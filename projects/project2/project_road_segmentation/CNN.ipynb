{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext lab_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from utils import *\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.compat.v2.keras.layers import BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16379839625501494973\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4937233203\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1827712432552967652\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(8)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training images, images loaded: 100 \n",
      "Loading groundtruth images, images loaded: 100 \n"
     ]
    }
   ],
   "source": [
    "image_dir_train = \"data/training/images/\"\n",
    "files = os.listdir(image_dir_train)\n",
    "n_train = len(files)\n",
    "print(f\"Loading training images, images loaded: {n_train} \")\n",
    "imgs_train = np.asarray(\n",
    "    [load_image(image_dir_train + files[i]) for i in range(n_train)]\n",
    ")\n",
    "gt_dir_train = \"data/training/groundtruth/\"\n",
    "print(f\"Loading groundtruth images, images loaded: {n_train} \")\n",
    "gt_imgs_train = np.asarray(\n",
    "    [load_image(gt_dir_train + files[i]) for i in range(n_train)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 400, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 400)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_imgs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 400\n",
    "# Patches for training\n",
    "img_patches_train = [\n",
    "    crop_image(imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "gt_patches_train = [\n",
    "    crop_image(gt_imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = np.asarray(\n",
    "    [\n",
    "        img_patches_train[i][j]\n",
    "        for i in range(len(img_patches_train))\n",
    "        for j in range(len(img_patches_train[i]))\n",
    "    ]\n",
    ")\n",
    "Y_train = np.asarray(\n",
    "    [\n",
    "        gt_patches_train[i][j]\n",
    "        for i in range(len(gt_patches_train))\n",
    "        for j in range(len(gt_patches_train[i]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 400, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 400)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_dir_val = \"data/validating/images/\"\n",
    "# files = os.listdir(image_dir_val)\n",
    "# n_val = len(files)\n",
    "# print(f\"Loading validating images, images loaded: {n_val} \")\n",
    "# imgs_val = np.asarray([load_image(image_dir_val + files[i]) for i in range(n_val)])\n",
    "# gt_dir_val = \"data/validating/groundtruth/\"\n",
    "# print(f\"Loading validating groundtruth, images loaded: {n_val} \")\n",
    "# gt_imgs_val = np.asarray([load_image(gt_dir_val + files[i]) for i in range(n_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_size = 400\n",
    "# # Patches for validating\n",
    "# img_patches_val = [\n",
    "#     crop_image(imgs_val[i], image_size, image_size) for i in range(n_val)\n",
    "# ]\n",
    "# gt_patches_val = [\n",
    "#     crop_image(gt_imgs_val[i], image_size, image_size) for i in range(n_val)\n",
    "# ]\n",
    "\n",
    "# # Separate features and labels\n",
    "# X_val = np.asarray(\n",
    "#     [\n",
    "#         img_patches_val[i][j]\n",
    "#         for i in range(len(img_patches_val))\n",
    "#         for j in range(len(img_patches_val[i]))\n",
    "#     ]\n",
    "# )\n",
    "# Y_val = np.asarray(\n",
    "#     [\n",
    "#         gt_patches_val[i][j]\n",
    "#         for i in range(len(gt_patches_val))\n",
    "#         for j in range(len(gt_patches_val[i]))\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = imag_rotation_aug(imgs_train, gt_imgs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "Y_train = np.asarray(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 456, 456, 3)\n",
      "(900, 456, 456)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "n_train = Y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val, Y_val = imag_rotation_aug(imgs_val, gt_imgs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val = np.asarray(X_val)\n",
    "# Y_val = np.asarray(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_val.shape)\n",
    "# print(Y_val.shape)\n",
    "# n_val = Y_val.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create functions to calcualte precision, recall and F-1 in the training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Compute the Precision for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "        Precision (numpy.float64): the Precision of the batch \n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Compute the Recall for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "       Recall (numpy.float64): the Recal of the batch \n",
    "    \"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    \"\"\"Compute the F-1 for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "       F-1 (numpy.float64): the F-1 of the batch \n",
    "    \"\"\"\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erick architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        self.model = self.initialize_U_NET(shape)\n",
    "\n",
    "    def initialize_U_NET(self, shape):\n",
    "        \"\"\"Create Network Architecture.\n",
    "        Args:\n",
    "            shape (triplet): Size of the input layer height x width x colors (64 x 64 x 3)\n",
    "        Returns:\n",
    "            model (Neural Network): Architecture of the model\n",
    "        \"\"\"\n",
    "        # INPUT\n",
    "        # shape     - Size of the input images\n",
    "        # OUTPUT\n",
    "        # model    - Compiled CNN\n",
    "\n",
    "        # Define hyperparamters\n",
    "        KERNEL3 = (3, 3)\n",
    "        KERNEL5 = (5, 5)\n",
    "\n",
    "        # Define a model\n",
    "        model = Sequential()\n",
    "\n",
    "        # Add the layers\n",
    "        # Selection of the model is described in the report\n",
    "        # We use padding = 'same' to avoid issues with the matrix sizes\n",
    "        model.add(Conv2D(64, KERNEL5, input_shape=shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(128, KERNEL3, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(512, KERNEL3, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(1024, KERNEL3, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        # Flatten it and use regularizers to avoid overfitting\n",
    "        # The parameters have been chosen empirically\n",
    "        model.add(Flatten())\n",
    "        model.add(\n",
    "            Dense(\n",
    "                128, kernel_regularizer=l2(0.000001), activity_regularizer=l2(0.000001)\n",
    "            )\n",
    "        )\n",
    "        # model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # Add output layer\n",
    "        model.add(\n",
    "            Dense(2, kernel_regularizer=l2(0.000001), activity_regularizer=l2(0.000001))\n",
    "        )\n",
    "        model.add(Activation(\"sigmoid\"))\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            metrics=[\"accuracy\", recall, f1],\n",
    "        )\n",
    "\n",
    "        # Print a summary of the model to see what has been generated\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the Model.\n",
    "\n",
    "        Returns:\n",
    "            History (History_Keras): History of the training\n",
    "        \"\"\"\n",
    "        # Early stopping callback after 10 steps\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"loss\", patience=10, verbose=1, restore_best_weights=True,\n",
    "        )\n",
    "        # Reduce learning rate on plateau after 5 steps\n",
    "        lr_callback = ReduceLROnPlateau(\n",
    "            monitor=\"loss\", factor=0.5, patience=4, verbose=1, cooldown=1,\n",
    "        )\n",
    "        save_best = ModelCheckpoint(\n",
    "            \"Erick_dropout_0.25_1024-{epoch:03d}-{f1:03f}.h5\",\n",
    "            save_best_only=True,\n",
    "            monitor=\"loss\",\n",
    "            verbose=1,\n",
    "        )\n",
    "        callbacks = [lr_callback, save_best, early_stopping]\n",
    "\n",
    "        # Train the model using the previously defined functions and callbacks\n",
    "        history = self.model.fit_generator(\n",
    "            create_minibatch(\n",
    "                X_train, Y_train, n_train, WINDOW_SIZE, BATCH_SIZE, PATCH_SIZE, WIDTH\n",
    "            ),\n",
    "            steps_per_epoch=STEPS_PER_EPOCH,\n",
    "            epochs=EPOCHS,\n",
    "            use_multiprocessing=False,\n",
    "            workers=1,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "            #             validation_data=create_minibatch(\n",
    "            #                 X_val, Y_val, n_val, WINDOW_SIZE, BATCH_SIZE, PATCH_SIZE, WIDTH\n",
    "            #             ),\n",
    "            #             validation_steps=STEPS_PER_EPOCH / 3,\n",
    "        )\n",
    "        return history\n",
    "\n",
    "    def classify_patches(self, X):\n",
    "        \"\"\"Classify image patches as either road or not.\n",
    "        Args:\n",
    "            X (image): part of the image to classify\n",
    "        Returns:\n",
    "            Predictions : Predictions for each patch\n",
    "        \"\"\"\n",
    "        # Subdivide the images into blocks with a stride and patch_size of 16\n",
    "        img_patches = create_patches(X, 16, 16, padding=24)\n",
    "\n",
    "        # Predict\n",
    "        predictions = self.model.predict(img_patches)\n",
    "        predictions = (predictions[:, 0] < predictions[:, 1]) * 1\n",
    "\n",
    "        # Regroup patches into images\n",
    "        return predictions.reshape(X.shape[0], -1)\n",
    "\n",
    "    #         return group_patches(predictions, X.shape[0])\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"Loads Saved Model.\n",
    "        Args:\n",
    "           filename (string): name of the model\n",
    "           \n",
    "        \"\"\"\n",
    "        # Load the model (used for submission)\n",
    "        dependencies = {\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "        self.model = load_model(filename, custom_objects=dependencies)\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Saves trained model.\n",
    "        Args:\n",
    "           filename (string): name of the model\n",
    "           \n",
    "        \"\"\"\n",
    "        self.model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 64, 64, 64)        4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 4, 4, 1024)        4719616   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 2, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 2, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 6,798,338\n",
      "Trainable params: 6,798,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We define parameters of the model\n",
    "BATCH_SIZE = 300\n",
    "WINDOW_SIZE = 64\n",
    "PATCH_SIZE = 16\n",
    "EPOCHS = 300\n",
    "STEPS_PER_EPOCH = 100\n",
    "WIDTH = 448\n",
    "model = CNN(shape=(WINDOW_SIZE, WINDOW_SIZE, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.5541 - accuracy: 0.7527 - recall: 0.7571 - f1: 0.7537\n",
      "Epoch 00001: loss improved from inf to 0.55361, saving model to Erick_dropout_0.25_1024-001-0.753668.h5\n",
      "100/100 [==============================] - 37s 366ms/step - loss: 0.5536 - accuracy: 0.7526 - recall: 0.7570 - f1: 0.7537\n",
      "Epoch 2/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.4843 - accuracy: 0.7574 - recall: 0.7551 - f1: 0.7568\n",
      "Epoch 00002: loss improved from 0.55361 to 0.48432, saving model to Erick_dropout_0.25_1024-002-0.756976.h5\n",
      "100/100 [==============================] - 36s 358ms/step - loss: 0.4843 - accuracy: 0.7575 - recall: 0.7553 - f1: 0.7570\n",
      "Epoch 3/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.4765 - accuracy: 0.7648 - recall: 0.7657 - f1: 0.7650\n",
      "Epoch 00003: loss improved from 0.48432 to 0.47615, saving model to Erick_dropout_0.25_1024-003-0.765160.h5\n",
      "100/100 [==============================] - 38s 381ms/step - loss: 0.4761 - accuracy: 0.7650 - recall: 0.7659 - f1: 0.7652\n",
      "Epoch 4/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.4449 - accuracy: 0.7836 - recall: 0.7841 - f1: 0.7837\n",
      "Epoch 00004: loss improved from 0.47615 to 0.44480, saving model to Erick_dropout_0.25_1024-004-0.783928.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.4448 - accuracy: 0.7839 - recall: 0.7842 - f1: 0.7839\n",
      "Epoch 5/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.4302 - accuracy: 0.8006 - recall: 0.7998 - f1: 0.8004\n",
      "Epoch 00005: loss improved from 0.44480 to 0.43061, saving model to Erick_dropout_0.25_1024-005-0.800146.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.4306 - accuracy: 0.8003 - recall: 0.7995 - f1: 0.8001\n",
      "Epoch 6/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.4844 - accuracy: 0.7735 - recall: 0.7738 - f1: 0.7735\n",
      "Epoch 00006: loss did not improve from 0.43061\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.4840 - accuracy: 0.7736 - recall: 0.7740 - f1: 0.7736\n",
      "Epoch 7/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3883 - accuracy: 0.8203 - recall: 0.8211 - f1: 0.8204\n",
      "Epoch 00007: loss improved from 0.43061 to 0.38844, saving model to Erick_dropout_0.25_1024-007-0.820445.h5\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 0.3884 - accuracy: 0.8203 - recall: 0.8210 - f1: 0.8204\n",
      "Epoch 8/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3504 - accuracy: 0.8414 - recall: 0.8414 - f1: 0.8414\n",
      "Epoch 00008: loss improved from 0.38844 to 0.34971, saving model to Erick_dropout_0.25_1024-008-0.841564.h5\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.3497 - accuracy: 0.8416 - recall: 0.8416 - f1: 0.8416\n",
      "Epoch 9/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3164 - accuracy: 0.8595 - recall: 0.8596 - f1: 0.8596\n",
      "Epoch 00009: loss improved from 0.34971 to 0.31637, saving model to Erick_dropout_0.25_1024-009-0.859447.h5\n",
      "100/100 [==============================] - 35s 354ms/step - loss: 0.3164 - accuracy: 0.8594 - recall: 0.8596 - f1: 0.8594\n",
      "Epoch 10/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3476 - accuracy: 0.8426 - recall: 0.8424 - f1: 0.8425\n",
      "Epoch 00010: loss did not improve from 0.31637\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.3471 - accuracy: 0.8429 - recall: 0.8428 - f1: 0.8429\n",
      "Epoch 11/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.8729 - recall: 0.8724 - f1: 0.8728\n",
      "Epoch 00011: loss improved from 0.31637 to 0.28794, saving model to Erick_dropout_0.25_1024-011-0.872826.h5\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.2879 - accuracy: 0.8729 - recall: 0.8724 - f1: 0.8728\n",
      "Epoch 12/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2738 - accuracy: 0.8791 - recall: 0.8787 - f1: 0.8790\n",
      "Epoch 00012: loss improved from 0.28794 to 0.27391, saving model to Erick_dropout_0.25_1024-012-0.879181.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.2739 - accuracy: 0.8792 - recall: 0.8788 - f1: 0.8792\n",
      "Epoch 13/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2678 - accuracy: 0.8832 - recall: 0.8824 - f1: 0.8831\n",
      "Epoch 00013: loss improved from 0.27391 to 0.26734, saving model to Erick_dropout_0.25_1024-013-0.883356.h5\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.2673 - accuracy: 0.8835 - recall: 0.8826 - f1: 0.8834\n",
      "Epoch 14/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2558 - accuracy: 0.8884 - recall: 0.8890 - f1: 0.8885\n",
      "Epoch 00014: loss improved from 0.26734 to 0.25524, saving model to Erick_dropout_0.25_1024-014-0.888621.h5\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.2552 - accuracy: 0.8886 - recall: 0.8891 - f1: 0.8886\n",
      "Epoch 15/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2422 - accuracy: 0.8963 - recall: 0.8965 - f1: 0.8963\n",
      "Epoch 00015: loss improved from 0.25524 to 0.24206, saving model to Erick_dropout_0.25_1024-015-0.896284.h5\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.2421 - accuracy: 0.8963 - recall: 0.8964 - f1: 0.8963\n",
      "Epoch 16/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2498 - accuracy: 0.8924 - recall: 0.8929 - f1: 0.8924\n",
      "Epoch 00016: loss did not improve from 0.24206\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.2496 - accuracy: 0.8925 - recall: 0.8930 - f1: 0.8925\n",
      "Epoch 17/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2338 - accuracy: 0.9008 - recall: 0.9010 - f1: 0.9008\n",
      "Epoch 00017: loss improved from 0.24206 to 0.23409, saving model to Erick_dropout_0.25_1024-017-0.900701.h5\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.2341 - accuracy: 0.9007 - recall: 0.9009 - f1: 0.9007\n",
      "Epoch 18/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2354 - accuracy: 0.8996 - recall: 0.8995 - f1: 0.8996\n",
      "Epoch 00018: loss did not improve from 0.23409\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.2348 - accuracy: 0.9000 - recall: 0.8999 - f1: 0.9000\n",
      "Epoch 19/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2223 - accuracy: 0.9062 - recall: 0.9054 - f1: 0.9061\n",
      "Epoch 00019: loss improved from 0.23409 to 0.22227, saving model to Erick_dropout_0.25_1024-019-0.905942.h5\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 0.2223 - accuracy: 0.9060 - recall: 0.9052 - f1: 0.9059\n",
      "Epoch 20/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2149 - accuracy: 0.9096 - recall: 0.9091 - f1: 0.9095\n",
      "Epoch 00020: loss improved from 0.22227 to 0.21521, saving model to Erick_dropout_0.25_1024-020-0.909534.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.2152 - accuracy: 0.9096 - recall: 0.9091 - f1: 0.9095\n",
      "Epoch 21/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2209 - accuracy: 0.9058 - recall: 0.9058 - f1: 0.9058\n",
      "Epoch 00021: loss did not improve from 0.21521\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.2208 - accuracy: 0.9059 - recall: 0.9060 - f1: 0.9059\n",
      "Epoch 22/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2141 - accuracy: 0.9098 - recall: 0.9094 - f1: 0.9098\n",
      "Epoch 00022: loss improved from 0.21521 to 0.21409, saving model to Erick_dropout_0.25_1024-022-0.909889.h5\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.2141 - accuracy: 0.9099 - recall: 0.9095 - f1: 0.9099\n",
      "Epoch 23/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2142 - accuracy: 0.9101 - recall: 0.9101 - f1: 0.9101\n",
      "Epoch 00023: loss did not improve from 0.21409\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.2143 - accuracy: 0.9101 - recall: 0.9101 - f1: 0.9101\n",
      "Epoch 24/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2165 - accuracy: 0.9092 - recall: 0.9092 - f1: 0.9092\n",
      "Epoch 00024: loss did not improve from 0.21409\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.2171 - accuracy: 0.9092 - recall: 0.9091 - f1: 0.9092\n",
      "Epoch 25/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2264 - accuracy: 0.9037 - recall: 0.9038 - f1: 0.9037\n",
      "Epoch 00025: loss did not improve from 0.21409\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.2263 - accuracy: 0.9039 - recall: 0.9041 - f1: 0.9039\n",
      "Epoch 26/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2050 - accuracy: 0.9129 - recall: 0.9126 - f1: 0.9129\n",
      "Epoch 00026: loss improved from 0.21409 to 0.20485, saving model to Erick_dropout_0.25_1024-026-0.912885.h5\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.2048 - accuracy: 0.9129 - recall: 0.9126 - f1: 0.9129\n",
      "Epoch 27/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1987 - accuracy: 0.9171 - recall: 0.9172 - f1: 0.9171\n",
      "Epoch 00027: loss improved from 0.20485 to 0.19920, saving model to Erick_dropout_0.25_1024-027-0.917033.h5\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.1992 - accuracy: 0.9170 - recall: 0.9171 - f1: 0.9170\n",
      "Epoch 28/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1932 - accuracy: 0.9202 - recall: 0.9202 - f1: 0.9202\n",
      "Epoch 00028: loss improved from 0.19920 to 0.19342, saving model to Erick_dropout_0.25_1024-028-0.920080.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1934 - accuracy: 0.9201 - recall: 0.9201 - f1: 0.9201\n",
      "Epoch 29/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2013 - accuracy: 0.9138 - recall: 0.9137 - f1: 0.9138\n",
      "Epoch 00029: loss did not improve from 0.19342\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.2010 - accuracy: 0.9140 - recall: 0.9140 - f1: 0.9140\n",
      "Epoch 30/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1851 - accuracy: 0.9233 - recall: 0.9234 - f1: 0.9233\n",
      "Epoch 00030: loss improved from 0.19342 to 0.18510, saving model to Erick_dropout_0.25_1024-030-0.923384.h5\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 0.1851 - accuracy: 0.9234 - recall: 0.9234 - f1: 0.9234\n",
      "Epoch 31/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1912 - accuracy: 0.9189 - recall: 0.9189 - f1: 0.9189\n",
      "Epoch 00031: loss did not improve from 0.18510\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.1910 - accuracy: 0.9190 - recall: 0.9190 - f1: 0.9190\n",
      "Epoch 32/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1855 - accuracy: 0.9246 - recall: 0.9247 - f1: 0.9247\n",
      "Epoch 00032: loss did not improve from 0.18510\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1851 - accuracy: 0.9249 - recall: 0.9250 - f1: 0.9249\n",
      "Epoch 33/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1930 - accuracy: 0.9209 - recall: 0.9211 - f1: 0.9209\n",
      "Epoch 00033: loss did not improve from 0.18510\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.1932 - accuracy: 0.9209 - recall: 0.9212 - f1: 0.9210\n",
      "Epoch 34/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1840 - accuracy: 0.9246 - recall: 0.9244 - f1: 0.9246\n",
      "Epoch 00034: loss improved from 0.18510 to 0.18391, saving model to Erick_dropout_0.25_1024-034-0.924681.h5\n",
      "100/100 [==============================] - 36s 356ms/step - loss: 0.1839 - accuracy: 0.9247 - recall: 0.9245 - f1: 0.9247\n",
      "Epoch 35/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1834 - accuracy: 0.9265 - recall: 0.9265 - f1: 0.9265\n",
      "Epoch 00035: loss did not improve from 0.18391\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.1841 - accuracy: 0.9261 - recall: 0.9261 - f1: 0.9261\n",
      "Epoch 36/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1791 - accuracy: 0.9266 - recall: 0.9265 - f1: 0.9266\n",
      "Epoch 00036: loss improved from 0.18391 to 0.17933, saving model to Erick_dropout_0.25_1024-036-0.926559.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1793 - accuracy: 0.9266 - recall: 0.9264 - f1: 0.9266\n",
      "Epoch 37/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1813 - accuracy: 0.9260 - recall: 0.9263 - f1: 0.9261\n",
      "Epoch 00037: loss did not improve from 0.17933\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.1809 - accuracy: 0.9262 - recall: 0.9264 - f1: 0.9262\n",
      "Epoch 38/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1787 - accuracy: 0.9270 - recall: 0.9269 - f1: 0.9270\n",
      "Epoch 00038: loss improved from 0.17933 to 0.17853, saving model to Erick_dropout_0.25_1024-038-0.927011.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1785 - accuracy: 0.9270 - recall: 0.9269 - f1: 0.9270\n",
      "Epoch 39/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1761 - accuracy: 0.9292 - recall: 0.9291 - f1: 0.9292\n",
      "Epoch 00039: loss improved from 0.17853 to 0.17652, saving model to Erick_dropout_0.25_1024-039-0.929125.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1765 - accuracy: 0.9291 - recall: 0.9291 - f1: 0.9291\n",
      "Epoch 40/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1764 - accuracy: 0.9283 - recall: 0.9283 - f1: 0.9283\n",
      "Epoch 00040: loss improved from 0.17652 to 0.17646, saving model to Erick_dropout_0.25_1024-040-0.928182.h5\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.1765 - accuracy: 0.9282 - recall: 0.9282 - f1: 0.9282\n",
      "Epoch 41/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1699 - accuracy: 0.9306 - recall: 0.9308 - f1: 0.9306\n",
      "Epoch 00041: loss improved from 0.17646 to 0.16988, saving model to Erick_dropout_0.25_1024-041-0.930717.h5\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.1699 - accuracy: 0.9307 - recall: 0.9310 - f1: 0.9307\n",
      "Epoch 42/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1716 - accuracy: 0.9304 - recall: 0.9306 - f1: 0.9304\n",
      "Epoch 00042: loss did not improve from 0.16988\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1720 - accuracy: 0.9303 - recall: 0.9304 - f1: 0.9303\n",
      "Epoch 43/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1755 - accuracy: 0.9281 - recall: 0.9281 - f1: 0.9281\n",
      "Epoch 00043: loss did not improve from 0.16988\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.1754 - accuracy: 0.9281 - recall: 0.9281 - f1: 0.9281\n",
      "Epoch 44/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1702 - accuracy: 0.9323 - recall: 0.9323 - f1: 0.9323\n",
      "Epoch 00044: loss did not improve from 0.16988\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.1699 - accuracy: 0.9323 - recall: 0.9324 - f1: 0.9324\n",
      "Epoch 45/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1679 - accuracy: 0.9307 - recall: 0.9307 - f1: 0.9307\n",
      "Epoch 00045: loss improved from 0.16988 to 0.16790, saving model to Erick_dropout_0.25_1024-045-0.930597.h5\n",
      "100/100 [==============================] - 35s 354ms/step - loss: 0.1679 - accuracy: 0.9306 - recall: 0.9306 - f1: 0.9306\n",
      "Epoch 46/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1749 - accuracy: 0.9297 - recall: 0.9297 - f1: 0.9297\n",
      "Epoch 00046: loss did not improve from 0.16790\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.1748 - accuracy: 0.9297 - recall: 0.9297 - f1: 0.9297\n",
      "Epoch 47/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1669 - accuracy: 0.9337 - recall: 0.9338 - f1: 0.9337\n",
      "Epoch 00047: loss improved from 0.16790 to 0.16678, saving model to Erick_dropout_0.25_1024-047-0.933799.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1668 - accuracy: 0.9338 - recall: 0.9340 - f1: 0.9338\n",
      "Epoch 48/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1693 - accuracy: 0.9310 - recall: 0.9310 - f1: 0.9310\n",
      "Epoch 00048: loss did not improve from 0.16678\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1695 - accuracy: 0.9310 - recall: 0.9310 - f1: 0.9310\n",
      "Epoch 49/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1708 - accuracy: 0.9314 - recall: 0.9314 - f1: 0.9314\n",
      "Epoch 00049: loss did not improve from 0.16678\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1711 - accuracy: 0.9314 - recall: 0.9314 - f1: 0.9314\n",
      "Epoch 50/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1695 - accuracy: 0.9312 - recall: 0.9311 - f1: 0.9312\n",
      "Epoch 00050: loss did not improve from 0.16678\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1698 - accuracy: 0.9312 - recall: 0.9311 - f1: 0.9312\n",
      "Epoch 51/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1612 - accuracy: 0.9346 - recall: 0.9345 - f1: 0.9346\n",
      "Epoch 00051: loss improved from 0.16678 to 0.16158, saving model to Erick_dropout_0.25_1024-051-0.934500.h5\n",
      "100/100 [==============================] - 36s 357ms/step - loss: 0.1616 - accuracy: 0.9345 - recall: 0.9345 - f1: 0.9345\n",
      "Epoch 52/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1643 - accuracy: 0.9333 - recall: 0.9332 - f1: 0.9333\n",
      "Epoch 00052: loss did not improve from 0.16158\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1645 - accuracy: 0.9333 - recall: 0.9333 - f1: 0.9333\n",
      "Epoch 53/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1598 - accuracy: 0.9361 - recall: 0.9361 - f1: 0.9361\n",
      "Epoch 00053: loss improved from 0.16158 to 0.16013, saving model to Erick_dropout_0.25_1024-053-0.935821.h5\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.1601 - accuracy: 0.9358 - recall: 0.9359 - f1: 0.9358\n",
      "Epoch 54/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1615 - accuracy: 0.9343 - recall: 0.9341 - f1: 0.9343\n",
      "Epoch 00054: loss did not improve from 0.16013\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1613 - accuracy: 0.9345 - recall: 0.9344 - f1: 0.9345\n",
      "Epoch 55/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1621 - accuracy: 0.9358 - recall: 0.9360 - f1: 0.9358\n",
      "Epoch 00055: loss did not improve from 0.16013\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1621 - accuracy: 0.9358 - recall: 0.9360 - f1: 0.9358\n",
      "Epoch 56/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1656 - accuracy: 0.9358 - recall: 0.9360 - f1: 0.9358\n",
      "Epoch 00056: loss did not improve from 0.16013\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.1654 - accuracy: 0.9357 - recall: 0.9359 - f1: 0.9357\n",
      "Epoch 57/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1580 - accuracy: 0.9357 - recall: 0.9360 - f1: 0.9357\n",
      "Epoch 00057: loss improved from 0.16013 to 0.15784, saving model to Erick_dropout_0.25_1024-057-0.935602.h5\n",
      "100/100 [==============================] - 36s 357ms/step - loss: 0.1578 - accuracy: 0.9356 - recall: 0.9359 - f1: 0.9356\n",
      "Epoch 58/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1680 - accuracy: 0.9321 - recall: 0.9321 - f1: 0.9321\n",
      "Epoch 00058: loss did not improve from 0.15784\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.1675 - accuracy: 0.9322 - recall: 0.9322 - f1: 0.9322\n",
      "Epoch 59/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1586 - accuracy: 0.9371 - recall: 0.9371 - f1: 0.9371\n",
      "Epoch 00059: loss did not improve from 0.15784\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1588 - accuracy: 0.9372 - recall: 0.9372 - f1: 0.9372\n",
      "Epoch 60/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1616 - accuracy: 0.9355 - recall: 0.9356 - f1: 0.9355\n",
      "Epoch 00060: loss did not improve from 0.15784\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.1618 - accuracy: 0.9356 - recall: 0.9358 - f1: 0.9357\n",
      "Epoch 61/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1527 - accuracy: 0.9389 - recall: 0.9390 - f1: 0.9389\n",
      "Epoch 00061: loss improved from 0.15784 to 0.15252, saving model to Erick_dropout_0.25_1024-061-0.939074.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1525 - accuracy: 0.9391 - recall: 0.9392 - f1: 0.9391\n",
      "Epoch 62/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1585 - accuracy: 0.9369 - recall: 0.9371 - f1: 0.9369\n",
      "Epoch 00062: loss did not improve from 0.15252\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1581 - accuracy: 0.9369 - recall: 0.9370 - f1: 0.9369\n",
      "Epoch 63/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1572 - accuracy: 0.9373 - recall: 0.9373 - f1: 0.9373\n",
      "Epoch 00063: loss did not improve from 0.15252\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.1575 - accuracy: 0.9374 - recall: 0.9374 - f1: 0.9374\n",
      "Epoch 64/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1542 - accuracy: 0.9384 - recall: 0.9384 - f1: 0.9384\n",
      "Epoch 00064: loss did not improve from 0.15252\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1544 - accuracy: 0.9383 - recall: 0.9383 - f1: 0.9383\n",
      "Epoch 65/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1556 - accuracy: 0.9404 - recall: 0.9404 - f1: 0.9404\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.15252\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.1561 - accuracy: 0.9402 - recall: 0.9402 - f1: 0.9402\n",
      "Epoch 66/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1482 - accuracy: 0.9419 - recall: 0.9419 - f1: 0.9419\n",
      "Epoch 00066: loss improved from 0.15252 to 0.14804, saving model to Erick_dropout_0.25_1024-066-0.941884.h5\n",
      "100/100 [==============================] - 35s 354ms/step - loss: 0.1480 - accuracy: 0.9419 - recall: 0.9419 - f1: 0.9419\n",
      "Epoch 67/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1361 - accuracy: 0.9467 - recall: 0.9467 - f1: 0.9467\n",
      "Epoch 00067: loss improved from 0.14804 to 0.13603, saving model to Erick_dropout_0.25_1024-067-0.946653.h5\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.1360 - accuracy: 0.9467 - recall: 0.9467 - f1: 0.9467\n",
      "Epoch 68/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.9448 - recall: 0.9448 - f1: 0.9448\n",
      "Epoch 00068: loss did not improve from 0.13603\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1365 - accuracy: 0.9450 - recall: 0.9450 - f1: 0.9450\n",
      "Epoch 69/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1375 - accuracy: 0.9461 - recall: 0.9461 - f1: 0.9461\n",
      "Epoch 00069: loss did not improve from 0.13603\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.1379 - accuracy: 0.9460 - recall: 0.9460 - f1: 0.9460\n",
      "Epoch 70/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1312 - accuracy: 0.9486 - recall: 0.9487 - f1: 0.9486\n",
      "Epoch 00070: loss improved from 0.13603 to 0.13104, saving model to Erick_dropout_0.25_1024-070-0.948738.h5\n",
      "100/100 [==============================] - 36s 355ms/step - loss: 0.1310 - accuracy: 0.9487 - recall: 0.9488 - f1: 0.9487\n",
      "Epoch 71/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1362 - accuracy: 0.9454 - recall: 0.9455 - f1: 0.9454\n",
      "Epoch 00071: loss did not improve from 0.13104\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.1360 - accuracy: 0.9454 - recall: 0.9454 - f1: 0.9454\n",
      "Epoch 72/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1338 - accuracy: 0.9465 - recall: 0.9466 - f1: 0.9466\n",
      "Epoch 00072: loss did not improve from 0.13104\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1338 - accuracy: 0.9466 - recall: 0.9466 - f1: 0.9466\n",
      "Epoch 73/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1249 - accuracy: 0.9504 - recall: 0.9505 - f1: 0.9504\n",
      "Epoch 00073: loss improved from 0.13104 to 0.12487, saving model to Erick_dropout_0.25_1024-073-0.950452.h5\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 0.1249 - accuracy: 0.9505 - recall: 0.9505 - f1: 0.9505\n",
      "Epoch 74/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1324 - accuracy: 0.9493 - recall: 0.9493 - f1: 0.9493\n",
      "Epoch 00074: loss did not improve from 0.12487\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.1327 - accuracy: 0.9490 - recall: 0.9490 - f1: 0.9490\n",
      "Epoch 75/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1294 - accuracy: 0.9485 - recall: 0.9485 - f1: 0.9485\n",
      "Epoch 00075: loss did not improve from 0.12487\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.1292 - accuracy: 0.9485 - recall: 0.9486 - f1: 0.9485\n",
      "Epoch 76/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 0.9502 - recall: 0.9502 - f1: 0.9502\n",
      "Epoch 00076: loss did not improve from 0.12487\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.1260 - accuracy: 0.9502 - recall: 0.9502 - f1: 0.9502\n",
      "Epoch 77/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9499 - recall: 0.9499 - f1: 0.9499\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00077: loss did not improve from 0.12487\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.1271 - accuracy: 0.9500 - recall: 0.9500 - f1: 0.9500\n",
      "Epoch 78/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1187 - accuracy: 0.9541 - recall: 0.9541 - f1: 0.9541\n",
      "Epoch 00078: loss improved from 0.12487 to 0.11825, saving model to Erick_dropout_0.25_1024-078-0.954384.h5\n",
      "100/100 [==============================] - 37s 367ms/step - loss: 0.1183 - accuracy: 0.9544 - recall: 0.9544 - f1: 0.9544\n",
      "Epoch 79/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9539 - recall: 0.9539 - f1: 0.9539\n",
      "Epoch 00079: loss improved from 0.11825 to 0.11778, saving model to Erick_dropout_0.25_1024-079-0.953850.h5\n",
      "100/100 [==============================] - 36s 358ms/step - loss: 0.1178 - accuracy: 0.9538 - recall: 0.9539 - f1: 0.9538\n",
      "Epoch 80/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1182 - accuracy: 0.9531 - recall: 0.9532 - f1: 0.9531\n",
      "Epoch 00080: loss did not improve from 0.11778\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.1178 - accuracy: 0.9534 - recall: 0.9534 - f1: 0.9534\n",
      "Epoch 81/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1134 - accuracy: 0.9554 - recall: 0.9554 - f1: 0.9554\n",
      "Epoch 00081: loss improved from 0.11778 to 0.11329, saving model to Erick_dropout_0.25_1024-081-0.955466.h5\n",
      "100/100 [==============================] - 35s 355ms/step - loss: 0.1133 - accuracy: 0.9555 - recall: 0.9555 - f1: 0.9555\n",
      "Epoch 82/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1168 - accuracy: 0.9530 - recall: 0.9530 - f1: 0.9530\n",
      "Epoch 00082: loss did not improve from 0.11329\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.1171 - accuracy: 0.9529 - recall: 0.9529 - f1: 0.9529\n",
      "Epoch 83/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1169 - accuracy: 0.9543 - recall: 0.9543 - f1: 0.9543\n",
      "Epoch 00083: loss did not improve from 0.11329\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1170 - accuracy: 0.9541 - recall: 0.9541 - f1: 0.9541\n",
      "Epoch 84/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1120 - accuracy: 0.9556 - recall: 0.9556 - f1: 0.9556\n",
      "Epoch 00084: loss improved from 0.11329 to 0.11207, saving model to Erick_dropout_0.25_1024-084-0.955467.h5\n",
      "100/100 [==============================] - 36s 356ms/step - loss: 0.1121 - accuracy: 0.9555 - recall: 0.9555 - f1: 0.9555\n",
      "Epoch 85/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1135 - accuracy: 0.9550 - recall: 0.9551 - f1: 0.9550\n",
      "Epoch 00085: loss did not improve from 0.11207\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.1134 - accuracy: 0.9552 - recall: 0.9552 - f1: 0.9552\n",
      "Epoch 86/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1153 - accuracy: 0.9534 - recall: 0.9534 - f1: 0.9534\n",
      "Epoch 00086: loss did not improve from 0.11207\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.1152 - accuracy: 0.9534 - recall: 0.9534 - f1: 0.9534\n",
      "Epoch 87/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1163 - accuracy: 0.9531 - recall: 0.9531 - f1: 0.9531\n",
      "Epoch 00087: loss did not improve from 0.11207\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1163 - accuracy: 0.9532 - recall: 0.9532 - f1: 0.9532\n",
      "Epoch 88/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1140 - accuracy: 0.9542 - recall: 0.9542 - f1: 0.9542\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00088: loss did not improve from 0.11207\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.1144 - accuracy: 0.9541 - recall: 0.9541 - f1: 0.9541\n",
      "Epoch 89/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1094 - accuracy: 0.9562 - recall: 0.9562 - f1: 0.9562\n",
      "Epoch 00089: loss improved from 0.11207 to 0.10993, saving model to Erick_dropout_0.25_1024-089-0.956000.h5\n",
      "100/100 [==============================] - 35s 355ms/step - loss: 0.1099 - accuracy: 0.9560 - recall: 0.9560 - f1: 0.9560\n",
      "Epoch 90/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1094 - accuracy: 0.9547 - recall: 0.9547 - f1: 0.9547\n",
      "Epoch 00090: loss improved from 0.10993 to 0.10916, saving model to Erick_dropout_0.25_1024-090-0.954800.h5\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 0.1092 - accuracy: 0.9548 - recall: 0.9548 - f1: 0.9548\n",
      "Epoch 91/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1086 - accuracy: 0.9560 - recall: 0.9560 - f1: 0.9560\n",
      "Epoch 00091: loss improved from 0.10916 to 0.10912, saving model to Erick_dropout_0.25_1024-091-0.955933.h5\n",
      "100/100 [==============================] - 35s 355ms/step - loss: 0.1091 - accuracy: 0.9559 - recall: 0.9559 - f1: 0.9559\n",
      "Epoch 92/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1080 - accuracy: 0.9561 - recall: 0.9561 - f1: 0.9561\n",
      "Epoch 00092: loss improved from 0.10912 to 0.10839, saving model to Erick_dropout_0.25_1024-092-0.955933.h5\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.1084 - accuracy: 0.9559 - recall: 0.9559 - f1: 0.9559\n",
      "Epoch 93/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1082 - accuracy: 0.9575 - recall: 0.9575 - f1: 0.9575\n",
      "Epoch 00093: loss did not improve from 0.10839\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.1085 - accuracy: 0.9574 - recall: 0.9574 - f1: 0.9574\n",
      "Epoch 94/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1052 - accuracy: 0.9587 - recall: 0.9587 - f1: 0.9587\n",
      "Epoch 00094: loss improved from 0.10839 to 0.10521, saving model to Erick_dropout_0.25_1024-094-0.958535.h5\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 0.1052 - accuracy: 0.9585 - recall: 0.9586 - f1: 0.9585\n",
      "Epoch 95/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1064 - accuracy: 0.9580 - recall: 0.9580 - f1: 0.9580\n",
      "Epoch 00095: loss did not improve from 0.10521\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1061 - accuracy: 0.9581 - recall: 0.9581 - f1: 0.9581\n",
      "Epoch 96/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1073 - accuracy: 0.9570 - recall: 0.9570 - f1: 0.9570\n",
      "Epoch 00096: loss did not improve from 0.10521\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1070 - accuracy: 0.9572 - recall: 0.9572 - f1: 0.9572\n",
      "Epoch 97/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1074 - accuracy: 0.9563 - recall: 0.9563 - f1: 0.9563\n",
      "Epoch 00097: loss did not improve from 0.10521\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.1072 - accuracy: 0.9564 - recall: 0.9565 - f1: 0.9565\n",
      "Epoch 98/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1020 - accuracy: 0.9597 - recall: 0.9597 - f1: 0.9597\n",
      "Epoch 00098: loss improved from 0.10521 to 0.10191, saving model to Erick_dropout_0.25_1024-098-0.959666.h5\n",
      "100/100 [==============================] - 35s 354ms/step - loss: 0.1019 - accuracy: 0.9597 - recall: 0.9597 - f1: 0.9597\n",
      "Epoch 99/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1079 - accuracy: 0.9578 - recall: 0.9578 - f1: 0.9578\n",
      "Epoch 00099: loss did not improve from 0.10191\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.1079 - accuracy: 0.9577 - recall: 0.9577 - f1: 0.9577\n",
      "Epoch 100/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1019 - accuracy: 0.9587 - recall: 0.9587 - f1: 0.9587\n",
      "Epoch 00100: loss did not improve from 0.10191\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1020 - accuracy: 0.9586 - recall: 0.9587 - f1: 0.9587\n",
      "Epoch 101/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1033 - accuracy: 0.9581 - recall: 0.9581 - f1: 0.9581\n",
      "Epoch 00101: loss did not improve from 0.10191\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1030 - accuracy: 0.9583 - recall: 0.9583 - f1: 0.9583\n",
      "Epoch 102/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1039 - accuracy: 0.9588 - recall: 0.9588 - f1: 0.9588\n",
      "Epoch 00102: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00102: loss did not improve from 0.10191\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1042 - accuracy: 0.9587 - recall: 0.9587 - f1: 0.9587\n",
      "Epoch 103/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1013 - accuracy: 0.9597 - recall: 0.9597 - f1: 0.9597\n",
      "Epoch 00103: loss improved from 0.10191 to 0.10133, saving model to Erick_dropout_0.25_1024-103-0.959767.h5\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.1013 - accuracy: 0.9598 - recall: 0.9598 - f1: 0.9598\n",
      "Epoch 104/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1045 - accuracy: 0.9578 - recall: 0.9578 - f1: 0.9578\n",
      "Epoch 00104: loss did not improve from 0.10133\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.1040 - accuracy: 0.9580 - recall: 0.9580 - f1: 0.9580\n",
      "Epoch 105/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1018 - accuracy: 0.9598 - recall: 0.9598 - f1: 0.9598\n",
      "Epoch 00105: loss did not improve from 0.10133\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.1021 - accuracy: 0.9595 - recall: 0.9595 - f1: 0.9595\n",
      "Epoch 106/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1025 - accuracy: 0.9599 - recall: 0.9599 - f1: 0.9599\n",
      "Epoch 00106: loss did not improve from 0.10133\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.1024 - accuracy: 0.9599 - recall: 0.9599 - f1: 0.9599\n",
      "Epoch 107/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9603 - recall: 0.9603 - f1: 0.9603\n",
      "Epoch 00107: loss improved from 0.10133 to 0.10115, saving model to Erick_dropout_0.25_1024-107-0.960300.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1012 - accuracy: 0.9603 - recall: 0.9603 - f1: 0.9603\n",
      "Epoch 108/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1009 - accuracy: 0.9599 - recall: 0.9599 - f1: 0.9599\n",
      "Epoch 00108: loss improved from 0.10115 to 0.10061, saving model to Erick_dropout_0.25_1024-108-0.960000.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1006 - accuracy: 0.9600 - recall: 0.9600 - f1: 0.9600\n",
      "Epoch 109/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1015 - accuracy: 0.9581 - recall: 0.9581 - f1: 0.9581\n",
      "Epoch 00109: loss did not improve from 0.10061\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.1015 - accuracy: 0.9580 - recall: 0.9580 - f1: 0.9580\n",
      "Epoch 110/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0999 - accuracy: 0.9610 - recall: 0.9609 - f1: 0.9610\n",
      "Epoch 00110: loss improved from 0.10061 to 0.09984, saving model to Erick_dropout_0.25_1024-110-0.961049.h5\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.0998 - accuracy: 0.9610 - recall: 0.9610 - f1: 0.9610\n",
      "Epoch 111/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1042 - accuracy: 0.9586 - recall: 0.9586 - f1: 0.9586\n",
      "Epoch 00111: loss did not improve from 0.09984\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.1041 - accuracy: 0.9587 - recall: 0.9587 - f1: 0.9587\n",
      "Epoch 112/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1003 - accuracy: 0.9597 - recall: 0.9597 - f1: 0.9597\n",
      "Epoch 00112: loss did not improve from 0.09984\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.1000 - accuracy: 0.9598 - recall: 0.9598 - f1: 0.9598\n",
      "Epoch 113/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1025 - accuracy: 0.9591 - recall: 0.9591 - f1: 0.9591\n",
      "Epoch 00113: loss did not improve from 0.09984\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.1024 - accuracy: 0.9591 - recall: 0.9591 - f1: 0.9591\n",
      "Epoch 114/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0982 - accuracy: 0.9609 - recall: 0.9609 - f1: 0.9609\n",
      "Epoch 00114: loss improved from 0.09984 to 0.09871, saving model to Erick_dropout_0.25_1024-114-0.960682.h5\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 0.0987 - accuracy: 0.9607 - recall: 0.9607 - f1: 0.9607\n",
      "Epoch 115/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0977 - accuracy: 0.9619 - recall: 0.9619 - f1: 0.9619\n",
      "Epoch 00115: loss improved from 0.09871 to 0.09772, saving model to Erick_dropout_0.25_1024-115-0.961982.h5\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.0977 - accuracy: 0.9620 - recall: 0.9620 - f1: 0.9620\n",
      "Epoch 116/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0982 - accuracy: 0.9608 - recall: 0.9608 - f1: 0.9608\n",
      "Epoch 00116: loss did not improve from 0.09772\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.0980 - accuracy: 0.9609 - recall: 0.9609 - f1: 0.9609\n",
      "Epoch 117/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1034 - accuracy: 0.9575 - recall: 0.9575 - f1: 0.9575\n",
      "Epoch 00117: loss did not improve from 0.09772\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.1037 - accuracy: 0.9575 - recall: 0.9575 - f1: 0.9575\n",
      "Epoch 118/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0998 - accuracy: 0.9616 - recall: 0.9616 - f1: 0.9616\n",
      "Epoch 00118: loss did not improve from 0.09772\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.0996 - accuracy: 0.9617 - recall: 0.9617 - f1: 0.9617\n",
      "Epoch 119/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0997 - accuracy: 0.9608 - recall: 0.9608 - f1: 0.9608\n",
      "Epoch 00119: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00119: loss did not improve from 0.09772\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.0999 - accuracy: 0.9607 - recall: 0.9607 - f1: 0.9607\n",
      "Epoch 120/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0959 - accuracy: 0.9613 - recall: 0.9613 - f1: 0.9613\n",
      "Epoch 00120: loss improved from 0.09772 to 0.09583, saving model to Erick_dropout_0.25_1024-120-0.961333.h5\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.0958 - accuracy: 0.9613 - recall: 0.9613 - f1: 0.9613\n",
      "Epoch 121/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9596 - recall: 0.9596 - f1: 0.9596\n",
      "Epoch 00121: loss did not improve from 0.09583\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1015 - accuracy: 0.9595 - recall: 0.9595 - f1: 0.9595\n",
      "Epoch 122/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1010 - accuracy: 0.9599 - recall: 0.9599 - f1: 0.9599\n",
      "Epoch 00122: loss did not improve from 0.09583\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1012 - accuracy: 0.9597 - recall: 0.9597 - f1: 0.9597\n",
      "Epoch 123/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0989 - accuracy: 0.9608 - recall: 0.9608 - f1: 0.9608\n",
      "Epoch 00123: loss did not improve from 0.09583\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.0988 - accuracy: 0.9608 - recall: 0.9608 - f1: 0.9608\n",
      "Epoch 124/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0959 - accuracy: 0.9608 - recall: 0.9608 - f1: 0.9608\n",
      "Epoch 00124: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00124: loss did not improve from 0.09583\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.0960 - accuracy: 0.9607 - recall: 0.9607 - f1: 0.9607\n",
      "Epoch 125/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0996 - accuracy: 0.9598 - recall: 0.9598 - f1: 0.9598\n",
      "Epoch 00125: loss did not improve from 0.09583\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.0995 - accuracy: 0.9598 - recall: 0.9598 - f1: 0.9598\n",
      "Epoch 126/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0994 - accuracy: 0.9604 - recall: 0.9604 - f1: 0.9604\n",
      "Epoch 00126: loss did not improve from 0.09583\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.0993 - accuracy: 0.9605 - recall: 0.9605 - f1: 0.9605\n",
      "Epoch 127/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0994 - accuracy: 0.9601 - recall: 0.9601 - f1: 0.9601\n",
      "Epoch 00127: loss did not improve from 0.09583\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.0995 - accuracy: 0.9601 - recall: 0.9601 - f1: 0.9601\n",
      "Epoch 128/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0981 - accuracy: 0.9599 - recall: 0.9599 - f1: 0.9599\n",
      "Epoch 00128: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 00128: loss did not improve from 0.09583\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.0982 - accuracy: 0.9598 - recall: 0.9598 - f1: 0.9598\n",
      "Epoch 129/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0975 - accuracy: 0.9595 - recall: 0.9595 - f1: 0.9595\n",
      "Epoch 00129: loss did not improve from 0.09583\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.0976 - accuracy: 0.9595 - recall: 0.9595 - f1: 0.9595\n",
      "Epoch 130/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0956 - accuracy: 0.9613 - recall: 0.9613 - f1: 0.9613\n",
      "Epoch 00130: loss improved from 0.09583 to 0.09552, saving model to Erick_dropout_0.25_1024-130-0.961335.h5\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.0955 - accuracy: 0.9613 - recall: 0.9614 - f1: 0.9613\n",
      "Epoch 131/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0981 - accuracy: 0.9614 - recall: 0.9614 - f1: 0.9614\n",
      "Epoch 00131: loss did not improve from 0.09552\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.0978 - accuracy: 0.9615 - recall: 0.9616 - f1: 0.9616\n",
      "Epoch 132/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0983 - accuracy: 0.9609 - recall: 0.9609 - f1: 0.9609\n",
      "Epoch 00132: loss did not improve from 0.09552\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.0980 - accuracy: 0.9610 - recall: 0.9610 - f1: 0.9610\n",
      "Epoch 133/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0975 - accuracy: 0.9602 - recall: 0.9602 - f1: 0.9602\n",
      "Epoch 00133: loss did not improve from 0.09552\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.0972 - accuracy: 0.9604 - recall: 0.9604 - f1: 0.9604\n",
      "Epoch 134/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0942 - accuracy: 0.9630 - recall: 0.9630 - f1: 0.9630\n",
      "Epoch 00134: loss improved from 0.09552 to 0.09419, saving model to Erick_dropout_0.25_1024-134-0.963000.h5\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 0.0942 - accuracy: 0.9630 - recall: 0.9630 - f1: 0.9630\n",
      "Epoch 135/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1006 - accuracy: 0.9591 - recall: 0.9591 - f1: 0.9591\n",
      "Epoch 00135: loss did not improve from 0.09419\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.1005 - accuracy: 0.9591 - recall: 0.9591 - f1: 0.9591\n",
      "Epoch 136/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0936 - accuracy: 0.9621 - recall: 0.9621 - f1: 0.9621\n",
      "Epoch 00136: loss improved from 0.09419 to 0.09361, saving model to Erick_dropout_0.25_1024-136-0.962033.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.0936 - accuracy: 0.9620 - recall: 0.9620 - f1: 0.9620\n",
      "Epoch 137/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0971 - accuracy: 0.9615 - recall: 0.9615 - f1: 0.9615\n",
      "Epoch 00137: loss did not improve from 0.09361\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.0971 - accuracy: 0.9615 - recall: 0.9615 - f1: 0.9615\n",
      "Epoch 138/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9616 - recall: 0.9616 - f1: 0.9616\n",
      "Epoch 00138: loss did not improve from 0.09361\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.0960 - accuracy: 0.9617 - recall: 0.9617 - f1: 0.9617\n",
      "Epoch 139/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0962 - accuracy: 0.9612 - recall: 0.9612 - f1: 0.9612\n",
      "Epoch 00139: loss did not improve from 0.09361\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.0963 - accuracy: 0.9611 - recall: 0.9611 - f1: 0.9611\n",
      "Epoch 140/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0972 - accuracy: 0.9615 - recall: 0.9615 - f1: 0.9615\n",
      "Epoch 00140: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 00140: loss did not improve from 0.09361\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.0974 - accuracy: 0.9614 - recall: 0.9614 - f1: 0.9614\n",
      "Epoch 141/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0939 - accuracy: 0.9627 - recall: 0.9627 - f1: 0.9627\n",
      "Epoch 00141: loss did not improve from 0.09361\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.0937 - accuracy: 0.9628 - recall: 0.9628 - f1: 0.9628\n",
      "Epoch 142/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0922 - accuracy: 0.9633 - recall: 0.9633 - f1: 0.9633\n",
      "Epoch 00142: loss improved from 0.09361 to 0.09246, saving model to Erick_dropout_0.25_1024-142-0.963100.h5\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 0.0925 - accuracy: 0.9631 - recall: 0.9631 - f1: 0.9631\n",
      "Epoch 143/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1001 - accuracy: 0.9600 - recall: 0.9600 - f1: 0.9600\n",
      "Epoch 00143: loss did not improve from 0.09246\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1000 - accuracy: 0.9601 - recall: 0.9601 - f1: 0.9601\n",
      "Epoch 144/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0994 - accuracy: 0.9593 - recall: 0.9593 - f1: 0.9593\n",
      "Epoch 00144: loss did not improve from 0.09246\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.0993 - accuracy: 0.9594 - recall: 0.9594 - f1: 0.9594\n",
      "Epoch 145/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0945 - accuracy: 0.9622 - recall: 0.9622 - f1: 0.9622\n",
      "Epoch 00145: loss did not improve from 0.09246\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.0943 - accuracy: 0.9623 - recall: 0.9623 - f1: 0.9623\n",
      "Epoch 146/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0955 - accuracy: 0.9626 - recall: 0.9626 - f1: 0.9626\n",
      "Epoch 00146: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\n",
      "Epoch 00146: loss did not improve from 0.09246\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.0957 - accuracy: 0.9625 - recall: 0.9626 - f1: 0.9626\n",
      "Epoch 147/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9608 - recall: 0.9608 - f1: 0.9608\n",
      "Epoch 00147: loss did not improve from 0.09246\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.0961 - accuracy: 0.9608 - recall: 0.9608 - f1: 0.9608\n",
      "Epoch 148/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0968 - accuracy: 0.9609 - recall: 0.9609 - f1: 0.9609\n",
      "Epoch 00148: loss did not improve from 0.09246\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.0967 - accuracy: 0.9610 - recall: 0.9609 - f1: 0.9609\n",
      "Epoch 149/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0953 - accuracy: 0.9622 - recall: 0.9622 - f1: 0.9622\n",
      "Epoch 00149: loss did not improve from 0.09246\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.0954 - accuracy: 0.9621 - recall: 0.9621 - f1: 0.9621\n",
      "Epoch 150/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0982 - accuracy: 0.9609 - recall: 0.9609 - f1: 0.9609\n",
      "Epoch 00150: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\n",
      "Epoch 00150: loss did not improve from 0.09246\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.0985 - accuracy: 0.9606 - recall: 0.9606 - f1: 0.9606\n",
      "Epoch 151/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0927 - accuracy: 0.9624 - recall: 0.9624 - f1: 0.9624\n",
      "Epoch 00151: loss did not improve from 0.09246\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.0926 - accuracy: 0.9625 - recall: 0.9625 - f1: 0.9625\n",
      "Epoch 152/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0979 - accuracy: 0.9613 - recall: 0.9613 - f1: 0.9613\n",
      "Epoch 00152: loss did not improve from 0.09246\n",
      "Restoring model weights from the end of the best epoch.\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.0980 - accuracy: 0.9613 - recall: 0.9613 - f1: 0.9613\n",
      "Epoch 00152: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(history.history[\"loss\"][1:], label=\"train_loss\")\n",
    "# plt.plot(history.history[\"val_loss\"][1:], label=\"val_loss\")\n",
    "plt.plot(history.history[\"accuracy\"][1:], label=\"train_acc\")\n",
    "# plt.plot(history.history[\"val_accuracy\"][1:], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"plots/Erick_dropout_0.25_1024.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_23 (Conv2D)           (None, 64, 64, 64)        4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 4, 4, 1024)        4719616   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)   (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 2, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 2, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 6,798,338\n",
      "Trainable params: 6,798,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 64)        4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 1024)        4719616   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 6,798,338\n",
      "Trainable params: 6,798,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Processing image => data/test_set_images/test_1/test_1.png\n",
      "Processing image => data/test_set_images/test_2/test_2.png\n",
      "Processing image => data/test_set_images/test_3/test_3.png\n",
      "Processing image => data/test_set_images/test_4/test_4.png\n",
      "Processing image => data/test_set_images/test_5/test_5.png\n",
      "Processing image => data/test_set_images/test_6/test_6.png\n",
      "Processing image => data/test_set_images/test_7/test_7.png\n",
      "Processing image => data/test_set_images/test_8/test_8.png\n",
      "Processing image => data/test_set_images/test_9/test_9.png\n",
      "Processing image => data/test_set_images/test_10/test_10.png\n",
      "Processing image => data/test_set_images/test_11/test_11.png\n",
      "Processing image => data/test_set_images/test_12/test_12.png\n",
      "Processing image => data/test_set_images/test_13/test_13.png\n",
      "Processing image => data/test_set_images/test_14/test_14.png\n",
      "Processing image => data/test_set_images/test_15/test_15.png\n",
      "Processing image => data/test_set_images/test_16/test_16.png\n",
      "Processing image => data/test_set_images/test_17/test_17.png\n",
      "Processing image => data/test_set_images/test_18/test_18.png\n",
      "Processing image => data/test_set_images/test_19/test_19.png\n",
      "Processing image => data/test_set_images/test_20/test_20.png\n",
      "Processing image => data/test_set_images/test_21/test_21.png\n",
      "Processing image => data/test_set_images/test_22/test_22.png\n",
      "Processing image => data/test_set_images/test_23/test_23.png\n",
      "Processing image => data/test_set_images/test_24/test_24.png\n",
      "Processing image => data/test_set_images/test_25/test_25.png\n",
      "Processing image => data/test_set_images/test_26/test_26.png\n",
      "Processing image => data/test_set_images/test_27/test_27.png\n",
      "Processing image => data/test_set_images/test_28/test_28.png\n",
      "Processing image => data/test_set_images/test_29/test_29.png\n",
      "Processing image => data/test_set_images/test_30/test_30.png\n",
      "Processing image => data/test_set_images/test_31/test_31.png\n",
      "Processing image => data/test_set_images/test_32/test_32.png\n",
      "Processing image => data/test_set_images/test_33/test_33.png\n",
      "Processing image => data/test_set_images/test_34/test_34.png\n",
      "Processing image => data/test_set_images/test_35/test_35.png\n",
      "Processing image => data/test_set_images/test_36/test_36.png\n",
      "Processing image => data/test_set_images/test_37/test_37.png\n",
      "Processing image => data/test_set_images/test_38/test_38.png\n",
      "Processing image => data/test_set_images/test_39/test_39.png\n",
      "Processing image => data/test_set_images/test_40/test_40.png\n",
      "Processing image => data/test_set_images/test_41/test_41.png\n",
      "Processing image => data/test_set_images/test_42/test_42.png\n",
      "Processing image => data/test_set_images/test_43/test_43.png\n",
      "Processing image => data/test_set_images/test_44/test_44.png\n",
      "Processing image => data/test_set_images/test_45/test_45.png\n",
      "Processing image => data/test_set_images/test_46/test_46.png\n",
      "Processing image => data/test_set_images/test_47/test_47.png\n",
      "Processing image => data/test_set_images/test_48/test_48.png\n",
      "Processing image => data/test_set_images/test_49/test_49.png\n",
      "Processing image => data/test_set_images/test_50/test_50.png\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = CNN(shape=(WINDOW_SIZE, WINDOW_SIZE, 3))\n",
    "\n",
    "# Load the model\n",
    "model.load(\"Erick_dropout_0.25_1024-130-0.977367.h5\")\n",
    "\n",
    "model.model.summary()\n",
    "\n",
    "# We add all test images to an array, used later for generating a submission\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "    image_filenames.append(image_filename)\n",
    "\n",
    "# Set-up submission filename\n",
    "submission_filename = \"Erick_dropout_0.25_1024-130-0.977367.csv\"\n",
    "\n",
    "# Generates the submission\n",
    "generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}