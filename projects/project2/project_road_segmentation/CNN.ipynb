{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext lab_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from utils import *\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.compat.v2.keras.layers import BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16379839625501494973\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4937233203\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1827712432552967652\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(8)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training images, images loaded: 100 \n",
      "Loading groundtruth images, images loaded: 100 \n"
     ]
    }
   ],
   "source": [
    "image_dir_train = \"data/training/images/\"\n",
    "files = os.listdir(image_dir_train)\n",
    "n_train = len(files)\n",
    "print(f\"Loading training images, images loaded: {n_train} \")\n",
    "imgs_train = np.asarray(\n",
    "    [load_image(image_dir_train + files[i]) for i in range(n_train)]\n",
    ")\n",
    "gt_dir_train = \"data/training/groundtruth/\"\n",
    "print(f\"Loading groundtruth images, images loaded: {n_train} \")\n",
    "gt_imgs_train = np.asarray(\n",
    "    [load_image(gt_dir_train + files[i]) for i in range(n_train)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 400, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 400)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_imgs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 400\n",
    "# Patches for training\n",
    "img_patches_train = [\n",
    "    crop_image(imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "gt_patches_train = [\n",
    "    crop_image(gt_imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = np.asarray(\n",
    "    [\n",
    "        img_patches_train[i][j]\n",
    "        for i in range(len(img_patches_train))\n",
    "        for j in range(len(img_patches_train[i]))\n",
    "    ]\n",
    ")\n",
    "Y_train = np.asarray(\n",
    "    [\n",
    "        gt_patches_train[i][j]\n",
    "        for i in range(len(gt_patches_train))\n",
    "        for j in range(len(gt_patches_train[i]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 400, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 400)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_dir_val = \"data/validating/images/\"\n",
    "# files = os.listdir(image_dir_val)\n",
    "# n_val = len(files)\n",
    "# print(f\"Loading validating images, images loaded: {n_val} \")\n",
    "# imgs_val = np.asarray([load_image(image_dir_val + files[i]) for i in range(n_val)])\n",
    "# gt_dir_val = \"data/validating/groundtruth/\"\n",
    "# print(f\"Loading validating groundtruth, images loaded: {n_val} \")\n",
    "# gt_imgs_val = np.asarray([load_image(gt_dir_val + files[i]) for i in range(n_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_size = 400\n",
    "# # Patches for validating\n",
    "# img_patches_val = [\n",
    "#     crop_image(imgs_val[i], image_size, image_size) for i in range(n_val)\n",
    "# ]\n",
    "# gt_patches_val = [\n",
    "#     crop_image(gt_imgs_val[i], image_size, image_size) for i in range(n_val)\n",
    "# ]\n",
    "\n",
    "# # Separate features and labels\n",
    "# X_val = np.asarray(\n",
    "#     [\n",
    "#         img_patches_val[i][j]\n",
    "#         for i in range(len(img_patches_val))\n",
    "#         for j in range(len(img_patches_val[i]))\n",
    "#     ]\n",
    "# )\n",
    "# Y_val = np.asarray(\n",
    "#     [\n",
    "#         gt_patches_val[i][j]\n",
    "#         for i in range(len(gt_patches_val))\n",
    "#         for j in range(len(gt_patches_val[i]))\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = imag_rotation_aug(imgs_train, gt_imgs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "Y_train = np.asarray(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 456, 456, 3)\n",
      "(900, 456, 456)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "n_train = Y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val, Y_val = imag_rotation_aug(imgs_val, gt_imgs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val = np.asarray(X_val)\n",
    "# Y_val = np.asarray(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_val.shape)\n",
    "# print(Y_val.shape)\n",
    "# n_val = Y_val.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create functions to calcualte precision, recall and F-1 in the training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Compute the Precision for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "        Precision (numpy.float64): the Precision of the batch \n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Compute the Recall for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "       Recall (numpy.float64): the Recal of the batch \n",
    "    \"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    \"\"\"Compute the F-1 for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "       F-1 (numpy.float64): the F-1 of the batch \n",
    "    \"\"\"\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erick architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        self.model = self.initialize_U_NET(shape)\n",
    "\n",
    "    def initialize_U_NET(self, shape):\n",
    "        \"\"\"Create Network Architecture.\n",
    "        Args:\n",
    "            shape (triplet): Size of the input layer height x width x colors (64 x 64 x 3)\n",
    "        Returns:\n",
    "            model (Neural Network): Architecture of the model\n",
    "        \"\"\"\n",
    "        # INPUT\n",
    "        # shape     - Size of the input images\n",
    "        # OUTPUT\n",
    "        # model    - Compiled CNN\n",
    "\n",
    "        # Define hyperparamters\n",
    "        KERNEL3 = (3, 3)\n",
    "        KERNEL5 = (5, 5)\n",
    "\n",
    "        # Define a model\n",
    "        model = Sequential()\n",
    "\n",
    "        # Add the layers\n",
    "        # Selection of the model is described in the report\n",
    "        # We use padding = 'same' to avoid issues with the matrix sizes\n",
    "        model.add(Conv2D(64, KERNEL5, input_shape=shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Conv2D(128, KERNEL3, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Conv2D(512, KERNEL3, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        # Flatten it and use regularizers to avoid overfitting\n",
    "        # The parameters have been chosen empirically\n",
    "        model.add(Flatten())\n",
    "        model.add(\n",
    "            Dense(\n",
    "                128, kernel_regularizer=l2(0.000001), activity_regularizer=l2(0.000001)\n",
    "            )\n",
    "        )\n",
    "        # model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # Add output layer\n",
    "        model.add(\n",
    "            Dense(2, kernel_regularizer=l2(0.000001), activity_regularizer=l2(0.000001))\n",
    "        )\n",
    "        model.add(Activation(\"sigmoid\"))\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            metrics=[\"accuracy\", recall, f1],\n",
    "        )\n",
    "\n",
    "        # Print a summary of the model to see what has been generated\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the Model.\n",
    "\n",
    "        Returns:\n",
    "            History (History_Keras): History of the training\n",
    "        \"\"\"\n",
    "        # Early stopping callback after 10 steps\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"loss\", patience=10, verbose=1, restore_best_weights=True,\n",
    "        )\n",
    "        # Reduce learning rate on plateau after 5 steps\n",
    "        lr_callback = ReduceLROnPlateau(\n",
    "            monitor=\"loss\", factor=0.8, patience=4, verbose=1, cooldown=1,\n",
    "        )\n",
    "        save_best = ModelCheckpoint(\n",
    "            \"Erick_dropout_0.2-{epoch:03d}-{f1:03f}.h5\",\n",
    "            save_best_only=True,\n",
    "            monitor=\"loss\",\n",
    "            verbose=1,\n",
    "        )\n",
    "        callbacks = [lr_callback, save_best, early_stopping]\n",
    "\n",
    "        # Train the model using the previously defined functions and callbacks\n",
    "        history = self.model.fit_generator(\n",
    "            create_minibatch(\n",
    "                X_train, Y_train, n_train, WINDOW_SIZE, BATCH_SIZE, PATCH_SIZE, WIDTH\n",
    "            ),\n",
    "            steps_per_epoch=STEPS_PER_EPOCH,\n",
    "            epochs=EPOCHS,\n",
    "            use_multiprocessing=False,\n",
    "            workers=1,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "            #             validation_data=create_minibatch(\n",
    "            #                 X_val, Y_val, n_val, WINDOW_SIZE, BATCH_SIZE, PATCH_SIZE, WIDTH\n",
    "            #             ),\n",
    "            #             validation_steps=STEPS_PER_EPOCH / 3,\n",
    "        )\n",
    "        return history\n",
    "\n",
    "    def classify(self, X):\n",
    "        \"\"\"Classify Image as either road or not.\n",
    "        Args:\n",
    "            X (image): part of the image to classify\n",
    "        Returns:\n",
    "            Predictions : Predictions for each patch\n",
    "        \"\"\"\n",
    "        # Subdivide the images into blocks with a stride and patch_size of 16\n",
    "        img_patches = create_patches(X, 16, 16, padding=24)\n",
    "\n",
    "        # Predict\n",
    "        predictions = self.model.predict(img_patches)\n",
    "        predictions = (predictions[:, 0] < predictions[:, 1]) * 1\n",
    "\n",
    "        # Regroup patches into images\n",
    "        return predictions.reshape(X.shape[0], -1)\n",
    "\n",
    "    #         return group_patches(predictions, X.shape[0])\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"Loads Saved Model.\n",
    "        Args:\n",
    "           filename (string): name of the model\n",
    "           \n",
    "        \"\"\"\n",
    "        # Load the model (used for submission)\n",
    "        dependencies = {\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "        self.model = load_model(filename, custom_objects=dependencies)\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Saves trained model.\n",
    "        Args:\n",
    "           filename (string): name of the model\n",
    "           \n",
    "        \"\"\"\n",
    "        self.model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 64)        4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1048704   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,603,010\n",
      "Trainable params: 2,603,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We define parameters of the model\n",
    "BATCH_SIZE = 300\n",
    "WINDOW_SIZE = 64\n",
    "PATCH_SIZE = 16\n",
    "EPOCHS = 300\n",
    "STEPS_PER_EPOCH = 100\n",
    "WIDTH = 448\n",
    "model = CNN(shape=(WINDOW_SIZE, WINDOW_SIZE, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.5463 - accuracy: 0.7569 - recall: 0.7585 - f1: 0.7575\n",
      "Epoch 00001: loss improved from inf to 0.54624, saving model to Erick_dropout_0.2-001-0.757395.h5\n",
      "100/100 [==============================] - 41s 411ms/step - loss: 0.5462 - accuracy: 0.7567 - recall: 0.7583 - f1: 0.7574\n",
      "Epoch 2/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.5066 - accuracy: 0.7514 - recall: 0.7523 - f1: 0.7516\n",
      "Epoch 00002: loss improved from 0.54624 to 0.50641, saving model to Erick_dropout_0.2-002-0.751483.h5\n",
      "100/100 [==============================] - 39s 387ms/step - loss: 0.5064 - accuracy: 0.7513 - recall: 0.7522 - f1: 0.7515\n",
      "Epoch 3/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.4686 - accuracy: 0.7684 - recall: 0.7675 - f1: 0.7682\n",
      "Epoch 00003: loss improved from 0.50641 to 0.46818, saving model to Erick_dropout_0.2-003-0.768587.h5\n",
      "100/100 [==============================] - 38s 378ms/step - loss: 0.4682 - accuracy: 0.7688 - recall: 0.7679 - f1: 0.7686\n",
      "Epoch 4/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.4081 - accuracy: 0.8022 - recall: 0.7980 - f1: 0.8014\n",
      "Epoch 00004: loss improved from 0.46818 to 0.40828, saving model to Erick_dropout_0.2-004-0.801557.h5\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.4083 - accuracy: 0.8023 - recall: 0.7982 - f1: 0.8016\n",
      "Epoch 5/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3541 - accuracy: 0.8355 - recall: 0.8332 - f1: 0.8352\n",
      "Epoch 00005: loss improved from 0.40828 to 0.35359, saving model to Erick_dropout_0.2-005-0.835361.h5\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.3536 - accuracy: 0.8357 - recall: 0.8334 - f1: 0.8354\n",
      "Epoch 6/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3521 - accuracy: 0.8406 - recall: 0.8390 - f1: 0.8403\n",
      "Epoch 00006: loss improved from 0.35359 to 0.35215, saving model to Erick_dropout_0.2-006-0.840300.h5\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.3522 - accuracy: 0.8406 - recall: 0.8389 - f1: 0.8403\n",
      "Epoch 7/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3464 - accuracy: 0.8404 - recall: 0.8387 - f1: 0.8401\n",
      "Epoch 00007: loss improved from 0.35215 to 0.34608, saving model to Erick_dropout_0.2-007-0.840463.h5\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.3461 - accuracy: 0.8407 - recall: 0.8391 - f1: 0.8405\n",
      "Epoch 8/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3176 - accuracy: 0.8557 - recall: 0.8554 - f1: 0.8556\n",
      "Epoch 00008: loss improved from 0.34608 to 0.31692, saving model to Erick_dropout_0.2-008-0.856141.h5\n",
      "100/100 [==============================] - 37s 372ms/step - loss: 0.3169 - accuracy: 0.8562 - recall: 0.8558 - f1: 0.8561\n",
      "Epoch 9/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.8725 - recall: 0.8721 - f1: 0.8724\n",
      "Epoch 00009: loss improved from 0.31692 to 0.29146, saving model to Erick_dropout_0.2-009-0.872253.h5\n",
      "100/100 [==============================] - 41s 412ms/step - loss: 0.2915 - accuracy: 0.8723 - recall: 0.8720 - f1: 0.8723\n",
      "Epoch 10/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2866 - accuracy: 0.8758 - recall: 0.8750 - f1: 0.8757\n",
      "Epoch 00010: loss improved from 0.29146 to 0.28669, saving model to Erick_dropout_0.2-010-0.875580.h5\n",
      "100/100 [==============================] - 35s 355ms/step - loss: 0.2867 - accuracy: 0.8757 - recall: 0.8749 - f1: 0.8756\n",
      "Epoch 11/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2756 - accuracy: 0.8781 - recall: 0.8780 - f1: 0.8781\n",
      "Epoch 00011: loss improved from 0.28669 to 0.27579, saving model to Erick_dropout_0.2-011-0.878036.h5\n",
      "100/100 [==============================] - 33s 330ms/step - loss: 0.2758 - accuracy: 0.8781 - recall: 0.8780 - f1: 0.8780\n",
      "Epoch 12/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2587 - accuracy: 0.8872 - recall: 0.8863 - f1: 0.8871\n",
      "Epoch 00012: loss improved from 0.27579 to 0.25903, saving model to Erick_dropout_0.2-012-0.886876.h5\n",
      "100/100 [==============================] - 32s 324ms/step - loss: 0.2590 - accuracy: 0.8870 - recall: 0.8861 - f1: 0.8869\n",
      "Epoch 13/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2473 - accuracy: 0.8936 - recall: 0.8932 - f1: 0.8936\n",
      "Epoch 00013: loss improved from 0.25903 to 0.24712, saving model to Erick_dropout_0.2-013-0.893582.h5\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.2471 - accuracy: 0.8936 - recall: 0.8932 - f1: 0.8936\n",
      "Epoch 14/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2438 - accuracy: 0.8931 - recall: 0.8930 - f1: 0.8931\n",
      "Epoch 00014: loss improved from 0.24712 to 0.24405, saving model to Erick_dropout_0.2-014-0.892905.h5\n",
      "100/100 [==============================] - 32s 318ms/step - loss: 0.2441 - accuracy: 0.8929 - recall: 0.8928 - f1: 0.8929\n",
      "Epoch 15/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2439 - accuracy: 0.8947 - recall: 0.8937 - f1: 0.8946\n",
      "Epoch 00015: loss improved from 0.24405 to 0.24347, saving model to Erick_dropout_0.2-015-0.894898.h5\n",
      "100/100 [==============================] - 38s 378ms/step - loss: 0.2435 - accuracy: 0.8950 - recall: 0.8940 - f1: 0.8949\n",
      "Epoch 16/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2344 - accuracy: 0.8980 - recall: 0.8975 - f1: 0.8979\n",
      "Epoch 00016: loss improved from 0.24347 to 0.23446, saving model to Erick_dropout_0.2-016-0.897828.h5\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.2345 - accuracy: 0.8979 - recall: 0.8974 - f1: 0.8978\n",
      "Epoch 17/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2334 - accuracy: 0.9010 - recall: 0.9011 - f1: 0.9010\n",
      "Epoch 00017: loss improved from 0.23446 to 0.23368, saving model to Erick_dropout_0.2-017-0.900748.h5\n",
      "100/100 [==============================] - 40s 401ms/step - loss: 0.2337 - accuracy: 0.9007 - recall: 0.9009 - f1: 0.9007\n",
      "Epoch 18/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2306 - accuracy: 0.9017 - recall: 0.9018 - f1: 0.9017\n",
      "Epoch 00018: loss improved from 0.23368 to 0.23068, saving model to Erick_dropout_0.2-018-0.901614.h5\n",
      "100/100 [==============================] - 34s 343ms/step - loss: 0.2307 - accuracy: 0.9016 - recall: 0.9017 - f1: 0.9016\n",
      "Epoch 19/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2283 - accuracy: 0.9034 - recall: 0.9030 - f1: 0.9033\n",
      "Epoch 00019: loss improved from 0.23068 to 0.22872, saving model to Erick_dropout_0.2-019-0.902892.h5\n",
      "100/100 [==============================] - 33s 334ms/step - loss: 0.2287 - accuracy: 0.9029 - recall: 0.9025 - f1: 0.9029\n",
      "Epoch 20/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2221 - accuracy: 0.9048 - recall: 0.9042 - f1: 0.9047\n",
      "Epoch 00020: loss improved from 0.22872 to 0.22211, saving model to Erick_dropout_0.2-020-0.904699.h5\n",
      "100/100 [==============================] - 34s 339ms/step - loss: 0.2221 - accuracy: 0.9047 - recall: 0.9042 - f1: 0.9047\n",
      "Epoch 21/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2169 - accuracy: 0.9079 - recall: 0.9082 - f1: 0.9079\n",
      "Epoch 00021: loss improved from 0.22211 to 0.21711, saving model to Erick_dropout_0.2-021-0.907857.h5\n",
      "100/100 [==============================] - 36s 359ms/step - loss: 0.2171 - accuracy: 0.9078 - recall: 0.9082 - f1: 0.9079\n",
      "Epoch 22/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2170 - accuracy: 0.9105 - recall: 0.9103 - f1: 0.9105\n",
      "Epoch 00022: loss did not improve from 0.21711\n",
      "100/100 [==============================] - 35s 354ms/step - loss: 0.2171 - accuracy: 0.9104 - recall: 0.9102 - f1: 0.9104\n",
      "Epoch 23/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2162 - accuracy: 0.9090 - recall: 0.9093 - f1: 0.9090\n",
      "Epoch 00023: loss improved from 0.21711 to 0.21613, saving model to Erick_dropout_0.2-023-0.909031.h5\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.2161 - accuracy: 0.9090 - recall: 0.9094 - f1: 0.9090\n",
      "Epoch 24/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2088 - accuracy: 0.9130 - recall: 0.9132 - f1: 0.9130\n",
      "Epoch 00024: loss improved from 0.21613 to 0.20868, saving model to Erick_dropout_0.2-024-0.913138.h5\n",
      "100/100 [==============================] - 36s 355ms/step - loss: 0.2087 - accuracy: 0.9131 - recall: 0.9134 - f1: 0.9131\n",
      "Epoch 25/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2072 - accuracy: 0.9128 - recall: 0.9125 - f1: 0.9128\n",
      "Epoch 00025: loss improved from 0.20868 to 0.20703, saving model to Erick_dropout_0.2-025-0.912838.h5\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.2070 - accuracy: 0.9129 - recall: 0.9126 - f1: 0.9128\n",
      "Epoch 26/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2038 - accuracy: 0.9164 - recall: 0.9162 - f1: 0.9164\n",
      "Epoch 00026: loss improved from 0.20703 to 0.20367, saving model to Erick_dropout_0.2-026-0.916346.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.2037 - accuracy: 0.9164 - recall: 0.9161 - f1: 0.9163\n",
      "Epoch 27/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2084 - accuracy: 0.9137 - recall: 0.9133 - f1: 0.9137\n",
      "Epoch 00027: loss did not improve from 0.20367\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.2081 - accuracy: 0.9137 - recall: 0.9134 - f1: 0.9137\n",
      "Epoch 28/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1999 - accuracy: 0.9164 - recall: 0.9161 - f1: 0.9164\n",
      "Epoch 00028: loss improved from 0.20367 to 0.19958, saving model to Erick_dropout_0.2-028-0.916523.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1996 - accuracy: 0.9165 - recall: 0.9163 - f1: 0.9165\n",
      "Epoch 29/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2070 - accuracy: 0.9127 - recall: 0.9127 - f1: 0.9127\n",
      "Epoch 00029: loss did not improve from 0.19958\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.2068 - accuracy: 0.9129 - recall: 0.9128 - f1: 0.9129\n",
      "Epoch 30/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2019 - accuracy: 0.9166 - recall: 0.9169 - f1: 0.9166\n",
      "Epoch 00030: loss did not improve from 0.19958\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.2018 - accuracy: 0.9167 - recall: 0.9170 - f1: 0.9167\n",
      "Epoch 31/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2084 - accuracy: 0.9139 - recall: 0.9140 - f1: 0.9139\n",
      "Epoch 00031: loss did not improve from 0.19958\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.2086 - accuracy: 0.9136 - recall: 0.9137 - f1: 0.9137\n",
      "Epoch 32/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2052 - accuracy: 0.9161 - recall: 0.9161 - f1: 0.9161\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.19958\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.2050 - accuracy: 0.9161 - recall: 0.9162 - f1: 0.9161\n",
      "Epoch 33/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1870 - accuracy: 0.9232 - recall: 0.9235 - f1: 0.9232\n",
      "Epoch 00033: loss improved from 0.19958 to 0.18641, saving model to Erick_dropout_0.2-033-0.923551.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1864 - accuracy: 0.9235 - recall: 0.9238 - f1: 0.9236\n",
      "Epoch 34/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1845 - accuracy: 0.9268 - recall: 0.9270 - f1: 0.9268\n",
      "Epoch 00034: loss improved from 0.18641 to 0.18413, saving model to Erick_dropout_0.2-034-0.926975.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1841 - accuracy: 0.9270 - recall: 0.9271 - f1: 0.9270\n",
      "Epoch 35/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1778 - accuracy: 0.9269 - recall: 0.9267 - f1: 0.9269\n",
      "Epoch 00035: loss improved from 0.18413 to 0.17769, saving model to Erick_dropout_0.2-035-0.926915.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1777 - accuracy: 0.9269 - recall: 0.9267 - f1: 0.9269\n",
      "Epoch 36/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1791 - accuracy: 0.9268 - recall: 0.9270 - f1: 0.9268\n",
      "Epoch 00036: loss did not improve from 0.17769\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1787 - accuracy: 0.9271 - recall: 0.9273 - f1: 0.9271\n",
      "Epoch 37/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1839 - accuracy: 0.9260 - recall: 0.9263 - f1: 0.9260\n",
      "Epoch 00037: loss did not improve from 0.17769\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.1835 - accuracy: 0.9261 - recall: 0.9263 - f1: 0.9261\n",
      "Epoch 38/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1792 - accuracy: 0.9278 - recall: 0.9282 - f1: 0.9278\n",
      "Epoch 00038: loss did not improve from 0.17769\n",
      "100/100 [==============================] - 34s 342ms/step - loss: 0.1790 - accuracy: 0.9277 - recall: 0.9282 - f1: 0.9278\n",
      "Epoch 39/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1811 - accuracy: 0.9261 - recall: 0.9262 - f1: 0.9261\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.17769\n",
      "100/100 [==============================] - 35s 355ms/step - loss: 0.1814 - accuracy: 0.9260 - recall: 0.9261 - f1: 0.9260\n",
      "Epoch 40/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1732 - accuracy: 0.9317 - recall: 0.9319 - f1: 0.9317\n",
      "Epoch 00040: loss improved from 0.17769 to 0.17296, saving model to Erick_dropout_0.2-040-0.931745.h5\n",
      "100/100 [==============================] - 36s 362ms/step - loss: 0.1730 - accuracy: 0.9317 - recall: 0.9319 - f1: 0.9317\n",
      "Epoch 41/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1723 - accuracy: 0.9294 - recall: 0.9297 - f1: 0.9294\n",
      "Epoch 00041: loss improved from 0.17296 to 0.17198, saving model to Erick_dropout_0.2-041-0.929722.h5\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.1720 - accuracy: 0.9297 - recall: 0.9300 - f1: 0.9297\n",
      "Epoch 42/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1699 - accuracy: 0.9308 - recall: 0.9309 - f1: 0.9308\n",
      "Epoch 00042: loss improved from 0.17198 to 0.16939, saving model to Erick_dropout_0.2-042-0.931042.h5\n",
      "100/100 [==============================] - 33s 332ms/step - loss: 0.1694 - accuracy: 0.9310 - recall: 0.9312 - f1: 0.9310\n",
      "Epoch 43/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1662 - accuracy: 0.9339 - recall: 0.9339 - f1: 0.9339\n",
      "Epoch 00043: loss improved from 0.16939 to 0.16649, saving model to Erick_dropout_0.2-043-0.933752.h5\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 0.1665 - accuracy: 0.9337 - recall: 0.9338 - f1: 0.9338\n",
      "Epoch 44/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1678 - accuracy: 0.9330 - recall: 0.9330 - f1: 0.9330\n",
      "Epoch 00044: loss did not improve from 0.16649\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 0.1684 - accuracy: 0.9327 - recall: 0.9326 - f1: 0.9327\n",
      "Epoch 45/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1713 - accuracy: 0.9303 - recall: 0.9305 - f1: 0.9303\n",
      "Epoch 00045: loss did not improve from 0.16649\n",
      "100/100 [==============================] - 34s 337ms/step - loss: 0.1707 - accuracy: 0.9305 - recall: 0.9307 - f1: 0.9305\n",
      "Epoch 46/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1645 - accuracy: 0.9346 - recall: 0.9347 - f1: 0.9347\n",
      "Epoch 00046: loss improved from 0.16649 to 0.16433, saving model to Erick_dropout_0.2-046-0.934771.h5\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.1643 - accuracy: 0.9348 - recall: 0.9348 - f1: 0.9348\n",
      "Epoch 47/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1595 - accuracy: 0.9358 - recall: 0.9360 - f1: 0.9358\n",
      "Epoch 00047: loss improved from 0.16433 to 0.15926, saving model to Erick_dropout_0.2-047-0.935779.h5\n",
      "100/100 [==============================] - 33s 326ms/step - loss: 0.1593 - accuracy: 0.9358 - recall: 0.9359 - f1: 0.9358\n",
      "Epoch 48/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1715 - accuracy: 0.9305 - recall: 0.9306 - f1: 0.9305\n",
      "Epoch 00048: loss did not improve from 0.15926\n",
      "100/100 [==============================] - 34s 337ms/step - loss: 0.1717 - accuracy: 0.9304 - recall: 0.9306 - f1: 0.9304\n",
      "Epoch 49/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1657 - accuracy: 0.9331 - recall: 0.9334 - f1: 0.9331\n",
      "Epoch 00049: loss did not improve from 0.15926\n",
      "100/100 [==============================] - 34s 335ms/step - loss: 0.1655 - accuracy: 0.9331 - recall: 0.9334 - f1: 0.9332\n",
      "Epoch 50/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1636 - accuracy: 0.9351 - recall: 0.9351 - f1: 0.9351\n",
      "Epoch 00050: loss did not improve from 0.15926\n",
      "100/100 [==============================] - 35s 355ms/step - loss: 0.1637 - accuracy: 0.9350 - recall: 0.9349 - f1: 0.9350\n",
      "Epoch 51/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1625 - accuracy: 0.9356 - recall: 0.9356 - f1: 0.9356\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "\n",
      "Epoch 00051: loss did not improve from 0.15926\n",
      "100/100 [==============================] - 36s 358ms/step - loss: 0.1626 - accuracy: 0.9355 - recall: 0.9355 - f1: 0.9355\n",
      "Epoch 52/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1516 - accuracy: 0.9392 - recall: 0.9391 - f1: 0.9392\n",
      "Epoch 00052: loss improved from 0.15926 to 0.15166, saving model to Erick_dropout_0.2-052-0.939131.h5\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.1517 - accuracy: 0.9391 - recall: 0.9391 - f1: 0.9391\n",
      "Epoch 53/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1506 - accuracy: 0.9394 - recall: 0.9394 - f1: 0.9394\n",
      "Epoch 00053: loss improved from 0.15166 to 0.15078, saving model to Erick_dropout_0.2-053-0.939165.h5\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 0.1508 - accuracy: 0.9392 - recall: 0.9391 - f1: 0.9392\n",
      "Epoch 54/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1550 - accuracy: 0.9371 - recall: 0.9372 - f1: 0.9371\n",
      "Epoch 00054: loss did not improve from 0.15078\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 0.1549 - accuracy: 0.9371 - recall: 0.9372 - f1: 0.9371\n",
      "Epoch 55/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1556 - accuracy: 0.9361 - recall: 0.9362 - f1: 0.9361\n",
      "Epoch 00055: loss did not improve from 0.15078\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.1555 - accuracy: 0.9361 - recall: 0.9363 - f1: 0.9361\n",
      "Epoch 56/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1571 - accuracy: 0.9379 - recall: 0.9379 - f1: 0.9379\n",
      "Epoch 00056: loss did not improve from 0.15078\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.1573 - accuracy: 0.9377 - recall: 0.9378 - f1: 0.9377\n",
      "Epoch 57/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1603 - accuracy: 0.9363 - recall: 0.9362 - f1: 0.9363\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "\n",
      "Epoch 00057: loss did not improve from 0.15078\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 0.1603 - accuracy: 0.9364 - recall: 0.9363 - f1: 0.9364\n",
      "Epoch 58/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1489 - accuracy: 0.9413 - recall: 0.9414 - f1: 0.9413\n",
      "Epoch 00058: loss improved from 0.15078 to 0.14894, saving model to Erick_dropout_0.2-058-0.941402.h5\n",
      "100/100 [==============================] - 36s 355ms/step - loss: 0.1489 - accuracy: 0.9414 - recall: 0.9414 - f1: 0.9414\n",
      "Epoch 59/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1460 - accuracy: 0.9429 - recall: 0.9429 - f1: 0.9429\n",
      "Epoch 00059: loss improved from 0.14894 to 0.14569, saving model to Erick_dropout_0.2-059-0.943165.h5\n",
      "100/100 [==============================] - 34s 343ms/step - loss: 0.1457 - accuracy: 0.9432 - recall: 0.9431 - f1: 0.9432\n",
      "Epoch 60/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1429 - accuracy: 0.9423 - recall: 0.9422 - f1: 0.9423\n",
      "Epoch 00060: loss improved from 0.14569 to 0.14289, saving model to Erick_dropout_0.2-060-0.942410.h5\n",
      "100/100 [==============================] - 34s 339ms/step - loss: 0.1429 - accuracy: 0.9424 - recall: 0.9423 - f1: 0.9424\n",
      "Epoch 61/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1539 - accuracy: 0.9374 - recall: 0.9375 - f1: 0.9374\n",
      "Epoch 00061: loss did not improve from 0.14289\n",
      "100/100 [==============================] - 33s 335ms/step - loss: 0.1547 - accuracy: 0.9371 - recall: 0.9372 - f1: 0.9371\n",
      "Epoch 62/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1457 - accuracy: 0.9421 - recall: 0.9422 - f1: 0.9421\n",
      "Epoch 00062: loss did not improve from 0.14289\n",
      "100/100 [==============================] - 33s 335ms/step - loss: 0.1451 - accuracy: 0.9423 - recall: 0.9424 - f1: 0.9423\n",
      "Epoch 63/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1417 - accuracy: 0.9441 - recall: 0.9441 - f1: 0.9441\n",
      "Epoch 00063: loss improved from 0.14289 to 0.14199, saving model to Erick_dropout_0.2-063-0.944085.h5\n",
      "100/100 [==============================] - 37s 370ms/step - loss: 0.1420 - accuracy: 0.9441 - recall: 0.9441 - f1: 0.9441\n",
      "Epoch 64/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1399 - accuracy: 0.9444 - recall: 0.9444 - f1: 0.9444\n",
      "Epoch 00064: loss improved from 0.14199 to 0.13973, saving model to Erick_dropout_0.2-064-0.944485.h5\n",
      "100/100 [==============================] - 36s 356ms/step - loss: 0.1397 - accuracy: 0.9445 - recall: 0.9445 - f1: 0.9445\n",
      "Epoch 65/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1442 - accuracy: 0.9425 - recall: 0.9428 - f1: 0.9425\n",
      "Epoch 00065: loss did not improve from 0.13973\n",
      "100/100 [==============================] - 36s 358ms/step - loss: 0.1438 - accuracy: 0.9428 - recall: 0.9431 - f1: 0.9428\n",
      "Epoch 66/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1442 - accuracy: 0.9403 - recall: 0.9403 - f1: 0.9403\n",
      "Epoch 00066: loss did not improve from 0.13973\n",
      "100/100 [==============================] - 37s 369ms/step - loss: 0.1440 - accuracy: 0.9402 - recall: 0.9402 - f1: 0.9402\n",
      "Epoch 67/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1436 - accuracy: 0.9424 - recall: 0.9425 - f1: 0.9424\n",
      "Epoch 00067: loss did not improve from 0.13973\n",
      "100/100 [==============================] - 33s 334ms/step - loss: 0.1432 - accuracy: 0.9427 - recall: 0.9427 - f1: 0.9427\n",
      "Epoch 68/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1417 - accuracy: 0.9430 - recall: 0.9432 - f1: 0.9431\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "\n",
      "Epoch 00068: loss did not improve from 0.13973\n",
      "100/100 [==============================] - 32s 318ms/step - loss: 0.1416 - accuracy: 0.9431 - recall: 0.9433 - f1: 0.9431\n",
      "Epoch 69/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1409 - accuracy: 0.9436 - recall: 0.9437 - f1: 0.9436\n",
      "Epoch 00069: loss did not improve from 0.13973\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 0.1405 - accuracy: 0.9438 - recall: 0.9439 - f1: 0.9438\n",
      "Epoch 70/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1339 - accuracy: 0.9468 - recall: 0.9469 - f1: 0.9468\n",
      "Epoch 00070: loss improved from 0.13973 to 0.13377, saving model to Erick_dropout_0.2-070-0.946905.h5\n",
      "100/100 [==============================] - 32s 317ms/step - loss: 0.1338 - accuracy: 0.9469 - recall: 0.9470 - f1: 0.9469\n",
      "Epoch 71/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1342 - accuracy: 0.9456 - recall: 0.9456 - f1: 0.9456\n",
      "Epoch 00071: loss did not improve from 0.13377\n",
      "100/100 [==============================] - 31s 315ms/step - loss: 0.1343 - accuracy: 0.9456 - recall: 0.9457 - f1: 0.9457\n",
      "Epoch 72/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1383 - accuracy: 0.9447 - recall: 0.9446 - f1: 0.9447\n",
      "Epoch 00072: loss did not improve from 0.13377\n",
      "100/100 [==============================] - 32s 316ms/step - loss: 0.1383 - accuracy: 0.9446 - recall: 0.9445 - f1: 0.9446\n",
      "Epoch 73/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1382 - accuracy: 0.9447 - recall: 0.9447 - f1: 0.9447\n",
      "Epoch 00073: loss did not improve from 0.13377\n",
      "100/100 [==============================] - 31s 315ms/step - loss: 0.1384 - accuracy: 0.9444 - recall: 0.9444 - f1: 0.9444\n",
      "Epoch 74/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1345 - accuracy: 0.9465 - recall: 0.9466 - f1: 0.9465\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "\n",
      "Epoch 00074: loss did not improve from 0.13377\n",
      "100/100 [==============================] - 32s 315ms/step - loss: 0.1347 - accuracy: 0.9463 - recall: 0.9463 - f1: 0.9463\n",
      "Epoch 75/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1343 - accuracy: 0.9467 - recall: 0.9469 - f1: 0.9467\n",
      "Epoch 00075: loss did not improve from 0.13377\n",
      "100/100 [==============================] - 31s 315ms/step - loss: 0.1338 - accuracy: 0.9470 - recall: 0.9472 - f1: 0.9470\n",
      "Epoch 76/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1346 - accuracy: 0.9453 - recall: 0.9453 - f1: 0.9453\n",
      "Epoch 00076: loss did not improve from 0.13377\n",
      "100/100 [==============================] - 32s 315ms/step - loss: 0.1344 - accuracy: 0.9453 - recall: 0.9454 - f1: 0.9453\n",
      "Epoch 77/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1297 - accuracy: 0.9484 - recall: 0.9485 - f1: 0.9484\n",
      "Epoch 00077: loss improved from 0.13377 to 0.12994, saving model to Erick_dropout_0.2-077-0.948189.h5\n",
      "100/100 [==============================] - 32s 316ms/step - loss: 0.1299 - accuracy: 0.9482 - recall: 0.9483 - f1: 0.9482\n",
      "Epoch 78/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1286 - accuracy: 0.9487 - recall: 0.9488 - f1: 0.9487\n",
      "Epoch 00078: loss improved from 0.12994 to 0.12840, saving model to Erick_dropout_0.2-078-0.948817.h5\n",
      "100/100 [==============================] - 32s 318ms/step - loss: 0.1284 - accuracy: 0.9488 - recall: 0.9488 - f1: 0.9488\n",
      "Epoch 79/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9493 - recall: 0.9493 - f1: 0.9493\n",
      "Epoch 00079: loss improved from 0.12840 to 0.12741, saving model to Erick_dropout_0.2-079-0.949266.h5\n",
      "100/100 [==============================] - 32s 317ms/step - loss: 0.1274 - accuracy: 0.9493 - recall: 0.9493 - f1: 0.9493\n",
      "Epoch 80/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1385 - accuracy: 0.9441 - recall: 0.9442 - f1: 0.9441\n",
      "Epoch 00080: loss did not improve from 0.12741\n",
      "100/100 [==============================] - 32s 316ms/step - loss: 0.1382 - accuracy: 0.9441 - recall: 0.9442 - f1: 0.9441\n",
      "Epoch 81/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488\n",
      "Epoch 00081: loss did not improve from 0.12741\n",
      "100/100 [==============================] - 33s 326ms/step - loss: 0.1310 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488\n",
      "Epoch 82/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1295 - accuracy: 0.9486 - recall: 0.9487 - f1: 0.9486\n",
      "Epoch 00082: loss did not improve from 0.12741\n",
      "100/100 [==============================] - 32s 316ms/step - loss: 0.1296 - accuracy: 0.9486 - recall: 0.9487 - f1: 0.9486\n",
      "Epoch 83/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1248 - accuracy: 0.9512 - recall: 0.9512 - f1: 0.9512\n",
      "Epoch 00083: loss improved from 0.12741 to 0.12500, saving model to Erick_dropout_0.2-083-0.951184.h5\n",
      "100/100 [==============================] - 32s 317ms/step - loss: 0.1250 - accuracy: 0.9512 - recall: 0.9512 - f1: 0.9512\n",
      "Epoch 84/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1295 - accuracy: 0.9474 - recall: 0.9475 - f1: 0.9474\n",
      "Epoch 00084: loss did not improve from 0.12500\n",
      "100/100 [==============================] - 32s 316ms/step - loss: 0.1292 - accuracy: 0.9474 - recall: 0.9476 - f1: 0.9475\n",
      "Epoch 85/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507\n",
      "Epoch 00085: loss improved from 0.12500 to 0.12365, saving model to Erick_dropout_0.2-085-0.950663.h5\n",
      "100/100 [==============================] - 32s 317ms/step - loss: 0.1237 - accuracy: 0.9507 - recall: 0.9506 - f1: 0.9507\n",
      "Epoch 86/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 0.9502 - recall: 0.9503 - f1: 0.9502\n",
      "Epoch 00086: loss did not improve from 0.12365\n",
      "100/100 [==============================] - 32s 315ms/step - loss: 0.1262 - accuracy: 0.9504 - recall: 0.9504 - f1: 0.9504\n",
      "Epoch 87/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9511 - recall: 0.9511 - f1: 0.9511\n",
      "Epoch 00087: loss improved from 0.12365 to 0.12142, saving model to Erick_dropout_0.2-087-0.951087.h5\n",
      "100/100 [==============================] - 32s 317ms/step - loss: 0.1214 - accuracy: 0.9511 - recall: 0.9512 - f1: 0.9511\n",
      "Epoch 88/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9512 - recall: 0.9511 - f1: 0.9512\n",
      "Epoch 00088: loss improved from 0.12142 to 0.12036, saving model to Erick_dropout_0.2-088-0.951031.h5\n",
      "100/100 [==============================] - 32s 316ms/step - loss: 0.1204 - accuracy: 0.9510 - recall: 0.9510 - f1: 0.9510\n",
      "Epoch 89/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1271 - accuracy: 0.9484 - recall: 0.9484 - f1: 0.9484\n",
      "Epoch 00089: loss did not improve from 0.12036\n",
      "100/100 [==============================] - 32s 316ms/step - loss: 0.1270 - accuracy: 0.9483 - recall: 0.9483 - f1: 0.9483\n",
      "Epoch 90/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9490 - recall: 0.9491 - f1: 0.9490\n",
      "Epoch 00090: loss did not improve from 0.12036\n",
      "100/100 [==============================] - 32s 315ms/step - loss: 0.1273 - accuracy: 0.9489 - recall: 0.9489 - f1: 0.9489\n",
      "Epoch 91/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9519 - recall: 0.9519 - f1: 0.9519\n",
      "Epoch 00091: loss did not improve from 0.12036\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 0.1237 - accuracy: 0.9519 - recall: 0.9520 - f1: 0.9519\n",
      "Epoch 92/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9530 - recall: 0.9530 - f1: 0.9530\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "\n",
      "Epoch 00092: loss did not improve from 0.12036\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 0.1215 - accuracy: 0.9530 - recall: 0.9530 - f1: 0.9530\n",
      "Epoch 93/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9491 - recall: 0.9491 - f1: 0.9491\n",
      "Epoch 00093: loss did not improve from 0.12036\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 0.1237 - accuracy: 0.9492 - recall: 0.9491 - f1: 0.9491\n",
      "Epoch 94/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1249 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507\n",
      "Epoch 00094: loss did not improve from 0.12036\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 0.1251 - accuracy: 0.9506 - recall: 0.9507 - f1: 0.9506\n",
      "Epoch 95/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9495 - recall: 0.9496 - f1: 0.9495\n",
      "Epoch 00095: loss did not improve from 0.12036\n",
      "100/100 [==============================] - 32s 316ms/step - loss: 0.1225 - accuracy: 0.9497 - recall: 0.9498 - f1: 0.9497\n",
      "Epoch 96/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9518 - recall: 0.9518 - f1: 0.9518\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "\n",
      "Epoch 00096: loss did not improve from 0.12036\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1239 - accuracy: 0.9518 - recall: 0.9517 - f1: 0.9518\n",
      "Epoch 97/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9524 - recall: 0.9524 - f1: 0.9524\n",
      "Epoch 00097: loss improved from 0.12036 to 0.11981, saving model to Erick_dropout_0.2-097-0.952547.h5\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 0.1198 - accuracy: 0.9525 - recall: 0.9525 - f1: 0.9525\n",
      "Epoch 98/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1167 - accuracy: 0.9540 - recall: 0.9540 - f1: 0.9540\n",
      "Epoch 00098: loss improved from 0.11981 to 0.11688, saving model to Erick_dropout_0.2-098-0.953864.h5\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 0.1169 - accuracy: 0.9539 - recall: 0.9538 - f1: 0.9539\n",
      "Epoch 99/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9507 - recall: 0.9508 - f1: 0.9507\n",
      "Epoch 00099: loss did not improve from 0.11688\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1219 - accuracy: 0.9508 - recall: 0.9508 - f1: 0.9508\n",
      "Epoch 100/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9537 - recall: 0.9537 - f1: 0.9537\n",
      "Epoch 00100: loss did not improve from 0.11688\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1176 - accuracy: 0.9536 - recall: 0.9537 - f1: 0.9536\n",
      "Epoch 101/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1171 - accuracy: 0.9543 - recall: 0.9543 - f1: 0.9543\n",
      "Epoch 00101: loss did not improve from 0.11688\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1171 - accuracy: 0.9544 - recall: 0.9544 - f1: 0.9544\n",
      "Epoch 102/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9524 - recall: 0.9524 - f1: 0.9524\n",
      "Epoch 00102: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "\n",
      "Epoch 00102: loss did not improve from 0.11688\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1203 - accuracy: 0.9524 - recall: 0.9524 - f1: 0.9524\n",
      "Epoch 103/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9520 - recall: 0.9521 - f1: 0.9520\n",
      "Epoch 00103: loss did not improve from 0.11688\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1185 - accuracy: 0.9521 - recall: 0.9521 - f1: 0.9521\n",
      "Epoch 104/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1179 - accuracy: 0.9533 - recall: 0.9533 - f1: 0.9533\n",
      "Epoch 00104: loss did not improve from 0.11688\n",
      "100/100 [==============================] - 31s 312ms/step - loss: 0.1178 - accuracy: 0.9533 - recall: 0.9533 - f1: 0.9533\n",
      "Epoch 105/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1095 - accuracy: 0.9582 - recall: 0.9583 - f1: 0.9582\n",
      "Epoch 00105: loss improved from 0.11688 to 0.11003, saving model to Erick_dropout_0.2-105-0.958154.h5\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 0.1100 - accuracy: 0.9582 - recall: 0.9582 - f1: 0.9582\n",
      "Epoch 106/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1162 - accuracy: 0.9542 - recall: 0.9543 - f1: 0.9542\n",
      "Epoch 00106: loss did not improve from 0.11003\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1163 - accuracy: 0.9541 - recall: 0.9541 - f1: 0.9541\n",
      "Epoch 107/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1161 - accuracy: 0.9538 - recall: 0.9537 - f1: 0.9538\n",
      "Epoch 00107: loss did not improve from 0.11003\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1158 - accuracy: 0.9539 - recall: 0.9538 - f1: 0.9539\n",
      "Epoch 108/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1162 - accuracy: 0.9534 - recall: 0.9534 - f1: 0.9534\n",
      "Epoch 00108: loss did not improve from 0.11003\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1159 - accuracy: 0.9535 - recall: 0.9536 - f1: 0.9535\n",
      "Epoch 109/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1182 - accuracy: 0.9524 - recall: 0.9523 - f1: 0.9524\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "\n",
      "Epoch 00109: loss did not improve from 0.11003\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1180 - accuracy: 0.9524 - recall: 0.9523 - f1: 0.9524\n",
      "Epoch 110/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1157 - accuracy: 0.9542 - recall: 0.9542 - f1: 0.9542\n",
      "Epoch 00110: loss did not improve from 0.11003\n",
      "100/100 [==============================] - 31s 311ms/step - loss: 0.1153 - accuracy: 0.9544 - recall: 0.9544 - f1: 0.9544\n",
      "Epoch 111/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1139 - accuracy: 0.9552 - recall: 0.9551 - f1: 0.9552\n",
      "Epoch 00111: loss did not improve from 0.11003\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1138 - accuracy: 0.9552 - recall: 0.9551 - f1: 0.9552\n",
      "Epoch 112/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1126 - accuracy: 0.9556 - recall: 0.9555 - f1: 0.9556\n",
      "Epoch 00112: loss did not improve from 0.11003\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1127 - accuracy: 0.9555 - recall: 0.9555 - f1: 0.9555\n",
      "Epoch 113/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1147 - accuracy: 0.9547 - recall: 0.9546 - f1: 0.9547\n",
      "Epoch 00113: ReduceLROnPlateau reducing learning rate to 8.589935605414213e-05.\n",
      "\n",
      "Epoch 00113: loss did not improve from 0.11003\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 0.1144 - accuracy: 0.9549 - recall: 0.9548 - f1: 0.9548\n",
      "Epoch 114/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1106 - accuracy: 0.9570 - recall: 0.9570 - f1: 0.9570\n",
      "Epoch 00114: loss did not improve from 0.11003\n",
      "100/100 [==============================] - 31s 312ms/step - loss: 0.1110 - accuracy: 0.9568 - recall: 0.9568 - f1: 0.9568\n",
      "Epoch 115/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1146 - accuracy: 0.9555 - recall: 0.9556 - f1: 0.9555\n",
      "Epoch 00115: loss did not improve from 0.11003\n",
      "Restoring model weights from the end of the best epoch.\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1146 - accuracy: 0.9555 - recall: 0.9556 - f1: 0.9555\n",
      "Epoch 00115: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(history.history[\"loss\"][1:], label=\"train_loss\")\n",
    "# plt.plot(history.history[\"val_loss\"][1:], label=\"val_loss\")\n",
    "plt.plot(history.history[\"accuracy\"][1:], label=\"train_acc\")\n",
    "# plt.plot(history.history[\"val_accuracy\"][1:], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"plots/Erick_dropout_0.2.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 64)        4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               1048704   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,603,010\n",
      "Trainable params: 2,603,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 64)        4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1048704   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,603,010\n",
      "Trainable params: 2,603,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Processing image => data/test_set_images/test_1/test_1.png\n",
      "Processing image => data/test_set_images/test_2/test_2.png\n",
      "Processing image => data/test_set_images/test_3/test_3.png\n",
      "Processing image => data/test_set_images/test_4/test_4.png\n",
      "Processing image => data/test_set_images/test_5/test_5.png\n",
      "Processing image => data/test_set_images/test_6/test_6.png\n",
      "Processing image => data/test_set_images/test_7/test_7.png\n",
      "Processing image => data/test_set_images/test_8/test_8.png\n",
      "Processing image => data/test_set_images/test_9/test_9.png\n",
      "Processing image => data/test_set_images/test_10/test_10.png\n",
      "Processing image => data/test_set_images/test_11/test_11.png\n",
      "Processing image => data/test_set_images/test_12/test_12.png\n",
      "Processing image => data/test_set_images/test_13/test_13.png\n",
      "Processing image => data/test_set_images/test_14/test_14.png\n",
      "Processing image => data/test_set_images/test_15/test_15.png\n",
      "Processing image => data/test_set_images/test_16/test_16.png\n",
      "Processing image => data/test_set_images/test_17/test_17.png\n",
      "Processing image => data/test_set_images/test_18/test_18.png\n",
      "Processing image => data/test_set_images/test_19/test_19.png\n",
      "Processing image => data/test_set_images/test_20/test_20.png\n",
      "Processing image => data/test_set_images/test_21/test_21.png\n",
      "Processing image => data/test_set_images/test_22/test_22.png\n",
      "Processing image => data/test_set_images/test_23/test_23.png\n",
      "Processing image => data/test_set_images/test_24/test_24.png\n",
      "Processing image => data/test_set_images/test_25/test_25.png\n",
      "Processing image => data/test_set_images/test_26/test_26.png\n",
      "Processing image => data/test_set_images/test_27/test_27.png\n",
      "Processing image => data/test_set_images/test_28/test_28.png\n",
      "Processing image => data/test_set_images/test_29/test_29.png\n",
      "Processing image => data/test_set_images/test_30/test_30.png\n",
      "Processing image => data/test_set_images/test_31/test_31.png\n",
      "Processing image => data/test_set_images/test_32/test_32.png\n",
      "Processing image => data/test_set_images/test_33/test_33.png\n",
      "Processing image => data/test_set_images/test_34/test_34.png\n",
      "Processing image => data/test_set_images/test_35/test_35.png\n",
      "Processing image => data/test_set_images/test_36/test_36.png\n",
      "Processing image => data/test_set_images/test_37/test_37.png\n",
      "Processing image => data/test_set_images/test_38/test_38.png\n",
      "Processing image => data/test_set_images/test_39/test_39.png\n",
      "Processing image => data/test_set_images/test_40/test_40.png\n",
      "Processing image => data/test_set_images/test_41/test_41.png\n",
      "Processing image => data/test_set_images/test_42/test_42.png\n",
      "Processing image => data/test_set_images/test_43/test_43.png\n",
      "Processing image => data/test_set_images/test_44/test_44.png\n",
      "Processing image => data/test_set_images/test_45/test_45.png\n",
      "Processing image => data/test_set_images/test_46/test_46.png\n",
      "Processing image => data/test_set_images/test_47/test_47.png\n",
      "Processing image => data/test_set_images/test_48/test_48.png\n",
      "Processing image => data/test_set_images/test_49/test_49.png\n",
      "Processing image => data/test_set_images/test_50/test_50.png\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = CNN(shape=(WINDOW_SIZE, WINDOW_SIZE, 3))\n",
    "\n",
    "# Load the model\n",
    "model.load(\"Erick_dropout_0.2-105-0.958154.h5\")\n",
    "\n",
    "model.model.summary()\n",
    "\n",
    "# We add all test images to an array, used later for generating a submission\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "    image_filenames.append(image_filename)\n",
    "\n",
    "# Set-up submission filename\n",
    "submission_filename = \"Erick_dropout_0.2-105-0.958154.csv\"\n",
    "\n",
    "# Generates the submission\n",
    "generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
