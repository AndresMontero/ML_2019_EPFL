{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "Project2_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYJ1kbG8jWTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# %reload_ext lab_black\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK_PP-SIjZGi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fc6feb71-3d8b-4e18-f283-ecf4c1ac4047"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# %reload_ext lab_black\n",
        "%matplotlib inline\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqgcLk7XjY4r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f7372c8a-b81a-466c-a2e0-15f3111d1267"
      },
      "source": [
        "cd drive/My\\ Drive/ML_EPFL/\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'drive/My Drive/ML_EPFL/'\n",
            "/content/drive/My Drive/ML_EPFL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rTyGwU_jWTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "821fdfea-78ab-4c7e-e7f2-6745098b769e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import (\n",
        "    ModelCheckpoint,\n",
        "    LearningRateScheduler,\n",
        "    ReduceLROnPlateau,\n",
        "    EarlyStopping,\n",
        "    TensorBoard,\n",
        ")\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import GaussianNoise\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.compat.v2.keras.layers import BatchNormalization\n",
        "from tensorflow.python.client import device_lib\n",
        "from utils import *\n",
        "from models.cnn import *"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD2PPhq4jWTf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "2a5074db-6db7-48e6-fb76-f765df7b610b"
      },
      "source": [
        "np.random.seed(8)\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 17972120363416876124\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 15597721349114775915\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 13919056416948921352\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15956161332\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 2649883283409126915\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvep_3MqjWTi",
        "colab_type": "text"
      },
      "source": [
        "# Loading images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmriRjRFjWTj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8a6a504f-8941-4187-c158-39acf26e3a16"
      },
      "source": [
        "image_dir_train = \"data/training/images/\"\n",
        "files = os.listdir(image_dir_train)\n",
        "n_train = len(files)\n",
        "print(f\"Loading training images, images loaded: {n_train} \")\n",
        "imgs_train = np.asarray([load_img(image_dir_train + files[i]) for i in range(n_train)])\n",
        "gt_dir_train = \"data/training/groundtruth/\"\n",
        "print(f\"Loading groundtruth images, images loaded: {n_train} \")\n",
        "gt_imgs_train = np.asarray([load_img(gt_dir_train + files[i]) for i in range(n_train)])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading training images, images loaded: 100 \n",
            "Loading groundtruth images, images loaded: 100 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHE_dHkrjWTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, Y_train = imag_rotation_aug(imgs_train, gt_imgs_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cClDq5s9jWTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.asarray(X_train)\n",
        "Y_train = np.asarray(Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK_TYNgvjWTp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "974b4d5b-21ca-4ab9-baf1-50f18074e2c0"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "n_train = Y_train.shape[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(900, 448, 448, 3)\n",
            "(900, 448, 448)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9aULk_ljWTr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "75da7341-6ad4-45e0-e517-7a28b9b8fa87"
      },
      "source": [
        "# We define parameters of the model\n",
        "BATCH_SIZE = 1200\n",
        "WINDOW_SIZE = 64\n",
        "PATCH_SIZE = 16\n",
        "EPOCHS = 200\n",
        "STEPS_PER_EPOCH = 100\n",
        "WIDTH = 448\n",
        "model = CNN(\n",
        "    shape=(WINDOW_SIZE, WINDOW_SIZE, 3),\n",
        "    BATCH_SIZE=BATCH_SIZE,\n",
        "    WINDOW_SIZE=WINDOW_SIZE,\n",
        "    PATCH_SIZE=PATCH_SIZE,\n",
        "    EPOCHS=EPOCHS,\n",
        "    STEPS_PER_EPOCH=STEPS_PER_EPOCH,\n",
        "    WIDTH=WIDTH,\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 64, 64, 64)        4864      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 4, 1024)        4719616   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 1024)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 1024)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 2, 2, 1024)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               524416    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 258       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 6,798,338\n",
            "Trainable params: 6,798,338\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnLR_jF9jWTt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8194e6a6-9821-444a-91a2-a5e33c4441eb"
      },
      "source": [
        "history = model.train(X_train, Y_train, n_train)\n",
        "model.save(\"best_cnn.h5\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.5355 - acc: 0.7517 - recall: 0.7517 - f1: 0.7518\n",
            "Epoch 00001: loss improved from inf to 0.53499, saving model to saved_models/CNN_dropout_0.25_1024-001-0.751717.h5\n",
            "100/100 [==============================] - 41s 413ms/step - loss: 0.5350 - acc: 0.7517 - recall: 0.7517 - f1: 0.7517\n",
            "Epoch 2/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.5061 - acc: 0.7535 - recall: 0.7535 - f1: 0.7536\n",
            "Epoch 00002: loss improved from 0.53499 to 0.50543, saving model to saved_models/CNN_dropout_0.25_1024-002-0.753999.h5\n",
            "100/100 [==============================] - 35s 347ms/step - loss: 0.5054 - acc: 0.7539 - recall: 0.7539 - f1: 0.7540\n",
            "Epoch 3/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.4683 - acc: 0.7706 - recall: 0.7704 - f1: 0.7706\n",
            "Epoch 00003: loss improved from 0.50543 to 0.46892, saving model to saved_models/CNN_dropout_0.25_1024-003-0.770607.h5\n",
            "100/100 [==============================] - 35s 346ms/step - loss: 0.4689 - acc: 0.7706 - recall: 0.7704 - f1: 0.7706\n",
            "Epoch 4/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.4764 - acc: 0.7640 - recall: 0.7624 - f1: 0.7636\n",
            "Epoch 00004: loss did not improve from 0.46892\n",
            "100/100 [==============================] - 34s 342ms/step - loss: 0.4765 - acc: 0.7639 - recall: 0.7625 - f1: 0.7636\n",
            "Epoch 5/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.4180 - acc: 0.7985 - recall: 0.7993 - f1: 0.7986\n",
            "Epoch 00005: loss improved from 0.46892 to 0.41758, saving model to saved_models/CNN_dropout_0.25_1024-005-0.798894.h5\n",
            "100/100 [==============================] - 34s 343ms/step - loss: 0.4176 - acc: 0.7988 - recall: 0.7996 - f1: 0.7989\n",
            "Epoch 6/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.3413 - acc: 0.8442 - recall: 0.8461 - f1: 0.8445\n",
            "Epoch 00006: loss improved from 0.41758 to 0.34144, saving model to saved_models/CNN_dropout_0.25_1024-006-0.844481.h5\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.3414 - acc: 0.8442 - recall: 0.8460 - f1: 0.8445\n",
            "Epoch 7/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.2928 - acc: 0.8705 - recall: 0.8708 - f1: 0.8705\n",
            "Epoch 00007: loss improved from 0.34144 to 0.29267, saving model to saved_models/CNN_dropout_0.25_1024-007-0.870570.h5\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.2927 - acc: 0.8705 - recall: 0.8708 - f1: 0.8706\n",
            "Epoch 8/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.2492 - acc: 0.8917 - recall: 0.8917 - f1: 0.8917\n",
            "Epoch 00008: loss improved from 0.29267 to 0.24896, saving model to saved_models/CNN_dropout_0.25_1024-008-0.891833.h5\n",
            "100/100 [==============================] - 35s 349ms/step - loss: 0.2490 - acc: 0.8918 - recall: 0.8918 - f1: 0.8918\n",
            "Epoch 9/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9010 - recall: 0.9008 - f1: 0.9009\n",
            "Epoch 00009: loss improved from 0.24896 to 0.22958, saving model to saved_models/CNN_dropout_0.25_1024-009-0.900880.h5\n",
            "100/100 [==============================] - 35s 349ms/step - loss: 0.2296 - acc: 0.9009 - recall: 0.9007 - f1: 0.9009\n",
            "Epoch 10/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.8995 - recall: 0.8994 - f1: 0.8995\n",
            "Epoch 00010: loss did not improve from 0.22958\n",
            "100/100 [==============================] - 35s 345ms/step - loss: 0.2330 - acc: 0.8994 - recall: 0.8994 - f1: 0.8994\n",
            "Epoch 11/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9143 - recall: 0.9147 - f1: 0.9144\n",
            "Epoch 00011: loss improved from 0.22958 to 0.20255, saving model to saved_models/CNN_dropout_0.25_1024-011-0.914377.h5\n",
            "100/100 [==============================] - 35s 347ms/step - loss: 0.2025 - acc: 0.9143 - recall: 0.9147 - f1: 0.9144\n",
            "Epoch 12/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1916 - acc: 0.9195 - recall: 0.9197 - f1: 0.9195\n",
            "Epoch 00012: loss improved from 0.20255 to 0.19152, saving model to saved_models/CNN_dropout_0.25_1024-012-0.919508.h5\n",
            "100/100 [==============================] - 35s 348ms/step - loss: 0.1915 - acc: 0.9195 - recall: 0.9198 - f1: 0.9195\n",
            "Epoch 13/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1829 - acc: 0.9238 - recall: 0.9238 - f1: 0.9238\n",
            "Epoch 00013: loss improved from 0.19152 to 0.18281, saving model to saved_models/CNN_dropout_0.25_1024-013-0.923889.h5\n",
            "100/100 [==============================] - 35s 350ms/step - loss: 0.1828 - acc: 0.9239 - recall: 0.9239 - f1: 0.9239\n",
            "Epoch 14/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9259 - recall: 0.9260 - f1: 0.9259\n",
            "Epoch 00014: loss improved from 0.18281 to 0.17572, saving model to saved_models/CNN_dropout_0.25_1024-014-0.926047.h5\n",
            "100/100 [==============================] - 35s 351ms/step - loss: 0.1757 - acc: 0.9260 - recall: 0.9261 - f1: 0.9260\n",
            "Epoch 15/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1693 - acc: 0.9301 - recall: 0.9301 - f1: 0.9301\n",
            "Epoch 00015: loss improved from 0.17572 to 0.16922, saving model to saved_models/CNN_dropout_0.25_1024-015-0.930166.h5\n",
            "100/100 [==============================] - 35s 349ms/step - loss: 0.1692 - acc: 0.9302 - recall: 0.9301 - f1: 0.9302\n",
            "Epoch 16/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9307 - recall: 0.9307 - f1: 0.9307\n",
            "Epoch 00016: loss improved from 0.16922 to 0.16394, saving model to saved_models/CNN_dropout_0.25_1024-016-0.930828.h5\n",
            "100/100 [==============================] - 35s 349ms/step - loss: 0.1639 - acc: 0.9308 - recall: 0.9308 - f1: 0.9308\n",
            "Epoch 17/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1620 - acc: 0.9330 - recall: 0.9330 - f1: 0.9330\n",
            "Epoch 00017: loss improved from 0.16394 to 0.16196, saving model to saved_models/CNN_dropout_0.25_1024-017-0.933058.h5\n",
            "100/100 [==============================] - 35s 349ms/step - loss: 0.1620 - acc: 0.9331 - recall: 0.9331 - f1: 0.9331\n",
            "Epoch 18/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9375 - recall: 0.9375 - f1: 0.9375\n",
            "Epoch 00018: loss improved from 0.16196 to 0.15140, saving model to saved_models/CNN_dropout_0.25_1024-018-0.937494.h5\n",
            "100/100 [==============================] - 35s 349ms/step - loss: 0.1514 - acc: 0.9375 - recall: 0.9375 - f1: 0.9375\n",
            "Epoch 19/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9378 - recall: 0.9378 - f1: 0.9378\n",
            "Epoch 00019: loss improved from 0.15140 to 0.15107, saving model to saved_models/CNN_dropout_0.25_1024-019-0.937930.h5\n",
            "100/100 [==============================] - 35s 349ms/step - loss: 0.1511 - acc: 0.9379 - recall: 0.9379 - f1: 0.9379\n",
            "Epoch 20/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9381 - recall: 0.9381 - f1: 0.9381\n",
            "Epoch 00020: loss improved from 0.15107 to 0.15023, saving model to saved_models/CNN_dropout_0.25_1024-020-0.938116.h5\n",
            "100/100 [==============================] - 35s 349ms/step - loss: 0.1502 - acc: 0.9381 - recall: 0.9382 - f1: 0.9381\n",
            "Epoch 21/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9420 - recall: 0.9419 - f1: 0.9420\n",
            "Epoch 00021: loss improved from 0.15023 to 0.14080, saving model to saved_models/CNN_dropout_0.25_1024-021-0.942123.h5\n",
            "100/100 [==============================] - 35s 349ms/step - loss: 0.1408 - acc: 0.9421 - recall: 0.9420 - f1: 0.9421\n",
            "Epoch 22/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9434 - recall: 0.9434 - f1: 0.9434\n",
            "Epoch 00022: loss improved from 0.14080 to 0.13794, saving model to saved_models/CNN_dropout_0.25_1024-022-0.943391.h5\n",
            "100/100 [==============================] - 35s 345ms/step - loss: 0.1379 - acc: 0.9434 - recall: 0.9434 - f1: 0.9434\n",
            "Epoch 23/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9429 - recall: 0.9428 - f1: 0.9429\n",
            "Epoch 00023: loss did not improve from 0.13794\n",
            "100/100 [==============================] - 34s 342ms/step - loss: 0.1394 - acc: 0.9430 - recall: 0.9429 - f1: 0.9430\n",
            "Epoch 24/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9416 - recall: 0.9415 - f1: 0.9416\n",
            "Epoch 00024: loss did not improve from 0.13794\n",
            "100/100 [==============================] - 34s 342ms/step - loss: 0.1404 - acc: 0.9415 - recall: 0.9414 - f1: 0.9415\n",
            "Epoch 25/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9449 - recall: 0.9448 - f1: 0.9449\n",
            "Epoch 00025: loss improved from 0.13794 to 0.13433, saving model to saved_models/CNN_dropout_0.25_1024-025-0.944850.h5\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.1343 - acc: 0.9449 - recall: 0.9448 - f1: 0.9448\n",
            "Epoch 26/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1265 - acc: 0.9482 - recall: 0.9482 - f1: 0.9482\n",
            "Epoch 00026: loss improved from 0.13433 to 0.12648, saving model to saved_models/CNN_dropout_0.25_1024-026-0.948202.h5\n",
            "100/100 [==============================] - 34s 345ms/step - loss: 0.1265 - acc: 0.9482 - recall: 0.9482 - f1: 0.9482\n",
            "Epoch 27/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9495 - recall: 0.9494 - f1: 0.9495\n",
            "Epoch 00027: loss improved from 0.12648 to 0.12460, saving model to saved_models/CNN_dropout_0.25_1024-027-0.949367.h5\n",
            "100/100 [==============================] - 34s 345ms/step - loss: 0.1246 - acc: 0.9494 - recall: 0.9493 - f1: 0.9494\n",
            "Epoch 28/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9470 - recall: 0.9471 - f1: 0.9470\n",
            "Epoch 00028: loss did not improve from 0.12460\n",
            "100/100 [==============================] - 34s 342ms/step - loss: 0.1289 - acc: 0.9470 - recall: 0.9470 - f1: 0.9470\n",
            "Epoch 29/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9484 - recall: 0.9484 - f1: 0.9484\n",
            "Epoch 00029: loss did not improve from 0.12460\n",
            "100/100 [==============================] - 34s 340ms/step - loss: 0.1261 - acc: 0.9484 - recall: 0.9484 - f1: 0.9484\n",
            "Epoch 30/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9488 - recall: 0.9489 - f1: 0.9488\n",
            "Epoch 00030: loss did not improve from 0.12460\n",
            "100/100 [==============================] - 34s 342ms/step - loss: 0.1250 - acc: 0.9489 - recall: 0.9490 - f1: 0.9489\n",
            "Epoch 31/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.9511 - recall: 0.9511 - f1: 0.9511\n",
            "Epoch 00031: loss improved from 0.12460 to 0.12002, saving model to saved_models/CNN_dropout_0.25_1024-031-0.951132.h5\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.1200 - acc: 0.9511 - recall: 0.9511 - f1: 0.9511\n",
            "Epoch 32/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9532 - recall: 0.9531 - f1: 0.9532\n",
            "Epoch 00032: loss improved from 0.12002 to 0.11525, saving model to saved_models/CNN_dropout_0.25_1024-032-0.953202.h5\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.1153 - acc: 0.9532 - recall: 0.9532 - f1: 0.9532\n",
            "Epoch 33/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9522 - recall: 0.9521 - f1: 0.9522\n",
            "Epoch 00033: loss did not improve from 0.11525\n",
            "100/100 [==============================] - 34s 342ms/step - loss: 0.1170 - acc: 0.9521 - recall: 0.9520 - f1: 0.9521\n",
            "Epoch 34/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9520 - recall: 0.9520 - f1: 0.9520\n",
            "Epoch 00034: loss did not improve from 0.11525\n",
            "100/100 [==============================] - 34s 341ms/step - loss: 0.1178 - acc: 0.9520 - recall: 0.9519 - f1: 0.9520\n",
            "Epoch 35/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9531 - recall: 0.9531 - f1: 0.9531\n",
            "Epoch 00035: loss did not improve from 0.11525\n",
            "100/100 [==============================] - 34s 341ms/step - loss: 0.1166 - acc: 0.9531 - recall: 0.9531 - f1: 0.9531\n",
            "Epoch 36/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9549 - recall: 0.9548 - f1: 0.9549\n",
            "Epoch 00036: loss improved from 0.11525 to 0.11182, saving model to saved_models/CNN_dropout_0.25_1024-036-0.954933.h5\n",
            "100/100 [==============================] - 34s 343ms/step - loss: 0.1118 - acc: 0.9549 - recall: 0.9549 - f1: 0.9549\n",
            "Epoch 37/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9533 - recall: 0.9532 - f1: 0.9533\n",
            "Epoch 00037: loss did not improve from 0.11182\n",
            "100/100 [==============================] - 34s 342ms/step - loss: 0.1139 - acc: 0.9533 - recall: 0.9532 - f1: 0.9533\n",
            "Epoch 38/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9564 - recall: 0.9564 - f1: 0.9564\n",
            "Epoch 00038: loss improved from 0.11182 to 0.10757, saving model to saved_models/CNN_dropout_0.25_1024-038-0.956362.h5\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.1076 - acc: 0.9564 - recall: 0.9564 - f1: 0.9564\n",
            "Epoch 39/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9548 - recall: 0.9549 - f1: 0.9548\n",
            "Epoch 00039: loss did not improve from 0.10757\n",
            "100/100 [==============================] - 34s 342ms/step - loss: 0.1128 - acc: 0.9547 - recall: 0.9548 - f1: 0.9547\n",
            "Epoch 40/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9559 - recall: 0.9559 - f1: 0.9559\n",
            "Epoch 00040: loss did not improve from 0.10757\n",
            "100/100 [==============================] - 34s 341ms/step - loss: 0.1097 - acc: 0.9560 - recall: 0.9560 - f1: 0.9560\n",
            "Epoch 41/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1081 - acc: 0.9566 - recall: 0.9566 - f1: 0.9566\n",
            "Epoch 00041: loss did not improve from 0.10757\n",
            "100/100 [==============================] - 34s 341ms/step - loss: 0.1083 - acc: 0.9566 - recall: 0.9566 - f1: 0.9566\n",
            "Epoch 42/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9558 - recall: 0.9558 - f1: 0.9558\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.10757\n",
            "100/100 [==============================] - 34s 343ms/step - loss: 0.1107 - acc: 0.9558 - recall: 0.9559 - f1: 0.9558\n",
            "Epoch 43/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9612 - recall: 0.9612 - f1: 0.9612\n",
            "Epoch 00043: loss improved from 0.10757 to 0.09635, saving model to saved_models/CNN_dropout_0.25_1024-043-0.961186.h5\n",
            "100/100 [==============================] - 34s 343ms/step - loss: 0.0963 - acc: 0.9612 - recall: 0.9612 - f1: 0.9612\n",
            "Epoch 44/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9618 - recall: 0.9617 - f1: 0.9618\n",
            "Epoch 00044: loss improved from 0.09635 to 0.09454, saving model to saved_models/CNN_dropout_0.25_1024-044-0.961885.h5\n",
            "100/100 [==============================] - 34s 345ms/step - loss: 0.0945 - acc: 0.9619 - recall: 0.9618 - f1: 0.9619\n",
            "Epoch 45/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9620 - recall: 0.9619 - f1: 0.9620\n",
            "Epoch 00045: loss improved from 0.09454 to 0.09356, saving model to saved_models/CNN_dropout_0.25_1024-045-0.961893.h5\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0936 - acc: 0.9619 - recall: 0.9618 - f1: 0.9619\n",
            "Epoch 46/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9635 - recall: 0.9636 - f1: 0.9635\n",
            "Epoch 00046: loss improved from 0.09356 to 0.09086, saving model to saved_models/CNN_dropout_0.25_1024-046-0.963436.h5\n",
            "100/100 [==============================] - 35s 345ms/step - loss: 0.0909 - acc: 0.9634 - recall: 0.9635 - f1: 0.9634\n",
            "Epoch 47/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9628 - recall: 0.9628 - f1: 0.9628\n",
            "Epoch 00047: loss did not improve from 0.09086\n",
            "100/100 [==============================] - 34s 342ms/step - loss: 0.0920 - acc: 0.9627 - recall: 0.9627 - f1: 0.9627\n",
            "Epoch 48/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9641 - recall: 0.9640 - f1: 0.9641\n",
            "Epoch 00048: loss improved from 0.09086 to 0.08881, saving model to saved_models/CNN_dropout_0.25_1024-048-0.964101.h5\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0888 - acc: 0.9641 - recall: 0.9640 - f1: 0.9641\n",
            "Epoch 49/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9631 - recall: 0.9631 - f1: 0.9631\n",
            "Epoch 00049: loss did not improve from 0.08881\n",
            "100/100 [==============================] - 34s 342ms/step - loss: 0.0914 - acc: 0.9632 - recall: 0.9632 - f1: 0.9632\n",
            "Epoch 50/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9635 - recall: 0.9635 - f1: 0.9635\n",
            "Epoch 00050: loss did not improve from 0.08881\n",
            "100/100 [==============================] - 34s 341ms/step - loss: 0.0893 - acc: 0.9635 - recall: 0.9635 - f1: 0.9635\n",
            "Epoch 51/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9651 - recall: 0.9652 - f1: 0.9651\n",
            "Epoch 00051: loss improved from 0.08881 to 0.08640, saving model to saved_models/CNN_dropout_0.25_1024-051-0.965115.h5\n",
            "100/100 [==============================] - 35s 346ms/step - loss: 0.0864 - acc: 0.9651 - recall: 0.9652 - f1: 0.9651\n",
            "Epoch 52/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9644 - recall: 0.9644 - f1: 0.9644\n",
            "Epoch 00052: loss did not improve from 0.08640\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0875 - acc: 0.9644 - recall: 0.9644 - f1: 0.9644\n",
            "Epoch 53/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9633 - recall: 0.9634 - f1: 0.9633\n",
            "Epoch 00053: loss did not improve from 0.08640\n",
            "100/100 [==============================] - 34s 340ms/step - loss: 0.0908 - acc: 0.9634 - recall: 0.9634 - f1: 0.9634\n",
            "Epoch 54/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9645 - recall: 0.9644 - f1: 0.9645\n",
            "Epoch 00054: loss did not improve from 0.08640\n",
            "100/100 [==============================] - 35s 345ms/step - loss: 0.0892 - acc: 0.9646 - recall: 0.9646 - f1: 0.9646\n",
            "Epoch 55/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9647 - recall: 0.9647 - f1: 0.9647\n",
            "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.08640\n",
            "100/100 [==============================] - 34s 345ms/step - loss: 0.0872 - acc: 0.9647 - recall: 0.9647 - f1: 0.9647\n",
            "Epoch 56/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9674 - recall: 0.9674 - f1: 0.9674\n",
            "Epoch 00056: loss improved from 0.08640 to 0.08095, saving model to saved_models/CNN_dropout_0.25_1024-056-0.967316.h5\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0810 - acc: 0.9673 - recall: 0.9673 - f1: 0.9673\n",
            "Epoch 57/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9679 - recall: 0.9679 - f1: 0.9679\n",
            "Epoch 00057: loss improved from 0.08095 to 0.07968, saving model to saved_models/CNN_dropout_0.25_1024-057-0.967963.h5\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0797 - acc: 0.9680 - recall: 0.9680 - f1: 0.9680\n",
            "Epoch 58/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9681 - recall: 0.9682 - f1: 0.9681\n",
            "Epoch 00058: loss improved from 0.07968 to 0.07921, saving model to saved_models/CNN_dropout_0.25_1024-058-0.968097.h5\n",
            "100/100 [==============================] - 34s 345ms/step - loss: 0.0792 - acc: 0.9681 - recall: 0.9681 - f1: 0.9681\n",
            "Epoch 59/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9680 - recall: 0.9680 - f1: 0.9680\n",
            "Epoch 00059: loss did not improve from 0.07921\n",
            "100/100 [==============================] - 35s 346ms/step - loss: 0.0799 - acc: 0.9679 - recall: 0.9679 - f1: 0.9679\n",
            "Epoch 60/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9689 - recall: 0.9689 - f1: 0.9689\n",
            "Epoch 00060: loss improved from 0.07921 to 0.07772, saving model to saved_models/CNN_dropout_0.25_1024-060-0.968817.h5\n",
            "100/100 [==============================] - 35s 346ms/step - loss: 0.0777 - acc: 0.9688 - recall: 0.9688 - f1: 0.9688\n",
            "Epoch 61/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9686 - recall: 0.9686 - f1: 0.9686\n",
            "Epoch 00061: loss did not improve from 0.07772\n",
            "100/100 [==============================] - 35s 345ms/step - loss: 0.0781 - acc: 0.9685 - recall: 0.9685 - f1: 0.9685\n",
            "Epoch 62/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9691 - recall: 0.9691 - f1: 0.9691\n",
            "Epoch 00062: loss improved from 0.07772 to 0.07760, saving model to saved_models/CNN_dropout_0.25_1024-062-0.969050.h5\n",
            "100/100 [==============================] - 35s 346ms/step - loss: 0.0776 - acc: 0.9690 - recall: 0.9691 - f1: 0.9691\n",
            "Epoch 63/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9697 - recall: 0.9697 - f1: 0.9697\n",
            "Epoch 00063: loss improved from 0.07760 to 0.07531, saving model to saved_models/CNN_dropout_0.25_1024-063-0.969742.h5\n",
            "100/100 [==============================] - 35s 349ms/step - loss: 0.0753 - acc: 0.9697 - recall: 0.9698 - f1: 0.9697\n",
            "Epoch 64/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9678 - recall: 0.9678 - f1: 0.9678\n",
            "Epoch 00064: loss did not improve from 0.07531\n",
            "100/100 [==============================] - 34s 345ms/step - loss: 0.0782 - acc: 0.9678 - recall: 0.9678 - f1: 0.9678\n",
            "Epoch 65/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9698 - recall: 0.9698 - f1: 0.9698\n",
            "Epoch 00065: loss did not improve from 0.07531\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0771 - acc: 0.9696 - recall: 0.9697 - f1: 0.9696\n",
            "Epoch 66/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9695 - recall: 0.9695 - f1: 0.9695\n",
            "Epoch 00066: loss did not improve from 0.07531\n",
            "100/100 [==============================] - 34s 345ms/step - loss: 0.0758 - acc: 0.9696 - recall: 0.9696 - f1: 0.9696\n",
            "Epoch 67/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9694 - recall: 0.9694 - f1: 0.9694\n",
            "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.07531\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0760 - acc: 0.9694 - recall: 0.9694 - f1: 0.9694\n",
            "Epoch 68/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9708 - recall: 0.9708 - f1: 0.9708\n",
            "Epoch 00068: loss improved from 0.07531 to 0.07241, saving model to saved_models/CNN_dropout_0.25_1024-068-0.970800.h5\n",
            "100/100 [==============================] - 35s 346ms/step - loss: 0.0724 - acc: 0.9708 - recall: 0.9708 - f1: 0.9708\n",
            "Epoch 69/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9703 - recall: 0.9703 - f1: 0.9703\n",
            "Epoch 00069: loss did not improve from 0.07241\n",
            "100/100 [==============================] - 34s 345ms/step - loss: 0.0743 - acc: 0.9704 - recall: 0.9704 - f1: 0.9704\n",
            "Epoch 70/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9722 - recall: 0.9722 - f1: 0.9722\n",
            "Epoch 00070: loss improved from 0.07241 to 0.06970, saving model to saved_models/CNN_dropout_0.25_1024-070-0.972279.h5\n",
            "100/100 [==============================] - 35s 347ms/step - loss: 0.0697 - acc: 0.9723 - recall: 0.9723 - f1: 0.9723\n",
            "Epoch 71/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9712 - recall: 0.9712 - f1: 0.9712\n",
            "Epoch 00071: loss did not improve from 0.06970\n",
            "100/100 [==============================] - 35s 345ms/step - loss: 0.0720 - acc: 0.9713 - recall: 0.9713 - f1: 0.9713\n",
            "Epoch 72/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9712 - recall: 0.9712 - f1: 0.9712\n",
            "Epoch 00072: loss did not improve from 0.06970\n",
            "100/100 [==============================] - 34s 345ms/step - loss: 0.0715 - acc: 0.9713 - recall: 0.9713 - f1: 0.9713\n",
            "Epoch 73/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9721 - recall: 0.9721 - f1: 0.9721\n",
            "Epoch 00073: loss did not improve from 0.06970\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0702 - acc: 0.9721 - recall: 0.9721 - f1: 0.9721\n",
            "Epoch 74/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9707 - recall: 0.9707 - f1: 0.9707\n",
            "Epoch 00074: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.06970\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0712 - acc: 0.9707 - recall: 0.9707 - f1: 0.9707\n",
            "Epoch 75/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9709 - recall: 0.9709 - f1: 0.9709\n",
            "Epoch 00075: loss did not improve from 0.06970\n",
            "100/100 [==============================] - 34s 343ms/step - loss: 0.0706 - acc: 0.9709 - recall: 0.9709 - f1: 0.9709\n",
            "Epoch 76/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9720 - recall: 0.9720 - f1: 0.9720\n",
            "Epoch 00076: loss improved from 0.06970 to 0.06899, saving model to saved_models/CNN_dropout_0.25_1024-076-0.972137.h5\n",
            "100/100 [==============================] - 35s 348ms/step - loss: 0.0690 - acc: 0.9721 - recall: 0.9721 - f1: 0.9721\n",
            "Epoch 77/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9725 - recall: 0.9725 - f1: 0.9725\n",
            "Epoch 00077: loss improved from 0.06899 to 0.06876, saving model to saved_models/CNN_dropout_0.25_1024-077-0.972483.h5\n",
            "100/100 [==============================] - 35s 347ms/step - loss: 0.0688 - acc: 0.9725 - recall: 0.9725 - f1: 0.9725\n",
            "Epoch 78/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9719 - recall: 0.9719 - f1: 0.9719\n",
            "Epoch 00078: loss did not improve from 0.06876\n",
            "100/100 [==============================] - 34s 345ms/step - loss: 0.0689 - acc: 0.9719 - recall: 0.9719 - f1: 0.9719\n",
            "Epoch 79/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9730 - recall: 0.9730 - f1: 0.9730\n",
            "Epoch 00079: loss improved from 0.06876 to 0.06801, saving model to saved_models/CNN_dropout_0.25_1024-079-0.972979.h5\n",
            "100/100 [==============================] - 35s 347ms/step - loss: 0.0680 - acc: 0.9730 - recall: 0.9730 - f1: 0.9730\n",
            "Epoch 80/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9718 - recall: 0.9718 - f1: 0.9718\n",
            "Epoch 00080: loss did not improve from 0.06801\n",
            "100/100 [==============================] - 34s 345ms/step - loss: 0.0688 - acc: 0.9718 - recall: 0.9718 - f1: 0.9718\n",
            "Epoch 81/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9729 - recall: 0.9729 - f1: 0.9729\n",
            "Epoch 00081: loss improved from 0.06801 to 0.06682, saving model to saved_models/CNN_dropout_0.25_1024-081-0.972888.h5\n",
            "100/100 [==============================] - 35s 346ms/step - loss: 0.0668 - acc: 0.9729 - recall: 0.9729 - f1: 0.9729\n",
            "Epoch 82/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9731 - recall: 0.9731 - f1: 0.9731\n",
            "Epoch 00082: loss did not improve from 0.06682\n",
            "100/100 [==============================] - 35s 346ms/step - loss: 0.0675 - acc: 0.9731 - recall: 0.9731 - f1: 0.9731\n",
            "Epoch 83/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9725 - recall: 0.9725 - f1: 0.9725\n",
            "Epoch 00083: loss did not improve from 0.06682\n",
            "100/100 [==============================] - 34s 345ms/step - loss: 0.0685 - acc: 0.9725 - recall: 0.9726 - f1: 0.9725\n",
            "Epoch 84/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9724 - recall: 0.9724 - f1: 0.9724\n",
            "Epoch 00084: loss did not improve from 0.06682\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0673 - acc: 0.9724 - recall: 0.9724 - f1: 0.9724\n",
            "Epoch 85/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9728 - recall: 0.9728 - f1: 0.9728\n",
            "Epoch 00085: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.06682\n",
            "100/100 [==============================] - 34s 343ms/step - loss: 0.0674 - acc: 0.9728 - recall: 0.9729 - f1: 0.9728\n",
            "Epoch 86/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9731 - recall: 0.9731 - f1: 0.9731\n",
            "Epoch 00086: loss did not improve from 0.06682\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0671 - acc: 0.9732 - recall: 0.9732 - f1: 0.9732\n",
            "Epoch 87/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9724 - recall: 0.9724 - f1: 0.9724\n",
            "Epoch 00087: loss improved from 0.06682 to 0.06674, saving model to saved_models/CNN_dropout_0.25_1024-087-0.972479.h5\n",
            "100/100 [==============================] - 35s 346ms/step - loss: 0.0667 - acc: 0.9725 - recall: 0.9725 - f1: 0.9725\n",
            "Epoch 88/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9730 - recall: 0.9730 - f1: 0.9730\n",
            "Epoch 00088: loss did not improve from 0.06674\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0679 - acc: 0.9730 - recall: 0.9730 - f1: 0.9730\n",
            "Epoch 89/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9740 - recall: 0.9740 - f1: 0.9740\n",
            "Epoch 00089: loss improved from 0.06674 to 0.06493, saving model to saved_models/CNN_dropout_0.25_1024-089-0.973996.h5\n",
            "100/100 [==============================] - 35s 347ms/step - loss: 0.0649 - acc: 0.9740 - recall: 0.9740 - f1: 0.9740\n",
            "Epoch 90/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9727 - recall: 0.9727 - f1: 0.9727\n",
            "Epoch 00090: loss did not improve from 0.06493\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0669 - acc: 0.9726 - recall: 0.9726 - f1: 0.9726\n",
            "Epoch 91/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9730 - recall: 0.9730 - f1: 0.9730\n",
            "Epoch 00091: loss did not improve from 0.06493\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0673 - acc: 0.9730 - recall: 0.9730 - f1: 0.9730\n",
            "Epoch 92/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9734 - recall: 0.9734 - f1: 0.9734\n",
            "Epoch 00092: loss did not improve from 0.06493\n",
            "100/100 [==============================] - 34s 342ms/step - loss: 0.0657 - acc: 0.9734 - recall: 0.9734 - f1: 0.9734\n",
            "Epoch 93/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9727 - recall: 0.9727 - f1: 0.9727\n",
            "Epoch 00093: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.06493\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0689 - acc: 0.9727 - recall: 0.9727 - f1: 0.9727\n",
            "Epoch 94/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9734 - recall: 0.9733 - f1: 0.9734\n",
            "Epoch 00094: loss improved from 0.06493 to 0.06486, saving model to saved_models/CNN_dropout_0.25_1024-094-0.973424.h5\n",
            "100/100 [==============================] - 35s 346ms/step - loss: 0.0649 - acc: 0.9734 - recall: 0.9734 - f1: 0.9734\n",
            "Epoch 95/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9734 - recall: 0.9734 - f1: 0.9734\n",
            "Epoch 00095: loss did not improve from 0.06486\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0655 - acc: 0.9735 - recall: 0.9735 - f1: 0.9735\n",
            "Epoch 96/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9736 - recall: 0.9736 - f1: 0.9736\n",
            "Epoch 00096: loss did not improve from 0.06486\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0661 - acc: 0.9736 - recall: 0.9736 - f1: 0.9736\n",
            "Epoch 97/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9729 - recall: 0.9729 - f1: 0.9729\n",
            "Epoch 00097: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.06486\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0663 - acc: 0.9730 - recall: 0.9730 - f1: 0.9730\n",
            "Epoch 98/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9738 - recall: 0.9738 - f1: 0.9738\n",
            "Epoch 00098: loss did not improve from 0.06486\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0652 - acc: 0.9738 - recall: 0.9738 - f1: 0.9738\n",
            "Epoch 99/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9731 - recall: 0.9730 - f1: 0.9731\n",
            "Epoch 00099: loss did not improve from 0.06486\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0660 - acc: 0.9730 - recall: 0.9730 - f1: 0.9730\n",
            "Epoch 100/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9737 - recall: 0.9737 - f1: 0.9737\n",
            "Epoch 00100: loss improved from 0.06486 to 0.06404, saving model to saved_models/CNN_dropout_0.25_1024-100-0.973804.h5\n",
            "100/100 [==============================] - 35s 347ms/step - loss: 0.0640 - acc: 0.9738 - recall: 0.9738 - f1: 0.9738\n",
            "Epoch 101/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9740 - recall: 0.9740 - f1: 0.9740\n",
            "Epoch 00101: loss did not improve from 0.06404\n",
            "100/100 [==============================] - 34s 345ms/step - loss: 0.0651 - acc: 0.9740 - recall: 0.9740 - f1: 0.9740\n",
            "Epoch 102/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9736 - recall: 0.9736 - f1: 0.9736\n",
            "Epoch 00102: loss did not improve from 0.06404\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0665 - acc: 0.9737 - recall: 0.9737 - f1: 0.9737\n",
            "Epoch 103/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9732 - recall: 0.9732 - f1: 0.9732\n",
            "Epoch 00103: loss did not improve from 0.06404\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0661 - acc: 0.9732 - recall: 0.9732 - f1: 0.9732\n",
            "Epoch 104/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9734 - recall: 0.9734 - f1: 0.9734\n",
            "Epoch 00104: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "\n",
            "Epoch 00104: loss did not improve from 0.06404\n",
            "100/100 [==============================] - 35s 346ms/step - loss: 0.0649 - acc: 0.9734 - recall: 0.9734 - f1: 0.9734\n",
            "Epoch 105/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9734 - recall: 0.9734 - f1: 0.9734\n",
            "Epoch 00105: loss did not improve from 0.06404\n",
            "100/100 [==============================] - 35s 348ms/step - loss: 0.0644 - acc: 0.9734 - recall: 0.9734 - f1: 0.9734\n",
            "Epoch 106/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9737 - recall: 0.9737 - f1: 0.9737\n",
            "Epoch 00106: loss improved from 0.06404 to 0.06396, saving model to saved_models/CNN_dropout_0.25_1024-106-0.973762.h5\n",
            "100/100 [==============================] - 35s 350ms/step - loss: 0.0640 - acc: 0.9738 - recall: 0.9738 - f1: 0.9738\n",
            "Epoch 107/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9737 - recall: 0.9737 - f1: 0.9737\n",
            "Epoch 00107: loss did not improve from 0.06396\n",
            "100/100 [==============================] - 35s 348ms/step - loss: 0.0651 - acc: 0.9736 - recall: 0.9736 - f1: 0.9736\n",
            "Epoch 108/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9734 - recall: 0.9734 - f1: 0.9734\n",
            "Epoch 00108: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "\n",
            "Epoch 00108: loss did not improve from 0.06396\n",
            "100/100 [==============================] - 35s 346ms/step - loss: 0.0654 - acc: 0.9734 - recall: 0.9734 - f1: 0.9734\n",
            "Epoch 109/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9730 - recall: 0.9730 - f1: 0.9730\n",
            "Epoch 00109: loss did not improve from 0.06396\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0668 - acc: 0.9729 - recall: 0.9730 - f1: 0.9729\n",
            "Epoch 110/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9737 - recall: 0.9737 - f1: 0.9737\n",
            "Epoch 00110: loss did not improve from 0.06396\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0654 - acc: 0.9737 - recall: 0.9737 - f1: 0.9737\n",
            "Epoch 111/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9737 - recall: 0.9737 - f1: 0.9737\n",
            "Epoch 00111: loss did not improve from 0.06396\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0654 - acc: 0.9737 - recall: 0.9737 - f1: 0.9737\n",
            "Epoch 112/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9741 - recall: 0.9741 - f1: 0.9741\n",
            "Epoch 00112: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "\n",
            "Epoch 00112: loss did not improve from 0.06396\n",
            "100/100 [==============================] - 34s 343ms/step - loss: 0.0643 - acc: 0.9741 - recall: 0.9741 - f1: 0.9741\n",
            "Epoch 113/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9726 - recall: 0.9726 - f1: 0.9726\n",
            "Epoch 00113: loss did not improve from 0.06396\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0676 - acc: 0.9726 - recall: 0.9726 - f1: 0.9726\n",
            "Epoch 114/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9746 - recall: 0.9746 - f1: 0.9746\n",
            "Epoch 00114: loss improved from 0.06396 to 0.06333, saving model to saved_models/CNN_dropout_0.25_1024-114-0.974663.h5\n",
            "100/100 [==============================] - 35s 348ms/step - loss: 0.0633 - acc: 0.9747 - recall: 0.9747 - f1: 0.9747\n",
            "Epoch 115/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9728 - recall: 0.9729 - f1: 0.9728\n",
            "Epoch 00115: loss did not improve from 0.06333\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0664 - acc: 0.9728 - recall: 0.9728 - f1: 0.9728\n",
            "Epoch 116/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9736 - recall: 0.9736 - f1: 0.9736\n",
            "Epoch 00116: loss did not improve from 0.06333\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0655 - acc: 0.9736 - recall: 0.9736 - f1: 0.9736\n",
            "Epoch 117/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9733 - recall: 0.9733 - f1: 0.9733\n",
            "Epoch 00117: loss did not improve from 0.06333\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0656 - acc: 0.9733 - recall: 0.9733 - f1: 0.9733\n",
            "Epoch 118/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9739 - recall: 0.9739 - f1: 0.9739\n",
            "Epoch 00118: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "\n",
            "Epoch 00118: loss did not improve from 0.06333\n",
            "100/100 [==============================] - 34s 343ms/step - loss: 0.0654 - acc: 0.9739 - recall: 0.9739 - f1: 0.9739\n",
            "Epoch 119/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9741 - recall: 0.9741 - f1: 0.9741\n",
            "Epoch 00119: loss did not improve from 0.06333\n",
            "100/100 [==============================] - 34s 343ms/step - loss: 0.0639 - acc: 0.9741 - recall: 0.9742 - f1: 0.9741\n",
            "Epoch 120/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9747 - recall: 0.9746 - f1: 0.9747\n",
            "Epoch 00120: loss did not improve from 0.06333\n",
            "100/100 [==============================] - 34s 343ms/step - loss: 0.0643 - acc: 0.9746 - recall: 0.9746 - f1: 0.9746\n",
            "Epoch 121/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9740 - recall: 0.9740 - f1: 0.9740\n",
            "Epoch 00121: loss did not improve from 0.06333\n",
            "100/100 [==============================] - 34s 345ms/step - loss: 0.0643 - acc: 0.9740 - recall: 0.9740 - f1: 0.9740\n",
            "Epoch 122/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9723 - recall: 0.9724 - f1: 0.9723\n",
            "Epoch 00122: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "\n",
            "Epoch 00122: loss did not improve from 0.06333\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0677 - acc: 0.9723 - recall: 0.9724 - f1: 0.9723\n",
            "Epoch 123/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9739 - recall: 0.9739 - f1: 0.9739\n",
            "Epoch 00123: loss did not improve from 0.06333\n",
            "100/100 [==============================] - 34s 345ms/step - loss: 0.0654 - acc: 0.9740 - recall: 0.9740 - f1: 0.9740\n",
            "Epoch 124/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9739 - recall: 0.9739 - f1: 0.9739\n",
            "Epoch 00124: loss did not improve from 0.06333\n",
            "Restoring model weights from the end of the best epoch.\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 0.0649 - acc: 0.9739 - recall: 0.9739 - f1: 0.9739\n",
            "Epoch 00124: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2Ap30SsjWTv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "b25163a9-bc90-40b1-cc00-a678b8ad1e49"
      },
      "source": [
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history.history[\"acc\"], label=\"train_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(\"CNN_dropout_0.25_1024.pdf\")\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3sAAAJhCAYAAAD496mqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXgyZ7RajWTx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d5af76bc-c67c-4a05-d751-0336d6e9d5cf"
      },
      "source": [
        "model = CNN(\n",
        "    shape=(WINDOW_SIZE, WINDOW_SIZE, 3),\n",
        "    BATCH_SIZE=BATCH_SIZE,\n",
        "    WINDOW_SIZE=WINDOW_SIZE,\n",
        "    PATCH_SIZE=PATCH_SIZE,\n",
        "    EPOCHS=EPOCHS,\n",
        "    STEPS_PER_EPOCH=STEPS_PER_EPOCH,\n",
        "    WIDTH=WIDTH,\n",
        ")\n",
        "model.load(\"best_cnn.h5\")\n",
        "image_filenames = []\n",
        "for i in range(1, 51):\n",
        "    image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
        "    image_filenames.append(image_filename)\n",
        "\n",
        "submission_filename = \"best_cnn.csv\"\n",
        "generate_submission(model, submission_filename, *image_filenames)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 64, 64, 64)        4864      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 4, 4, 1024)        4719616   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 4, 4, 1024)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 2, 2, 1024)        0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 2, 2, 1024)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               524416    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 258       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 6,798,338\n",
            "Trainable params: 6,798,338\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Generating predictions for image:  data/test_set_images/test_1/test_1.png\n",
            "Generating predictions for image:  data/test_set_images/test_2/test_2.png\n",
            "Generating predictions for image:  data/test_set_images/test_3/test_3.png\n",
            "Generating predictions for image:  data/test_set_images/test_4/test_4.png\n",
            "Generating predictions for image:  data/test_set_images/test_5/test_5.png\n",
            "Generating predictions for image:  data/test_set_images/test_6/test_6.png\n",
            "Generating predictions for image:  data/test_set_images/test_7/test_7.png\n",
            "Generating predictions for image:  data/test_set_images/test_8/test_8.png\n",
            "Generating predictions for image:  data/test_set_images/test_9/test_9.png\n",
            "Generating predictions for image:  data/test_set_images/test_10/test_10.png\n",
            "Generating predictions for image:  data/test_set_images/test_11/test_11.png\n",
            "Generating predictions for image:  data/test_set_images/test_12/test_12.png\n",
            "Generating predictions for image:  data/test_set_images/test_13/test_13.png\n",
            "Generating predictions for image:  data/test_set_images/test_14/test_14.png\n",
            "Generating predictions for image:  data/test_set_images/test_15/test_15.png\n",
            "Generating predictions for image:  data/test_set_images/test_16/test_16.png\n",
            "Generating predictions for image:  data/test_set_images/test_17/test_17.png\n",
            "Generating predictions for image:  data/test_set_images/test_18/test_18.png\n",
            "Generating predictions for image:  data/test_set_images/test_19/test_19.png\n",
            "Generating predictions for image:  data/test_set_images/test_20/test_20.png\n",
            "Generating predictions for image:  data/test_set_images/test_21/test_21.png\n",
            "Generating predictions for image:  data/test_set_images/test_22/test_22.png\n",
            "Generating predictions for image:  data/test_set_images/test_23/test_23.png\n",
            "Generating predictions for image:  data/test_set_images/test_24/test_24.png\n",
            "Generating predictions for image:  data/test_set_images/test_25/test_25.png\n",
            "Generating predictions for image:  data/test_set_images/test_26/test_26.png\n",
            "Generating predictions for image:  data/test_set_images/test_27/test_27.png\n",
            "Generating predictions for image:  data/test_set_images/test_28/test_28.png\n",
            "Generating predictions for image:  data/test_set_images/test_29/test_29.png\n",
            "Generating predictions for image:  data/test_set_images/test_30/test_30.png\n",
            "Generating predictions for image:  data/test_set_images/test_31/test_31.png\n",
            "Generating predictions for image:  data/test_set_images/test_32/test_32.png\n",
            "Generating predictions for image:  data/test_set_images/test_33/test_33.png\n",
            "Generating predictions for image:  data/test_set_images/test_34/test_34.png\n",
            "Generating predictions for image:  data/test_set_images/test_35/test_35.png\n",
            "Generating predictions for image:  data/test_set_images/test_36/test_36.png\n",
            "Generating predictions for image:  data/test_set_images/test_37/test_37.png\n",
            "Generating predictions for image:  data/test_set_images/test_38/test_38.png\n",
            "Generating predictions for image:  data/test_set_images/test_39/test_39.png\n",
            "Generating predictions for image:  data/test_set_images/test_40/test_40.png\n",
            "Generating predictions for image:  data/test_set_images/test_41/test_41.png\n",
            "Generating predictions for image:  data/test_set_images/test_42/test_42.png\n",
            "Generating predictions for image:  data/test_set_images/test_43/test_43.png\n",
            "Generating predictions for image:  data/test_set_images/test_44/test_44.png\n",
            "Generating predictions for image:  data/test_set_images/test_45/test_45.png\n",
            "Generating predictions for image:  data/test_set_images/test_46/test_46.png\n",
            "Generating predictions for image:  data/test_set_images/test_47/test_47.png\n",
            "Generating predictions for image:  data/test_set_images/test_48/test_48.png\n",
            "Generating predictions for image:  data/test_set_images/test_49/test_49.png\n",
            "Generating predictions for image:  data/test_set_images/test_50/test_50.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuG2kleEjWT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbM_-D0pjWT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}