{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5UtF5J4gPg3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "37577151-4dae-46f8-cf63-99d1bb23bba9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-Vr6E4PsBtV",
        "colab_type": "text"
      },
      "source": [
        "Here write the path where you cloned the repository + ML_2019_EPFL/projects/project2/project_road_segmentation/\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0fQ0i96gQJh",
        "colab_type": "code",
        "outputId": "07d496a7-c8ef-405c-e4e4-8866ceb364d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd drive/My\\ Drive/ML_2019_EPFL/projects/project2/project_road_segmentation/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ML_2019_EPFL/projects/project2/project_road_segmentation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCz8Br3eTDbb",
        "colab_type": "code",
        "outputId": "8b3cd0c8-9afa-4ec1-a4f7-d10d7bb62894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.callbacks import Callback\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import GaussianNoise\n",
        "from utils import *\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.compat.v2.keras.layers import BatchNormalization\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
        "from tensorflow.python.client import device_lib"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHW88-aqToBV",
        "colab_type": "code",
        "outputId": "dd8cdb24-f747-4d6e-f18e-9d22dc39fbf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "image_dir_train = \"data/training/images/\"\n",
        "files = os.listdir(image_dir_train)\n",
        "n_train = len(files)\n",
        "print(f\"Loading training images, images loaded: {n_train} \")\n",
        "imgs_train = np.asarray(\n",
        "    [load_image(image_dir_train + files[i]) for i in range(n_train)]\n",
        ")\n",
        "gt_dir_train = \"data/training/groundtruth/\"\n",
        "print(f\"Loading groundtruth images, images loaded: {n_train} \")\n",
        "gt_imgs_train = np.asarray(\n",
        "    [load_image(gt_dir_train + files[i]) for i in range(n_train)]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading training images, images loaded: 100 \n",
            "Loading groundtruth images, images loaded: 100 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "123TwbsVUDB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, Y_train = imag_rotation_aug(imgs_train, gt_imgs_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTUu1tAtUHL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.asarray(X_train)\n",
        "Y_train = np.asarray(Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77ZuF7I2ULFM",
        "colab_type": "code",
        "outputId": "918c9abe-0a1f-4c10-cdc0-a1d95a68626d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "n_train = Y_train.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(900, 456, 456, 3)\n",
            "(900, 456, 456)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxJC9fvmVqI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Compute the Precision for the batch.\n",
        "    Args:\n",
        "        y_true (numpy.ndarray): the ground truth labels\n",
        "        y_pred (numpy.ndarray): the predicted labels \n",
        "    Returns:\n",
        "        Precision (numpy.float64): the Precision of the batch \n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Compute the Recall for the batch.\n",
        "    Args:\n",
        "        y_true (numpy.ndarray): the ground truth labels\n",
        "        y_pred (numpy.ndarray): the predicted labels \n",
        "    Returns:\n",
        "       Recall (numpy.float64): the Recal of the batch \n",
        "    \"\"\"\n",
        "\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    \"\"\"Compute the F-1 for the batch.\n",
        "    Args:\n",
        "        y_true (numpy.ndarray): the ground truth labels\n",
        "        y_pred (numpy.ndarray): the predicted labels \n",
        "    Returns:\n",
        "       F-1 (numpy.float64): the F-1 of the batch \n",
        "    \"\"\"\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcojasQqTBkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN:\n",
        "    def __init__(self, shape):\n",
        "        self.shape = shape\n",
        "        self.model = self.initialize_U_NET(shape)\n",
        "\n",
        "    def initialize_U_NET(self, shape):\n",
        "        \"\"\"Create Network Architecture.\n",
        "        Args:\n",
        "            shape (triplet): Size of the input layer height x width x colors (64 x 64 x 3)\n",
        "        Returns:\n",
        "            model (Neural Network): Architecture of the model\n",
        "        \"\"\"\n",
        "        # INPUT\n",
        "        # shape     - Size of the input images\n",
        "        # OUTPUT\n",
        "        # model    - Compiled CNN\n",
        "\n",
        "        # Define hyperparamters\n",
        "        KERNEL3 = (3, 3)\n",
        "        KERNEL5 = (5, 5)\n",
        "\n",
        "        # Define a model\n",
        "        model = Sequential()\n",
        "\n",
        "        # Add the layers\n",
        "        # Selection of the model is described in the report\n",
        "        # We use padding = 'same' to avoid issues with the matrix sizes\n",
        "        model.add(Conv2D(64, KERNEL5, input_shape=shape, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.1))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        model.add(Conv2D(128, KERNEL3, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.1))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.1))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        model.add(Conv2D(512, KERNEL3, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.1))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        # Flatten it and use regularizers to avoid overfitting\n",
        "        # The parameters have been chosen empirically\n",
        "        model.add(Flatten())\n",
        "        model.add(\n",
        "            Dense(\n",
        "                128, kernel_regularizer=l2(0.000001), activity_regularizer=l2(0.000001)\n",
        "            )\n",
        "        )\n",
        "        # model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha=0.1))\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "        # Add output layer\n",
        "        model.add(\n",
        "            Dense(2, kernel_regularizer=l2(0.000001), activity_regularizer=l2(0.000001))\n",
        "        )\n",
        "        model.add(Activation(\"sigmoid\"))\n",
        "        model.compile(\n",
        "            loss=\"binary_crossentropy\",\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            metrics=[\"accuracy\", recall, f1],\n",
        "        )\n",
        "\n",
        "        # Print a summary of the model to see what has been generated\n",
        "        model.summary()\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train the Model.\n",
        "\n",
        "        Returns:\n",
        "            History (History_Keras): History of the training\n",
        "        \"\"\"\n",
        "        # Early stopping callback after 10 steps\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor=\"loss\", patience=10, verbose=1, restore_best_weights=True,\n",
        "        )\n",
        "        # Reduce learning rate on plateau after 5 steps\n",
        "        lr_callback = ReduceLROnPlateau(\n",
        "            monitor=\"loss\", factor=0.8, patience=4, verbose=1, cooldown=1,\n",
        "        )\n",
        "        save_best = ModelCheckpoint(\n",
        "            \"Erick_dropout_0.2-{epoch:03d}-{f1:03f}.h5\",\n",
        "            save_best_only=True,\n",
        "            monitor=\"loss\",\n",
        "            verbose=1,\n",
        "        )\n",
        "        callbacks = [lr_callback, save_best, early_stopping]\n",
        "\n",
        "        # Train the model using the previously defined functions and callbacks\n",
        "        history = self.model.fit_generator(\n",
        "            create_minibatch(\n",
        "                X_train, Y_train, n_train, WINDOW_SIZE, BATCH_SIZE, PATCH_SIZE, WIDTH\n",
        "            ),\n",
        "            steps_per_epoch=STEPS_PER_EPOCH,\n",
        "            epochs=EPOCHS,\n",
        "            use_multiprocessing=False,\n",
        "            workers=1,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1,\n",
        "            #             validation_data=create_minibatch(\n",
        "            #                 X_val, Y_val, n_val, WINDOW_SIZE, BATCH_SIZE, PATCH_SIZE, WIDTH\n",
        "            #             ),\n",
        "            #             validation_steps=STEPS_PER_EPOCH / 3,\n",
        "        )\n",
        "        return history\n",
        "\n",
        "    def classify(self, X):\n",
        "        \"\"\"Classify Image as either road or not.\n",
        "        Args:\n",
        "            X (image): part of the image to classify\n",
        "        Returns:\n",
        "            Predictions : Predictions for each patch\n",
        "        \"\"\"\n",
        "        # Subdivide the images into blocks with a stride and patch_size of 16\n",
        "        img_patches = create_patches(X, 16, 16, padding=24)\n",
        "\n",
        "        # Predict\n",
        "        predictions = self.model.predict(img_patches)\n",
        "        predictions = (predictions[:, 0] < predictions[:, 1]) * 1\n",
        "\n",
        "        # Regroup patches into images\n",
        "        return predictions.reshape(X.shape[0], -1)\n",
        "\n",
        "    #         return group_patches(predictions, X.shape[0])\n",
        "\n",
        "    def load(self, filename):\n",
        "        \"\"\"Loads Saved Model.\n",
        "        Args:\n",
        "           filename (string): name of the model\n",
        "           \n",
        "        \"\"\"\n",
        "        # Load the model (used for submission)\n",
        "        dependencies = {\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "        }\n",
        "        self.model = load_model(filename, custom_objects=dependencies)\n",
        "\n",
        "    def save(self, filename):\n",
        "        \"\"\"Saves trained model.\n",
        "        Args:\n",
        "           filename (string): name of the model\n",
        "           \n",
        "        \"\"\"\n",
        "        self.model.save(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnntdYHZU8lz",
        "colab_type": "code",
        "outputId": "ee53c0c3-3666-42a3-eab2-6b686530857b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "# We define parameters of the model\n",
        "BATCH_SIZE = 1200\n",
        "WINDOW_SIZE = 64\n",
        "PATCH_SIZE = 16\n",
        "EPOCHS = 100\n",
        "STEPS_PER_EPOCH = 200\n",
        "WIDTH = 448\n",
        "model = CNN(shape=(WINDOW_SIZE, WINDOW_SIZE, 3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4132d5d7be31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mSTEPS_PER_EPOCH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mWIDTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m448\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWINDOW_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWINDOW_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-83751e11298a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_U_NET\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize_U_NET\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-83751e11298a>\u001b[0m in \u001b[0;36minitialize_U_NET\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Define a model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Add the layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6m6g7T3VAee",
        "colab_type": "code",
        "outputId": "480310cc-b0fb-412b-cf3e-06dfb81a3cba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.5212 - acc: 0.7560 - recall: 0.7567 - f1: 0.7563\n",
            "Epoch 00001: loss improved from inf to 0.52085, saving model to Erick_dropout_0.2-001-0.756379.h5\n",
            "200/200 [==============================] - 69s 343ms/step - loss: 0.5208 - acc: 0.7560 - recall: 0.7567 - f1: 0.7564\n",
            "Epoch 2/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.4547 - acc: 0.7798 - recall: 0.7796 - f1: 0.7798\n",
            "Epoch 00002: loss improved from 0.52085 to 0.45442, saving model to Erick_dropout_0.2-002-0.779993.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.4544 - acc: 0.7800 - recall: 0.7798 - f1: 0.7800\n",
            "Epoch 3/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.3324 - acc: 0.8523 - recall: 0.8517 - f1: 0.8522\n",
            "Epoch 00003: loss improved from 0.45442 to 0.33228, saving model to Erick_dropout_0.2-003-0.852234.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.3323 - acc: 0.8523 - recall: 0.8517 - f1: 0.8522\n",
            "Epoch 4/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2778 - acc: 0.8799 - recall: 0.8798 - f1: 0.8799\n",
            "Epoch 00004: loss improved from 0.33228 to 0.27747, saving model to Erick_dropout_0.2-004-0.880019.h5\n",
            "200/200 [==============================] - 62s 309ms/step - loss: 0.2775 - acc: 0.8800 - recall: 0.8800 - f1: 0.8800\n",
            "Epoch 5/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9051 - recall: 0.9049 - f1: 0.9051\n",
            "Epoch 00005: loss improved from 0.27747 to 0.22478, saving model to Erick_dropout_0.2-005-0.905156.h5\n",
            "200/200 [==============================] - 62s 309ms/step - loss: 0.2248 - acc: 0.9052 - recall: 0.9049 - f1: 0.9052\n",
            "Epoch 6/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.9113 - recall: 0.9114 - f1: 0.9114\n",
            "Epoch 00006: loss improved from 0.22478 to 0.21050, saving model to Erick_dropout_0.2-006-0.911406.h5\n",
            "200/200 [==============================] - 62s 309ms/step - loss: 0.2105 - acc: 0.9114 - recall: 0.9114 - f1: 0.9114\n",
            "Epoch 7/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9211 - recall: 0.9213 - f1: 0.9212\n",
            "Epoch 00007: loss improved from 0.21050 to 0.18997, saving model to Erick_dropout_0.2-007-0.921162.h5\n",
            "200/200 [==============================] - 62s 308ms/step - loss: 0.1900 - acc: 0.9211 - recall: 0.9213 - f1: 0.9212\n",
            "Epoch 8/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9243 - recall: 0.9243 - f1: 0.9243\n",
            "Epoch 00008: loss improved from 0.18997 to 0.18358, saving model to Erick_dropout_0.2-008-0.924238.h5\n",
            "200/200 [==============================] - 62s 309ms/step - loss: 0.1836 - acc: 0.9242 - recall: 0.9242 - f1: 0.9242\n",
            "Epoch 9/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9297 - recall: 0.9295 - f1: 0.9297\n",
            "Epoch 00009: loss improved from 0.18358 to 0.17055, saving model to Erick_dropout_0.2-009-0.929701.h5\n",
            "200/200 [==============================] - 62s 309ms/step - loss: 0.1706 - acc: 0.9297 - recall: 0.9296 - f1: 0.9297\n",
            "Epoch 10/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9314 - recall: 0.9313 - f1: 0.9314\n",
            "Epoch 00010: loss improved from 0.17055 to 0.16740, saving model to Erick_dropout_0.2-010-0.931323.h5\n",
            "200/200 [==============================] - 62s 309ms/step - loss: 0.1674 - acc: 0.9313 - recall: 0.9312 - f1: 0.9313\n",
            "Epoch 11/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1587 - acc: 0.9357 - recall: 0.9357 - f1: 0.9357\n",
            "Epoch 00011: loss improved from 0.16740 to 0.15857, saving model to Erick_dropout_0.2-011-0.935798.h5\n",
            "200/200 [==============================] - 62s 309ms/step - loss: 0.1586 - acc: 0.9358 - recall: 0.9358 - f1: 0.9358\n",
            "Epoch 12/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9386 - recall: 0.9386 - f1: 0.9386\n",
            "Epoch 00012: loss improved from 0.15857 to 0.15028, saving model to Erick_dropout_0.2-012-0.938592.h5\n",
            "200/200 [==============================] - 62s 309ms/step - loss: 0.1503 - acc: 0.9386 - recall: 0.9386 - f1: 0.9386\n",
            "Epoch 13/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9400 - recall: 0.9401 - f1: 0.9400\n",
            "Epoch 00013: loss improved from 0.15028 to 0.14972, saving model to Erick_dropout_0.2-013-0.940012.h5\n",
            "200/200 [==============================] - 62s 309ms/step - loss: 0.1497 - acc: 0.9400 - recall: 0.9401 - f1: 0.9400\n",
            "Epoch 14/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9423 - recall: 0.9423 - f1: 0.9423\n",
            "Epoch 00014: loss improved from 0.14972 to 0.14353, saving model to Erick_dropout_0.2-014-0.942374.h5\n",
            "200/200 [==============================] - 62s 309ms/step - loss: 0.1435 - acc: 0.9424 - recall: 0.9423 - f1: 0.9424\n",
            "Epoch 15/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9449 - recall: 0.9450 - f1: 0.9449\n",
            "Epoch 00015: loss improved from 0.14353 to 0.13770, saving model to Erick_dropout_0.2-015-0.944850.h5\n",
            "200/200 [==============================] - 62s 309ms/step - loss: 0.1377 - acc: 0.9448 - recall: 0.9450 - f1: 0.9448\n",
            "Epoch 16/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9457 - recall: 0.9458 - f1: 0.9457\n",
            "Epoch 00016: loss improved from 0.13770 to 0.13558, saving model to Erick_dropout_0.2-016-0.945732.h5\n",
            "200/200 [==============================] - 62s 309ms/step - loss: 0.1356 - acc: 0.9457 - recall: 0.9458 - f1: 0.9457\n",
            "Epoch 17/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9460 - recall: 0.9461 - f1: 0.9460\n",
            "Epoch 00017: loss improved from 0.13558 to 0.13409, saving model to Erick_dropout_0.2-017-0.946009.h5\n",
            "200/200 [==============================] - 62s 312ms/step - loss: 0.1341 - acc: 0.9460 - recall: 0.9460 - f1: 0.9460\n",
            "Epoch 18/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9477 - recall: 0.9478 - f1: 0.9477\n",
            "Epoch 00018: loss improved from 0.13409 to 0.13108, saving model to Erick_dropout_0.2-018-0.947708.h5\n",
            "200/200 [==============================] - 63s 313ms/step - loss: 0.1311 - acc: 0.9477 - recall: 0.9477 - f1: 0.9477\n",
            "Epoch 19/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9483 - recall: 0.9484 - f1: 0.9483\n",
            "Epoch 00019: loss improved from 0.13108 to 0.12957, saving model to Erick_dropout_0.2-019-0.948323.h5\n",
            "200/200 [==============================] - 63s 313ms/step - loss: 0.1296 - acc: 0.9483 - recall: 0.9484 - f1: 0.9483\n",
            "Epoch 20/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9500 - recall: 0.9500 - f1: 0.9500\n",
            "Epoch 00020: loss improved from 0.12957 to 0.12589, saving model to Erick_dropout_0.2-020-0.949984.h5\n",
            "200/200 [==============================] - 63s 313ms/step - loss: 0.1259 - acc: 0.9500 - recall: 0.9499 - f1: 0.9500\n",
            "Epoch 21/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9514 - recall: 0.9514 - f1: 0.9514\n",
            "Epoch 00021: loss improved from 0.12589 to 0.12318, saving model to Erick_dropout_0.2-021-0.951448.h5\n",
            "200/200 [==============================] - 63s 314ms/step - loss: 0.1232 - acc: 0.9514 - recall: 0.9514 - f1: 0.9514\n",
            "Epoch 22/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9513 - recall: 0.9513 - f1: 0.9513\n",
            "Epoch 00022: loss did not improve from 0.12318\n",
            "200/200 [==============================] - 63s 313ms/step - loss: 0.1237 - acc: 0.9513 - recall: 0.9513 - f1: 0.9513\n",
            "Epoch 23/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9533 - recall: 0.9533 - f1: 0.9533\n",
            "Epoch 00023: loss improved from 0.12318 to 0.11894, saving model to Erick_dropout_0.2-023-0.953397.h5\n",
            "200/200 [==============================] - 63s 313ms/step - loss: 0.1189 - acc: 0.9534 - recall: 0.9534 - f1: 0.9534\n",
            "Epoch 24/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9533 - recall: 0.9533 - f1: 0.9533\n",
            "Epoch 00024: loss improved from 0.11894 to 0.11827, saving model to Erick_dropout_0.2-024-0.953347.h5\n",
            "200/200 [==============================] - 63s 314ms/step - loss: 0.1183 - acc: 0.9533 - recall: 0.9533 - f1: 0.9533\n",
            "Epoch 25/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9526 - recall: 0.9526 - f1: 0.9526\n",
            "Epoch 00025: loss did not improve from 0.11827\n",
            "200/200 [==============================] - 63s 313ms/step - loss: 0.1197 - acc: 0.9526 - recall: 0.9526 - f1: 0.9526\n",
            "Epoch 26/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9552 - recall: 0.9552 - f1: 0.9552\n",
            "Epoch 00026: loss improved from 0.11827 to 0.11446, saving model to Erick_dropout_0.2-026-0.955149.h5\n",
            "200/200 [==============================] - 63s 314ms/step - loss: 0.1145 - acc: 0.9552 - recall: 0.9551 - f1: 0.9551\n",
            "Epoch 27/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9544 - recall: 0.9544 - f1: 0.9544\n",
            "Epoch 00027: loss did not improve from 0.11446\n",
            "200/200 [==============================] - 63s 313ms/step - loss: 0.1153 - acc: 0.9544 - recall: 0.9544 - f1: 0.9544\n",
            "Epoch 28/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9552 - recall: 0.9552 - f1: 0.9552\n",
            "Epoch 00028: loss did not improve from 0.11446\n",
            "200/200 [==============================] - 63s 313ms/step - loss: 0.1151 - acc: 0.9552 - recall: 0.9552 - f1: 0.9552\n",
            "Epoch 29/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9553 - recall: 0.9553 - f1: 0.9553\n",
            "Epoch 00029: loss improved from 0.11446 to 0.11346, saving model to Erick_dropout_0.2-029-0.955327.h5\n",
            "200/200 [==============================] - 63s 313ms/step - loss: 0.1135 - acc: 0.9553 - recall: 0.9553 - f1: 0.9553\n",
            "Epoch 30/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9574 - recall: 0.9574 - f1: 0.9574\n",
            "Epoch 00030: loss improved from 0.11346 to 0.10970, saving model to Erick_dropout_0.2-030-0.957360.h5\n",
            "200/200 [==============================] - 62s 312ms/step - loss: 0.1097 - acc: 0.9574 - recall: 0.9574 - f1: 0.9574\n",
            "Epoch 31/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9580 - recall: 0.9580 - f1: 0.9580\n",
            "Epoch 00031: loss improved from 0.10970 to 0.10909, saving model to Erick_dropout_0.2-031-0.957945.h5\n",
            "200/200 [==============================] - 62s 312ms/step - loss: 0.1091 - acc: 0.9579 - recall: 0.9579 - f1: 0.9579\n",
            "Epoch 32/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9579 - recall: 0.9579 - f1: 0.9579\n",
            "Epoch 00032: loss did not improve from 0.10909\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.1096 - acc: 0.9579 - recall: 0.9579 - f1: 0.9579\n",
            "Epoch 33/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9569 - recall: 0.9569 - f1: 0.9569\n",
            "Epoch 00033: loss did not improve from 0.10909\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.1108 - acc: 0.9569 - recall: 0.9569 - f1: 0.9569\n",
            "Epoch 34/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9578 - recall: 0.9578 - f1: 0.9578\n",
            "Epoch 00034: loss improved from 0.10909 to 0.10903, saving model to Erick_dropout_0.2-034-0.957774.h5\n",
            "200/200 [==============================] - 62s 312ms/step - loss: 0.1090 - acc: 0.9578 - recall: 0.9578 - f1: 0.9578\n",
            "Epoch 35/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9590 - recall: 0.9590 - f1: 0.9590\n",
            "Epoch 00035: loss improved from 0.10903 to 0.10706, saving model to Erick_dropout_0.2-035-0.959037.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.1071 - acc: 0.9590 - recall: 0.9590 - f1: 0.9590\n",
            "Epoch 36/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9590 - recall: 0.9590 - f1: 0.9590\n",
            "Epoch 00036: loss improved from 0.10706 to 0.10674, saving model to Erick_dropout_0.2-036-0.958950.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.1067 - acc: 0.9589 - recall: 0.9589 - f1: 0.9589\n",
            "Epoch 37/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9587 - recall: 0.9587 - f1: 0.9587\n",
            "Epoch 00037: loss improved from 0.10674 to 0.10623, saving model to Erick_dropout_0.2-037-0.958733.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.1062 - acc: 0.9587 - recall: 0.9587 - f1: 0.9587\n",
            "Epoch 38/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9595 - recall: 0.9595 - f1: 0.9595\n",
            "Epoch 00038: loss improved from 0.10623 to 0.10433, saving model to Erick_dropout_0.2-038-0.959470.h5\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.1043 - acc: 0.9595 - recall: 0.9595 - f1: 0.9595\n",
            "Epoch 39/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9590 - recall: 0.9590 - f1: 0.9590\n",
            "Epoch 00039: loss did not improve from 0.10433\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.1057 - acc: 0.9591 - recall: 0.9591 - f1: 0.9591\n",
            "Epoch 40/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9593 - recall: 0.9593 - f1: 0.9593\n",
            "Epoch 00040: loss did not improve from 0.10433\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.1050 - acc: 0.9593 - recall: 0.9593 - f1: 0.9593\n",
            "Epoch 41/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9612 - recall: 0.9612 - f1: 0.9612\n",
            "Epoch 00041: loss improved from 0.10433 to 0.10152, saving model to Erick_dropout_0.2-041-0.961242.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.1015 - acc: 0.9612 - recall: 0.9612 - f1: 0.9612\n",
            "Epoch 42/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9606 - recall: 0.9606 - f1: 0.9606\n",
            "Epoch 00042: loss did not improve from 0.10152\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.1031 - acc: 0.9606 - recall: 0.9606 - f1: 0.9606\n",
            "Epoch 43/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9611 - recall: 0.9611 - f1: 0.9611\n",
            "Epoch 00043: loss did not improve from 0.10152\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.1033 - acc: 0.9610 - recall: 0.9610 - f1: 0.9610\n",
            "Epoch 44/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9609 - recall: 0.9609 - f1: 0.9609\n",
            "Epoch 00044: loss did not improve from 0.10152\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.1034 - acc: 0.9610 - recall: 0.9610 - f1: 0.9610\n",
            "Epoch 45/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9633 - recall: 0.9633 - f1: 0.9633\n",
            "Epoch 00045: loss improved from 0.10152 to 0.09799, saving model to Erick_dropout_0.2-045-0.963262.h5\n",
            "200/200 [==============================] - 62s 312ms/step - loss: 0.0980 - acc: 0.9633 - recall: 0.9633 - f1: 0.9633\n",
            "Epoch 46/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9615 - recall: 0.9615 - f1: 0.9615\n",
            "Epoch 00046: loss did not improve from 0.09799\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.1006 - acc: 0.9616 - recall: 0.9616 - f1: 0.9616\n",
            "Epoch 47/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9612 - recall: 0.9612 - f1: 0.9612\n",
            "Epoch 00047: loss did not improve from 0.09799\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.1016 - acc: 0.9612 - recall: 0.9612 - f1: 0.9612\n",
            "Epoch 48/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9623 - recall: 0.9623 - f1: 0.9623\n",
            "Epoch 00048: loss did not improve from 0.09799\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0999 - acc: 0.9624 - recall: 0.9624 - f1: 0.9624\n",
            "Epoch 49/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9621 - recall: 0.9621 - f1: 0.9621\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.09799\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0997 - acc: 0.9621 - recall: 0.9621 - f1: 0.9621\n",
            "Epoch 50/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9637 - recall: 0.9637 - f1: 0.9637\n",
            "Epoch 00050: loss improved from 0.09799 to 0.09518, saving model to Erick_dropout_0.2-050-0.963663.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0952 - acc: 0.9637 - recall: 0.9637 - f1: 0.9637\n",
            "Epoch 51/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9646 - recall: 0.9646 - f1: 0.9646\n",
            "Epoch 00051: loss improved from 0.09518 to 0.09395, saving model to Erick_dropout_0.2-051-0.964650.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0940 - acc: 0.9646 - recall: 0.9646 - f1: 0.9646\n",
            "Epoch 52/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9653 - recall: 0.9653 - f1: 0.9653\n",
            "Epoch 00052: loss improved from 0.09395 to 0.09219, saving model to Erick_dropout_0.2-052-0.965233.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0922 - acc: 0.9652 - recall: 0.9652 - f1: 0.9652\n",
            "Epoch 53/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9654 - recall: 0.9654 - f1: 0.9654\n",
            "Epoch 00053: loss improved from 0.09219 to 0.09164, saving model to Erick_dropout_0.2-053-0.965396.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0916 - acc: 0.9654 - recall: 0.9654 - f1: 0.9654\n",
            "Epoch 54/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9645 - recall: 0.9645 - f1: 0.9645\n",
            "Epoch 00054: loss did not improve from 0.09164\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0939 - acc: 0.9645 - recall: 0.9645 - f1: 0.9645\n",
            "Epoch 55/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9657 - recall: 0.9657 - f1: 0.9657\n",
            "Epoch 00055: loss did not improve from 0.09164\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0917 - acc: 0.9657 - recall: 0.9657 - f1: 0.9657\n",
            "Epoch 56/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9653 - recall: 0.9653 - f1: 0.9653\n",
            "Epoch 00056: loss did not improve from 0.09164\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0921 - acc: 0.9653 - recall: 0.9653 - f1: 0.9653\n",
            "Epoch 57/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9654 - recall: 0.9654 - f1: 0.9654\n",
            "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.09164\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0921 - acc: 0.9654 - recall: 0.9654 - f1: 0.9654\n",
            "Epoch 58/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9665 - recall: 0.9665 - f1: 0.9665\n",
            "Epoch 00058: loss improved from 0.09164 to 0.08747, saving model to Erick_dropout_0.2-058-0.966466.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0875 - acc: 0.9665 - recall: 0.9665 - f1: 0.9665\n",
            "Epoch 59/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9676 - recall: 0.9676 - f1: 0.9676\n",
            "Epoch 00059: loss did not improve from 0.08747\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0875 - acc: 0.9676 - recall: 0.9676 - f1: 0.9676\n",
            "Epoch 60/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9673 - recall: 0.9673 - f1: 0.9673\n",
            "Epoch 00060: loss improved from 0.08747 to 0.08720, saving model to Erick_dropout_0.2-060-0.967358.h5\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0872 - acc: 0.9674 - recall: 0.9674 - f1: 0.9674\n",
            "Epoch 61/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9681 - recall: 0.9681 - f1: 0.9681\n",
            "Epoch 00061: loss improved from 0.08720 to 0.08540, saving model to Erick_dropout_0.2-061-0.968092.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0854 - acc: 0.9681 - recall: 0.9681 - f1: 0.9681\n",
            "Epoch 62/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9679 - recall: 0.9679 - f1: 0.9679\n",
            "Epoch 00062: loss improved from 0.08540 to 0.08534, saving model to Erick_dropout_0.2-062-0.967900.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0853 - acc: 0.9679 - recall: 0.9679 - f1: 0.9679\n",
            "Epoch 63/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9677 - recall: 0.9677 - f1: 0.9677\n",
            "Epoch 00063: loss did not improve from 0.08534\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0863 - acc: 0.9677 - recall: 0.9677 - f1: 0.9677\n",
            "Epoch 64/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9673 - recall: 0.9673 - f1: 0.9673\n",
            "Epoch 00064: loss did not improve from 0.08534\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0868 - acc: 0.9673 - recall: 0.9673 - f1: 0.9673\n",
            "Epoch 65/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9680 - recall: 0.9680 - f1: 0.9680\n",
            "Epoch 00065: loss improved from 0.08534 to 0.08501, saving model to Erick_dropout_0.2-065-0.967958.h5\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0850 - acc: 0.9680 - recall: 0.9680 - f1: 0.9680\n",
            "Epoch 66/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9678 - recall: 0.9678 - f1: 0.9678\n",
            "Epoch 00066: loss improved from 0.08501 to 0.08486, saving model to Erick_dropout_0.2-066-0.967767.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0849 - acc: 0.9678 - recall: 0.9678 - f1: 0.9678\n",
            "Epoch 67/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9682 - recall: 0.9682 - f1: 0.9682\n",
            "Epoch 00067: loss improved from 0.08486 to 0.08470, saving model to Erick_dropout_0.2-067-0.968146.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0847 - acc: 0.9681 - recall: 0.9681 - f1: 0.9681\n",
            "Epoch 68/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9684 - recall: 0.9684 - f1: 0.9684\n",
            "Epoch 00068: loss improved from 0.08470 to 0.08392, saving model to Erick_dropout_0.2-068-0.968358.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0839 - acc: 0.9684 - recall: 0.9684 - f1: 0.9684\n",
            "Epoch 69/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9683 - recall: 0.9683 - f1: 0.9683\n",
            "Epoch 00069: loss did not improve from 0.08392\n",
            "200/200 [==============================] - 62s 309ms/step - loss: 0.0843 - acc: 0.9682 - recall: 0.9682 - f1: 0.9682\n",
            "Epoch 70/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9686 - recall: 0.9686 - f1: 0.9686\n",
            "Epoch 00070: loss improved from 0.08392 to 0.08375, saving model to Erick_dropout_0.2-070-0.968521.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0837 - acc: 0.9685 - recall: 0.9685 - f1: 0.9685\n",
            "Epoch 71/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9686 - recall: 0.9686 - f1: 0.9686\n",
            "Epoch 00071: loss did not improve from 0.08375\n",
            "200/200 [==============================] - 62s 309ms/step - loss: 0.0838 - acc: 0.9686 - recall: 0.9686 - f1: 0.9686\n",
            "Epoch 72/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9693 - recall: 0.9693 - f1: 0.9693\n",
            "Epoch 00072: loss improved from 0.08375 to 0.08267, saving model to Erick_dropout_0.2-072-0.969221.h5\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0827 - acc: 0.9692 - recall: 0.9692 - f1: 0.9692\n",
            "Epoch 73/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9696 - recall: 0.9696 - f1: 0.9696\n",
            "Epoch 00073: loss improved from 0.08267 to 0.08135, saving model to Erick_dropout_0.2-073-0.969583.h5\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0814 - acc: 0.9696 - recall: 0.9696 - f1: 0.9696\n",
            "Epoch 74/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9692 - recall: 0.9692 - f1: 0.9692\n",
            "Epoch 00074: loss did not improve from 0.08135\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0831 - acc: 0.9691 - recall: 0.9691 - f1: 0.9691\n",
            "Epoch 75/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9695 - recall: 0.9695 - f1: 0.9695\n",
            "Epoch 00075: loss did not improve from 0.08135\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0817 - acc: 0.9696 - recall: 0.9696 - f1: 0.9696\n",
            "Epoch 76/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9696 - recall: 0.9696 - f1: 0.9696\n",
            "Epoch 00076: loss did not improve from 0.08135\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0817 - acc: 0.9696 - recall: 0.9696 - f1: 0.9696\n",
            "Epoch 77/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9687 - recall: 0.9687 - f1: 0.9687\n",
            "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.08135\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0836 - acc: 0.9687 - recall: 0.9687 - f1: 0.9687\n",
            "Epoch 78/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9713 - recall: 0.9713 - f1: 0.9713\n",
            "Epoch 00078: loss improved from 0.08135 to 0.07835, saving model to Erick_dropout_0.2-078-0.971283.h5\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0783 - acc: 0.9713 - recall: 0.9713 - f1: 0.9713\n",
            "Epoch 79/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9710 - recall: 0.9710 - f1: 0.9710\n",
            "Epoch 00079: loss improved from 0.07835 to 0.07816, saving model to Erick_dropout_0.2-079-0.971029.h5\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0782 - acc: 0.9710 - recall: 0.9710 - f1: 0.9710\n",
            "Epoch 80/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9705 - recall: 0.9705 - f1: 0.9705\n",
            "Epoch 00080: loss did not improve from 0.07816\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0786 - acc: 0.9705 - recall: 0.9705 - f1: 0.9705\n",
            "Epoch 81/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9710 - recall: 0.9710 - f1: 0.9710\n",
            "Epoch 00081: loss improved from 0.07816 to 0.07741, saving model to Erick_dropout_0.2-081-0.970925.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0774 - acc: 0.9709 - recall: 0.9709 - f1: 0.9709\n",
            "Epoch 82/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9707 - recall: 0.9707 - f1: 0.9707\n",
            "Epoch 00082: loss did not improve from 0.07741\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0789 - acc: 0.9706 - recall: 0.9706 - f1: 0.9706\n",
            "Epoch 83/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9715 - recall: 0.9715 - f1: 0.9715\n",
            "Epoch 00083: loss improved from 0.07741 to 0.07696, saving model to Erick_dropout_0.2-083-0.971496.h5\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0770 - acc: 0.9715 - recall: 0.9715 - f1: 0.9715\n",
            "Epoch 84/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9712 - recall: 0.9712 - f1: 0.9712\n",
            "Epoch 00084: loss did not improve from 0.07696\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0777 - acc: 0.9712 - recall: 0.9712 - f1: 0.9712\n",
            "Epoch 85/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9711 - recall: 0.9711 - f1: 0.9711\n",
            "Epoch 00085: loss did not improve from 0.07696\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0778 - acc: 0.9711 - recall: 0.9711 - f1: 0.9711\n",
            "Epoch 86/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9712 - recall: 0.9712 - f1: 0.9712\n",
            "Epoch 00086: loss improved from 0.07696 to 0.07659, saving model to Erick_dropout_0.2-086-0.971150.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0766 - acc: 0.9711 - recall: 0.9711 - f1: 0.9711\n",
            "Epoch 87/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9716 - recall: 0.9716 - f1: 0.9716\n",
            "Epoch 00087: loss improved from 0.07659 to 0.07636, saving model to Erick_dropout_0.2-087-0.971629.h5\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0764 - acc: 0.9716 - recall: 0.9716 - f1: 0.9716\n",
            "Epoch 88/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9717 - recall: 0.9717 - f1: 0.9717\n",
            "Epoch 00088: loss improved from 0.07636 to 0.07608, saving model to Erick_dropout_0.2-088-0.971725.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0761 - acc: 0.9717 - recall: 0.9717 - f1: 0.9717\n",
            "Epoch 89/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9717 - recall: 0.9717 - f1: 0.9717\n",
            "Epoch 00089: loss improved from 0.07608 to 0.07587, saving model to Erick_dropout_0.2-089-0.971709.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0759 - acc: 0.9717 - recall: 0.9717 - f1: 0.9717\n",
            "Epoch 90/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9712 - recall: 0.9712 - f1: 0.9712\n",
            "Epoch 00090: loss did not improve from 0.07587\n",
            "200/200 [==============================] - 62s 309ms/step - loss: 0.0771 - acc: 0.9711 - recall: 0.9711 - f1: 0.9711\n",
            "Epoch 91/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9716 - recall: 0.9716 - f1: 0.9716\n",
            "Epoch 00091: loss improved from 0.07587 to 0.07546, saving model to Erick_dropout_0.2-091-0.971642.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0755 - acc: 0.9716 - recall: 0.9716 - f1: 0.9716\n",
            "Epoch 92/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9714 - recall: 0.9714 - f1: 0.9714\n",
            "Epoch 00092: loss did not improve from 0.07546\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0765 - acc: 0.9714 - recall: 0.9714 - f1: 0.9714\n",
            "Epoch 93/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9718 - recall: 0.9718 - f1: 0.9718\n",
            "Epoch 00093: loss improved from 0.07546 to 0.07491, saving model to Erick_dropout_0.2-093-0.971808.h5\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0749 - acc: 0.9718 - recall: 0.9718 - f1: 0.9718\n",
            "Epoch 94/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9723 - recall: 0.9723 - f1: 0.9723\n",
            "Epoch 00094: loss improved from 0.07491 to 0.07472, saving model to Erick_dropout_0.2-094-0.972250.h5\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0747 - acc: 0.9722 - recall: 0.9722 - f1: 0.9722\n",
            "Epoch 95/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9723 - recall: 0.9723 - f1: 0.9723\n",
            "Epoch 00095: loss improved from 0.07472 to 0.07445, saving model to Erick_dropout_0.2-095-0.972363.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0744 - acc: 0.9724 - recall: 0.9724 - f1: 0.9724\n",
            "Epoch 96/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9721 - recall: 0.9721 - f1: 0.9721\n",
            "Epoch 00096: loss did not improve from 0.07445\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0748 - acc: 0.9721 - recall: 0.9721 - f1: 0.9721\n",
            "Epoch 97/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9720 - recall: 0.9720 - f1: 0.9720\n",
            "Epoch 00097: loss did not improve from 0.07445\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0747 - acc: 0.9721 - recall: 0.9721 - f1: 0.9721\n",
            "Epoch 98/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9725 - recall: 0.9725 - f1: 0.9725\n",
            "Epoch 00098: loss improved from 0.07445 to 0.07378, saving model to Erick_dropout_0.2-098-0.972454.h5\n",
            "200/200 [==============================] - 62s 311ms/step - loss: 0.0738 - acc: 0.9725 - recall: 0.9725 - f1: 0.9725\n",
            "Epoch 99/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9726 - recall: 0.9726 - f1: 0.9726\n",
            "Epoch 00099: loss did not improve from 0.07378\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0740 - acc: 0.9725 - recall: 0.9725 - f1: 0.9725\n",
            "Epoch 100/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9730 - recall: 0.9730 - f1: 0.9730\n",
            "Epoch 00100: loss improved from 0.07378 to 0.07332, saving model to Erick_dropout_0.2-100-0.972971.h5\n",
            "200/200 [==============================] - 62s 310ms/step - loss: 0.0733 - acc: 0.9730 - recall: 0.9730 - f1: 0.9730\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yy1BVFlVvtz",
        "colab_type": "code",
        "outputId": "316a5b28-c479-493b-c25d-d48f0aaad7a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        }
      },
      "source": [
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.plot(history.history[\"loss\"][1:], label=\"train_loss\")\n",
        "# plt.plot(history.history[\"val_loss\"][1:], label=\"val_loss\")\n",
        "plt.plot(history.history[\"accuracy\"][1:], label=\"train_acc\")\n",
        "# plt.plot(history.history[\"val_accuracy\"][1:], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(\"plots/Erick_dropout_0.2.pdf\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-958ba6d276c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# plt.plot(history.history[\"val_loss\"][1:], label=\"val_loss\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train_acc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# plt.plot(history.history[\"val_accuracy\"][1:], label=\"val_acc\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Loss and Accuracy on Dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAI/CAYAAAA2kzvaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzde5hedXkv/O96ZsLMhIRkDiEhEhCG\ngCAgxtHGaFtiUqXaXVJsS497b3Gr9VTFvqIoVbBiUxHoSd9d2Skvlt1ewQO2ngAj9USKBjWCokgE\nqUgkZIYQAknI8Kz3j0nGRsDM4ZlnzWQ+n+vK9ZzW88y9uH788b3u31p3UZZlGQAAACa9WtUFAAAA\nMDICHAAAwBQhwAEAAEwRAhwAAMAUIcABAABMEQIcAADAFCHAAQAATBGtVRfwZO67776qS3iCnp6e\nbN26teoymCasN5rFWqNZrDWayXqjWSZqrS1cuPApP9OBAwAAmCIEOAAAgClCgAMAAJgiBDgAAIAp\nQoADAACYIgQ4AACAKUKAAwAAmCIEOAAAgClCgAMAAJgiBDgAAIApQoADAACYIgQ4AACAKUKAAwAA\nmCIEOAAAgClCgAMAAJgiBDgAAIApQoADAACYIgQ4AACAKUKAAwAAmCIEOAAAgClCgAMAAJgiBDgA\nAIApQoAbgfKH38/WN/xeyns2VV0KAAAwjQlwI1GWefwn/5nseLjqSgAAgGlMgBuJ9vahx107q60D\nAACY1lpHctDGjRtz5ZVXpl6vZ8WKFVm1atWTHnfzzTfnsssuy1/+5V+mt7c3W7ZsybnnnpuFCxcm\nSRYvXpxXv/rVjau+Wdo6kiTl7p0pKi4FAACYvg4Y4Or1etasWZMLLrgg3d3dOf/889PX15cjjzxy\nv+N27tyZz33uc1m8ePF+7y9YsCCXXHJJY6tutvahAKcDBwAAVOmAWyg3bdqUBQsWZP78+Wltbc2y\nZcuyYcOGJxy3du3anHnmmZkxY8aEFFqpfQFu965q6wAAAKa1Awa4gYGBdHd3D7/u7u7OwMDAfsfc\ndddd2bp1a5YsWfKE72/ZsiXnnXde3v3ud+d73/teA0quQOuMpNaiAwcAAFRqRNfA/SL1ej0f+chH\n8rrXve4Jn3V2duZDH/pQZs+enbvuuiuXXHJJLr300sycOXO/49atW5d169YlSVavXp2enp7xltVw\nWzpmpr1IDpuEtXHwaW1tnZT/H3DwsdZoFmuNZrLeaJYq1toBA1xXV1f6+/uHX/f396erq2v49a5d\nu/LjH/84F110UZJk27Ztef/735/zzjsvvb29w1sqjz322MyfPz+bN29Ob2/vfn9j5cqVWbly5fDr\nrVu3ju+sJkDR3pFd2wby2CSsjYNPT0/PpPz/gIOPtUazWGs0k/VGs0zUWtt3E8gnc8AA19vbm82b\nN2fLli3p6urK+vXr86d/+qfDn8+cOTNr1qwZfn3hhRfmj//4j9Pb25vt27dn1qxZqdVquf/++7N5\n8+bMnz9/nKdTjaJjZspdroEDAACqc8AA19LSknPOOScXX3xx6vV6li9fnkWLFmXt2rXp7e1NX1/f\nU3739ttvzzXXXJOWlpbUarW86lWvyqxZsxp6As1SdMxMuds1cAAAQHWKsizLqov4effdd1/VJTxB\ny99elMd2PpqWt/1V1aUwDdj6QbNYazSLtUYzWW80SxVbKA94F0qGFB0zE1soAQCACglwI1S0dyS2\nUAIAABUS4Eao6DjUHDgAAKBSAtwI6cABAABVE+BGqOiYmTz2WMr641WXAgAATFMC3AgV7R1DT9zI\nBAAAqIgAN0JFx8yhJ7sFOAAAoBoC3AgVHfs6cK6DAwAAqiHAjVCt/dChJwIcAABQEQFuhH62hVKA\nAwAAqiHAjZAtlAAAQNUEuBEq2oc6cKWbmAAAABUR4EZIBw4AAKiaADdCroEDAACqJsCNUNGmAwcA\nAFRLgBuholZL2toN8gYAACojwI1GW7sOHAAAUBkBbjTaO5JdOnAAAEA1BLjRaGtP6SYmAABARQS4\n0WjvsIUSAACojAA3Gu0zBTgAAKAyAtwoFG3t5sABAACVEeBGw01MAACACglwo6EDBwAAVEiAG429\nHbiyLKuuBAAAmIYEuNFo60jKevLYY1VXAgAATEMC3Gi0dww92kYJAABUQIAbjbb2oUejBAAAgAoI\ncKNQ7OvACXAAAEAFBLjRGN5CaZQAAADQfALcaNhCCQAAVEiAG432mUOPbmICAABUQIAbjfahDlyp\nAwcAAFRAgBsN18ABAAAVEuBGwzVwAABAhQS4UShaZyStra6BAwAAKiHAjVZbhw4cAABQCQFutNoF\nOAAAoBoC3Gi1tad0ExMAAKACAtxo6cABAAAVEeBGq73DGAEAAKASAtxotbXrwAEAAJUQ4EapsIUS\nAACoiAA3Wm0d5sABAACVEOBGq70j2eUaOAAAoPkEuNFqa08G96QcHKy6EgAAYJoR4EarvWPo0TZK\nAACgyQS40WprH3q0jRIAAGgyAW602mcOPboTJQAA0GQC3CgV7Xs7cLZQAgAATSbAjVbb3mvgdOAA\nAIAmE+BGa/gmJq6BAwAAmkuAG629WyhLHTgAAKDJBLjRMkYAAACoiAA3Wq6BAwAAKjKiALdx48a8\n6U1vyhvf+MZ88pOffMrjbr755vzu7/5ufvjDHw6/d+211+aNb3xj3vSmN2Xjxo3jr7hqh7QlReEa\nOAAAoOkOGODq9XrWrFmTd7zjHbn88stz00035d57733CcTt37sznPve5LF68ePi9e++9N+vXr89l\nl12Wd77znVmzZk3q9Xpjz6DJiqIYGuatAwcAADTZAQPcpk2bsmDBgsyfPz+tra1ZtmxZNmzY8ITj\n1q5dmzPPPDMzZswYfm/Dhg1ZtmxZZsyYkcMPPzwLFizIpk2bGnsGVWjvEOAAAICmO2CAGxgYSHd3\n9/Dr7u7uDAwM7HfMXXfdla1bt2bJkiW/8LtdXV1P+O6U1NZhCyUAANB0reP9gXq9no985CN53ete\nN+bfWLduXdatW5ckWb16dXp6esZbVsO1trYO19U/a3Zq9cF0TsI6OTj81/UGE8lao1msNZrJeqNZ\nqlhrBwxwXV1d6e/vH37d39+frq6u4de7du3Kj3/841x00UVJkm3btuX9739/zjvvvCd8d2BgYL/v\n7rNy5cqsXLly+PXWrVvHdjYTqKenZ7iux1tnJA9vn5R1cnD4r+sNJpK1RrNYazST9UazTNRaW7hw\n4VN+dsAtlL29vdm8eXO2bNmSwcHBrF+/Pn19fcOfz5w5M2vWrMkHP/jBfPCDH8zixYtz3nnnpbe3\nN319fVm/fn327NmTLVu2ZPPmzTnuuOMac1ZVchMTAACgAgfswLW0tOScc87JxRdfnHq9nuXLl2fR\nokVZu3btcEh7KosWLcrzn//8vOUtb0mtVssrX/nK1GpTf/Rc0d6Rcpdr4AAAgOYqyrIsqy7i5913\n331Vl/AE/7U9Wv/I36e8dUNaPnBVxVVxsLL1g2ax1mgWa41mst5olkm5hZIn0d6R6MABAABNJsCN\nRVtHsntnyik+lBwAAJhaBLixaO8YenxMFw4AAGgeAW4s2tqHHm2jBAAAmkiAG4t9HTijBAAAgCYS\n4MagaN/bgdutAwcAADSPADcWbTpwAABA8wlwY9E+c+hxtwAHAAA0jwA3Fnu3UJY6cAAAQBMJcGOx\nbwula+AAAIAmEuDGYt9NTHTgAACAJhLgxsJNTAAAgAoIcGNQtLQkMw5xExMAAKCpBLixau/QgQMA\nAJpKgBurtnY3MQEAAJpKgBur9g5jBAAAgKYS4MZKBw4AAGgyAW6sXAMHAAA0mQA3VgIcAADQZALc\nGBVtHcYIAAAATSXAjVV7R7LLNXAAAEDzCHBj1dae7N6ZsiyrrgQAAJgmBLixau9IHn88GdxTdSUA\nAMA0IcCNVVvH0KNtlAAAQJMIcGPVvi/APVptHQAAwLQhwI1R0d4+9MQwbwAAoEkEuLEa3kJplAAA\nANAcAtxY7dtCqQMHAAA0iQA3Vvu2UOrAAQAATSLAjdXeLZTlbgEOAABoDgFurNpdAwcAADSXADdW\nAhwAANBkAtxYtc5IajU3MQEAAJpGgBujoiiGunA6cAAAQJMIcOPR1pG4iQkAANAkAtx4tHek1IED\nAACaRIAbj7Z218ABAABNI8CNh2vgAACAJhLgxqO9I9mlAwcAADSHADcORVu7m5gAAABNI8CNhy2U\nAABAEwlw42GMAAAA0EQC3Hi0dySPPZby8cerrgQAAJgGBLjxaGsfejRKAAAAaAIBbjzaO4YeXQcH\nAAA0gQA3HjpwAABAEwlw41C0zxx6ogMHAAA0gQA3Hu37OnACHAAAMPEEuPFwDRwAANBEAtx4tA0F\nuNI1cAAAQBMIcOOxbwulDhwAANAEAtx42EIJAAA0kQA3Hoe4iQkAANA8Atw4FLXa0Cw4HTgAAKAJ\nBLjxams3yBsAAGgKAW682jt04AAAgKZoHclBGzduzJVXXpl6vZ4VK1Zk1apV+31+ww035Prrr0+t\nVkt7e3te85rX5Mgjj8yWLVty7rnnZuHChUmSxYsX59WvfnXjz6JKbe3GCAAAAE1xwABXr9ezZs2a\nXHDBBenu7s7555+fvr6+HHnkkcPHvPCFL8yLX/ziJMktt9ySq666Ku985zuTJAsWLMgll1wyQeVP\nAjpwAABAkxxwC+WmTZuyYMGCzJ8/P62trVm2bFk2bNiw3zEzZ84cfr5r164URdH4Sier9pkCHAAA\n0BQH7MANDAyku7t7+HV3d3fuvPPOJxx33XXX5TOf+UwGBwfzrne9a/j9LVu25LzzzktHR0d+7/d+\nLyeeeGKDSp8cirb2lMYIAAAATTCia+BG4owzzsgZZ5yRr371q/n4xz+eN7zhDens7MyHPvShzJ49\nO3fddVcuueSSXHrppft17JJk3bp1WbduXZJk9erV6enpaVRZDdPa2vqkdT00tzOP3fX9SVkzU9dT\nrTdoNGuNZrHWaCbrjWapYq0dMMB1dXWlv79/+HV/f3+6urqe8vhly5bliiuuSJLMmDEjM2bMSJIc\ne+yxmT9/fjZv3pze3t79vrNy5cqsXLly+PXWrVtHdxZN0NPT86R11cuk3PnopKyZqeup1hs0mrVG\ns1hrNJP1RrNM1FrbdxPIJ3PAa+B6e3uzefPmbNmyJYODg1m/fn36+vr2O2bz5s3Dz7/5zW/miCOO\nSJJs37499Xo9SXL//fdn8+bNmT9//phOYtJq70h27UpZllVXAgAAHOQO2IFraWnJOeeck4svvjj1\nej3Lly/PokWLsnbt2vT29qavry/XXXddbrvttrS0tGTWrFl5/etfnyS5/fbbc80116SlpSW1Wi2v\netWrMmvWrAk/qaZq60jKevLYY0lbW9XVAAAAB7ERXQO3ZMmSLFmyZL/3zj777OHnr3jFK570e0uX\nLs3SpUvHUd4U0N4x9Lj7UQEOAACYUAfcQskBtLUPPe4yzBsAAJhYAtw4Ffs6cGbBAQAAE0yAG6/2\nvR243TpwAADAxBLgxqtNBw4AAGgOAW682oeGkpcCHAAAMMEEuPEa3kIpwAEAABNLgBsvNzEBAACa\nRIAbrzY3MQEAAJpDgBunonVG0tqqAwcAAEw4Aa4R2jpcAwcAAEw4Aa4R2jt04AAAgAknwDVCW3tK\n18ABAAATTIBrBB04AACgCQS4RmhrdxdKAABgwglwjaADBwAANIEA1wCFAAcAADSBANcIxggAAABN\nIMA1gg4cAADQBAJcI8w8NBkcTLl7d9WVAAAABzEBrhHmdg89buuvtg4AAOCgJsA1QNG5N8A9uLXa\nQgAAgIOaANcInT1JkvJBHTgAAGDiCHCNMFcHDgAAmHgCXAMUbW3JobMTHTgAAGACCXCN0tmdUgcO\nAACYQAJco3T26MABAAATSoBrkKKz2zVwAADAhBLgGqWzO3n4oZR79lRdCQAAcJAS4Bpl7ygBw7wB\nAICJIsA1yM+GeQtwAADAxBDgGmXvLDh3ogQAACaKANcotlACAAATTIBrkKJjZtLeYQslAAAwYQS4\nRurssYUSAACYMAJcI3V268ABAAATRoBrIMO8AQCAiSTANVJnT/LQgykHB6uuBAAAOAgJcI3U2Z2U\nZbL9waorAQAADkICXAMV+0YJuA4OAACYAAJcI3UODfM2Cw4AAJgIAlwj7e3AGSUAAABMBAGukWbO\nSg45xBZKAABgQghwDVQURTK3R4ADAAAmhADXaJ3dtlACAAATQoBrsKJTBw4AAJgYAlyjdXYn2/pT\n1utVVwIAABxkBLhG6+xJHn88efihqisBAAAOMgJcgxX7ZsG5Dg4AAGgwAa7R9s6Ccx0cAADQaAJc\no+3twLkTJQAA0GgCXKPNOixpbdWBAwAAGk6Aa7CiVkvmdrsGDgAAaDgBbiJ0dqfUgQMAABpMgJsA\nQ8O8deAAAIDGEuAmwtzu5MH+lGVZdSUAAMBBRICbCJ3dyeCeZMfDVVcCAAAcRFpHctDGjRtz5ZVX\npl6vZ8WKFVm1atV+n99www25/vrrU6vV0t7ente85jU58sgjkyTXXnttbrzxxtRqtbziFa/Iaaed\n1vizmGSKzp6UydA2ytmHVV0OAABwkDhgB65er2fNmjV5xzvekcsvvzw33XRT7r333v2OeeELX5hL\nL700l1xySc4888xcddVVSZJ7770369evz2WXXZZ3vvOdWbNmTer1+sScyWSydxacUQIAAEAjHTDA\nbdq0KQsWLMj8+fPT2tqaZcuWZcOGDfsdM3PmzOHnu3btSlEUSZINGzZk2bJlmTFjRg4//PAsWLAg\nmzZtavApTEKdPUkM8wYAABrrgFsoBwYG0t3dPfy6u7s7d9555xOOu+666/KZz3wmg4ODede73jX8\n3cWLFw8f09XVlYGBgUbUPbnNmZvUajpwAABAQ43oGriROOOMM3LGGWfkq1/9aj7+8Y/nDW94w4i/\nu27duqxbty5Jsnr16vT09DSqrIZpbW0dVV0PdPbkkJ07MmcSnguT32jXG4yVtUazWGs0k/VGs1Sx\n1g4Y4Lq6utLf/7NOUn9/f7q6up7y+GXLluWKK6540u8ODAw86XdXrlyZlStXDr/eunXybT3s6ekZ\nVV31OZ3Z9dOfZM8kPBcmv9GuNxgra41msdZoJuuNZpmotbZw4cKn/OyA18D19vZm8+bN2bJlSwYH\nB7N+/fr09fXtd8zmzZuHn3/zm9/MEUcckSTp6+vL+vXrs2fPnmzZsiWbN2/OcccdN9bzmFo6uw3z\nBgAAGuqAHbiWlpacc845ufjii1Ov17N8+fIsWrQoa9euTW9vb/r6+nLdddfltttuS0tLS2bNmpXX\nv/71SZJFixbl+c9/ft7ylrekVqvlla98ZWq16TF6rujsSfmdb6Ysy+GbugAAAIzHiK6BW7JkSZYs\nWbLfe2efffbw81e84hVP+d2zzjorZ5111hjLm8I6u5Pdu5KdjyQzZ1VdDQAAcBCYHu2wKuwdJeBO\nlAAAQKMIcBOkMMwbAABoMAFuohjmDQAANJgAN1HmdCZFoQMHAAA0jAA3QYrWGclhc5NtAhwAANAY\nAtxEmtttCyUAANAwAtxE6uyxhRIAAGgYAW4CFZ3diQ4cAADQIALcROrsSR59JOWunVVXAgAAHAQE\nuIm0bxacG5kAAAANIMBNoGLvLDjXwQEAAI0gwE2kvR04d6IEAAAaQYCbSPu2UOrAAQAADSDATaBi\nxiHJrNnuRAkAADSEADfR5vak1IEDAAAaQICbaGbBAQAADSLATbCis8c1cAAAQEMIcBOtszvZsT3l\nnseqrgQAAJjiBLiJZhYcAADQIALcBCuMEgAAABpEgJtoeztwhnkDAADjJcBNtM6uoUcdOAAAYJwE\nuAlWtM9MOg41SgAAABg3Aa4ZOrsN8wYAAMZNgGsGw7wBAIAGEOCaoOiaJ8ABAADjJsA1Q2dPsn1b\nyj17qq4EAACYwgS4ZujaN8xbFw4AABg7Aa4Jik4BDgAAGD8Brhn2duDKAQEOAAAYOwGuGTrnDT0O\nPFBtHQAAwJQmwDVB0daWHDrbFkoAAGBcBLhm6eyxhRIAABgXAa5Zunp04AAAgHER4Jqk6JqX6MAB\nAADjIMA1S1dP8uiOlLt2Vl0JAAAwRQlwzWIWHAAAME4CXJMUe2fB2UYJAACMlQDXLJ37hnmbBQcA\nAIyNANcsnd1JUdhCCQAAjJkA1yRF64zksLm2UAIAAGMmwDVTZ09KHTgAAGCMBLhm6urRgQMAAMZM\ngGuiorMneXBryrKsuhQAAGAKEuCaqWtesntX8ugjVVcCAABMQQJcEw3PgnvQKAEAAGD0BLhm6jTM\nGwAAGDsBrpm65iUxzBsAABgbAa6Z5sxNWlp04AAAgDER4JqoqLUkc7oSs+AAAIAxEOCarasnpQ4c\nAAAwBgJck+2bBQcAADBaAlyzde0d5l2vV10JAAAwxQhwzdY5LxkcTHY8VHUlAADAFCPANdnwMG/X\nwQEAAKMkwDXb3llwAhwAADBarSM5aOPGjbnyyitTr9ezYsWKrFq1ar/PP/3pT+cLX/hCWlpacthh\nh+W1r31t5s0bCipnn312jjrqqCRJT09P3va2tzX4FKaYvR248sGtKSouBQAAmFoOGODq9XrWrFmT\nCy64IN3d3Tn//PPT19eXI488cviYpz/96Vm9enXa2tpyww035Oqrr865556bJDnkkENyySWXTNwZ\nTDWzDktmHJIMPFB1JQAAwBRzwC2UmzZtyoIFCzJ//vy0trZm2bJl2bBhw37HnHzyyWlra0uSLF68\nOAMDAxNT7UGgKIqks9sWSgAAYNQO2IEbGBhId3f38Ovu7u7ceeedT3n8jTfemNNOO2349Z49e/L2\nt789LS0tOfPMM/O85z1vnCUfBDp7UpoFBwAAjNKIroEbqS9/+cu56667cuGFFw6/96EPfShdXV25\n//778573vCdHHXVUFixYsN/31q1bl3Xr1iVJVq9enZ6enkaW1RCtra0Nq+uhI47MY7d9Y1KeJ5ND\nI9cb/CLWGs1irdFM1hvNUsVaO2CA6+rqSn9///Dr/v7+dHV1PeG4W2+9Nddee20uvPDCzJgxY7/v\nJ8n8+fNz0kkn5Uc/+tETAtzKlSuzcuXK4ddbt06+7lRPT0/D6qrPnJ1yYGseuP/+FC0tDflNDi6N\nXG/wi1hrNIu1RjNZbzTLRK21hQsXPuVnB7wGrre3N5s3b86WLVsyODiY9evXp6+vb79j7r777lxx\nxRU577zzMmfOnOH3d+zYkT179iRJtm/fnjvuuGO/m59MW109SVlPHnKtIAAAMHIH7MC1tLTknHPO\nycUXX5x6vZ7ly5dn0aJFWbt2bXp7e9PX15err746u3btymWXXZbkZ+MCfvKTn+TDH/5warVa6vV6\nVq1aJcBlaJh3mQzdyGTfXDgAAIADGNE1cEuWLMmSJUv2e+/ss88efv7nf/7nT/q9E044IZdeeuk4\nyjtIdZoFBwAAjN4Bt1AyAfZ13YwSAAAARkGAq0Ax89CkvSMxSgAAABgFAa4qnT0p+x+ougoAAGAK\nEeCq0tWjAwcAAIyKAFeRomteMqADBwAAjJwAV5XOnuThh1LunZMHAABwIAJcVbqGRgnYRgkAAIyU\nAFeRolOAAwAARkeAq8reDlxpFhwAADBCAlxVOvcN83YjEwAAYGQEuIoUbW3JobNtoQQAAEZMgKtS\nV48tlAAAwIgJcFUyCw4AABgFAa5CRWePLZQAAMCICXBV6upJHn0k5a6dVVcCAABMAQJclcyCAwAA\nRkGAq1CxdxZc3MgEAAAYAQGuSp37hnm7kQkAAHBgAlyVOruTorCFEgAAGBEBrkJF64zksLm2UAIA\nACMiwFWtsyelDhwAADACAlzVuubpwAEAACMiwFWs6OpJBh5IWZZVlwIAAExyAlzVOnuSx3Ynj+6o\nuhIAAGCSE+AqZhYcAAAwUgJc1ToFOAAAYGQEuKp1zUuSlA8a5g0AAPxiAlzV5sxNWlqTn/6k6koA\nAIBJToCrWFFrSU55TsqvfSnlnseqLgcAAJjEBLhJoLb8ZcmO7SlvuanqUgAAgElMgJsMTnxWsuBp\nKf/9M1VXAgAATGIC3CRQFEWK01+a3P2DlD+6s+pyAACASUqAmySK578oaWtP+e+frboUAABgkhLg\nJoli5qEplp6e8utfTrlje9XlAAAAk5AAN4kUy1+WDO5J+dXPV10KAAAwCQlwk0jxtKOT409O+cXP\npaw/XnU5AADAJCPATTK15S9N+rckt32j6lIAAIBJRoCbbE5bmsztSt1IAQAA4OcIcJNM0dqa4lfO\nSL77rZT331d1OQAAwCQiwE1CxS+/OGlpSflFIwUAAICfEeAmoWJuV4oly1Le9IWUu3dVXQ4AADBJ\nCHCTVLH8ZcnOR1J+7UtVlwIAAEwSAtxkddyJyZFPT/nvn0lZllVXAwAATAIC3CRVFEWK5S9N7v1R\nsul7VZcDAABMAgLcJFb80ulJx6EpjRQAAAAiwE1qRVt7ihesSPnN9Sm3DVRdDgAAUDEBbpIrfuUl\nyeOPp/zm+qpLAQAAKibATXLFEYuSw49Ieds3qi4FAAComAA3BRSn9CV33JZy9+6qSwEAACokwE0B\nxal9yZ7Hku/fWnUpAABAhQS4qWDxyUlbe8rv3FJ1JQAAQIUEuCmgmDEjOfFZKW+9xVBvAACYxgS4\nKaI4pS8ZeCC57z+rLgUAAKiIADdFFCc/J0lS3mYbJQAATFcC3BRRdPUkRx4jwAEAwDQmwE0hxal9\nyabvpXxkR9WlAAAAFWgdyUEbN27MlVdemXq9nhUrVmTVqlX7ff7pT386X/jCF9LS0pLDDjssr33t\nazNv3rwkyRe/+MV84hOfSJKcddZZOf300xt7BtNIccpzUn72oylv35jiuS+suhwAAKDJDtiBq9fr\nWbNmTd7xjnfk8ssvz0033ZR77713v2Oe/vSnZ/Xq1fnABz6QpUuX5uqrr06S7NixIx/72Mfyvve9\nL+973/vysY99LDt26B6N2bEnJIfOTm7bUHUlAABABQ4Y4DZt2pQFCxZk/vz5aW1tzbJly7Jhw/4B\n4uSTT05bW1uSZPHixRkYGLzo1BYAACAASURBVEgy1Lk79dRTM2vWrMyaNSunnnpqNm7cOAGnMT0U\ntZYUz1yS8jvfTFmvV10OAADQZAcMcAMDA+nu7h5+3d3dPRzQnsyNN96Y00477Um/29XV9Qu/ywic\n8pzk4YeSezZVXQkAANBkI7oGbqS+/OUv56677sqFF144qu+tW7cu69atS5KsXr06PT09jSyrIVpb\nWydFXfVf+bU8cOVfp2PT7Zn13GVVl8MEmSzrjYOftUazWGs0k/VGs1Sx1g4Y4Lq6utLf3z/8ur+/\nP11dXU847tZbb821116bCy+8MDNmzBj+7u233z58zMDAQE466aQnfHflypVZuXLl8OutW7eO7iya\noKenZ/LUdczxeeRrX86uX1t14GOZkibVeuOgZq3RLNYazWS90SwTtdYWLlz4lJ8dcAtlb29vNm/e\nnC1btmRwcDDr169PX1/ffsfcfffdueKKK3Leeedlzpw5w++fdtpp+fa3v50dO3Zkx44d+fa3vz28\nvZKxK07pS+7ZlPKhB6suBQAAaKIDduBaWlpyzjnn5OKLL069Xs/y5cuzaNGirF27Nr29venr68vV\nV1+dXbt25bLLLksylETf9ra3ZdasWXn5y1+e888/P0ny27/925k1a9bEntE0UJzSl/KTV6f8zjdS\nvGDlgb8AAAAcFIqyLMuqi/h59913X9UlPMFkasWXZZn6ea9Ijn1GWl779qrLYQJMpvXGwc1ao1ms\nNZrJeqNZJuUWSiafoiiGtlHe/q2Ug4NVlwMAADSJADdFFaf0Jbt2JptuP/DBAADAQUGAm6pOfFbS\n0prytluqrgQAAGgSAW6KKto7kuOfmfK2b1RdCgAA0CQC3BRWnNqXbP5xygd+WnUpAABAEwhwU1hx\nynOTxDZKAACYJgS4KayYvzA5/AjbKAEAYJoQ4Ka44pS+5Pu3ptwy+WbnAQAAjSXATXHFi16WtLen\nftm7Uj7YX3U5AADABBLgprji8IWpvenC5JGHU7/8XSkf3l51SQAAwAQR4A4CxdMXp/aGP0+23p/6\n31yYcuejVZcEAABMAAHuIFGccHJqf/K25N67U//7v0j52O6qSwIAABpMgDuIFKc+N8Ur3pzceXvq\n//uvUg4OVl0SAADQQALcQab2S7+a4g9fm9x2S8p/vDxl/fGqSwIAABqkteoCaLzar56R+qOPpPzE\nVUnHockfvTZFUVRdFgAAME4C3EGq9usvT33njpSf+3hy2NwUZ/5B1SUBAADjZAvlQaz4rf+eYunp\nKT/3MTPiAADgICDAHcSKokjxm3+Q1Ospv/BvVZcDAACMkwB3kCvmLUjR94KUX7ou5aOPVF0OAAAw\nDgLcNFC85Kxk186UX76u6lIAAIBxEOCmgeLo3uTEZ6X8wqdS7tlTdTkAAMAYCXDTRO0lZyXbBlJ+\n/UtVlwIAAIyRADddnHRacuQxKa+/NmW9XnU1AADAGAhw00RRFCnOOCvZ/OPktluqLgcAABgDAW4a\nKZ7zgqRrXurXf6LqUgAAgDEQ4KaRorU1xa+dmdx5e8offr/qcgAAgFES4KaZ4oW/lsycpQsHAABT\nkAA3zRTtHSlOf2my8Wspf/qTqssBAABGQYCbhooVL0taWlN+/pNVlwIAAIyCADcNFYd1pli2IuX6\nG1M+9GDV5QAAACMkwE1TxYtXJY8Pprzx01WXAgAAjJAAN00V8xcmz16a8oufTblrZ9XlAAAAIyDA\nTWO1l5yVPPpIyo/+Y8qdj1ZdDgAAcAAC3DRWHHtCil95ScovX5/6O16V+g3Xpnxsd9VlAQAAT0GA\nm+Zqf/z61N5xaXJUb8qPXpn6O/8k9S9fn/Lxx6suDQAA+DkCHCmOWZyWc9+T2p+9N+nqSflPH0z9\nXa9P/etfTlmvV10eAACwlwDHsOIZp6b29ven9vp3JjNmpLziA6m/99yUWzZXXRoAABABjp9TFEWK\n034ptXf9dYpXviXpfyD1NZfZUgkAAJOAAMeTKmotqS09PcUf/kly1x0pb7i26pIAAGDaE+D4hYrn\n/nLynGUp/+2fU977o6rLAQCAaU2A4xcqiiK1P3xt0nFo6lf+dcrBPVWXBAAA05YAxwEVs+ek9sev\nT/7zrpSf+WjV5QAAwLQlwDEixbOXpli6POVnr0l5z6aqywEAgGlJgGPEit97VXLY3NTXXJ5yz2NV\nlwMAANOOAMeIFYfOSu1/vDHZ/OOU//rPVZcDAADTjgDHqBQnPyfFL7845Q3Xptz0varLAQCAaUWA\nY9SK3z0n6Zo3dFfK3buqLgcAAKYNAY5RK9pnpvY//zTZsjnlJz5SdTkAADBtCHCMSfGMU1Os+G8p\nb/x0yu9+q+pyAABgWhDgGLPit/57csSi1P/x8pTbt1VdDgAAHPQEOMasaGtL7dVvTR59JPX/729T\nlmXVJQEAwEFNgGNciiOfnuJ3XpHcdkvKGz9ddTkAAHBQE+AYt2L5y5JnPS/lx65M+eO7qy4HAAAO\nWgIc41YURWr/40+TQw9L/cOXpNy9u+qSAADgoCTA0RDF7MNSe+W5yf0/SXnN/6m6HAAAOCgJcDRM\nceKzUrz4t1J++fqU31xfdTkAAHDQaR3JQRs3bsyVV16Zer2eFStWZNWqVft9fvvtt+eqq67KPffc\nkze/+c1ZunTp8Gdnn312jjrqqCRJT09P3va2tzWwfCabYtUfpvz+ralf9fepPX1xiq55VZcEAAAH\njQMGuHq9njVr1uSCCy5Id3d3zj///PT19eXII48cPqanpyeve93r8qlPfeoJ3z/kkENyySWXNLZq\nJq2idUZqr/5/Un/PuamvuSy1P3tvilpL1WUBAMBB4YBbKDdt2pQFCxZk/vz5aW1tzbJly7Jhw4b9\njjn88MNz9NFHpyiKCSuUqaM4fGGKP3hN8oPvprz6/015y1dT/uC7KX/6k5Q7HzUvDgAAxuiAHbiB\ngYF0d3cPv+7u7s6dd9454j+wZ8+evP3tb09LS0vOPPPMPO95zxtbpUwpxfOXJ3d+N+VXbkj5lRv2\n//CQQ5LZc5M5nSlesDK1X3lJNUUCAMAUM6Jr4MbjQx/6ULq6unL//ffnPe95T4466qgsWLBgv2PW\nrVuXdevWJUlWr16dnp6eiS5r1FpbWydlXZNZ+ZYLU77yTXn8oQdTf7A/9W39qW97cOjxwf4M/vju\nDP7TBzOzvT0zX/ryqsudVKw3msVao1msNZrJeqNZqlhrBwxwXV1d6e/vH37d39+frq6uEf+BfcfO\nnz8/J510Un70ox89IcCtXLkyK1euHH69devWEf9+s/T09EzKuqaEQ+cM/Tvy2P3eLgcHk3/4qzx8\nxaXZsXt3ar/84ooKnHysN5rFWqNZrDWayXqjWSZqrS1cuPApPzvgNXC9vb3ZvHlztmzZksHBwaxf\nvz59fX0j+sM7duzInj17kiTbt2/PHXfcsd/NT5jeitbW1F59XnLykpT/9MHUb/5i1SUBAMCkdsAO\nXEtLS84555xcfPHFqdfrWb58eRYtWpS1a9emt7c3fX192bRpUz7wgQ/kkUceyTe+8Y1cc801ueyy\ny/KTn/wkH/7wh1Or1VKv17Nq1SoBjv0UM2ak9trzU//b96S88q9TzpiR4jkvqLosAACYlIpyEt4S\n8L777qu6hCfQip9Y5a6dqf/NhcndP0jtteeneNb0vtmN9UazWGs0i7VGM1lvNMuk3EIJzVC0d6T2\np+9OFh2b+v9enfK736q6JAAAmHQEOCaNomNmam++MFmwKPUPXpzyjtuqLgkAACYVAY5JpTh0dmpv\neU/SMz/1v/uLlHfeXnVJAAAwaQhwTDrF7Dmp/dl7k7ndqf/NhTpxAACwlwDHpFTM6Uztre9Luual\n/rcXpbzdNXEAACDAMWkNh7jDn5b637035W23VF0SAABUSoBjUitmz0nt/3lv8rSjU//g+1J+6+aq\nSwIAgMoIcEx6wzc2Obo39X/4q9Q3fLXqkgAAoBICHFNCMXNWaudelBxzQsorPpD6zf9edUkAANB0\nAhxTRtG+d07c8c9M+Y9/nfpXbqi6JAAAaCoBjimlaGtP7Y3vSk48LeVH/j6PX/Sm1K/7eMqBB6ou\nDQAAJlxr1QXAaBVtbam94YKUX7k+5c1fTPnxq1J+4iPJ4mem+KVfTfGcF6Q4dFbVZQIAQMMJcExJ\nxYwZKV70G8mLfiPlls0pv/6llF/7Usp/+mDKf/6H5JTnpOh7YYpnnJpiTmfV5QIAQEMIcEx5xeFH\npPiN30v5srOT//xhypu/lHLDV1Ju/FrKJFnwtBTHn5wcf3KKE05OMbe76pIBAGBMBDgOGkVRJEcf\nl+Lo41L+zv9M7rkr5Q++k/KO21Ju+Ery5euHAt3hR6Q44ZQUS56fPHPJ0PcAAGAKEOA4KBW1luSY\nxSmOWZy85LdS1h9Pfnx3yju+MxTqvnFTyq/ckBxzfGq/+fuCHAAAU4IAx7RQ1FqGu3N58aqUg3tS\nrr8x5WeuSf1vLkqOPSG13/yD5KTTBDkAACYtAY5pqWidkeJXXpJy2YtS3vSFlJ+9JvW/fnfS+4yh\nIHfiswQ5AAAmHQGOaa1onZHiV89IuWxFypvWpfzsR1O//F3JcSelWPaiFCecksxbIMwBADApCHCQ\nvWMJTv/1lC9YmfKrn0953cdTfuTvh2560tWT4vhTkmecMnTzk575VZcLAMA0JcDBf1HMmJFi+UtT\nnv7ryU/vTXnHbcn3b0v53W8mN//7UKDrPjzFic9KccbLU8xfWHXJAABMIwIcPImiKJIjFqU4YlFy\n+ktTlmVy338OjSS447aUG76a8uYvpvj1307x6y9PMeOQqksGAGAaEOBgBIqiSJ52dIqnHZ286DdS\nbutPec0/pvzUv6T82pdS+8PXpDjp2VWXCQDAQa5WdQEwFRVzu1N79VtTO/eiJGXql7879Q9fknLb\nQNWlAQBwEBPgYByKk56d2oV/l+K//X7Kb92c+rtel/qNnx4aHA4AAA0mwME4FTMOSe03fz+1i/4u\nOeaElP/y4dT/4i2pb/hKyscFOQAAGsc1cNAgxeELU3vzhSlvuSnlJ69O+eFLUnYfnmLlb6Z44coU\n7TOrLhEAgClOgIMGKooixXNfmPI5y5Jbv576DZ9Mufb/pPy3f0nxq2ekeNFvpOjs/oW/UZZlk6oF\nAGCqEeBgAhS1WnLa0rSctjTl3T9IecMnU15/bcrP/2uK5/1y8oxTk4cfSrZvS7ZvS7n3Mdu3JY88\nnIdesCLl7/6vFG3tVZ8KAACTiAAHE6w45vgUrzkv5QM/TfmFT6X86ueT//j3oQ8POSSZPTc5bO7Q\ngPBjjk+S7PrK55O7fpDa696RYt6CCqsHAGAyEeCgSYp5C1L83qtSnvmHycPbhkJbW8fQjLmfM/v0\nM7Lt0j9P/b1vSe01bzVjDgCAJO5CCU1XdMxMcfjCFO0znzS8JUnbkqWpvfOypLM79b++KPXrPu7a\nOAAABDiYrIrDj0jt7e9P8ZxlKT9+1dBdLXfvqrosAAAqZAslTGJFe0fy6rcmR/em/MQ/pdz846Hr\n4g4/YsS/Ue55LLn7Byl/8N2Ud343xdzuFL//6qHfBgBgShHgYJIriiLFGS9PuejY1D98ServPTc5\n9oQUXfOSzp6kq2doNEHnvKSrJ0mR3PX9ocD2g+8kd92RDO4Z+rGFR6X83q0pf3JPam/88xRzOis9\nNwAARkeAgymieOazU7vgspTX/lPKLZtT/vjuobEDSfa7Oq6oJWV96PGoY1Msf2mK45+ZLH5mikNn\np7x1Q+r/8P7U//Ktqb3pwhRHHFnJ+QAAMHoCHEwhxbwFKV791uHX5Z49ybb+ZGBryge3Jg9uTR7b\nneLYZyS9z0gx89An/sapz03tre9L/W/fk/rq81J7wwUpFp/UzNMAAGCMBDiYwooZM5J5C5J5C/Lk\n97N8iu89fXFq51+S+t9clPplf57aK89N0ffCiSoTAIAGcRdKmKaKeQtSe/tfJUf3Dl1b9/l//YXH\nl2WZ8pEdQ/9270o5OGi0AQBAk+nAwTRWzDostbf8ReprLk95zZrU+7ekePGq5IGfptyyOXlgc7Ll\npykf2Jw88NNk56M/9wNF0tKatO7913FoiuNPTnHykuSk01LMnFXNiQEAHKQEOJjmikPaUnvNW1N+\n9MqU6/4t5Rc+9bMPW1qTnvlDWzSPOynpPjyp1ZLHB5PBff/2DD0+viflQw+m/NZ/pLxp3dBxx56Q\n4plLhgLdUb0papr+AADjIcABKWotKc7+XykXPzPlww+lmLcgOfyIoREFtZZR/Vb5+OPJ3Xek/M43\nh/796/9N+a//N5k9J8VzlqU48w9TzDpsgs4EAODgJsABw4olzx/VzVCe9DdaWpLjThrq2K36o5Tb\nt6W8fWNy2zdSfuWGlLfclOJ3X5li6ekpivH+NQCA6cV+JmBCFYfNTW3p6am96s9Su+Dy5PAjUv7j\n5alf/q6UW+6rujwAgClFgAOapjjy6am97a9S/MGfJD+6M/UL/zT1z1yTcnDPhP3Ncs+elPdsSnnH\ndyb07wAANIMtlEBTFbVaiuUvTfnsX0r9X65I+cmrU274Smp/9LoUx504rt8uBweT+/4z5T2bkh9t\nGnq890dDN11Jko6ZKZ65JHnWc1Oc0pfi0NnjPyEAgCYS4IBKFHO70/Lat6f89tdT/+f/nfpfvS05\n5vikszvFnK5k7tC/4eeHzU127Uy2b0se3pbyoW1Dz7dvS7l9WzLwwFBY29dl6zg0efpxKX7tzBRH\n9yYtrSlvuyXlrRuSW76aslZLjjsxxbOel+LU56VY8LRK/3sAAIyEAAdUqnjW81I74ZSUn/t4yrvv\nSDbfm/L7tyaPPpIkOeCo8ENnD4W7uV0pXvSy5OjjUjz9uGTeEU+4SUrx7KUp6/Xknk0pN3495a1f\nHxqf8NErkyMWDd0l8zkvSJ52tBusAACTkgAHVK5o70jxW3+033vl7t3JQwPJtoGUDz2YbH8wae9I\ncdjcocA2e25y2JwUrTNG97dqteSY41Mcc3zyW3+Usn/LUJj71n+k/MxHU356bbLgaSmWvCDFc5Yl\ni44R5gCASUOAAyaloq1taBbd4UeMe7TBL/w73YenWPEbyYrfSLn9wZTf+lrKb9yU8nMfS/nZa4b+\n/pJlyYIjU3R0JO0dSfvMn3tsH/W8PACAsRDgAPYqDutM8atnJL96RsqHH0q58Wspb7kp5Q3XJvX6\nU2/nLIqhrZyz5ySHzU0xe87e53OS2XNTdB+enHByihmHNPN0AICDkAAH8CSK2XNS/PKLk19+ccpd\nO5OHH0p270x27kx27Uy569Fk56NDN1bZ+UiyY3vK7Q8N3WDl3ruT7Q8lj+5Isvc6vraOFCcvSZ69\nNMWpz03RMfMp/3ZZlkn/lpQ//H7yozuTltake16KrnlJ17yke17ScaitnQAwDQlwAAdQtO/dOvlf\n3xvB98rBPcmO7cmPf5Ry480pv3Vz8o2bUra0Jic+K8Wzl6Y47XlDd8y8Z1PKH96R8offS+66I3no\nwaEfOeSQpF5PBgf37wC2dwyFuXkLUlvx31Kc+KxGnS4AMIkVZVke8CZvzXbfffdVXcIT9PT0ZOvW\nrVWXwTRhvR2cyvrjyQ/vGLphyrduTrbeP7T9stbys1l18xak6H1Gcuwzhh6fdvTQMQ8/NDQqYeCB\nlP1bkoGtKfsfGOrQbetPTlua2u+8IsXhR4yqJmuNZrHWaCbrjWaZqLW2cOHCp/xMBw6gSYpaS7L4\npBSLT0r5O+ck9/5oKMjteSxF7wlDoe2wuU/+5TmdQ/+OOX6/7l+557GUn//XlJ/9aOrvfn2KFb+Z\n4mW/+wu3aCYZGqdw793Z9YNbUx8YGAqQg4NDc/T+6+Pj+14PDh2zZ0/y+GDKwcGhoey//vIUR/U2\n7j8SAPALjSjAbdy4MVdeeWXq9XpWrFiRVatW7ff57bffnquuuir33HNP3vzmN2fp0qXDn33xi1/M\nJz7xiSTJWWedldNPP71x1QNMUUVRDI0oWHTM+H5nxiEpXvo7KZe9KOUn/inl9Z9I+R83pvitP06x\nbMXQ2IS9yv4HUt7+reR73075vW8nO7bnoQP9gZbWpHVG0to69K/lZ4/l9m0pb/tGaq9/hy2cANAk\nBwxw9Xo9a9asyQUXXJDu7u6cf/756evry5FHHjl8TE9PT173utflU5/61H7f3bFjRz72sY9l9erV\nSZK3v/3t6evry6xZsxp8GgDTWzG3O8U5b065/GWpr70i5VV/l/LfPzs03PyeTSlv/3Zy/0+GDp7T\nOXRDlRNPS+epS7JtxyN7A9p/CWqtM5KW1l94o5Tywf7U/+bC1P/2ohTnvCW1575w3OdR1h8f2lp6\n338ms+YMdRxbjGgAgH0OGOA2bdqUBQsW5P9v7+6joyzv/I+/r3nMAyHJTAiBBAgJwQLhQQShVEWE\ntbXq1tpt+yu1Z939ba1yVttu2y11T3tsz/bU1lJYXVy6rQf9/XTbumdXtnZ160EqWpEKhiCCQqIQ\nDE8hmQTyPEnua/+4QiLyFCDMMMnndc597nsy98xcyfmea/hyPXxHjx4NwIIFC9iyZctJCVx+fj7A\nKV/0lZWVzJgxoy9hmzFjBpWVlVxzzcV/yYuIyKnMxDJ83/4x9vWXsf/xBPbxhyEUhsnlmIWfwEyd\nBWPH9/XXwbw8zAXO3Te5UXzf+hHeP/8j9hcP4TU34bvhlgG91loLTTE4UIM9WAMH9mMP1MCh/RCP\n99+YkYmZMgvKZ2OmzcbkRi+orSIiIkPFORO4WCxGNNr/hRmNRqmqqhrQm3/4tZFIhFgsdgHNFBGR\ngTLGYOYtxM6aD4ffh7ETMMHgpfmszBH4vv59vH99CPurf8U73oT51BfPOHJn453YP23EvvgsHKjp\nfyI7AoXjMdfd5M5jxkFjPfatCuzOCrd7J0DhBEz5VZhpV0Ju1CWnoTAEwxAMnjRlVEREZCi6LDYx\nWb9+PevXrwfgwQcfJC8vL8ktOlUgELgs2yVDk+JNBk1h4VmfHqxYs9/9Kc1rHqL9v58mrbOdrLu/\nhfH3f8X01NfR9j//SfsL/4VtPkaguIy0v/4qwYmTCYwvwTcy+/Rv/InbsNbSvf894hWbiW/bTPzF\n32J//5+nvz8UxoTC+HIiZP3frxKeNe+ifzcZHOrXJJEUb5IoyYi1cyZwkUiEhoaGvscNDQ1EIpEB\nvXkkEmHXrl19j2OxGFOnTj3lviVLlrBkyZK+x5fjtq/ajlYSSfEmiTKYsWY/9zeYcDrt//007fV1\n+L78TXh/L/bFZ7FvvOoqms+6Gt/iP8ebPI12Y2gHiHfBudqQmQ3Xfhyu/Ti+jnaofhvb2gzxTjfl\nMt7Zf3R10vPODpp+8HeYmz+PufXzbgdQSSr1a5JIijdJlMuyjEBpaSmHDh2irq6OSCTCpk2buO++\n+wb0wbNmzeJXv/oVLS0tAGzfvp2lS5cOsNkiIpJKjDGY2+7Ay8rB/uYXeN/+a2hphvQMzOJbMYtu\nxowquPjPSUt3a+LOco/t7MA+tQb7u19jq3fh+/I3MCNzL/qzRUREkm1AhbwrKip44okn8DyPRYsW\ncfvtt/Ob3/yG0tJS5syZQ3V1NT/96U9pbW0lGAySk5PDz372MwA2bNjAM888A7gyAosWLTpno1TI\nW4Y7xZskyqWKNW/LH7Ev/hZz9XWYBTdg0s5el+5S8V5dj31qDWRk4vvyNzFXTE9KO0T9miSW4k0S\nJRkjcANK4BJNCZwMd4o3SZThEGu2dh/ez38MRw5hPrUUc9NfaLOTJBgOsSaXD8WbJEoyEjh9g4mI\nyJBmiorx/cMKzJyPYdc9iffID7DNx5PdLBERkQtyWexCKSIicimZtAz48jdhcrlbn/edv4EJpZgJ\nk2DCJMzEMhg15qyFy0VERC4HSuBERGRYMMZgrr8JW/oR7CsvYGuqsS89D11xV2MuI9Mlc8WTMGXT\n4IrpmFB4UNtga/dC3SGYeqXbjEVEROQ8KYETEZFhxYybiFn6FQBsdzcc3I+tqYZ9Vdh91dgX1mGf\n/w8IheAjMzHT52BmzMFERl3Q59m2FuzrL2P/uB5qqt0Pw2mY2QswH1sMZdO0Jk9ERAZMCZyIiAxb\nJhCA8SWY8SVw7Y0A2Hgn7NmJ3bEV++YWdzwFFBX3JXOMGQ8ZmWeccmk9D3bvwL66HlvxGnTFoWgi\n5v/chRk7DrvlFXe8tgGi+ZiPLnJH/pkXrVtrobMDrAWfH/w+8PmV/ImIDDPahXKAtJuRJJLiTRJF\nsXZ21lo4XIt9cyt2x1ao3gU9Pe7JQABG5sDIXBiZgzlxjcW+/jLUH4H0TMy8hZhrlsD40pMSPtvZ\nia3cjN20Ad6udInZpCluXV5bC7a1BVqb4cS5rRV6uk9tpDEuofP5IC0d8+dLMQs/cdmt51OsSSIp\n3iRRLstC3iIiIsOVMQbGjMOMGQcf/zS2rQXe2YFtqIPjTXCsEXu8ERrr3TTM48fAem793Ke+iJn9\n0TOuozPhMGbeQpi3ENvYgN38Eva1DdhX10NmFmSOgMwsTE6093EmZIxwyZrX4xJJz/vAdY+bAvrU\nv7hE845lWmcnIjIEKYETEREZIJMxAmZ/lDONbVmvB+Lx806cTG4Uc9Nn4KbPXFT7rOdhn/t37G//\nDbv/PXz3LHfJp4iIDBmaOC8iIjJIjM+f1FEv4/Phu+Xz+L72fWg5jvfDb+D9aWPS2iMiIoNPCZyI\niMgQY6bOwvfdVTCuBPvLFXhP/Qu2qyvZzRIRkUGgKZQiIiJDkMmN4vvGP2Kf+f/YF57B7q3Cd9e3\nIDfPrZ2zPeBZt2bP89zZ54e0DLc7p4iIXJbUQ4uIiAxRJhDAfPavsJOm4K39J7x/+MrAXhgKQXom\npGVAev9hxk3EzLgaxk1MyC6X1lpoa3EbxmTnujWIIiLDnBI4ERGRIc5cOR9fUTF2yyv9deR8PncY\n03/d3Q0dbdDeDu2tgJZjsAAAEGVJREFU0NGObW+F9jZobMBu24z9r3+D3DxX3Hzm1W7HzTPstHku\ntq0VjhzAHqp1ZRdO7Op5rPc43ujaBOD3w+RyzMyr3ZE3ehD/QiIiqUMJnIiIyDBgRhVgPvnZi3oP\ne7wRu+MN7PbXXdmDjf8DoTBMmYkpvwqTld2fDPp8YPqvOzMy8KrednX1DtXC4QNwLHbyB2Rlu9p6\n2bmYgkJXVy871/38YA228nXsr3+B/fUvXGH1WfNcEjlh0mVX905E5FJRIe8BUkFISSTFmySKYk0u\nlO3qgt07sG++jt2+BWJHB/bCjExXW6+gEAqKMAVFUFAEefmYQPDcn3vkIHb7n7DbX4eqt93avewI\n5ERccfVA8KSz8QchGAB/EIK9z5342Yl70zMx0XzIG+1GF/3+i/zrSLKpb5NEUSFvERERSQkmGITy\n2Zjy2dgvfAWOHobOjv5NUT54WI/sSJRj4QzIyr6o0TIzeizmxk/DjZ/GthzHvrkVdm5zUz27u9zR\n1gk93dDVhe3p7v15d//zXd2unR/Q97/Zfj9ERkHeaDdNM2+0S/JaW6C1GVpbsK3Nfde0tbo1gtm5\nkBPBnEgms3MxORHIynHv++Hi6yfOwbAbTczU+j4RGRglcCIiInJRjDGQP+as94Ty8jCD/L/UZsRI\nzIIbYMEN5/1a6/W4RK67y22UUn8EW3/EJaK917byT9B8zL3A54PMLMgc4c7ZEUzhBLfZS3sb9lgM\njhzE7tnpkjs+kBQORN5oGF+CGV+KGV/qrrNzz/v36vv92lqhphrGFGFyohf8Pie9Z3c3vPMmtmIT\ndmcFZsoszBfvccm8iCSMEjgREREZdozPD2E/hMMuKRtVwOnGBW1Hu9v4JS19wCOHtisOTTG3xq/5\nuNsoxu93awL9/pM3kWlvw77/Hux/D7v/XWzFa/2JX3YEiidhJk7GlH4EisvOWCjeWgsHatwaxbe2\nwrvvuBE/cKOJk6ZC2RR3LijC+AZWCth2xd0IZ8UmN221rRXC6VAyGfvqemz9EXzLvqMdQkUSSAmc\niIiIyBmcKWE662uCIRhV4I6B3F8+u+/atrVC7V7s/neh5j3svj1u0xhwCWDhBEzpFVByBWZCGdQd\nxO7Yin2rAhp7RzjHTcR8/HbMpKnYw7XY6l3YnRWw+Q/ufUZkQekUzLgSl1CCSzJPapSB2n1uimpn\nO2RkYmbOw1y1AKbOwgRDeJv/gH38EbwHv43vqw9goqPO+28lIudPm5gMkBbDSiIp3iRRFGuSKIq1\nC2dbm2HvHuy7u7Hv7Ya9e1yZhxPSM1xSVX4Vpnz2aadMWmuh7hC2ehdU7cJWvw1HDpz9g7Oy3U6f\nV33MlYs4TYF3+86beI/+CEJhfPd9DzO+5GJ/3UGheJNEScYmJkrgBkgdgSSS4k0SRbEmiaJYGzzW\n81w5hn3VbvfM0o+cNrk65/ucmGLZ909Be9IJv39AUy3tgRq8h78Pra347v72SSOKyaJ4k0RJRgI3\nsAnQIiIiInJZMD4fZux4fAtuwFxRfkHJG4Dx+90RCPQeQXcEe48BrpMzhRPwfechyC/Ae+QHeK+8\ncEHtEZGB0Ro4EREREbkoJieK7+9/hLfmx9j/98949XWY+ddDKNRbfy8EwZCrzde73s52dfWWY+g9\nWpr7SzQEgm465vgSTFrGWT/bWut2Dt29A3a/hd1XRdOkK7DTroLyqy5oHaPI5UwJnIiIiIhcNJOW\nge9vv4t98lHsc09jn3v69DcGQ26TlHjnWd/PbdxiYPRYV1phQilmwiQYVwItx13Ctuct7O63+jdw\nycqG4jLib76B/eOL7rOmzcZc9VHMjLln3C3TdsWh4Sg01GGbj7k6gj29tfv6rnvP2TluA5nCCRc8\n+ilyMRR1IiIiIjIoTCAAf3kvZt5C7PGm3sLpcej60Nl6kDGit67eSMyIrL5rRmS5nS9r3sXWVGNr\n3sVW7YLXXz61tl5WNuaK6XBFuTsXFGGMIZqbS/3mV1z5g4rXsJWbsf4ATJmJmTLDFWGvr8M2HIH6\nOlfy4TxYgEDQ7fhZPMmVeJhQBmMKXYmKBLHWQqwe9lVha/dCZBRm2pWYiHYEHcq0ickAaTGsJJLi\nTRJFsSaJoliTi2WPN8H+d7H733NlDT6QsH3YB+PNep7bxbPiNWzFJqg/4son5OZBNB+Tlw/R0f3X\nI3PAH3BHwN97faJ+nx9iR7E11S5p2lcFNe9CZ4f74HAajBgJobA7gqG+axMKu7qDobCrpZeW5u4P\np2NOXIfSeqedhnvPof7HgQAcb4J91dh9VX1t6Cs2/0EFRS6Rm3YlTC7HhNNO/lv29MCRA9jafa5s\nRW0NtDZjyqa510ya4sphyDlpF8peSuBkuFO8SaIo1iRRFGuSSGeKN2utS3gyszD+wRkps14PHD7g\nkrn970FrC7arE+JxN030pCMO8Q7o6HCjkOfD+PpfY3wwpghTXOaKvRe7KZ0cPYzduQ27axvs2elG\nOwMBmDQVM2kKxOpd0nZwvxsdBZecFhS5chR7q9xU0VAIyqZhpvYmgWPHD7iQ/XCjBK6XEjgZ7hRv\nkiiKNUkUxZok0uUeb9Zal1x1dkBHuzt3dripo/G4W5MXj8OJRPDE48wRmOKy3s1dzr45i413QvUu\n7M5KV8j9QI0bXSyaiCkqhqJizLhiN1oXCLrXdLS7dYW7KrE7t8HhWvdm2REYVwzBkBuZCwbd6GDf\nEewfWQynuRG/E0daursn3gkdbdDeju1og/beo6PNPWeMG+H0+12C6veDz+d+lpGJGT0WCgohK+ey\nSiaTkcBpDZyIiIiISAIZY/qnWWZln/r8YHxGKAxTr8RMvRI++1fYrvg5p0WatHSYMRczYy4ANnYU\nu6sSdlVi6w5Bd5fbPbT7Q+sae7pPep/zHh0KhcCzboTxRH3CD+l7z/RMKCjEjC5054JClyh2dbnE\nt7vrQ23rghEjMbl5kBOF3Kh7PMAyGZcjJXAiIiIiIkPchaxpM5FRmGv+DK75s7PeZ72e3hG2D4wk\ndnb2njuwnZ2YcBjSMtyIXHqGu07PcCN2H0qmrOeB54HX484tx9001SMH+s529w7Y/IcBJ4sn3ecP\nQE4EcvMwuVHMZ/4SE80/r79NMimBExERERGRC2Z8/t7k7PQ1+853RNH4fG765IlUJS0d8kZjymef\ndJ/t7IAjB91o24mpnIHgB65D4Pe5BLCxARobsI0N0NQAjfXYphi2phpjUms0TgmciIiIiIikHBNO\ng/El574xJ+qOiYMzPTXZUivdFBERERERGcaUwImIiIiIiKQIJXAiIiIiIiIpQgmciIiIiIhIilAC\nJyIiIiIikiKUwImIiIiIiKQIJXAiIiIiIiIpQgmciIiIiIhIilACJyIiIiIikiKUwImIiIiIiKQI\nJXAiIiIiIiIpQgmciIiIiIhIilACJyIiIiIikiKUwImIiIiIiKQIJXAiIiIiIiIpQgmciIiIiIhI\nilACJyIiIiIikiKUwImIiIiIiKQIJXAiIiIiIiIpQgmciIiIiIhIilACJyIiIiIikiKUwImIiIiI\niKQIJXAiIiIiIiIpwlhrbbIbISIiIiIiIuemEbgBWr58ebKbIMOI4k0SRbEmiaJYk0RSvEmiJCPW\nlMCJiIiIiIikCCVwIiIiIiIiKcL/wAMPPJDsRqSKkpKSZDdBhhHFmySKYk0SRbEmiaR4k0RJdKxp\nExMREREREZEUoSmUIiIiIiIiKSKQ7AakgsrKStauXYvneSxevJjbbrst2U2SIaK+vp7Vq1fT1NSE\nMYYlS5bwyU9+kpaWFlauXMnRo0cZNWoUX//61xkxYkSymytDgOd5LF++nEgkwvLly6mrq2PVqlU0\nNzdTUlLCvffeSyCgrwa5eK2traxZs4b3338fYwz33HMPY8eOVd8mg+53v/sdGzZswBjDuHHjWLZs\nGU1NTerbZFA8+uijVFRUkJ2dzYoVKwDO+O80ay1r165l27ZthMNhli1bdkmmV2oE7hw8z+Oxxx7j\n/vvvZ+XKlbz66qvU1tYmu1kyRPj9fr70pS+xcuVKfvjDH/L73/+e2tpa1q1bx/Tp03n44YeZPn06\n69atS3ZTZYh47rnnKCws7Hv85JNPcvPNN/PII4+QmZnJhg0bktg6GUrWrl3LrFmzWLVqFQ899BCF\nhYXq22TQxWIxnn/+eR588EFWrFiB53ls2rRJfZsMmuuvv57777//pJ+dqS/btm0bhw8f5uGHH+au\nu+7il7/85SVpkxK4c6iurqagoIDRo0cTCARYsGABW7ZsSXazZIjIzc3t+5+Z9PR0CgsLicVibNmy\nhYULFwKwcOFCxZwMioaGBioqKli8eDEA1lp27tzJ/PnzAfclpViTwdDW1sbbb7/NDTfcAEAgECAz\nM1N9m1wSnucRj8fp6ekhHo+Tk5Ojvk0GzdSpU0+ZKXCmvmzr1q1cd911GGOYPHkyra2tNDY2Dnqb\nNJZ8DrFYjGg02vc4Go1SVVWVxBbJUFVXV8fevXuZNGkSx44dIzc3F4CcnByOHTuW5NbJUPD4449z\nxx130N7eDkBzczMZGRn4/X4AIpEIsVgsmU2UIaKuro6RI0fy6KOPUlNTQ0lJCXfeeaf6Nhl0kUiE\nW2+9lXvuuYdQKMTMmTMpKSlR3yaX1Jn6slgsRl5eXt990WiUWCzWd+9g0QicyGWgo6ODFStWcOed\nd5KRkXHSc8YYjDFJapkMFW+88QbZ2dnaVlsSoqenh71793LjjTfyk5/8hHA4fMp0SfVtMhhaWlrY\nsmULq1ev5uc//zkdHR1UVlYmu1kyjCSjL9MI3DlEIhEaGhr6Hjc0NBCJRJLYIhlquru7WbFiBdde\ney3z5s0DIDs7m8bGRnJzc2lsbGTkyJFJbqWkut27d7N161a2bdtGPB6nvb2dxx9/nLa2Nnp6evD7\n/cRiMfVvMiii0SjRaJSysjIA5s+fz7p169S3yaDbsWMH+fn5fbE0b948du/erb5NLqkz9WWRSIT6\n+vq++y5V3qARuHMoLS3l0KFD1NXV0d3dzaZNm5gzZ06ymyVDhLWWNWvWUFhYyC233NL38zlz5rBx\n40YANm7cyNy5c5PVRBkili5dypo1a1i9ejVf+9rXKC8v57777mPatGls3rwZgJdeekn9mwyKnJwc\notEoBw8eBNw/souKitS3yaDLy8ujqqqKzs5OrLV9saa+TS6lM/Vlc+bM4eWXX8Zay549e8jIyBj0\n6ZOgQt4DUlFRwRNPPIHneSxatIjbb7892U2SIeKdd97he9/7HuPHj+8bfv/CF75AWVkZK1eupL6+\nXltty6DbuXMnzz77LMuXL+fIkSOsWrWKlpYWJk6cyL333kswGEx2E2UI2LdvH2vWrKG7u5v8/HyW\nLVuGtVZ9mwy6p59+mk2bNuH3+ykuLubuu+8mFoupb5NBsWrVKnbt2kVzczPZ2dl87nOfY+7cuaft\ny6y1PPbYY2zfvp1QKMSyZcsoLS0d9DYpgRMREREREUkRmkIpIiIiIiKSIpTAiYiIiIiIpAglcCIi\nIiIiIilCCZyIiIiIiEiKUAInIiIiIiKSIpTAiYiIiIiIpAglcCIiIiIiIilCCZyIiIiIiEiK+F/T\nnm6vRU92DwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnBHIeohWRK2",
        "colab_type": "code",
        "outputId": "747291d1-12f1-44b0-9285-3288c3d65de2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Instantiate the model\n",
        "model = CNN(shape=(WINDOW_SIZE, WINDOW_SIZE, 3))\n",
        "\n",
        "# Load the model\n",
        "model.load(\"Erick_dropout_0.2-098-0.972454.h5\")\n",
        "\n",
        "model.model.summary()\n",
        "\n",
        "# We add all test images to an array, used later for generating a submission\n",
        "image_filenames = []\n",
        "for i in range(1, 51):\n",
        "    image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
        "    image_filenames.append(image_filename)\n",
        "\n",
        "# Set-up submission filename\n",
        "submission_filename = \"asdfasdf12-2.csv\"\n",
        "\n",
        "# Generates the submission\n",
        "generate_submission(model, submission_filename, *image_filenames)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 64, 64, 64)        4864      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               1048704   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 258       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 2,603,010\n",
            "Trainable params: 2,603,010\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-889f0e093cbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Erick_dropout_0.2-098-0.972454.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-83751e11298a>\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;34m\"f1\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         }\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdependencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     81\u001b[0m                   (export_dir,\n\u001b[1;32m     82\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: Erick_dropout_0.2-098-0.972454.h5/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mWJzYo7iyFG",
        "colab_type": "code",
        "outputId": "56682ee5-e827-45a6-fb2c-2b87344661c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load the model\n",
        "model.load(\"Erick_dropout_0.2-010-0.931323.h5\") # cargar tu modelo con los mejores weights\n",
        "\n",
        "# Print a summary to make sure the correct model is used\n",
        "model.model.summary()\n",
        "\n",
        "# We add all test images to an array, used later for generating a submission\n",
        "image_filenames = []\n",
        "for i in range(1, 51):\n",
        "    image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
        "    image_filenames.append(image_filename)\n",
        "\n",
        "# Set-up submission filename\n",
        "submission_filename = \"Erick_dropout_0.2-010-0.931323.h5.csv\"\n",
        "\n",
        "# Generates the submission\n",
        "generate_submission(model, submission_filename, *image_filenames)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 64, 64, 64)        4864      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 8, 8, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)   (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               1048704   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 2)                 258       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 2,603,010\n",
            "Trainable params: 2,603,010\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Processing image => data/test_set_images/test_1/test_1.png\n",
            "Processing image => data/test_set_images/test_2/test_2.png\n",
            "Processing image => data/test_set_images/test_3/test_3.png\n",
            "Processing image => data/test_set_images/test_4/test_4.png\n",
            "Processing image => data/test_set_images/test_5/test_5.png\n",
            "Processing image => data/test_set_images/test_6/test_6.png\n",
            "Processing image => data/test_set_images/test_7/test_7.png\n",
            "Processing image => data/test_set_images/test_8/test_8.png\n",
            "Processing image => data/test_set_images/test_9/test_9.png\n",
            "Processing image => data/test_set_images/test_10/test_10.png\n",
            "Processing image => data/test_set_images/test_11/test_11.png\n",
            "Processing image => data/test_set_images/test_12/test_12.png\n",
            "Processing image => data/test_set_images/test_13/test_13.png\n",
            "Processing image => data/test_set_images/test_14/test_14.png\n",
            "Processing image => data/test_set_images/test_15/test_15.png\n",
            "Processing image => data/test_set_images/test_16/test_16.png\n",
            "Processing image => data/test_set_images/test_17/test_17.png\n",
            "Processing image => data/test_set_images/test_18/test_18.png\n",
            "Processing image => data/test_set_images/test_19/test_19.png\n",
            "Processing image => data/test_set_images/test_20/test_20.png\n",
            "Processing image => data/test_set_images/test_21/test_21.png\n",
            "Processing image => data/test_set_images/test_22/test_22.png\n",
            "Processing image => data/test_set_images/test_23/test_23.png\n",
            "Processing image => data/test_set_images/test_24/test_24.png\n",
            "Processing image => data/test_set_images/test_25/test_25.png\n",
            "Processing image => data/test_set_images/test_26/test_26.png\n",
            "Processing image => data/test_set_images/test_27/test_27.png\n",
            "Processing image => data/test_set_images/test_28/test_28.png\n",
            "Processing image => data/test_set_images/test_29/test_29.png\n",
            "Processing image => data/test_set_images/test_30/test_30.png\n",
            "Processing image => data/test_set_images/test_31/test_31.png\n",
            "Processing image => data/test_set_images/test_32/test_32.png\n",
            "Processing image => data/test_set_images/test_33/test_33.png\n",
            "Processing image => data/test_set_images/test_34/test_34.png\n",
            "Processing image => data/test_set_images/test_35/test_35.png\n",
            "Processing image => data/test_set_images/test_36/test_36.png\n",
            "Processing image => data/test_set_images/test_37/test_37.png\n",
            "Processing image => data/test_set_images/test_38/test_38.png\n",
            "Processing image => data/test_set_images/test_39/test_39.png\n",
            "Processing image => data/test_set_images/test_40/test_40.png\n",
            "Processing image => data/test_set_images/test_41/test_41.png\n",
            "Processing image => data/test_set_images/test_42/test_42.png\n",
            "Processing image => data/test_set_images/test_43/test_43.png\n",
            "Processing image => data/test_set_images/test_44/test_44.png\n",
            "Processing image => data/test_set_images/test_45/test_45.png\n",
            "Processing image => data/test_set_images/test_46/test_46.png\n",
            "Processing image => data/test_set_images/test_47/test_47.png\n",
            "Processing image => data/test_set_images/test_48/test_48.png\n",
            "Processing image => data/test_set_images/test_49/test_49.png\n",
            "Processing image => data/test_set_images/test_50/test_50.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEIRCxOelfqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}