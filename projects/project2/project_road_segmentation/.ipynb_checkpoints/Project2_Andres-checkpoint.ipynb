{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext lab_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13558312203158259327\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4937233203\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10434379150270298809\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Create some tensors\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as scipy\n",
    "import os\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend as K\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary for our model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    LeakyReLU,\n",
    ")\n",
    "from tensorflow.compat.v2.keras.layers import BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "# keras, model definition...\n",
    "cb = TQDMNotebookCallback()\n",
    "setattr(cb, \"on_train_batch_begin\", lambda x, y: None)\n",
    "setattr(cb, \"on_train_batch_end\", lambda x, y: None)\n",
    "\n",
    "# model.fit(X_train, Y_train, verbose=0, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 85 images\n",
      "Loading 85 groundtruth images\n"
     ]
    }
   ],
   "source": [
    "# Load a set of images\n",
    "root_dir = \"data/\"\n",
    "\n",
    "# Select the directory for the images and load them\n",
    "image_dir_train = root_dir + \"training/images/\"\n",
    "files = os.listdir(image_dir_train)\n",
    "n_train = len(files)\n",
    "\n",
    "print(\"Loading \" + str(n_train) + \" images\")\n",
    "imgs_train = np.asarray(\n",
    "    [load_image(image_dir_train + files[i]) for i in range(n_train)]\n",
    ")\n",
    "\n",
    "# Select the directory for groundtruth images and load them\n",
    "gt_dir_train = root_dir + \"training/groundtruth/\"\n",
    "print(\"Loading \" + str(n_train) + \" groundtruth images\")\n",
    "gt_imgs_train = np.asarray(\n",
    "    [load_image(gt_dir_train + files[i]) for i in range(n_train)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 400, 400, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 400, 400)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_imgs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 400\n",
    "\n",
    "# We separate the images from the groundtruth images\n",
    "img_patches_train = [\n",
    "    img_crop(imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "gt_patches_train = [\n",
    "    img_crop(gt_imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "\n",
    "# Linearize the list and labeling them X and Y\n",
    "X_train = np.asarray(\n",
    "    [\n",
    "        img_patches_train[i][j]\n",
    "        for i in range(len(img_patches_train))\n",
    "        for j in range(len(img_patches_train[i]))\n",
    "    ]\n",
    ")\n",
    "Y_train = np.asarray(\n",
    "    [\n",
    "        gt_patches_train[i][j]\n",
    "        for i in range(len(gt_patches_train))\n",
    "        for j in range(len(gt_patches_train[i]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 400, 400, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 400, 400)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 15 images\n",
      "Loading 15 groundtruth images\n"
     ]
    }
   ],
   "source": [
    "# Select the directory for the images and load them\n",
    "image_dir_val = root_dir + \"validating/images/\"\n",
    "files = os.listdir(image_dir_val)\n",
    "n_val = len(files)\n",
    "\n",
    "print(\"Loading \" + str(n_val) + \" images\")\n",
    "imgs_val = np.asarray([load_image(image_dir_val + files[i]) for i in range(n_val)])\n",
    "\n",
    "# Select the directory for groundtruth images and load them\n",
    "gt_dir_val = root_dir + \"validating/groundtruth/\"\n",
    "print(\"Loading \" + str(n_val) + \" groundtruth images\")\n",
    "gt_imgs_val = np.asarray([load_image(gt_dir_val + files[i]) for i in range(n_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 400, 400, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 400, 400)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 400\n",
    "\n",
    "# We separate the images from the groundtruth images\n",
    "img_patches_val = [img_crop(imgs_val[i], image_size, image_size) for i in range(n_val)]\n",
    "gt_patches_val = [\n",
    "    img_crop(gt_imgs_val[i], image_size, image_size) for i in range(n_val)\n",
    "]\n",
    "\n",
    "# Linearize the list and labeling them X and Y\n",
    "X_val = np.asarray(\n",
    "    [\n",
    "        img_patches_val[i][j]\n",
    "        for i in range(len(img_patches_val))\n",
    "        for j in range(len(img_patches_val[i]))\n",
    "    ]\n",
    ")\n",
    "Y_val = np.asarray(\n",
    "    [\n",
    "        gt_patches_val[i][j]\n",
    "        for i in range(len(gt_patches_val))\n",
    "        for j in range(len(gt_patches_val[i]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 400, 400, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 400, 400)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating mini-batch and running data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_minibatch(X, Y, n):\n",
    "\n",
    "#     # Fix the seed\n",
    "#     np.random.seed(1)\n",
    "\n",
    "#     # We define the window size of 72, batch size of 100 (empirically chosen)\n",
    "#     # and patch size should correspond to 16\n",
    "#     w_size = 72\n",
    "#     batch_size = 100\n",
    "#     patch_size = 16\n",
    "#     num_images = n\n",
    "\n",
    "#     while True:\n",
    "#         # Generate one minibatch\n",
    "#         batch_image = np.empty((batch_size, w_size, w_size, 3))\n",
    "#         batch_label = np.empty((batch_size, 2))\n",
    "\n",
    "#         for i in range(batch_size):\n",
    "\n",
    "#             # Select a random index represnting an image\n",
    "#             random_index = np.random.choice(num_images)\n",
    "\n",
    "#             # Width of original image\n",
    "#             width = 400\n",
    "\n",
    "#             # Sample a random window from the image\n",
    "#             random_sample = np.random.randint(w_size // 2, width - w_size // 2, 2)\n",
    "\n",
    "#             # Create a sub image of size 72x72\n",
    "#             sampled_image = X[random_index][\n",
    "#                 random_sample[0] - w_size // 2 : random_sample[0] + w_size // 2,\n",
    "#                 random_sample[1] - w_size // 2 : random_sample[1] + w_size // 2,\n",
    "#             ]\n",
    "\n",
    "#             # Take its corresponding ground-truth image\n",
    "#             correspond_ground_truth = Y[random_index][\n",
    "#                 random_sample[0] - patch_size // 2 : random_sample[0] + patch_size // 2,\n",
    "#                 random_sample[1] - patch_size // 2 : random_sample[1] + patch_size // 2,\n",
    "#             ]\n",
    "\n",
    "#             # We set in the label depending on the threshold of 0.25\n",
    "#             # The label becomes either 0 or 1 by applying to_categorical with parameter 2\n",
    "#             label = to_categorical(\n",
    "#                 (np.array([np.mean(correspond_ground_truth)]) > 0.25) * 1, 2\n",
    "#             )\n",
    "\n",
    "#             # The image augmentation is based on both flipping and rotating (randomly in steps of 45째)\n",
    "#             # Random vertical and horizontal flip\n",
    "#             if np.random.choice(2) == 1:\n",
    "#                 sampled_image = np.flipud(sampled_image)\n",
    "\n",
    "#             if np.random.choice(2) == 1:\n",
    "#                 sampled_image = np.fliplr(sampled_image)\n",
    "\n",
    "#             # Random rotation in steps of 45째\n",
    "#             rotations = [0, 45, 90, 135, 180, 225, 270, 315]\n",
    "\n",
    "#             # We select a rotation degree randomly\n",
    "#             rotation_choice = np.random.choice(len(rotations))\n",
    "\n",
    "#             # Rotate it using the random value (uses the scipy library)\n",
    "#             sampled_image = scipy.ndimage.rotate(\n",
    "#                 sampled_image,\n",
    "#                 rotations[rotation_choice],\n",
    "#                 order=1,\n",
    "#                 reshape=False,\n",
    "#                 mode=\"reflect\",\n",
    "#             )\n",
    "\n",
    "#             # We put in the sub image and its corresponding label before yielding it\n",
    "#             batch_image[i] = sampled_image\n",
    "#             batch_label[i] = label\n",
    "\n",
    "#         # Yield the mini_batch to the model\n",
    "#         yield (batch_image, batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as scipy\n",
    "\n",
    "\n",
    "def pad_matrix(mat, h_pad, w_pad, val=0):\n",
    "    h_pad = int(h_pad)\n",
    "    w_pad = int(w_pad)\n",
    "    if len(mat.shape) == 3:\n",
    "        padded_mat = np.pad(\n",
    "            mat,\n",
    "            ((h_pad, h_pad), (w_pad, w_pad), (0, 0)),\n",
    "            mode=\"constant\",\n",
    "            constant_values=((val, val), (val, val), (0, 0)),\n",
    "        )\n",
    "    elif len(mat.shape) == 2:\n",
    "        padded_mat = np.pad(\n",
    "            mat,\n",
    "            ((h_pad, h_pad), (w_pad, w_pad)),\n",
    "            mode=\"constant\",\n",
    "            constant_values=((val, val), (val, val)),\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"This method can only handle 2d or 3d arrays\")\n",
    "    return padded_mat\n",
    "\n",
    "\n",
    "def imag_rotation(X, Y):\n",
    "    theta = np.random.uniform(0, 2 * np.pi) * 180 / np.pi\n",
    "    Xr = pad_matrix(X, 82, 82)\n",
    "    Yr = pad_matrix(Y, 82, 82)\n",
    "    Xr = scipy.ndimage.rotate(Xr, theta, reshape=False)\n",
    "    Yr = scipy.ndimage.rotate(Yr, theta, reshape=False)\n",
    "\n",
    "    return Xr, Yr\n",
    "\n",
    "\n",
    "def create_minibatch(X, Y, n):\n",
    "\n",
    "    # Fix the seed\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # We define the window size of 72, batch size of 100 (empirically chosen)\n",
    "    # and patch size should correspond to 16\n",
    "    w_size = 72\n",
    "    batch_size = 85\n",
    "    patch_size = 16\n",
    "    num_images = n\n",
    "\n",
    "    while True:\n",
    "        # Generate one minibatch\n",
    "        batch_image = np.empty((batch_size, w_size, w_size, 3))\n",
    "        batch_label = np.empty((batch_size, 2))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            # Select a random index representing an image\n",
    "            random_index = np.random.choice(num_images)\n",
    "\n",
    "            # Width of original image\n",
    "            width = 400\n",
    "\n",
    "            ## Erick's rotation\n",
    "\n",
    "            # Sample a random window from the image\n",
    "\n",
    "            sampled_image = X[random_index]\n",
    "            correspond_ground_truth = Y[random_index]\n",
    "\n",
    "            # print(sampled_image.shape)\n",
    "            # print(correspond_ground_truth.shape)\n",
    "\n",
    "            random_sample = np.random.randint(w_size // 2, width - w_size // 2, 2)\n",
    "\n",
    "            ## Erick's rotation\n",
    "\n",
    "            if np.random.randint(2) == 1:\n",
    "                sampled_image, correspond_ground_truth = imag_rotation(\n",
    "                    sampled_image, correspond_ground_truth\n",
    "                )\n",
    "                rand_theta = np.random.uniform(0, 2 * np.pi)\n",
    "                rand_modulus = np.random.uniform(0, width // 2)\n",
    "                random_sample[0] = int(rand_modulus * np.cos(rand_theta) + 564 / 2)\n",
    "                random_sample[1] = int(rand_modulus * np.sin(rand_theta) + 564 / 2)\n",
    "\n",
    "            # Create a sub image of size 72x72\n",
    "            sampled_image = sampled_image[\n",
    "                random_sample[0] - w_size // 2 : random_sample[0] + w_size // 2,\n",
    "                random_sample[1] - w_size // 2 : random_sample[1] + w_size // 2,\n",
    "            ]\n",
    "\n",
    "            # Take its corresponding ground-truth image\n",
    "            correspond_ground_truth = correspond_ground_truth[\n",
    "                random_sample[0] - patch_size // 2 : random_sample[0] + patch_size // 2,\n",
    "                random_sample[1] - patch_size // 2 : random_sample[1] + patch_size // 2,\n",
    "            ]\n",
    "\n",
    "            # We set in the label depending on the threshold of 0.25\n",
    "            # The label becomes either 0 or 1 by applying to_categorical with parameter 2\n",
    "            label = to_categorical(\n",
    "                (np.array([np.mean(correspond_ground_truth)]) > 0.25) * 1, 2\n",
    "            )\n",
    "\n",
    "            # The image augmentation is based on both flipping and rotating (randomly in steps of 45째)\n",
    "            # Random vertical and horizontal flip\n",
    "            if np.random.choice(2) == 1:\n",
    "                sampled_image = np.flipud(sampled_image)\n",
    "\n",
    "            if np.random.choice(2) == 1:\n",
    "                sampled_image = np.fliplr(sampled_image)\n",
    "\n",
    "            \"\"\"        \n",
    "            # Random rotation in steps of 45째\n",
    "            rotations = [0, 45, 90, 135, 180, 225, 270, 315, 350]\n",
    "        \n",
    "            # We select a rotation degree randomly\n",
    "            rotation_choice = np.random.choice(len(rotations))\n",
    "            \n",
    "            # Rotate it using the random value (uses the scipy library)\n",
    "            sampled_image = scipy.ndimage.rotate(sampled_image, rotations[rotation_choice], order=1,\n",
    "                                                         reshape=False, mode='reflect')\n",
    "            \"\"\"\n",
    "            # We put in the sub image and its corresponding label before yielding it\n",
    "            batch_image[i] = sampled_image\n",
    "            batch_label[i] = label\n",
    "\n",
    "        # Yield the mini_batch to the model\n",
    "        yield (batch_image, batch_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the class (Same as in cnn_model.py, but provided here for better readability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_model:\n",
    "\n",
    "    # Initialize the class\n",
    "    def __init__(self, shape, batch_normalization, activation):\n",
    "        self.shape = shape\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.activation = activation\n",
    "        self.model = self.initialize_cnn_model(shape, batch_normalization, activation)\n",
    "\n",
    "    def initialize_cnn_model(self, shape, batch_normalization, activation):\n",
    "        #         print(activation)\n",
    "\n",
    "        # INPUT\n",
    "        # shape     - Size of the input images\n",
    "        # OUTPUT\n",
    "        # model    - Compiled CNN\n",
    "\n",
    "        # Define hyperparamters\n",
    "        KERNEL3 = (3, 3)\n",
    "        KERNEL5 = (5, 5)\n",
    "\n",
    "        # Define a model\n",
    "        model = Sequential()\n",
    "\n",
    "        # Add the layers\n",
    "        # Selection of the model is described in the report\n",
    "        # We use padding = 'same' to avoid issues with the matrix sizes\n",
    "        model.add(Conv2D(64, KERNEL5, input_shape=shape, padding=\"same\"))\n",
    "        if batch_normalization:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "            Activation(activation)\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(128, KERNEL3, padding=\"same\"))\n",
    "        if batch_normalization:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "            Activation(activation)\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "        if batch_normalization:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "            Activation(activation)\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "        if batch_normalization:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "            Activation(activation)\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "        if batch_normalization:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "            Activation(activation)\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "        if batch_normalization:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "            Activation(activation)\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        # Flatten it and use regularizers to avoid overfitting\n",
    "        # The parameters have been chosen empirically\n",
    "        model.add(Flatten())\n",
    "        model.add(\n",
    "            Dense(\n",
    "                128, kernel_regularizer=l2(0.000001), activity_regularizer=l2(0.000001)\n",
    "            )\n",
    "        )\n",
    "        #         if batch_normalization:\n",
    "        #             model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "            Activation(activation)\n",
    "        )\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # Add output layer\n",
    "        model.add(\n",
    "            Dense(2, kernel_regularizer=l2(0.000001), activity_regularizer=l2(0.000001))\n",
    "        )\n",
    "        model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "        # Compile the model using the binary crossentropy loss and the Adam optimizer for it\n",
    "        # We used the accuracy as a metric, but F1 score is also a plausible choice\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(lr=0.001),\n",
    "            metrics=[\"accuracy\", recall, f1],\n",
    "        )\n",
    "\n",
    "        # Print a summary of the model to see what has been generated\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # Early stopping callback after 10 steps\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0,\n",
    "            patience=15,\n",
    "            verbose=1,\n",
    "            mode=\"auto\",\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "\n",
    "        # Reduce learning rate on plateau after 4 steps\n",
    "        lr_callback = ReduceLROnPlateau(\n",
    "            monitor=\"loss\", factor=0.5, patience=4, verbose=1, mode=\"auto\"\n",
    "        )\n",
    "        save_best = ModelCheckpoint(\n",
    "            \"batch_LeakyReLU_validation_160_dropout-0.25_erickAug-{epoch:03d}-{accuracy:03f}-{val_accuracy:03f}.h5\",\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"auto\",\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        # Place the callbacks in a list to be used when training\n",
    "        #         callbacks = [cb, early_stopping, lr_callback]\n",
    "        callbacks = [save_best, lr_callback]\n",
    "\n",
    "        # Train the model using the previously defined functions and callbacks\n",
    "        history = self.model.fit_generator(\n",
    "            create_minibatch(X_train, Y_train, n_train),\n",
    "            steps_per_epoch=STEPS_PER_EPOCH,\n",
    "            epochs=EPOCHS,\n",
    "            use_multiprocessing=False,\n",
    "            workers=1,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "            validation_data=create_minibatch(X_val, Y_val, n_val),\n",
    "            validation_steps=STEPS_PER_EPOCH,\n",
    "        )\n",
    "        #         to_plot = self.model.fit_generator(\n",
    "        #             create_minibatch(X_train, Y_train, n_train),\n",
    "        #             steps_per_epoch=100,\n",
    "        #             epochs=EPOCHS,\n",
    "        #             use_multiprocessing=False,\n",
    "        #             workers=1,\n",
    "        #             callbacks=callbacks,\n",
    "        #             verbose=1,\n",
    "        #             validation_data=create_minibatch(X_val, Y_val, n_val),\n",
    "        #             validation_steps=100,\n",
    "        #         )\n",
    "        return history\n",
    "\n",
    "    def classify(self, X):\n",
    "        # Subdivide the images into blocks with a stride and patch_size of 16\n",
    "        img_patches = create_patches(X, 16, 16, padding=28)\n",
    "\n",
    "        # Predict\n",
    "        predictions = self.model.predict(img_patches)\n",
    "        predictions = (predictions[:, 0] < predictions[:, 1]) * 1\n",
    "\n",
    "        # Regroup patches into images\n",
    "        return group_patches(predictions, X.shape[0])\n",
    "\n",
    "    def load(self, filename):\n",
    "        # Load the model (used for submission)\n",
    "        dependencies = {\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "        self.model = load_model(filename, custom_objects=dependencies)\n",
    "\n",
    "\n",
    "#     def save(self, filename):\n",
    "#         # Save the model (used to then load to submit)\n",
    "#         self.model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,450\n",
      "Trainable params: 2,278,018\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n",
      "Epoch 1/160\n",
      "Executing op Shape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op StridedSlice in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2D in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op BiasAdd in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalAnd in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op FusedBatchNormV3 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignSubVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LeakyRelu in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MaxPool in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Shape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op GreaterEqual in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Square in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RealDiv in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sigmoid in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Minimum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Maximum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Log in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Neg in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mean in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DivNoNan in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op BroadcastGradientArgs in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Tile in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Maximum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op FloorDiv in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reciprocal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ZerosLike in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Select in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LessEqual in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op SigmoidGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op BiasAddGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LeakyReluGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MaxPoolGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op FusedBatchNormGradV3 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ShapeN in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2DBackpropInput in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2DBackpropFilter in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Pow in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sqrt in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RealDiv in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ResourceApplyAdam in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignAddVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Greater in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Equal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignAddVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Round in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "149/150 [============================>.] - ETA: 7s - loss: 0.5608 - accuracy: 0.7290 - recall: 0.7281 - f1: 0.7280 Executing op FusedBatchNormV3 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67234, saving model to batch_LeakyReLU_validation_160_dropout-0.25_erickAug-001-0.728667-0.768078.h5\n",
      "150/150 [==============================] - 2282s 15s/step - loss: 0.5609 - accuracy: 0.7287 - recall: 0.7278 - f1: 0.7277 - val_loss: 0.6723 - val_accuracy: 0.7681 - val_recall: 0.7681 - val_f1: 0.7681\n",
      "Epoch 2/160\n",
      "149/150 [============================>.] - ETA: 7s - loss: 0.4409 - accuracy: 0.7878 - recall: 0.7850 - f1: 0.7872 \n",
      "Epoch 00002: val_loss improved from 0.67234 to 0.50583, saving model to batch_LeakyReLU_validation_160_dropout-0.25_erickAug-002-0.787922-0.744274.h5\n",
      "150/150 [==============================] - 2142s 14s/step - loss: 0.4407 - accuracy: 0.7879 - recall: 0.7851 - f1: 0.7873 - val_loss: 0.5058 - val_accuracy: 0.7443 - val_recall: 0.7268 - val_f1: 0.7397\n",
      "Epoch 3/160\n",
      "149/150 [============================>.] - ETA: 6s - loss: 0.3785 - accuracy: 0.8276 - recall: 0.8295 - f1: 0.8280 \n",
      "Epoch 00003: val_loss did not improve from 0.50583\n",
      "150/150 [==============================] - 1909s 13s/step - loss: 0.3774 - accuracy: 0.8282 - recall: 0.8301 - f1: 0.8286 - val_loss: 0.6279 - val_accuracy: 0.6104 - val_recall: 0.6011 - val_f1: 0.6068\n",
      "Epoch 4/160\n",
      "149/150 [============================>.] - ETA: 6s - loss: 0.3311 - accuracy: 0.8547 - recall: 0.8547 - f1: 0.8547 \n",
      "Epoch 00004: val_loss improved from 0.50583 to 0.39156, saving model to batch_LeakyReLU_validation_160_dropout-0.25_erickAug-004-0.854706-0.792745.h5\n",
      "150/150 [==============================] - 1894s 13s/step - loss: 0.3309 - accuracy: 0.8547 - recall: 0.8550 - f1: 0.8547 - val_loss: 0.3916 - val_accuracy: 0.7927 - val_recall: 0.7955 - val_f1: 0.7933\n",
      "Epoch 5/160\n",
      "149/150 [============================>.] - ETA: 6s - loss: 0.3041 - accuracy: 0.8683 - recall: 0.8685 - f1: 0.8683 \n",
      "Epoch 00005: val_loss did not improve from 0.39156\n",
      "150/150 [==============================] - 1875s 13s/step - loss: 0.3045 - accuracy: 0.8681 - recall: 0.8682 - f1: 0.8681 - val_loss: 0.5122 - val_accuracy: 0.7975 - val_recall: 0.7937 - val_f1: 0.7967\n",
      "Epoch 6/160\n",
      "149/150 [============================>.] - ETA: 5s - loss: 0.2920 - accuracy: 0.8740 - recall: 0.8740 - f1: 0.8740 \n",
      "Epoch 00006: val_loss did not improve from 0.39156\n",
      "150/150 [==============================] - 1848s 12s/step - loss: 0.2920 - accuracy: 0.8741 - recall: 0.8740 - f1: 0.8740 - val_loss: 0.5909 - val_accuracy: 0.7751 - val_recall: 0.7748 - val_f1: 0.7750\n",
      "Epoch 7/160\n",
      "149/150 [============================>.] - ETA: 5s - loss: 0.2620 - accuracy: 0.8897 - recall: 0.8882 - f1: 0.8895 \n",
      "Epoch 00007: val_loss did not improve from 0.39156\n",
      "150/150 [==============================] - 1845s 12s/step - loss: 0.2620 - accuracy: 0.8896 - recall: 0.8882 - f1: 0.8895 - val_loss: 0.4404 - val_accuracy: 0.8178 - val_recall: 0.8195 - val_f1: 0.8181\n",
      "Epoch 8/160\n",
      "149/150 [============================>.] - ETA: 5s - loss: 0.2610 - accuracy: 0.8859 - recall: 0.8846 - f1: 0.8857 \n",
      "Epoch 00008: val_loss did not improve from 0.39156\n",
      "150/150 [==============================] - 1861s 12s/step - loss: 0.2618 - accuracy: 0.8854 - recall: 0.8842 - f1: 0.8853 - val_loss: 0.4202 - val_accuracy: 0.7875 - val_recall: 0.7860 - val_f1: 0.7871\n",
      "Epoch 9/160\n",
      "149/150 [============================>.] - ETA: 6s - loss: 0.2650 - accuracy: 0.8889 - recall: 0.8884 - f1: 0.8888 \n",
      "Epoch 00009: val_loss did not improve from 0.39156\n",
      "150/150 [==============================] - 1888s 13s/step - loss: 0.2654 - accuracy: 0.8884 - recall: 0.8879 - f1: 0.8883 - val_loss: 0.6263 - val_accuracy: 0.7714 - val_recall: 0.7712 - val_f1: 0.7714\n",
      "Epoch 10/160\n",
      "149/150 [============================>.] - ETA: 5s - loss: 0.2448 - accuracy: 0.8953 - recall: 0.8952 - f1: 0.8953 \n",
      "Epoch 00010: val_loss improved from 0.39156 to 0.37896, saving model to batch_LeakyReLU_validation_160_dropout-0.25_erickAug-010-0.895176-0.818078.h5\n",
      "150/150 [==============================] - 1883s 13s/step - loss: 0.2449 - accuracy: 0.8952 - recall: 0.8951 - f1: 0.8952 - val_loss: 0.3790 - val_accuracy: 0.8181 - val_recall: 0.8190 - val_f1: 0.8182\n",
      "Epoch 11/160\n",
      "149/150 [============================>.] - ETA: 5s - loss: 0.2382 - accuracy: 0.9021 - recall: 0.9028 - f1: 0.9022 \n",
      "Epoch 00011: val_loss did not improve from 0.37896\n",
      "150/150 [==============================] - 1853s 12s/step - loss: 0.2384 - accuracy: 0.9022 - recall: 0.9028 - f1: 0.9023 - val_loss: 0.3897 - val_accuracy: 0.8093 - val_recall: 0.8101 - val_f1: 0.8094\n",
      "Epoch 12/160\n",
      "149/150 [============================>.] - ETA: 5s - loss: 0.2305 - accuracy: 0.9008 - recall: 0.9008 - f1: 0.9008 \n",
      "Epoch 00012: val_loss did not improve from 0.37896\n",
      "150/150 [==============================] - 1875s 12s/step - loss: 0.2308 - accuracy: 0.9008 - recall: 0.9008 - f1: 0.9008 - val_loss: 0.4757 - val_accuracy: 0.8150 - val_recall: 0.8142 - val_f1: 0.8148\n",
      "Epoch 13/160\n",
      "149/150 [============================>.] - ETA: 6s - loss: 0.2197 - accuracy: 0.9080 - recall: 0.9084 - f1: 0.9080 \n",
      "Epoch 00013: val_loss did not improve from 0.37896\n",
      "150/150 [==============================] - 1887s 13s/step - loss: 0.2206 - accuracy: 0.9072 - recall: 0.9077 - f1: 0.9073 - val_loss: 0.4429 - val_accuracy: 0.7995 - val_recall: 0.8004 - val_f1: 0.7997\n",
      "Epoch 14/160\n",
      "149/150 [============================>.] - ETA: 5s - loss: 0.2287 - accuracy: 0.9049 - recall: 0.9047 - f1: 0.9049 \n",
      "Epoch 00014: val_loss improved from 0.37896 to 0.34642, saving model to batch_LeakyReLU_validation_160_dropout-0.25_erickAug-014-0.904706-0.832353.h5\n",
      "150/150 [==============================] - 1874s 12s/step - loss: 0.2292 - accuracy: 0.9047 - recall: 0.9045 - f1: 0.9047 - val_loss: 0.3464 - val_accuracy: 0.8324 - val_recall: 0.8327 - val_f1: 0.8324\n",
      "Epoch 15/160\n",
      "149/150 [============================>.] - ETA: 5s - loss: 0.2149 - accuracy: 0.9105 - recall: 0.9099 - f1: 0.9105 \n",
      "Epoch 00015: val_loss did not improve from 0.34642\n",
      "150/150 [==============================] - 1871s 12s/step - loss: 0.2160 - accuracy: 0.9100 - recall: 0.9093 - f1: 0.9099 - val_loss: 0.3815 - val_accuracy: 0.8187 - val_recall: 0.8169 - val_f1: 0.8184\n",
      "Epoch 16/160\n",
      "149/150 [============================>.] - ETA: 5s - loss: 0.2104 - accuracy: 0.9098 - recall: 0.9099 - f1: 0.9098 \n",
      "Epoch 00016: val_loss improved from 0.34642 to 0.33406, saving model to batch_LeakyReLU_validation_160_dropout-0.25_erickAug-016-0.910000-0.847608.h5\n",
      "150/150 [==============================] - 1893s 13s/step - loss: 0.2098 - accuracy: 0.9100 - recall: 0.9100 - f1: 0.9100 - val_loss: 0.3341 - val_accuracy: 0.8476 - val_recall: 0.8450 - val_f1: 0.8472\n",
      "Epoch 17/160\n",
      "149/150 [============================>.] - ETA: 6s - loss: 0.2150 - accuracy: 0.9101 - recall: 0.9098 - f1: 0.9101 \n",
      "Epoch 00017: val_loss did not improve from 0.33406\n",
      "150/150 [==============================] - 1887s 13s/step - loss: 0.2150 - accuracy: 0.9102 - recall: 0.9100 - f1: 0.9102 - val_loss: 0.3612 - val_accuracy: 0.8222 - val_recall: 0.8229 - val_f1: 0.8223\n",
      "Epoch 18/160\n",
      "149/150 [============================>.] - ETA: 6s - loss: 0.2027 - accuracy: 0.9138 - recall: 0.9146 - f1: 0.9139 \n",
      "Epoch 00018: val_loss did not improve from 0.33406\n",
      "150/150 [==============================] - 1901s 13s/step - loss: 0.2023 - accuracy: 0.9138 - recall: 0.9147 - f1: 0.9139 - val_loss: 0.9197 - val_accuracy: 0.7765 - val_recall: 0.7766 - val_f1: 0.7765\n",
      "Epoch 19/160\n",
      " 91/150 [=================>............] - ETA: 5:43 - loss: 0.1932 - accuracy: 0.9221 - recall: 0.9223 - f1: 0.9221"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-fc145ceab4ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m# model.save(\"no_batch_LeakyRelu_validation_1.h5\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-989b007d0b18>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m             \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m         )\n\u001b[0;32m    156\u001b[0m         \u001b[1;31m#         to_plot = self.model.fit_generator(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m       \u001b[0mbatch_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[1;34m(generator)\u001b[0m\n\u001b[0;32m    361\u001b[0m   \u001b[1;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m     \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    892\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 894\u001b[1;33m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    895\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "# We define the number of epochs and steps per epochs\n",
    "EPOCHS = 160\n",
    "STEPS_PER_EPOCH = 150\n",
    "batch_normalization = True\n",
    "activation = \"LeakyReLU\"\n",
    "model = cnn_model(\n",
    "    shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "# Train the model\n",
    "history = model.train()\n",
    "# model.save(\"no_batch_LeakyRelu_validation_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-4520269d93b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ggplot\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train_loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train_acc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "plt.savefig(\"batch_LeakyReLU_validation_160_dropout-0.25.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,450\n",
      "Trainable params: 2,278,018\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,450\n",
      "Trainable params: 2,278,018\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n",
      "Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_distributed_function_2945860 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_1/test_1.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_2/test_2.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_3/test_3.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_4/test_4.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_5/test_5.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_6/test_6.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_7/test_7.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_8/test_8.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_9/test_9.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_10/test_10.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_11/test_11.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_12/test_12.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_13/test_13.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_14/test_14.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_15/test_15.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_16/test_16.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_17/test_17.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_18/test_18.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_19/test_19.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_20/test_20.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_21/test_21.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_22/test_22.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_23/test_23.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_24/test_24.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_25/test_25.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_26/test_26.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_27/test_27.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_28/test_28.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_29/test_29.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_30/test_30.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_31/test_31.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_32/test_32.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_33/test_33.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_34/test_34.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_35/test_35.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_36/test_36.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_37/test_37.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_38/test_38.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_39/test_39.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_40/test_40.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_41/test_41.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_42/test_42.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_43/test_43.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_44/test_44.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_45/test_45.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_46/test_46.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_47/test_47.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_48/test_48.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_49/test_49.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_50/test_50.png\n"
     ]
    }
   ],
   "source": [
    "from helpers import *\n",
    "\n",
    "# from cnn_model import cnn_model\n",
    "\n",
    "# Instantiate the model\n",
    "batch_normalization = True\n",
    "activation = \"LeakyReLU\"\n",
    "model = cnn_model(\n",
    "    shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model.load(\n",
    "    \"batch_LeakyReLU_validation_160_dropout-0.25_erickAug-016-0.910000-0.847608.h5\"\n",
    ")\n",
    "\n",
    "# Print a summary to make sure the correct model is used\n",
    "model.model.summary()\n",
    "\n",
    "# We add all test images to an array, used later for generating a submission\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "    image_filenames.append(image_filename)\n",
    "\n",
    "# Set-up submission filename\n",
    "submission_filename = (\n",
    "    \"batch_LeakyReLU_validation_160_dropout-0.25_erickAug-016-0.910000-0.847608.csv\"\n",
    ")\n",
    "\n",
    "# Generates the submission\n",
    "generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,450\n",
      "Trainable params: 2,278,018\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n",
      "Epoch 1/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.5724 - accuracy: 0.7374 - recall: 0.7419 - f1: 0.7388\n",
      "Epoch 00001: val_loss improved from inf to 0.62171, saving model to batch_LeakyReLU_validation_160_dropout-0.25-001-0.737467-0.750800.h5\n",
      "150/150 [==============================] - 78s 520ms/step - loss: 0.5720 - accuracy: 0.7375 - recall: 0.7419 - f1: 0.7388 - val_loss: 0.6217 - val_accuracy: 0.7508 - val_recall: 0.7508 - val_f1: 0.7508\n",
      "Epoch 2/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.4866 - accuracy: 0.7578 - recall: 0.7530 - f1: 0.7565\n",
      "Epoch 00002: val_loss did not improve from 0.62171\n",
      "150/150 [==============================] - 69s 462ms/step - loss: 0.4865 - accuracy: 0.7577 - recall: 0.7529 - f1: 0.7565 - val_loss: 0.7564 - val_accuracy: 0.3563 - val_recall: 0.3494 - val_f1: 0.3518\n",
      "Epoch 3/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3946 - accuracy: 0.8190 - recall: 0.8168 - f1: 0.8186\n",
      "Epoch 00003: val_loss improved from 0.62171 to 0.53719, saving model to batch_LeakyReLU_validation_160_dropout-0.25-003-0.819133-0.702733.h5\n",
      "150/150 [==============================] - 67s 448ms/step - loss: 0.3944 - accuracy: 0.8191 - recall: 0.8168 - f1: 0.8187 - val_loss: 0.5372 - val_accuracy: 0.7027 - val_recall: 0.7037 - val_f1: 0.7030\n",
      "Epoch 4/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3643 - accuracy: 0.8347 - recall: 0.8347 - f1: 0.8347\n",
      "Epoch 00004: val_loss improved from 0.53719 to 0.40577, saving model to batch_LeakyReLU_validation_160_dropout-0.25-004-0.834700-0.815500.h5\n",
      "150/150 [==============================] - 68s 452ms/step - loss: 0.3648 - accuracy: 0.8347 - recall: 0.8347 - f1: 0.8347 - val_loss: 0.4058 - val_accuracy: 0.8155 - val_recall: 0.8119 - val_f1: 0.8148\n",
      "Epoch 5/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3295 - accuracy: 0.8538 - recall: 0.8528 - f1: 0.8537\n",
      "Epoch 00005: val_loss did not improve from 0.40577\n",
      "150/150 [==============================] - 69s 458ms/step - loss: 0.3288 - accuracy: 0.8542 - recall: 0.8533 - f1: 0.8540 - val_loss: 0.4356 - val_accuracy: 0.7808 - val_recall: 0.7815 - val_f1: 0.7810\n",
      "Epoch 6/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3186 - accuracy: 0.8601 - recall: 0.8605 - f1: 0.8601\n",
      "Epoch 00006: val_loss improved from 0.40577 to 0.37243, saving model to batch_LeakyReLU_validation_160_dropout-0.25-006-0.860167-0.813767.h5\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.3184 - accuracy: 0.8602 - recall: 0.8606 - f1: 0.8602 - val_loss: 0.3724 - val_accuracy: 0.8138 - val_recall: 0.8119 - val_f1: 0.8134\n",
      "Epoch 7/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.8728 - recall: 0.8729 - f1: 0.8728\n",
      "Epoch 00007: val_loss did not improve from 0.37243\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.2868 - accuracy: 0.8732 - recall: 0.8733 - f1: 0.8732 - val_loss: 0.3788 - val_accuracy: 0.8103 - val_recall: 0.8125 - val_f1: 0.8107\n",
      "Epoch 8/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2699 - accuracy: 0.8876 - recall: 0.8878 - f1: 0.8876\n",
      "Epoch 00008: val_loss improved from 0.37243 to 0.36826, saving model to batch_LeakyReLU_validation_160_dropout-0.25-008-0.887767-0.825067.h5\n",
      "150/150 [==============================] - 68s 457ms/step - loss: 0.2697 - accuracy: 0.8878 - recall: 0.8879 - f1: 0.8878 - val_loss: 0.3683 - val_accuracy: 0.8251 - val_recall: 0.8227 - val_f1: 0.8246\n",
      "Epoch 9/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2615 - accuracy: 0.8901 - recall: 0.8900 - f1: 0.8901\n",
      "Epoch 00009: val_loss improved from 0.36826 to 0.31509, saving model to batch_LeakyReLU_validation_160_dropout-0.25-009-0.890167-0.863167.h5\n",
      "150/150 [==============================] - 69s 458ms/step - loss: 0.2614 - accuracy: 0.8902 - recall: 0.8901 - f1: 0.8902 - val_loss: 0.3151 - val_accuracy: 0.8632 - val_recall: 0.8651 - val_f1: 0.8634\n",
      "Epoch 10/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2536 - accuracy: 0.8929 - recall: 0.8921 - f1: 0.8928\n",
      "Epoch 00010: val_loss did not improve from 0.31509\n",
      "150/150 [==============================] - 68s 451ms/step - loss: 0.2532 - accuracy: 0.8929 - recall: 0.8921 - f1: 0.8928 - val_loss: 0.5587 - val_accuracy: 0.8009 - val_recall: 0.8029 - val_f1: 0.8013\n",
      "Epoch 11/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2430 - accuracy: 0.8976 - recall: 0.8984 - f1: 0.8976\n",
      "Epoch 00011: val_loss did not improve from 0.31509\n",
      "150/150 [==============================] - 70s 463ms/step - loss: 0.2432 - accuracy: 0.8975 - recall: 0.8983 - f1: 0.8976 - val_loss: 0.4471 - val_accuracy: 0.7999 - val_recall: 0.8003 - val_f1: 0.8000\n",
      "Epoch 12/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2341 - accuracy: 0.9021 - recall: 0.9025 - f1: 0.9022\n",
      "Epoch 00012: val_loss did not improve from 0.31509\n",
      "150/150 [==============================] - 82s 550ms/step - loss: 0.2341 - accuracy: 0.9021 - recall: 0.9024 - f1: 0.9021 - val_loss: 0.3904 - val_accuracy: 0.8233 - val_recall: 0.8233 - val_f1: 0.8233\n",
      "Epoch 13/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2348 - accuracy: 0.9038 - recall: 0.9037 - f1: 0.9038\n",
      "Epoch 00013: val_loss did not improve from 0.31509\n",
      "150/150 [==============================] - 75s 502ms/step - loss: 0.2345 - accuracy: 0.9038 - recall: 0.9037 - f1: 0.9038 - val_loss: 0.3618 - val_accuracy: 0.8291 - val_recall: 0.8289 - val_f1: 0.8291\n",
      "Epoch 14/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2312 - accuracy: 0.9008 - recall: 0.9003 - f1: 0.9008\n",
      "Epoch 00014: val_loss did not improve from 0.31509\n",
      "150/150 [==============================] - 75s 503ms/step - loss: 0.2313 - accuracy: 0.9008 - recall: 0.9003 - f1: 0.9008 - val_loss: 0.3567 - val_accuracy: 0.8316 - val_recall: 0.8319 - val_f1: 0.8316\n",
      "Epoch 15/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2180 - accuracy: 0.9080 - recall: 0.9084 - f1: 0.9080\n",
      "Epoch 00015: val_loss did not improve from 0.31509\n",
      "150/150 [==============================] - 94s 628ms/step - loss: 0.2178 - accuracy: 0.9082 - recall: 0.9086 - f1: 0.9082 - val_loss: 0.3749 - val_accuracy: 0.8544 - val_recall: 0.8541 - val_f1: 0.8544\n",
      "Epoch 16/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2132 - accuracy: 0.9118 - recall: 0.9109 - f1: 0.9117\n",
      "Epoch 00016: val_loss improved from 0.31509 to 0.29161, saving model to batch_LeakyReLU_validation_160_dropout-0.25-016-0.912100-0.882367.h5\n",
      "150/150 [==============================] - 78s 521ms/step - loss: 0.2127 - accuracy: 0.9121 - recall: 0.9111 - f1: 0.9120 - val_loss: 0.2916 - val_accuracy: 0.8824 - val_recall: 0.8810 - val_f1: 0.8822\n",
      "Epoch 17/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2125 - accuracy: 0.9130 - recall: 0.9127 - f1: 0.9130\n",
      "Epoch 00017: val_loss improved from 0.29161 to 0.25331, saving model to batch_LeakyReLU_validation_160_dropout-0.25-017-0.913000-0.890467.h5\n",
      "150/150 [==============================] - 78s 522ms/step - loss: 0.2125 - accuracy: 0.9130 - recall: 0.9127 - f1: 0.9130 - val_loss: 0.2533 - val_accuracy: 0.8905 - val_recall: 0.8899 - val_f1: 0.8904\n",
      "Epoch 18/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2079 - accuracy: 0.9140 - recall: 0.9145 - f1: 0.9140\n",
      "Epoch 00018: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 77s 516ms/step - loss: 0.2080 - accuracy: 0.9139 - recall: 0.9145 - f1: 0.9140 - val_loss: 1.0854 - val_accuracy: 0.7716 - val_recall: 0.7717 - val_f1: 0.7716\n",
      "Epoch 19/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2009 - accuracy: 0.9202 - recall: 0.9198 - f1: 0.9202\n",
      "Epoch 00019: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 68s 451ms/step - loss: 0.2007 - accuracy: 0.9203 - recall: 0.9199 - f1: 0.9202 - val_loss: 0.3170 - val_accuracy: 0.8553 - val_recall: 0.8551 - val_f1: 0.8553\n",
      "Epoch 20/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1977 - accuracy: 0.9163 - recall: 0.9159 - f1: 0.9162\n",
      "Epoch 00020: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 79s 524ms/step - loss: 0.1974 - accuracy: 0.9165 - recall: 0.9161 - f1: 0.9164 - val_loss: 0.2989 - val_accuracy: 0.8611 - val_recall: 0.8603 - val_f1: 0.8610\n",
      "Epoch 21/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1934 - accuracy: 0.9225 - recall: 0.9226 - f1: 0.9225\n",
      "Epoch 00021: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 73s 486ms/step - loss: 0.1933 - accuracy: 0.9225 - recall: 0.9226 - f1: 0.9225 - val_loss: 0.5145 - val_accuracy: 0.8027 - val_recall: 0.8027 - val_f1: 0.8027\n",
      "Epoch 22/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1943 - accuracy: 0.9185 - recall: 0.9182 - f1: 0.9184\n",
      "Epoch 00022: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 74s 496ms/step - loss: 0.1945 - accuracy: 0.9183 - recall: 0.9180 - f1: 0.9183 - val_loss: 0.3091 - val_accuracy: 0.8474 - val_recall: 0.8465 - val_f1: 0.8473\n",
      "Epoch 23/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1944 - accuracy: 0.9179 - recall: 0.9185 - f1: 0.9179\n",
      "Epoch 00023: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 78s 520ms/step - loss: 0.1948 - accuracy: 0.9177 - recall: 0.9183 - f1: 0.9178 - val_loss: 0.3409 - val_accuracy: 0.8375 - val_recall: 0.8379 - val_f1: 0.8375\n",
      "Epoch 24/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1979 - accuracy: 0.9191 - recall: 0.9195 - f1: 0.9192\n",
      "Epoch 00024: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 79s 526ms/step - loss: 0.1982 - accuracy: 0.9190 - recall: 0.9193 - f1: 0.9190 - val_loss: 0.3191 - val_accuracy: 0.8504 - val_recall: 0.8512 - val_f1: 0.8505\n",
      "Epoch 25/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1874 - accuracy: 0.9210 - recall: 0.9217 - f1: 0.9211\n",
      "Epoch 00025: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 78s 521ms/step - loss: 0.1873 - accuracy: 0.9210 - recall: 0.9217 - f1: 0.9211 - val_loss: 0.3768 - val_accuracy: 0.8315 - val_recall: 0.8319 - val_f1: 0.8315\n",
      "Epoch 26/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1867 - accuracy: 0.9267 - recall: 0.9271 - f1: 0.9268\n",
      "Epoch 00026: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 77s 512ms/step - loss: 0.1869 - accuracy: 0.9267 - recall: 0.9271 - f1: 0.9267 - val_loss: 0.2946 - val_accuracy: 0.8677 - val_recall: 0.8679 - val_f1: 0.8677\n",
      "Epoch 27/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1792 - accuracy: 0.9259 - recall: 0.9258 - f1: 0.9259\n",
      "Epoch 00027: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 77s 511ms/step - loss: 0.1791 - accuracy: 0.9259 - recall: 0.9259 - f1: 0.9259 - val_loss: 0.2674 - val_accuracy: 0.8904 - val_recall: 0.8899 - val_f1: 0.8904\n",
      "Epoch 28/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1755 - accuracy: 0.9284 - recall: 0.9281 - f1: 0.9284\n",
      "Epoch 00028: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 77s 514ms/step - loss: 0.1757 - accuracy: 0.9283 - recall: 0.9279 - f1: 0.9282 - val_loss: 0.4680 - val_accuracy: 0.8014 - val_recall: 0.8029 - val_f1: 0.8017\n",
      "Epoch 29/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1776 - accuracy: 0.9280 - recall: 0.9283 - f1: 0.9280\n",
      "Epoch 00029: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 80s 534ms/step - loss: 0.1779 - accuracy: 0.9278 - recall: 0.9281 - f1: 0.9278 - val_loss: 0.3898 - val_accuracy: 0.8406 - val_recall: 0.8401 - val_f1: 0.8405\n",
      "Epoch 30/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1802 - accuracy: 0.9276 - recall: 0.9274 - f1: 0.9276\n",
      "Epoch 00030: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 88s 587ms/step - loss: 0.1801 - accuracy: 0.9275 - recall: 0.9273 - f1: 0.9275 - val_loss: 0.2979 - val_accuracy: 0.8707 - val_recall: 0.8714 - val_f1: 0.8708\n",
      "Epoch 31/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1777 - accuracy: 0.9264 - recall: 0.9264 - f1: 0.9264\n",
      "Epoch 00031: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 80s 532ms/step - loss: 0.1775 - accuracy: 0.9263 - recall: 0.9263 - f1: 0.9263 - val_loss: 0.3168 - val_accuracy: 0.8541 - val_recall: 0.8549 - val_f1: 0.8543\n",
      "Epoch 32/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1733 - accuracy: 0.9294 - recall: 0.9293 - f1: 0.9294\n",
      "Epoch 00032: val_loss improved from 0.25331 to 0.24671, saving model to batch_LeakyReLU_validation_160_dropout-0.25-032-0.929200-0.892433.h5\n",
      "150/150 [==============================] - 82s 550ms/step - loss: 0.1736 - accuracy: 0.9292 - recall: 0.9291 - f1: 0.9292 - val_loss: 0.2467 - val_accuracy: 0.8924 - val_recall: 0.8928 - val_f1: 0.8925\n",
      "Epoch 33/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.9330 - recall: 0.9330 - f1: 0.9330\n",
      "Epoch 00033: val_loss did not improve from 0.24671\n",
      "150/150 [==============================] - 83s 556ms/step - loss: 0.1655 - accuracy: 0.9328 - recall: 0.9328 - f1: 0.9328 - val_loss: 0.8405 - val_accuracy: 0.7871 - val_recall: 0.7870 - val_f1: 0.7871\n",
      "Epoch 34/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1722 - accuracy: 0.9329 - recall: 0.9327 - f1: 0.9328\n",
      "Epoch 00034: val_loss did not improve from 0.24671\n",
      "150/150 [==============================] - 77s 514ms/step - loss: 0.1720 - accuracy: 0.9331 - recall: 0.9329 - f1: 0.9331 - val_loss: 0.2583 - val_accuracy: 0.8829 - val_recall: 0.8825 - val_f1: 0.8828\n",
      "Epoch 35/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1661 - accuracy: 0.9316 - recall: 0.9317 - f1: 0.9316\n",
      "Epoch 00035: val_loss did not improve from 0.24671\n",
      "150/150 [==============================] - 77s 515ms/step - loss: 0.1662 - accuracy: 0.9315 - recall: 0.9317 - f1: 0.9315 - val_loss: 0.2525 - val_accuracy: 0.8886 - val_recall: 0.8885 - val_f1: 0.8886\n",
      "Epoch 36/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1689 - accuracy: 0.9295 - recall: 0.9297 - f1: 0.9295\n",
      "Epoch 00036: val_loss did not improve from 0.24671\n",
      "150/150 [==============================] - 82s 544ms/step - loss: 0.1688 - accuracy: 0.9295 - recall: 0.9297 - f1: 0.9295 - val_loss: 0.3462 - val_accuracy: 0.8695 - val_recall: 0.8695 - val_f1: 0.8695\n",
      "Epoch 37/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1700 - accuracy: 0.9303 - recall: 0.9301 - f1: 0.9303\n",
      "Epoch 00037: val_loss did not improve from 0.24671\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "150/150 [==============================] - 88s 590ms/step - loss: 0.1703 - accuracy: 0.9302 - recall: 0.9299 - f1: 0.9302 - val_loss: 0.3406 - val_accuracy: 0.8578 - val_recall: 0.8578 - val_f1: 0.8578\n",
      "Epoch 38/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1555 - accuracy: 0.9364 - recall: 0.9365 - f1: 0.9364\n",
      "Epoch 00038: val_loss did not improve from 0.24671\n",
      "150/150 [==============================] - 77s 512ms/step - loss: 0.1554 - accuracy: 0.9365 - recall: 0.9366 - f1: 0.9365 - val_loss: 0.2486 - val_accuracy: 0.8966 - val_recall: 0.8963 - val_f1: 0.8966\n",
      "Epoch 39/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1457 - accuracy: 0.9425 - recall: 0.9428 - f1: 0.9425\n",
      "Epoch 00039: val_loss did not improve from 0.24671\n",
      "150/150 [==============================] - 77s 513ms/step - loss: 0.1457 - accuracy: 0.9426 - recall: 0.9429 - f1: 0.9427 - val_loss: 0.2826 - val_accuracy: 0.8822 - val_recall: 0.8824 - val_f1: 0.8822\n",
      "Epoch 40/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1530 - accuracy: 0.9391 - recall: 0.9390 - f1: 0.9391\n",
      "Epoch 00040: val_loss did not improve from 0.24671\n",
      "150/150 [==============================] - 79s 529ms/step - loss: 0.1535 - accuracy: 0.9390 - recall: 0.9389 - f1: 0.9390 - val_loss: 0.4809 - val_accuracy: 0.8468 - val_recall: 0.8470 - val_f1: 0.8469\n",
      "Epoch 41/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9387 - recall: 0.9388 - f1: 0.9387\n",
      "Epoch 00041: val_loss did not improve from 0.24671\n",
      "150/150 [==============================] - 73s 490ms/step - loss: 0.1491 - accuracy: 0.9387 - recall: 0.9388 - f1: 0.9387 - val_loss: 0.2488 - val_accuracy: 0.8922 - val_recall: 0.8925 - val_f1: 0.8923\n",
      "Epoch 42/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1532 - accuracy: 0.9348 - recall: 0.9347 - f1: 0.9348\n",
      "Epoch 00042: val_loss improved from 0.24671 to 0.21734, saving model to batch_LeakyReLU_validation_160_dropout-0.25-042-0.934667-0.906233.h5\n",
      "150/150 [==============================] - 71s 476ms/step - loss: 0.1532 - accuracy: 0.9347 - recall: 0.9346 - f1: 0.9347 - val_loss: 0.2173 - val_accuracy: 0.9062 - val_recall: 0.9060 - val_f1: 0.9062\n",
      "Epoch 43/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1465 - accuracy: 0.9384 - recall: 0.9383 - f1: 0.9384\n",
      "Epoch 00043: val_loss did not improve from 0.21734\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "150/150 [==============================] - 75s 498ms/step - loss: 0.1465 - accuracy: 0.9383 - recall: 0.9382 - f1: 0.9383 - val_loss: 0.2562 - val_accuracy: 0.8901 - val_recall: 0.8900 - val_f1: 0.8901\n",
      "Epoch 44/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1483 - accuracy: 0.9434 - recall: 0.9435 - f1: 0.9434\n",
      "Epoch 00044: val_loss improved from 0.21734 to 0.20064, saving model to batch_LeakyReLU_validation_160_dropout-0.25-044-0.943500-0.913967.h5\n",
      "150/150 [==============================] - 84s 559ms/step - loss: 0.1480 - accuracy: 0.9435 - recall: 0.9436 - f1: 0.9435 - val_loss: 0.2006 - val_accuracy: 0.9140 - val_recall: 0.9141 - val_f1: 0.9140\n",
      "Epoch 45/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1364 - accuracy: 0.9461 - recall: 0.9463 - f1: 0.9462\n",
      "Epoch 00045: val_loss improved from 0.20064 to 0.18885, saving model to batch_LeakyReLU_validation_160_dropout-0.25-045-0.946233-0.916167.h5\n",
      "150/150 [==============================] - 78s 522ms/step - loss: 0.1361 - accuracy: 0.9462 - recall: 0.9464 - f1: 0.9462 - val_loss: 0.1889 - val_accuracy: 0.9162 - val_recall: 0.9162 - val_f1: 0.9162\n",
      "Epoch 46/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1327 - accuracy: 0.9478 - recall: 0.9478 - f1: 0.9478\n",
      "Epoch 00046: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 79s 528ms/step - loss: 0.1326 - accuracy: 0.9480 - recall: 0.9479 - f1: 0.9480 - val_loss: 0.1969 - val_accuracy: 0.9193 - val_recall: 0.9192 - val_f1: 0.9193\n",
      "Epoch 47/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.9452 - recall: 0.9450 - f1: 0.9452\n",
      "Epoch 00047: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 80s 536ms/step - loss: 0.1364 - accuracy: 0.9453 - recall: 0.9451 - f1: 0.9453 - val_loss: 0.3870 - val_accuracy: 0.8570 - val_recall: 0.8570 - val_f1: 0.8570\n",
      "Epoch 48/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1360 - accuracy: 0.9454 - recall: 0.9455 - f1: 0.9454\n",
      "Epoch 00048: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 69s 460ms/step - loss: 0.1357 - accuracy: 0.9455 - recall: 0.9456 - f1: 0.9455 - val_loss: 0.2036 - val_accuracy: 0.9154 - val_recall: 0.9153 - val_f1: 0.9154\n",
      "Epoch 49/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1325 - accuracy: 0.9460 - recall: 0.9460 - f1: 0.9460\n",
      "Epoch 00049: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 71s 472ms/step - loss: 0.1328 - accuracy: 0.9459 - recall: 0.9459 - f1: 0.9459 - val_loss: 0.2504 - val_accuracy: 0.8917 - val_recall: 0.8916 - val_f1: 0.8917\n",
      "Epoch 50/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1392 - accuracy: 0.9416 - recall: 0.9417 - f1: 0.9416\n",
      "Epoch 00050: val_loss did not improve from 0.18885\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "150/150 [==============================] - 82s 545ms/step - loss: 0.1392 - accuracy: 0.9418 - recall: 0.9418 - f1: 0.9418 - val_loss: 0.2183 - val_accuracy: 0.9081 - val_recall: 0.9082 - val_f1: 0.9081\n",
      "Epoch 51/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1368 - accuracy: 0.9460 - recall: 0.9462 - f1: 0.9460\n",
      "Epoch 00051: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 80s 530ms/step - loss: 0.1373 - accuracy: 0.9459 - recall: 0.9461 - f1: 0.9459 - val_loss: 0.2017 - val_accuracy: 0.9138 - val_recall: 0.9135 - val_f1: 0.9138\n",
      "Epoch 52/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1296 - accuracy: 0.9469 - recall: 0.9468 - f1: 0.9469\n",
      "Epoch 00052: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 77s 510ms/step - loss: 0.1293 - accuracy: 0.9470 - recall: 0.9469 - f1: 0.9470 - val_loss: 0.1925 - val_accuracy: 0.9194 - val_recall: 0.9193 - val_f1: 0.9194\n",
      "Epoch 53/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1294 - accuracy: 0.9482 - recall: 0.9481 - f1: 0.9482\n",
      "Epoch 00053: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 85s 566ms/step - loss: 0.1290 - accuracy: 0.9484 - recall: 0.9483 - f1: 0.9484 - val_loss: 0.2066 - val_accuracy: 0.9141 - val_recall: 0.9140 - val_f1: 0.9141\n",
      "Epoch 54/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1325 - accuracy: 0.9480 - recall: 0.9479 - f1: 0.9480\n",
      "Epoch 00054: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 74s 493ms/step - loss: 0.1323 - accuracy: 0.9480 - recall: 0.9479 - f1: 0.9480 - val_loss: 0.1966 - val_accuracy: 0.9188 - val_recall: 0.9187 - val_f1: 0.9188\n",
      "Epoch 55/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9466 - recall: 0.9465 - f1: 0.9466\n",
      "Epoch 00055: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 73s 485ms/step - loss: 0.1261 - accuracy: 0.9468 - recall: 0.9467 - f1: 0.9468 - val_loss: 0.2087 - val_accuracy: 0.9116 - val_recall: 0.9113 - val_f1: 0.9116\n",
      "Epoch 56/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9464 - recall: 0.9460 - f1: 0.9464\n",
      "Epoch 00056: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 80s 534ms/step - loss: 0.1304 - accuracy: 0.9463 - recall: 0.9459 - f1: 0.9463 - val_loss: 0.2095 - val_accuracy: 0.9089 - val_recall: 0.9089 - val_f1: 0.9089\n",
      "Epoch 57/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1350 - accuracy: 0.9459 - recall: 0.9456 - f1: 0.9459\n",
      "Epoch 00057: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 76s 505ms/step - loss: 0.1350 - accuracy: 0.9458 - recall: 0.9455 - f1: 0.9458 - val_loss: 0.2200 - val_accuracy: 0.9053 - val_recall: 0.9050 - val_f1: 0.9052\n",
      "Epoch 58/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9474 - recall: 0.9474 - f1: 0.9474\n",
      "Epoch 00058: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 72s 480ms/step - loss: 0.1304 - accuracy: 0.9474 - recall: 0.9474 - f1: 0.9474 - val_loss: 0.2203 - val_accuracy: 0.9085 - val_recall: 0.9085 - val_f1: 0.9085\n",
      "Epoch 59/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9465 - recall: 0.9467 - f1: 0.9465\n",
      "Epoch 00059: val_loss did not improve from 0.18885\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "150/150 [==============================] - 74s 492ms/step - loss: 0.1268 - accuracy: 0.9463 - recall: 0.9465 - f1: 0.9463 - val_loss: 0.1960 - val_accuracy: 0.9171 - val_recall: 0.9173 - val_f1: 0.9171\n",
      "Epoch 60/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9509 - recall: 0.9509 - f1: 0.9509\n",
      "Epoch 00060: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 78s 519ms/step - loss: 0.1232 - accuracy: 0.9510 - recall: 0.9510 - f1: 0.9510 - val_loss: 0.1898 - val_accuracy: 0.9224 - val_recall: 0.9223 - val_f1: 0.9224\n",
      "Epoch 61/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9501 - recall: 0.9500 - f1: 0.9501\n",
      "Epoch 00061: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 86s 573ms/step - loss: 0.1242 - accuracy: 0.9500 - recall: 0.9499 - f1: 0.9500 - val_loss: 0.1955 - val_accuracy: 0.9207 - val_recall: 0.9207 - val_f1: 0.9207\n",
      "Epoch 62/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9483 - recall: 0.9484 - f1: 0.9483\n",
      "Epoch 00062: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 74s 496ms/step - loss: 0.1273 - accuracy: 0.9485 - recall: 0.9486 - f1: 0.9485 - val_loss: 0.1927 - val_accuracy: 0.9195 - val_recall: 0.9196 - val_f1: 0.9195\n",
      "Epoch 63/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9478 - recall: 0.9477 - f1: 0.9478\n",
      "Epoch 00063: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 78s 521ms/step - loss: 0.1267 - accuracy: 0.9479 - recall: 0.9478 - f1: 0.9479 - val_loss: 0.2019 - val_accuracy: 0.9148 - val_recall: 0.9151 - val_f1: 0.9149\n",
      "Epoch 64/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9472 - recall: 0.9473 - f1: 0.9473\n",
      "Epoch 00064: val_loss did not improve from 0.18885\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "150/150 [==============================] - 76s 505ms/step - loss: 0.1280 - accuracy: 0.9474 - recall: 0.9475 - f1: 0.9474 - val_loss: 0.1978 - val_accuracy: 0.9174 - val_recall: 0.9173 - val_f1: 0.9174\n",
      "Epoch 65/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1175 - accuracy: 0.9527 - recall: 0.9526 - f1: 0.9527\n",
      "Epoch 00065: val_loss improved from 0.18885 to 0.17894, saving model to batch_LeakyReLU_validation_160_dropout-0.25-065-0.952900-0.925367.h5\n",
      "150/150 [==============================] - 80s 536ms/step - loss: 0.1173 - accuracy: 0.9529 - recall: 0.9527 - f1: 0.9529 - val_loss: 0.1789 - val_accuracy: 0.9254 - val_recall: 0.9255 - val_f1: 0.9254\n",
      "Epoch 66/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9482 - recall: 0.9485 - f1: 0.9482\n",
      "Epoch 00066: val_loss did not improve from 0.17894\n",
      "150/150 [==============================] - 91s 609ms/step - loss: 0.1281 - accuracy: 0.9483 - recall: 0.9487 - f1: 0.9483 - val_loss: 0.1962 - val_accuracy: 0.9200 - val_recall: 0.9201 - val_f1: 0.9200\n",
      "Epoch 67/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9484 - recall: 0.9485 - f1: 0.9484\n",
      "Epoch 00067: val_loss did not improve from 0.17894\n",
      "150/150 [==============================] - 136s 905ms/step - loss: 0.1239 - accuracy: 0.9486 - recall: 0.9486 - f1: 0.9486 - val_loss: 0.1831 - val_accuracy: 0.9264 - val_recall: 0.9266 - val_f1: 0.9264\n",
      "Epoch 68/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9509 - recall: 0.9509 - f1: 0.9509\n",
      "Epoch 00068: val_loss did not improve from 0.17894\n",
      "150/150 [==============================] - 120s 802ms/step - loss: 0.1223 - accuracy: 0.9505 - recall: 0.9505 - f1: 0.9505 - val_loss: 0.1817 - val_accuracy: 0.9248 - val_recall: 0.9249 - val_f1: 0.9248\n",
      "Epoch 69/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9485 - recall: 0.9487 - f1: 0.9485\n",
      "Epoch 00069: val_loss did not improve from 0.17894\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "150/150 [==============================] - 109s 728ms/step - loss: 0.1247 - accuracy: 0.9485 - recall: 0.9487 - f1: 0.9485 - val_loss: 0.1805 - val_accuracy: 0.9276 - val_recall: 0.9278 - val_f1: 0.9276\n",
      "Epoch 70/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9486 - recall: 0.9488 - f1: 0.9486\n",
      "Epoch 00070: val_loss improved from 0.17894 to 0.17232, saving model to batch_LeakyReLU_validation_160_dropout-0.25-070-0.948567-0.928000.h5\n",
      "150/150 [==============================] - 82s 550ms/step - loss: 0.1267 - accuracy: 0.9486 - recall: 0.9487 - f1: 0.9486 - val_loss: 0.1723 - val_accuracy: 0.9280 - val_recall: 0.9282 - val_f1: 0.9280\n",
      "Epoch 71/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9485 - recall: 0.9484 - f1: 0.9485\n",
      "Epoch 00071: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 81s 543ms/step - loss: 0.1245 - accuracy: 0.9484 - recall: 0.9483 - f1: 0.9484 - val_loss: 0.1776 - val_accuracy: 0.9262 - val_recall: 0.9262 - val_f1: 0.9262\n",
      "Epoch 72/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9509 - recall: 0.9513 - f1: 0.9510\n",
      "Epoch 00072: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 85s 564ms/step - loss: 0.1233 - accuracy: 0.9511 - recall: 0.9514 - f1: 0.9511 - val_loss: 0.1814 - val_accuracy: 0.9236 - val_recall: 0.9236 - val_f1: 0.9236\n",
      "Epoch 73/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9487 - recall: 0.9485 - f1: 0.9487\n",
      "Epoch 00073: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "150/150 [==============================] - 96s 640ms/step - loss: 0.1234 - accuracy: 0.9488 - recall: 0.9486 - f1: 0.9488 - val_loss: 0.1877 - val_accuracy: 0.9220 - val_recall: 0.9221 - val_f1: 0.9220\n",
      "Epoch 74/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9510 - recall: 0.9511 - f1: 0.9510\n",
      "Epoch 00074: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 96s 639ms/step - loss: 0.1221 - accuracy: 0.9510 - recall: 0.9511 - f1: 0.9510 - val_loss: 0.1777 - val_accuracy: 0.9259 - val_recall: 0.9259 - val_f1: 0.9259\n",
      "Epoch 75/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9497 - recall: 0.9497 - f1: 0.9497\n",
      "Epoch 00075: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 76s 509ms/step - loss: 0.1212 - accuracy: 0.9495 - recall: 0.9496 - f1: 0.9495 - val_loss: 0.1915 - val_accuracy: 0.9196 - val_recall: 0.9198 - val_f1: 0.9196\n",
      "Epoch 76/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9506 - recall: 0.9505 - f1: 0.9506\n",
      "Epoch 00076: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 75s 502ms/step - loss: 0.1198 - accuracy: 0.9505 - recall: 0.9505 - f1: 0.9505 - val_loss: 0.1870 - val_accuracy: 0.9224 - val_recall: 0.9225 - val_f1: 0.9224\n",
      "Epoch 77/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9519 - recall: 0.9522 - f1: 0.9520\n",
      "Epoch 00077: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "150/150 [==============================] - 75s 499ms/step - loss: 0.1213 - accuracy: 0.9521 - recall: 0.9523 - f1: 0.9521 - val_loss: 0.1805 - val_accuracy: 0.9251 - val_recall: 0.9252 - val_f1: 0.9251\n",
      "Epoch 78/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9493 - recall: 0.9492 - f1: 0.9493\n",
      "Epoch 00078: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 78s 522ms/step - loss: 0.1214 - accuracy: 0.9492 - recall: 0.9491 - f1: 0.9492 - val_loss: 0.1802 - val_accuracy: 0.9244 - val_recall: 0.9244 - val_f1: 0.9244\n",
      "Epoch 79/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488\n",
      "Epoch 00079: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 77s 511ms/step - loss: 0.1217 - accuracy: 0.9488 - recall: 0.9490 - f1: 0.9488 - val_loss: 0.1870 - val_accuracy: 0.9229 - val_recall: 0.9230 - val_f1: 0.9229\n",
      "Epoch 80/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1205 - accuracy: 0.9514 - recall: 0.9516 - f1: 0.9514\n",
      "Epoch 00080: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1204 - accuracy: 0.9514 - recall: 0.9516 - f1: 0.9514 - val_loss: 0.1811 - val_accuracy: 0.9245 - val_recall: 0.9245 - val_f1: 0.9245\n",
      "Epoch 81/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9465 - recall: 0.9466 - f1: 0.9465\n",
      "Epoch 00081: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1264 - accuracy: 0.9467 - recall: 0.9467 - f1: 0.9467 - val_loss: 0.1865 - val_accuracy: 0.9239 - val_recall: 0.9239 - val_f1: 0.9239\n",
      "Epoch 82/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9501 - recall: 0.9502 - f1: 0.9501\n",
      "Epoch 00082: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 65s 434ms/step - loss: 0.1240 - accuracy: 0.9503 - recall: 0.9503 - f1: 0.9503 - val_loss: 0.1865 - val_accuracy: 0.9233 - val_recall: 0.9233 - val_f1: 0.9233\n",
      "Epoch 83/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9514 - recall: 0.9513 - f1: 0.9514\n",
      "Epoch 00083: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1216 - accuracy: 0.9514 - recall: 0.9514 - f1: 0.9514 - val_loss: 0.1804 - val_accuracy: 0.9253 - val_recall: 0.9253 - val_f1: 0.9253\n",
      "Epoch 84/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1281 - accuracy: 0.9487 - recall: 0.9489 - f1: 0.9487\n",
      "Epoch 00084: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 65s 437ms/step - loss: 0.1282 - accuracy: 0.9486 - recall: 0.9489 - f1: 0.9486 - val_loss: 0.1801 - val_accuracy: 0.9240 - val_recall: 0.9241 - val_f1: 0.9240\n",
      "Epoch 85/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9508 - recall: 0.9509 - f1: 0.9508\n",
      "Epoch 00085: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1229 - accuracy: 0.9509 - recall: 0.9510 - f1: 0.9509 - val_loss: 0.1878 - val_accuracy: 0.9245 - val_recall: 0.9245 - val_f1: 0.9245\n",
      "Epoch 86/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9501 - recall: 0.9503 - f1: 0.9501\n",
      "Epoch 00086: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 65s 434ms/step - loss: 0.1220 - accuracy: 0.9503 - recall: 0.9504 - f1: 0.9503 - val_loss: 0.1841 - val_accuracy: 0.9249 - val_recall: 0.9249 - val_f1: 0.9249\n",
      "Epoch 87/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9468 - recall: 0.9467 - f1: 0.9468\n",
      "Epoch 00087: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 65s 435ms/step - loss: 0.1250 - accuracy: 0.9469 - recall: 0.9468 - f1: 0.9469 - val_loss: 0.1841 - val_accuracy: 0.9267 - val_recall: 0.9266 - val_f1: 0.9267\n",
      "Epoch 88/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9482 - recall: 0.9481 - f1: 0.9482\n",
      "Epoch 00088: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1235 - accuracy: 0.9482 - recall: 0.9481 - f1: 0.9482 - val_loss: 0.1828 - val_accuracy: 0.9250 - val_recall: 0.9251 - val_f1: 0.9250\n",
      "Epoch 89/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1179 - accuracy: 0.9522 - recall: 0.9522 - f1: 0.9522\n",
      "Epoch 00089: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1180 - accuracy: 0.9521 - recall: 0.9521 - f1: 0.9521 - val_loss: 0.1863 - val_accuracy: 0.9219 - val_recall: 0.9219 - val_f1: 0.9219\n",
      "Epoch 90/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9516 - recall: 0.9519 - f1: 0.9517\n",
      "Epoch 00090: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1233 - accuracy: 0.9516 - recall: 0.9518 - f1: 0.9516 - val_loss: 0.1875 - val_accuracy: 0.9234 - val_recall: 0.9234 - val_f1: 0.9234\n",
      "Epoch 91/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9504 - recall: 0.9505 - f1: 0.9504\n",
      "Epoch 00091: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 72s 479ms/step - loss: 0.1219 - accuracy: 0.9504 - recall: 0.9505 - f1: 0.9504 - val_loss: 0.1834 - val_accuracy: 0.9237 - val_recall: 0.9236 - val_f1: 0.9237\n",
      "Epoch 92/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1257 - accuracy: 0.9510 - recall: 0.9513 - f1: 0.9510\n",
      "Epoch 00092: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 70s 468ms/step - loss: 0.1251 - accuracy: 0.9513 - recall: 0.9516 - f1: 0.9513 - val_loss: 0.1908 - val_accuracy: 0.9217 - val_recall: 0.9218 - val_f1: 0.9217\n",
      "Epoch 93/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488\n",
      "Epoch 00093: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "150/150 [==============================] - 70s 465ms/step - loss: 0.1262 - accuracy: 0.9487 - recall: 0.9488 - f1: 0.9487 - val_loss: 0.1798 - val_accuracy: 0.9273 - val_recall: 0.9273 - val_f1: 0.9273\n",
      "Epoch 94/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9479 - recall: 0.9481 - f1: 0.9479\n",
      "Epoch 00094: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 73s 489ms/step - loss: 0.1247 - accuracy: 0.9479 - recall: 0.9480 - f1: 0.9479 - val_loss: 0.1853 - val_accuracy: 0.9232 - val_recall: 0.9232 - val_f1: 0.9232\n",
      "Epoch 95/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9519 - recall: 0.9519 - f1: 0.9519\n",
      "Epoch 00095: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 74s 496ms/step - loss: 0.1208 - accuracy: 0.9522 - recall: 0.9522 - f1: 0.9522 - val_loss: 0.1833 - val_accuracy: 0.9259 - val_recall: 0.9259 - val_f1: 0.9259\n",
      "Epoch 96/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9493 - recall: 0.9493 - f1: 0.9493\n",
      "Epoch 00096: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 76s 504ms/step - loss: 0.1253 - accuracy: 0.9492 - recall: 0.9491 - f1: 0.9492 - val_loss: 0.1818 - val_accuracy: 0.9275 - val_recall: 0.9276 - val_f1: 0.9275\n",
      "Epoch 97/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1189 - accuracy: 0.9505 - recall: 0.9507 - f1: 0.9505\n",
      "Epoch 00097: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "150/150 [==============================] - 79s 525ms/step - loss: 0.1191 - accuracy: 0.9504 - recall: 0.9506 - f1: 0.9504 - val_loss: 0.1901 - val_accuracy: 0.9218 - val_recall: 0.9219 - val_f1: 0.9218\n",
      "Epoch 98/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9491 - recall: 0.9490 - f1: 0.9491\n",
      "Epoch 00098: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 77s 510ms/step - loss: 0.1284 - accuracy: 0.9490 - recall: 0.9489 - f1: 0.9490 - val_loss: 0.1841 - val_accuracy: 0.9236 - val_recall: 0.9235 - val_f1: 0.9236\n",
      "Epoch 99/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9497 - recall: 0.9496 - f1: 0.9497\n",
      "Epoch 00099: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 105s 699ms/step - loss: 0.1234 - accuracy: 0.9497 - recall: 0.9496 - f1: 0.9497 - val_loss: 0.1841 - val_accuracy: 0.9220 - val_recall: 0.9221 - val_f1: 0.9220\n",
      "Epoch 100/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1295 - accuracy: 0.9462 - recall: 0.9462 - f1: 0.9462\n",
      "Epoch 00100: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 82s 546ms/step - loss: 0.1299 - accuracy: 0.9459 - recall: 0.9459 - f1: 0.9459 - val_loss: 0.1819 - val_accuracy: 0.9240 - val_recall: 0.9241 - val_f1: 0.9240\n",
      "Epoch 101/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9519 - recall: 0.9521 - f1: 0.9519\n",
      "Epoch 00101: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "150/150 [==============================] - 75s 502ms/step - loss: 0.1216 - accuracy: 0.9520 - recall: 0.9521 - f1: 0.9520 - val_loss: 0.1862 - val_accuracy: 0.9226 - val_recall: 0.9227 - val_f1: 0.9226\n",
      "Epoch 102/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1249 - accuracy: 0.9472 - recall: 0.9474 - f1: 0.9473\n",
      "Epoch 00102: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 75s 502ms/step - loss: 0.1249 - accuracy: 0.9472 - recall: 0.9473 - f1: 0.9472 - val_loss: 0.1883 - val_accuracy: 0.9227 - val_recall: 0.9227 - val_f1: 0.9227\n",
      "Epoch 103/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9499 - recall: 0.9499 - f1: 0.9499\n",
      "Epoch 00103: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 73s 489ms/step - loss: 0.1248 - accuracy: 0.9500 - recall: 0.9500 - f1: 0.9500 - val_loss: 0.1862 - val_accuracy: 0.9227 - val_recall: 0.9227 - val_f1: 0.9227\n",
      "Epoch 104/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1203 - accuracy: 0.9509 - recall: 0.9508 - f1: 0.9509\n",
      "Epoch 00104: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 79s 528ms/step - loss: 0.1203 - accuracy: 0.9509 - recall: 0.9509 - f1: 0.9509 - val_loss: 0.1851 - val_accuracy: 0.9248 - val_recall: 0.9249 - val_f1: 0.9248\n",
      "Epoch 105/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1183 - accuracy: 0.9505 - recall: 0.9503 - f1: 0.9505\n",
      "Epoch 00105: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "150/150 [==============================] - 76s 503ms/step - loss: 0.1185 - accuracy: 0.9504 - recall: 0.9502 - f1: 0.9504 - val_loss: 0.1882 - val_accuracy: 0.9185 - val_recall: 0.9186 - val_f1: 0.9185\n",
      "Epoch 106/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9479 - recall: 0.9480 - f1: 0.9479\n",
      "Epoch 00106: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 75s 497ms/step - loss: 0.1253 - accuracy: 0.9480 - recall: 0.9481 - f1: 0.9480 - val_loss: 0.1830 - val_accuracy: 0.9253 - val_recall: 0.9255 - val_f1: 0.9253\n",
      "Epoch 107/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9514 - recall: 0.9515 - f1: 0.9514\n",
      "Epoch 00107: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 77s 512ms/step - loss: 0.1217 - accuracy: 0.9515 - recall: 0.9516 - f1: 0.9515 - val_loss: 0.1894 - val_accuracy: 0.9216 - val_recall: 0.9217 - val_f1: 0.9216\n",
      "Epoch 108/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9492 - recall: 0.9492 - f1: 0.9492\n",
      "Epoch 00108: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 80s 531ms/step - loss: 0.1270 - accuracy: 0.9492 - recall: 0.9492 - f1: 0.9492 - val_loss: 0.1849 - val_accuracy: 0.9216 - val_recall: 0.9216 - val_f1: 0.9216\n",
      "Epoch 109/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9499 - recall: 0.9500 - f1: 0.9499\n",
      "Epoch 00109: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "150/150 [==============================] - 95s 631ms/step - loss: 0.1233 - accuracy: 0.9498 - recall: 0.9499 - f1: 0.9498 - val_loss: 0.1827 - val_accuracy: 0.9255 - val_recall: 0.9255 - val_f1: 0.9255\n",
      "Epoch 110/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9489 - recall: 0.9489 - f1: 0.9489\n",
      "Epoch 00110: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 78s 518ms/step - loss: 0.1256 - accuracy: 0.9485 - recall: 0.9486 - f1: 0.9485 - val_loss: 0.1844 - val_accuracy: 0.9251 - val_recall: 0.9251 - val_f1: 0.9251\n",
      "Epoch 111/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9503 - recall: 0.9505 - f1: 0.9503\n",
      "Epoch 00111: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 77s 512ms/step - loss: 0.1203 - accuracy: 0.9503 - recall: 0.9505 - f1: 0.9503 - val_loss: 0.1886 - val_accuracy: 0.9234 - val_recall: 0.9235 - val_f1: 0.9234\n",
      "Epoch 112/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9496 - recall: 0.9494 - f1: 0.9496\n",
      "Epoch 00112: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 74s 492ms/step - loss: 0.1235 - accuracy: 0.9498 - recall: 0.9496 - f1: 0.9498 - val_loss: 0.1772 - val_accuracy: 0.9248 - val_recall: 0.9247 - val_f1: 0.9248\n",
      "Epoch 113/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9484 - recall: 0.9485 - f1: 0.9484\n",
      "Epoch 00113: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00113: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "150/150 [==============================] - 73s 488ms/step - loss: 0.1246 - accuracy: 0.9484 - recall: 0.9485 - f1: 0.9484 - val_loss: 0.1813 - val_accuracy: 0.9253 - val_recall: 0.9252 - val_f1: 0.9253\n",
      "Epoch 114/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1181 - accuracy: 0.9528 - recall: 0.9528 - f1: 0.9528\n",
      "Epoch 00114: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 72s 482ms/step - loss: 0.1182 - accuracy: 0.9528 - recall: 0.9529 - f1: 0.9528 - val_loss: 0.1882 - val_accuracy: 0.9215 - val_recall: 0.9217 - val_f1: 0.9216\n",
      "Epoch 115/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1233 - accuracy: 0.9507 - recall: 0.9506 - f1: 0.9507\n",
      "Epoch 00115: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 71s 474ms/step - loss: 0.1229 - accuracy: 0.9509 - recall: 0.9508 - f1: 0.9509 - val_loss: 0.1788 - val_accuracy: 0.9255 - val_recall: 0.9254 - val_f1: 0.9255\n",
      "Epoch 116/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9530 - recall: 0.9528 - f1: 0.9530\n",
      "Epoch 00116: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 72s 482ms/step - loss: 0.1198 - accuracy: 0.9529 - recall: 0.9527 - f1: 0.9529 - val_loss: 0.1791 - val_accuracy: 0.9259 - val_recall: 0.9259 - val_f1: 0.9259\n",
      "Epoch 117/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9502 - recall: 0.9504 - f1: 0.9502\n",
      "Epoch 00117: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00117: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "150/150 [==============================] - 73s 487ms/step - loss: 0.1209 - accuracy: 0.9503 - recall: 0.9505 - f1: 0.9503 - val_loss: 0.1843 - val_accuracy: 0.9247 - val_recall: 0.9245 - val_f1: 0.9247\n",
      "Epoch 118/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1180 - accuracy: 0.9525 - recall: 0.9525 - f1: 0.9525\n",
      "Epoch 00118: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 72s 479ms/step - loss: 0.1180 - accuracy: 0.9526 - recall: 0.9525 - f1: 0.9526 - val_loss: 0.1834 - val_accuracy: 0.9236 - val_recall: 0.9237 - val_f1: 0.9236\n",
      "Epoch 119/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1248 - accuracy: 0.9487 - recall: 0.9488 - f1: 0.9487\n",
      "Epoch 00119: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 72s 477ms/step - loss: 0.1245 - accuracy: 0.9487 - recall: 0.9488 - f1: 0.9487 - val_loss: 0.1819 - val_accuracy: 0.9239 - val_recall: 0.9240 - val_f1: 0.9239\n",
      "Epoch 120/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9493 - recall: 0.9491 - f1: 0.9493\n",
      "Epoch 00120: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 73s 485ms/step - loss: 0.1256 - accuracy: 0.9495 - recall: 0.9493 - f1: 0.9495 - val_loss: 0.1768 - val_accuracy: 0.9265 - val_recall: 0.9265 - val_f1: 0.9265\n",
      "Epoch 121/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9494 - recall: 0.9494 - f1: 0.9494\n",
      "Epoch 00121: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00121: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "150/150 [==============================] - 81s 540ms/step - loss: 0.1234 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495 - val_loss: 0.1984 - val_accuracy: 0.9184 - val_recall: 0.9185 - val_f1: 0.9184\n",
      "Epoch 122/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1283 - accuracy: 0.9494 - recall: 0.9495 - f1: 0.9494\n",
      "Epoch 00122: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 73s 486ms/step - loss: 0.1279 - accuracy: 0.9495 - recall: 0.9496 - f1: 0.9495 - val_loss: 0.1787 - val_accuracy: 0.9253 - val_recall: 0.9253 - val_f1: 0.9253\n",
      "Epoch 123/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9521 - recall: 0.9521 - f1: 0.9521\n",
      "Epoch 00123: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 79s 526ms/step - loss: 0.1221 - accuracy: 0.9518 - recall: 0.9519 - f1: 0.9518 - val_loss: 0.1827 - val_accuracy: 0.9223 - val_recall: 0.9225 - val_f1: 0.9223\n",
      "Epoch 124/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9486 - recall: 0.9487 - f1: 0.9486\n",
      "Epoch 00124: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 83s 551ms/step - loss: 0.1241 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488 - val_loss: 0.1827 - val_accuracy: 0.9241 - val_recall: 0.9241 - val_f1: 0.9241\n",
      "Epoch 125/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9536 - recall: 0.9536 - f1: 0.9536\n",
      "Epoch 00125: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00125: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "150/150 [==============================] - 86s 576ms/step - loss: 0.1193 - accuracy: 0.9535 - recall: 0.9535 - f1: 0.9535 - val_loss: 0.1857 - val_accuracy: 0.9232 - val_recall: 0.9233 - val_f1: 0.9232\n",
      "Epoch 126/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9480 - recall: 0.9479 - f1: 0.9480\n",
      "Epoch 00126: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 90s 597ms/step - loss: 0.1241 - accuracy: 0.9479 - recall: 0.9479 - f1: 0.9479 - val_loss: 0.1840 - val_accuracy: 0.9230 - val_recall: 0.9229 - val_f1: 0.9230\n",
      "Epoch 127/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9494 - recall: 0.9491 - f1: 0.9494\n",
      "Epoch 00127: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 104s 693ms/step - loss: 0.1240 - accuracy: 0.9496 - recall: 0.9493 - f1: 0.9496 - val_loss: 0.1754 - val_accuracy: 0.9265 - val_recall: 0.9265 - val_f1: 0.9265\n",
      "Epoch 128/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1285 - accuracy: 0.9485 - recall: 0.9486 - f1: 0.9485\n",
      "Epoch 00128: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 91s 605ms/step - loss: 0.1287 - accuracy: 0.9485 - recall: 0.9486 - f1: 0.9485 - val_loss: 0.1896 - val_accuracy: 0.9227 - val_recall: 0.9227 - val_f1: 0.9227\n",
      "Epoch 129/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1305 - accuracy: 0.9453 - recall: 0.9452 - f1: 0.9453\n",
      "Epoch 00129: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00129: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "150/150 [==============================] - 81s 542ms/step - loss: 0.1305 - accuracy: 0.9453 - recall: 0.9451 - f1: 0.9453 - val_loss: 0.1861 - val_accuracy: 0.9238 - val_recall: 0.9238 - val_f1: 0.9238\n",
      "Epoch 130/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9475 - recall: 0.9476 - f1: 0.9475\n",
      "Epoch 00130: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 82s 545ms/step - loss: 0.1254 - accuracy: 0.9473 - recall: 0.9474 - f1: 0.9473 - val_loss: 0.1799 - val_accuracy: 0.9269 - val_recall: 0.9268 - val_f1: 0.9269\n",
      "Epoch 131/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9499 - recall: 0.9498 - f1: 0.9499\n",
      "Epoch 00131: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 85s 565ms/step - loss: 0.1247 - accuracy: 0.9502 - recall: 0.9501 - f1: 0.9502 - val_loss: 0.1908 - val_accuracy: 0.9199 - val_recall: 0.9199 - val_f1: 0.9199\n",
      "Epoch 132/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9506 - recall: 0.9507 - f1: 0.9506\n",
      "Epoch 00132: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 82s 546ms/step - loss: 0.1208 - accuracy: 0.9506 - recall: 0.9507 - f1: 0.9506 - val_loss: 0.1952 - val_accuracy: 0.9186 - val_recall: 0.9185 - val_f1: 0.9186\n",
      "Epoch 133/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9497 - recall: 0.9497 - f1: 0.9497\n",
      "Epoch 00133: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00133: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "150/150 [==============================] - 82s 544ms/step - loss: 0.1207 - accuracy: 0.9499 - recall: 0.9498 - f1: 0.9499 - val_loss: 0.1880 - val_accuracy: 0.9211 - val_recall: 0.9212 - val_f1: 0.9211\n",
      "Epoch 134/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9502 - recall: 0.9504 - f1: 0.9502\n",
      "Epoch 00134: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 82s 549ms/step - loss: 0.1185 - accuracy: 0.9504 - recall: 0.9506 - f1: 0.9504 - val_loss: 0.1892 - val_accuracy: 0.9214 - val_recall: 0.9214 - val_f1: 0.9214\n",
      "Epoch 135/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9476 - recall: 0.9476 - f1: 0.9476\n",
      "Epoch 00135: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 87s 582ms/step - loss: 0.1266 - accuracy: 0.9476 - recall: 0.9477 - f1: 0.9476 - val_loss: 0.1871 - val_accuracy: 0.9225 - val_recall: 0.9225 - val_f1: 0.9225\n",
      "Epoch 136/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1182 - accuracy: 0.9510 - recall: 0.9510 - f1: 0.9510\n",
      "Epoch 00136: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 84s 559ms/step - loss: 0.1181 - accuracy: 0.9510 - recall: 0.9510 - f1: 0.9510 - val_loss: 0.1851 - val_accuracy: 0.9208 - val_recall: 0.9210 - val_f1: 0.9208\n",
      "Epoch 137/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9505 - recall: 0.9506 - f1: 0.9505\n",
      "Epoch 00137: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00137: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      "150/150 [==============================] - 87s 582ms/step - loss: 0.1266 - accuracy: 0.9507 - recall: 0.9508 - f1: 0.9507 - val_loss: 0.1859 - val_accuracy: 0.9231 - val_recall: 0.9232 - val_f1: 0.9231\n",
      "Epoch 138/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9509 - recall: 0.9510 - f1: 0.9509\n",
      "Epoch 00138: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 85s 567ms/step - loss: 0.1229 - accuracy: 0.9509 - recall: 0.9509 - f1: 0.9509 - val_loss: 0.1866 - val_accuracy: 0.9221 - val_recall: 0.9221 - val_f1: 0.9221\n",
      "Epoch 139/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9482 - recall: 0.9484 - f1: 0.9482\n",
      "Epoch 00139: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 84s 559ms/step - loss: 0.1219 - accuracy: 0.9481 - recall: 0.9483 - f1: 0.9481 - val_loss: 0.1820 - val_accuracy: 0.9231 - val_recall: 0.9232 - val_f1: 0.9231\n",
      "Epoch 140/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1212 - accuracy: 0.9508 - recall: 0.9508 - f1: 0.9508\n",
      "Epoch 00140: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 100s 670ms/step - loss: 0.1210 - accuracy: 0.9508 - recall: 0.9508 - f1: 0.9508 - val_loss: 0.1814 - val_accuracy: 0.9244 - val_recall: 0.9244 - val_f1: 0.9244\n",
      "Epoch 141/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1249 - accuracy: 0.9496 - recall: 0.9495 - f1: 0.9496\n",
      "Epoch 00141: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00141: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
      "150/150 [==============================] - 86s 575ms/step - loss: 0.1250 - accuracy: 0.9496 - recall: 0.9495 - f1: 0.9496 - val_loss: 0.1910 - val_accuracy: 0.9226 - val_recall: 0.9228 - val_f1: 0.9226\n",
      "Epoch 142/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 0.9486 - recall: 0.9487 - f1: 0.9486\n",
      "Epoch 00142: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 91s 610ms/step - loss: 0.1262 - accuracy: 0.9487 - recall: 0.9488 - f1: 0.9487 - val_loss: 0.1798 - val_accuracy: 0.9269 - val_recall: 0.9268 - val_f1: 0.9269\n",
      "Epoch 143/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9509 - recall: 0.9508 - f1: 0.9509\n",
      "Epoch 00143: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 85s 568ms/step - loss: 0.1195 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9508 - val_loss: 0.1875 - val_accuracy: 0.9225 - val_recall: 0.9225 - val_f1: 0.9225\n",
      "Epoch 144/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1203 - accuracy: 0.9521 - recall: 0.9521 - f1: 0.9521\n",
      "Epoch 00144: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 98s 656ms/step - loss: 0.1203 - accuracy: 0.9523 - recall: 0.9523 - f1: 0.9523 - val_loss: 0.1907 - val_accuracy: 0.9226 - val_recall: 0.9226 - val_f1: 0.9226\n",
      "Epoch 145/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9500 - recall: 0.9500 - f1: 0.9500\n",
      "Epoch 00145: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00145: ReduceLROnPlateau reducing learning rate to 2.980232380322967e-11.\n",
      "150/150 [==============================] - 77s 513ms/step - loss: 0.1241 - accuracy: 0.9500 - recall: 0.9500 - f1: 0.9500 - val_loss: 0.1756 - val_accuracy: 0.9266 - val_recall: 0.9269 - val_f1: 0.9266\n",
      "Epoch 146/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1181 - accuracy: 0.9538 - recall: 0.9539 - f1: 0.9538\n",
      "Epoch 00146: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 81s 543ms/step - loss: 0.1180 - accuracy: 0.9538 - recall: 0.9539 - f1: 0.9538 - val_loss: 0.1852 - val_accuracy: 0.9245 - val_recall: 0.9243 - val_f1: 0.9245\n",
      "Epoch 147/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9506 - recall: 0.9506 - f1: 0.9506\n",
      "Epoch 00147: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 75s 499ms/step - loss: 0.1217 - accuracy: 0.9506 - recall: 0.9506 - f1: 0.9506 - val_loss: 0.1844 - val_accuracy: 0.9251 - val_recall: 0.9250 - val_f1: 0.9251\n",
      "Epoch 148/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9522 - recall: 0.9521 - f1: 0.9522\n",
      "Epoch 00148: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 76s 508ms/step - loss: 0.1185 - accuracy: 0.9522 - recall: 0.9522 - f1: 0.9522 - val_loss: 0.1897 - val_accuracy: 0.9225 - val_recall: 0.9227 - val_f1: 0.9225\n",
      "Epoch 149/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9509 - recall: 0.9507 - f1: 0.9509\n",
      "Epoch 00149: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00149: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
      "150/150 [==============================] - 75s 502ms/step - loss: 0.1199 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9508 - val_loss: 0.1849 - val_accuracy: 0.9242 - val_recall: 0.9243 - val_f1: 0.9242\n",
      "Epoch 150/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1209 - accuracy: 0.9519 - recall: 0.9517 - f1: 0.9519\n",
      "Epoch 00150: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 75s 500ms/step - loss: 0.1205 - accuracy: 0.9520 - recall: 0.9518 - f1: 0.9520 - val_loss: 0.1841 - val_accuracy: 0.9231 - val_recall: 0.9232 - val_f1: 0.9231\n",
      "Epoch 151/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9523 - recall: 0.9523 - f1: 0.9523\n",
      "Epoch 00151: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 74s 496ms/step - loss: 0.1218 - accuracy: 0.9525 - recall: 0.9525 - f1: 0.9525 - val_loss: 0.1788 - val_accuracy: 0.9257 - val_recall: 0.9257 - val_f1: 0.9257\n",
      "Epoch 152/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1291 - accuracy: 0.9461 - recall: 0.9460 - f1: 0.9461\n",
      "Epoch 00152: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 74s 496ms/step - loss: 0.1289 - accuracy: 0.9462 - recall: 0.9461 - f1: 0.9462 - val_loss: 0.1897 - val_accuracy: 0.9223 - val_recall: 0.9222 - val_f1: 0.9223\n",
      "Epoch 153/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9490 - recall: 0.9490 - f1: 0.9490\n",
      "Epoch 00153: val_loss did not improve from 0.17232\n",
      "\n",
      "Epoch 00153: ReduceLROnPlateau reducing learning rate to 7.450580950807417e-12.\n",
      "150/150 [==============================] - 75s 500ms/step - loss: 0.1238 - accuracy: 0.9490 - recall: 0.9491 - f1: 0.9490 - val_loss: 0.1855 - val_accuracy: 0.9245 - val_recall: 0.9245 - val_f1: 0.9245\n",
      "Epoch 154/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1165 - accuracy: 0.9526 - recall: 0.9529 - f1: 0.9526\n",
      "Epoch 00154: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 75s 502ms/step - loss: 0.1168 - accuracy: 0.9525 - recall: 0.9528 - f1: 0.9525 - val_loss: 0.1802 - val_accuracy: 0.9248 - val_recall: 0.9249 - val_f1: 0.9248\n",
      "Epoch 155/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9490 - recall: 0.9491 - f1: 0.9490\n",
      "Epoch 00155: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 77s 512ms/step - loss: 0.1228 - accuracy: 0.9492 - recall: 0.9493 - f1: 0.9492 - val_loss: 0.1855 - val_accuracy: 0.9219 - val_recall: 0.9219 - val_f1: 0.9219\n",
      "Epoch 156/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1163 - accuracy: 0.9530 - recall: 0.9531 - f1: 0.9530\n",
      "Epoch 00156: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 77s 513ms/step - loss: 0.1161 - accuracy: 0.9529 - recall: 0.9530 - f1: 0.9529 - val_loss: 0.1847 - val_accuracy: 0.9257 - val_recall: 0.9259 - val_f1: 0.9257\n",
      "Epoch 157/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1159 - accuracy: 0.9538 - recall: 0.9537 - f1: 0.9538\n",
      "Epoch 00157: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 78s 517ms/step - loss: 0.1155 - accuracy: 0.9541 - recall: 0.9539 - f1: 0.9541 - val_loss: 0.1796 - val_accuracy: 0.9256 - val_recall: 0.9256 - val_f1: 0.9256\n",
      "Epoch 158/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9519 - recall: 0.9521 - f1: 0.9520\n",
      "Epoch 00158: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 80s 534ms/step - loss: 0.1181 - accuracy: 0.9523 - recall: 0.9524 - f1: 0.9523 - val_loss: 0.1742 - val_accuracy: 0.9269 - val_recall: 0.9269 - val_f1: 0.9269\n",
      "Epoch 159/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 0.9497 - recall: 0.9498 - f1: 0.9497\n",
      "Epoch 00159: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 76s 510ms/step - loss: 0.1254 - accuracy: 0.9495 - recall: 0.9496 - f1: 0.9495 - val_loss: 0.1852 - val_accuracy: 0.9259 - val_recall: 0.9259 - val_f1: 0.9259\n",
      "Epoch 160/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1143 - accuracy: 0.9530 - recall: 0.9530 - f1: 0.9530\n",
      "Epoch 00160: val_loss did not improve from 0.17232\n",
      "150/150 [==============================] - 77s 511ms/step - loss: 0.1149 - accuracy: 0.9528 - recall: 0.9527 - f1: 0.9528 - val_loss: 0.1878 - val_accuracy: 0.9211 - val_recall: 0.9211 - val_f1: 0.9211\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "# We define the number of epochs and steps per epochs\n",
    "EPOCHS = 160\n",
    "STEPS_PER_EPOCH = 150\n",
    "batch_normalization = True\n",
    "activation = \"LeakyReLU\"\n",
    "model = cnn_model(\n",
    "    shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "# Train the model\n",
    "history = model.train()\n",
    "# model.save(\"no_batch_LeakyRelu_validation_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "plt.savefig(\"batch_LeakyReLU_validation_160_dropout-0.25.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_36 (Conv2D)           (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,450\n",
      "Trainable params: 2,278,018\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,450\n",
      "Trainable params: 2,278,018\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_distributed_function_88617440 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Processing image => data/test_set_images/test_1/test_1.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_2/test_2.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_3/test_3.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_4/test_4.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_5/test_5.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_6/test_6.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_7/test_7.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_8/test_8.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_9/test_9.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_10/test_10.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_11/test_11.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_12/test_12.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_13/test_13.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_14/test_14.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_15/test_15.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_16/test_16.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_17/test_17.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_18/test_18.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_19/test_19.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_20/test_20.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_21/test_21.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_22/test_22.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_23/test_23.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_24/test_24.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_25/test_25.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_26/test_26.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_27/test_27.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_28/test_28.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_29/test_29.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_30/test_30.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_31/test_31.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_32/test_32.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_33/test_33.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_34/test_34.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_35/test_35.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_36/test_36.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_37/test_37.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_38/test_38.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_39/test_39.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_40/test_40.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_41/test_41.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_42/test_42.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_43/test_43.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_44/test_44.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_45/test_45.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_46/test_46.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_47/test_47.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_48/test_48.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_49/test_49.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_50/test_50.png\n"
     ]
    }
   ],
   "source": [
    "from helpers import *\n",
    "\n",
    "# from cnn_model import cnn_model\n",
    "\n",
    "# Instantiate the model\n",
    "batch_normalization = True\n",
    "activation = \"LeakyReLU\"\n",
    "model = cnn_model(\n",
    "    shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model.load(\n",
    "    \"saved_models/batch_LeakyReLU_validation_160_dropout-0.25-070-0.948567-0.928000.h5\"\n",
    ")\n",
    "\n",
    "# Print a summary to make sure the correct model is used\n",
    "model.model.summary()\n",
    "\n",
    "# We add all test images to an array, used later for generating a submission\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "    image_filenames.append(image_filename)\n",
    "\n",
    "# Set-up submission filename\n",
    "submission_filename = \"to_submit_csv/batch_LeakyReLU_validation_160_dropout-0.25-070-0.948567-0.928000.csv\"\n",
    "\n",
    "# Generates the submission\n",
    "generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # history.history[\"loss\"]\n",
    "# print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history.history[\"accuracy\"])\n",
    "# plt.plot(history.history[\"val_accuracy\"])\n",
    "# plt.title(\"model accuracy\")\n",
    "# plt.ylabel(\"accuracy\")\n",
    "# plt.xlabel(\"epoch\")\n",
    "# plt.legend([\"train\", \"val\"], loc=\"upper left\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the training loss and accuracy\n",
    "# N = EPOCHS\n",
    "# print(N)\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure()\n",
    "# plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "# plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Loss/Accuracy\")\n",
    "# plt.legend(loc=\"lower left\")\n",
    "# plt.show()\n",
    "# plt.savefig(\"model1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "# batch_normalization = False\n",
    "# EPOCHS = 200\n",
    "\n",
    "# activation = \"relu\"\n",
    "# model = cnn_model(\n",
    "#     shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    "# )\n",
    "# # Train the model with batch\n",
    "# model.train()\n",
    "# model.save(\"no_batch_relu_validation.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "# batch_normalization = True\n",
    "# EPOCHS = 200\n",
    "\n",
    "# activation = \"LeakyReLU\"\n",
    "# model = cnn_model(\n",
    "#     shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    "# )\n",
    "# # Train the model with batch\n",
    "# model.train()\n",
    "# model.save(\"batch_LeakyReLU_validation.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,962\n",
      "Trainable params: 2,278,274\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "Executing op BiasAdd in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalAnd in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op FusedBatchNormV3 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignSubVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Relu in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MaxPool in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Shape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op GreaterEqual in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Square in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RealDiv in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mean in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op StopGradient in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op SquaredDifference in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Squeeze in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Rsqrt in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sigmoid in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Minimum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Maximum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Log in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Neg in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mean in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DivNoNan in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op BroadcastGradientArgs in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Tile in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Maximum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op FloorDiv in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reciprocal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ZerosLike in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Select in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LessEqual in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op SigmoidGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op BiasAddGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReluGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RsqrtGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MaxPoolGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op FusedBatchNormGradV3 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ShapeN in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2DBackpropInput in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2DBackpropFilter in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Pow in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sqrt in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RealDiv in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ResourceApplyAdam in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignAddVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Greater in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Equal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignAddVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Round in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.5348 - accuracy: 0.7299 - recall: 0.7180 - f1: 0.7254Executing op FusedBatchNormV3 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.76977, saving model to batch_relu_validation_200-001-0.729967-0.299667.h5\n",
      "150/150 [==============================] - 69s 463ms/step - loss: 0.5342 - accuracy: 0.7300 - recall: 0.7183 - f1: 0.7256 - val_loss: 0.7698 - val_accuracy: 0.2997 - val_recall: 0.2601 - val_f1: 0.2708\n",
      "Epoch 2/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.4046 - accuracy: 0.8150 - recall: 0.8141 - f1: 0.8148\n",
      "Epoch 00002: val_loss did not improve from 0.76977\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.4042 - accuracy: 0.8152 - recall: 0.8142 - f1: 0.8149 - val_loss: 1.4418 - val_accuracy: 0.3293 - val_recall: 0.3305 - val_f1: 0.3301\n",
      "Epoch 3/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3579 - accuracy: 0.8358 - recall: 0.8350 - f1: 0.8357\n",
      "Epoch 00003: val_loss improved from 0.76977 to 0.60472, saving model to batch_relu_validation_200-003-0.836100-0.722567.h5\n",
      "150/150 [==============================] - 68s 450ms/step - loss: 0.3575 - accuracy: 0.8361 - recall: 0.8353 - f1: 0.8359 - val_loss: 0.6047 - val_accuracy: 0.7226 - val_recall: 0.7277 - val_f1: 0.7240\n",
      "Epoch 4/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3371 - accuracy: 0.8480 - recall: 0.8479 - f1: 0.8479\n",
      "Epoch 00004: val_loss did not improve from 0.60472\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.3374 - accuracy: 0.8476 - recall: 0.8475 - f1: 0.8476 - val_loss: 2.5459 - val_accuracy: 0.3527 - val_recall: 0.3521 - val_f1: 0.3523\n",
      "Epoch 5/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3004 - accuracy: 0.8666 - recall: 0.8666 - f1: 0.8666\n",
      "Epoch 00005: val_loss did not improve from 0.60472\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.3001 - accuracy: 0.8668 - recall: 0.8669 - f1: 0.8668 - val_loss: 0.6048 - val_accuracy: 0.7483 - val_recall: 0.7490 - val_f1: 0.7485\n",
      "Epoch 6/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.8753 - recall: 0.8746 - f1: 0.8752\n",
      "Epoch 00006: val_loss improved from 0.60472 to 0.52884, saving model to batch_relu_validation_200-006-0.875400-0.801233.h5\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.2911 - accuracy: 0.8754 - recall: 0.8748 - f1: 0.8753 - val_loss: 0.5288 - val_accuracy: 0.8012 - val_recall: 0.8015 - val_f1: 0.8013\n",
      "Epoch 7/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2592 - accuracy: 0.8859 - recall: 0.8860 - f1: 0.8859\n",
      "Epoch 00007: val_loss improved from 0.52884 to 0.36932, saving model to batch_relu_validation_200-007-0.886033-0.818900.h5\n",
      "150/150 [==============================] - 67s 445ms/step - loss: 0.2589 - accuracy: 0.8860 - recall: 0.8862 - f1: 0.8861 - val_loss: 0.3693 - val_accuracy: 0.8189 - val_recall: 0.8157 - val_f1: 0.8183\n",
      "Epoch 8/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2446 - accuracy: 0.8984 - recall: 0.8973 - f1: 0.8983\n",
      "Epoch 00008: val_loss did not improve from 0.36932\n",
      "150/150 [==============================] - 68s 451ms/step - loss: 0.2442 - accuracy: 0.8986 - recall: 0.8975 - f1: 0.8985 - val_loss: 0.4621 - val_accuracy: 0.7692 - val_recall: 0.7690 - val_f1: 0.7691\n",
      "Epoch 9/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2412 - accuracy: 0.9003 - recall: 0.8985 - f1: 0.9001\n",
      "Epoch 00009: val_loss improved from 0.36932 to 0.36775, saving model to batch_relu_validation_200-009-0.900133-0.816667.h5\n",
      "150/150 [==============================] - 67s 450ms/step - loss: 0.2413 - accuracy: 0.9001 - recall: 0.8983 - f1: 0.8999 - val_loss: 0.3678 - val_accuracy: 0.8167 - val_recall: 0.8115 - val_f1: 0.8157\n",
      "Epoch 10/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2381 - accuracy: 0.9005 - recall: 0.9007 - f1: 0.9005\n",
      "Epoch 00010: val_loss did not improve from 0.36775\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.2380 - accuracy: 0.9005 - recall: 0.9008 - f1: 0.9006 - val_loss: 0.3807 - val_accuracy: 0.8101 - val_recall: 0.8083 - val_f1: 0.8097\n",
      "Epoch 11/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2279 - accuracy: 0.9068 - recall: 0.9077 - f1: 0.9069\n",
      "Epoch 00011: val_loss did not improve from 0.36775\n",
      "150/150 [==============================] - 68s 454ms/step - loss: 0.2276 - accuracy: 0.9071 - recall: 0.9079 - f1: 0.9072 - val_loss: 0.4172 - val_accuracy: 0.7904 - val_recall: 0.7915 - val_f1: 0.7906\n",
      "Epoch 12/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2258 - accuracy: 0.9060 - recall: 0.9060 - f1: 0.9060\n",
      "Epoch 00012: val_loss did not improve from 0.36775\n",
      "150/150 [==============================] - 70s 468ms/step - loss: 0.2254 - accuracy: 0.9061 - recall: 0.9061 - f1: 0.9061 - val_loss: 0.5616 - val_accuracy: 0.7799 - val_recall: 0.7797 - val_f1: 0.7798\n",
      "Epoch 13/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2214 - accuracy: 0.9072 - recall: 0.9067 - f1: 0.9072\n",
      "Epoch 00013: val_loss did not improve from 0.36775\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.2212 - accuracy: 0.9073 - recall: 0.9068 - f1: 0.9073 - val_loss: 0.5648 - val_accuracy: 0.7388 - val_recall: 0.7327 - val_f1: 0.7372\n",
      "Epoch 14/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2235 - accuracy: 0.9080 - recall: 0.9068 - f1: 0.9078\n",
      "Epoch 00014: val_loss did not improve from 0.36775\n",
      "150/150 [==============================] - 69s 462ms/step - loss: 0.2237 - accuracy: 0.9078 - recall: 0.9068 - f1: 0.9077 - val_loss: 0.5860 - val_accuracy: 0.8134 - val_recall: 0.8133 - val_f1: 0.8133\n",
      "Epoch 15/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2093 - accuracy: 0.9114 - recall: 0.9115 - f1: 0.9114\n",
      "Epoch 00015: val_loss did not improve from 0.36775\n",
      "150/150 [==============================] - 68s 452ms/step - loss: 0.2090 - accuracy: 0.9115 - recall: 0.9116 - f1: 0.9115 - val_loss: 0.4771 - val_accuracy: 0.8019 - val_recall: 0.8017 - val_f1: 0.8018\n",
      "Epoch 16/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2046 - accuracy: 0.9185 - recall: 0.9182 - f1: 0.9184\n",
      "Epoch 00016: val_loss did not improve from 0.36775\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.2041 - accuracy: 0.9186 - recall: 0.9183 - f1: 0.9186 - val_loss: 0.6455 - val_accuracy: 0.8043 - val_recall: 0.8048 - val_f1: 0.8044\n",
      "Epoch 17/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2025 - accuracy: 0.9173 - recall: 0.9169 - f1: 0.9173\n",
      "Epoch 00017: val_loss improved from 0.36775 to 0.24256, saving model to batch_relu_validation_200-017-0.917400-0.897000.h5\n",
      "150/150 [==============================] - 68s 451ms/step - loss: 0.2025 - accuracy: 0.9174 - recall: 0.9171 - f1: 0.9174 - val_loss: 0.2426 - val_accuracy: 0.8970 - val_recall: 0.8962 - val_f1: 0.8969\n",
      "Epoch 18/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2041 - accuracy: 0.9160 - recall: 0.9160 - f1: 0.9160\n",
      "Epoch 00018: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 70s 466ms/step - loss: 0.2042 - accuracy: 0.9158 - recall: 0.9157 - f1: 0.9158 - val_loss: 0.5773 - val_accuracy: 0.8211 - val_recall: 0.8211 - val_f1: 0.8211\n",
      "Epoch 19/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1970 - accuracy: 0.9216 - recall: 0.9212 - f1: 0.9215\n",
      "Epoch 00019: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 69s 458ms/step - loss: 0.1969 - accuracy: 0.9217 - recall: 0.9213 - f1: 0.9216 - val_loss: 0.3018 - val_accuracy: 0.8634 - val_recall: 0.8643 - val_f1: 0.8635\n",
      "Epoch 20/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1964 - accuracy: 0.9198 - recall: 0.9198 - f1: 0.9198\n",
      "Epoch 00020: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1963 - accuracy: 0.9198 - recall: 0.9199 - f1: 0.9198 - val_loss: 0.3267 - val_accuracy: 0.8405 - val_recall: 0.8393 - val_f1: 0.8403\n",
      "Epoch 21/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1914 - accuracy: 0.9219 - recall: 0.9218 - f1: 0.9219\n",
      "Epoch 00021: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 67s 445ms/step - loss: 0.1915 - accuracy: 0.9220 - recall: 0.9219 - f1: 0.9220 - val_loss: 1.0318 - val_accuracy: 0.7651 - val_recall: 0.7646 - val_f1: 0.7650\n",
      "Epoch 22/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1948 - accuracy: 0.9173 - recall: 0.9177 - f1: 0.9174\n",
      "Epoch 00022: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.1948 - accuracy: 0.9173 - recall: 0.9177 - f1: 0.9174 - val_loss: 0.5788 - val_accuracy: 0.7910 - val_recall: 0.7910 - val_f1: 0.7910\n",
      "Epoch 23/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1939 - accuracy: 0.9183 - recall: 0.9182 - f1: 0.9183\n",
      "Epoch 00023: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.1939 - accuracy: 0.9182 - recall: 0.9181 - f1: 0.9182 - val_loss: 0.3419 - val_accuracy: 0.8397 - val_recall: 0.8395 - val_f1: 0.8396\n",
      "Epoch 24/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1921 - accuracy: 0.9218 - recall: 0.9221 - f1: 0.9219\n",
      "Epoch 00024: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1925 - accuracy: 0.9217 - recall: 0.9219 - f1: 0.9217 - val_loss: 0.3078 - val_accuracy: 0.8572 - val_recall: 0.8568 - val_f1: 0.8571\n",
      "Epoch 25/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1872 - accuracy: 0.9224 - recall: 0.9223 - f1: 0.9224\n",
      "Epoch 00025: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1872 - accuracy: 0.9224 - recall: 0.9223 - f1: 0.9224 - val_loss: 0.3463 - val_accuracy: 0.8407 - val_recall: 0.8415 - val_f1: 0.8409\n",
      "Epoch 26/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1849 - accuracy: 0.9247 - recall: 0.9245 - f1: 0.9247\n",
      "Epoch 00026: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1852 - accuracy: 0.9246 - recall: 0.9243 - f1: 0.9245 - val_loss: 0.2951 - val_accuracy: 0.8612 - val_recall: 0.8609 - val_f1: 0.8611\n",
      "Epoch 27/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1814 - accuracy: 0.9243 - recall: 0.9246 - f1: 0.9244\n",
      "Epoch 00027: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1811 - accuracy: 0.9243 - recall: 0.9246 - f1: 0.9243 - val_loss: 0.3892 - val_accuracy: 0.8394 - val_recall: 0.8399 - val_f1: 0.8395\n",
      "Epoch 28/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1758 - accuracy: 0.9283 - recall: 0.9281 - f1: 0.9283\n",
      "Epoch 00028: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 67s 448ms/step - loss: 0.1755 - accuracy: 0.9283 - recall: 0.9280 - f1: 0.9282 - val_loss: 0.2735 - val_accuracy: 0.8776 - val_recall: 0.8780 - val_f1: 0.8776\n",
      "Epoch 29/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1765 - accuracy: 0.9298 - recall: 0.9298 - f1: 0.9298\n",
      "Epoch 00029: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 67s 450ms/step - loss: 0.1771 - accuracy: 0.9294 - recall: 0.9293 - f1: 0.9294 - val_loss: 0.2866 - val_accuracy: 0.8688 - val_recall: 0.8696 - val_f1: 0.8689\n",
      "Epoch 30/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1800 - accuracy: 0.9252 - recall: 0.9251 - f1: 0.9252\n",
      "Epoch 00030: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1798 - accuracy: 0.9252 - recall: 0.9251 - f1: 0.9252 - val_loss: 0.6177 - val_accuracy: 0.8203 - val_recall: 0.8202 - val_f1: 0.8203\n",
      "Epoch 31/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1778 - accuracy: 0.9281 - recall: 0.9279 - f1: 0.9280\n",
      "Epoch 00031: val_loss improved from 0.24256 to 0.24005, saving model to batch_relu_validation_200-031-0.928000-0.894400.h5\n",
      "150/150 [==============================] - 67s 445ms/step - loss: 0.1779 - accuracy: 0.9280 - recall: 0.9279 - f1: 0.9280 - val_loss: 0.2401 - val_accuracy: 0.8944 - val_recall: 0.8942 - val_f1: 0.8944\n",
      "Epoch 32/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1699 - accuracy: 0.9332 - recall: 0.9330 - f1: 0.9331\n",
      "Epoch 00032: val_loss did not improve from 0.24005\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1700 - accuracy: 0.9333 - recall: 0.9331 - f1: 0.9332 - val_loss: 0.2408 - val_accuracy: 0.8957 - val_recall: 0.8957 - val_f1: 0.8957\n",
      "Epoch 33/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1630 - accuracy: 0.9326 - recall: 0.9326 - f1: 0.9326\n",
      "Epoch 00033: val_loss did not improve from 0.24005\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1634 - accuracy: 0.9327 - recall: 0.9327 - f1: 0.9327 - val_loss: 0.6738 - val_accuracy: 0.8099 - val_recall: 0.8099 - val_f1: 0.8099\n",
      "Epoch 34/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1732 - accuracy: 0.9306 - recall: 0.9305 - f1: 0.9306\n",
      "Epoch 00034: val_loss did not improve from 0.24005\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1730 - accuracy: 0.9307 - recall: 0.9306 - f1: 0.9307 - val_loss: 0.3861 - val_accuracy: 0.8195 - val_recall: 0.8203 - val_f1: 0.8196\n",
      "Epoch 35/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1702 - accuracy: 0.9308 - recall: 0.9311 - f1: 0.9308\n",
      "Epoch 00035: val_loss did not improve from 0.24005\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1700 - accuracy: 0.9309 - recall: 0.9312 - f1: 0.9309 - val_loss: 0.2610 - val_accuracy: 0.8821 - val_recall: 0.8821 - val_f1: 0.8821\n",
      "Epoch 36/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1692 - accuracy: 0.9282 - recall: 0.9281 - f1: 0.9282\n",
      "Epoch 00036: val_loss improved from 0.24005 to 0.23758, saving model to batch_relu_validation_200-036-0.928267-0.895667.h5\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1690 - accuracy: 0.9283 - recall: 0.9282 - f1: 0.9283 - val_loss: 0.2376 - val_accuracy: 0.8957 - val_recall: 0.8954 - val_f1: 0.8956\n",
      "Epoch 37/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1700 - accuracy: 0.9293 - recall: 0.9296 - f1: 0.9294\n",
      "Epoch 00037: val_loss did not improve from 0.23758\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1701 - accuracy: 0.9294 - recall: 0.9297 - f1: 0.9294 - val_loss: 0.4739 - val_accuracy: 0.8300 - val_recall: 0.8297 - val_f1: 0.8300\n",
      "Epoch 38/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1536 - accuracy: 0.9376 - recall: 0.9377 - f1: 0.9376\n",
      "Epoch 00038: val_loss did not improve from 0.23758\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1533 - accuracy: 0.9379 - recall: 0.9379 - f1: 0.9379 - val_loss: 0.2818 - val_accuracy: 0.8797 - val_recall: 0.8799 - val_f1: 0.8797\n",
      "Epoch 39/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1501 - accuracy: 0.9399 - recall: 0.9400 - f1: 0.9399\n",
      "Epoch 00039: val_loss improved from 0.23758 to 0.23655, saving model to batch_relu_validation_200-039-0.939800-0.898233.h5\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1504 - accuracy: 0.9398 - recall: 0.9399 - f1: 0.9398 - val_loss: 0.2365 - val_accuracy: 0.8982 - val_recall: 0.8982 - val_f1: 0.8982\n",
      "Epoch 40/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1555 - accuracy: 0.9372 - recall: 0.9374 - f1: 0.9372\n",
      "Epoch 00040: val_loss did not improve from 0.23655\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1558 - accuracy: 0.9372 - recall: 0.9373 - f1: 0.9372 - val_loss: 0.3707 - val_accuracy: 0.8761 - val_recall: 0.8761 - val_f1: 0.8761\n",
      "Epoch 41/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1469 - accuracy: 0.9384 - recall: 0.9386 - f1: 0.9384\n",
      "Epoch 00041: val_loss improved from 0.23655 to 0.23002, saving model to batch_relu_validation_200-041-0.938367-0.903433.h5\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1467 - accuracy: 0.9384 - recall: 0.9386 - f1: 0.9384 - val_loss: 0.2300 - val_accuracy: 0.9034 - val_recall: 0.9034 - val_f1: 0.9034\n",
      "Epoch 42/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1489 - accuracy: 0.9402 - recall: 0.9399 - f1: 0.9402\n",
      "Epoch 00042: val_loss did not improve from 0.23002\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1489 - accuracy: 0.9400 - recall: 0.9397 - f1: 0.9400 - val_loss: 0.2344 - val_accuracy: 0.9055 - val_recall: 0.9055 - val_f1: 0.9055\n",
      "Epoch 43/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1477 - accuracy: 0.9405 - recall: 0.9407 - f1: 0.9405\n",
      "Epoch 00043: val_loss did not improve from 0.23002\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1478 - accuracy: 0.9404 - recall: 0.9406 - f1: 0.9404 - val_loss: 0.2849 - val_accuracy: 0.8847 - val_recall: 0.8847 - val_f1: 0.8847\n",
      "Epoch 44/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1513 - accuracy: 0.9395 - recall: 0.9396 - f1: 0.9395\n",
      "Epoch 00044: val_loss did not improve from 0.23002\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1508 - accuracy: 0.9397 - recall: 0.9398 - f1: 0.9397 - val_loss: 0.2475 - val_accuracy: 0.8939 - val_recall: 0.8940 - val_f1: 0.8939\n",
      "Epoch 45/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1379 - accuracy: 0.9463 - recall: 0.9464 - f1: 0.9463\n",
      "Epoch 00045: val_loss did not improve from 0.23002\n",
      "150/150 [==============================] - 67s 448ms/step - loss: 0.1377 - accuracy: 0.9463 - recall: 0.9465 - f1: 0.9463 - val_loss: 0.2551 - val_accuracy: 0.9064 - val_recall: 0.9063 - val_f1: 0.9064\n",
      "Epoch 46/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1411 - accuracy: 0.9428 - recall: 0.9426 - f1: 0.9427\n",
      "Epoch 00046: val_loss did not improve from 0.23002\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1410 - accuracy: 0.9427 - recall: 0.9425 - f1: 0.9427 - val_loss: 0.2369 - val_accuracy: 0.9015 - val_recall: 0.9016 - val_f1: 0.9015\n",
      "Epoch 47/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1439 - accuracy: 0.9424 - recall: 0.9424 - f1: 0.9424\n",
      "Epoch 00047: val_loss did not improve from 0.23002\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1437 - accuracy: 0.9425 - recall: 0.9425 - f1: 0.9425 - val_loss: 0.2907 - val_accuracy: 0.8718 - val_recall: 0.8717 - val_f1: 0.8718\n",
      "Epoch 48/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1444 - accuracy: 0.9426 - recall: 0.9423 - f1: 0.9426\n",
      "Epoch 00048: val_loss did not improve from 0.23002\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1444 - accuracy: 0.9427 - recall: 0.9424 - f1: 0.9427 - val_loss: 0.5852 - val_accuracy: 0.8341 - val_recall: 0.8342 - val_f1: 0.8341\n",
      "Epoch 49/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1380 - accuracy: 0.9440 - recall: 0.9442 - f1: 0.9440\n",
      "Epoch 00049: val_loss did not improve from 0.23002\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1379 - accuracy: 0.9440 - recall: 0.9442 - f1: 0.9440 - val_loss: 0.2356 - val_accuracy: 0.8998 - val_recall: 0.8996 - val_f1: 0.8998\n",
      "Epoch 50/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1381 - accuracy: 0.9448 - recall: 0.9446 - f1: 0.9448\n",
      "Epoch 00050: val_loss did not improve from 0.23002\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1380 - accuracy: 0.9449 - recall: 0.9447 - f1: 0.9449 - val_loss: 0.2405 - val_accuracy: 0.9005 - val_recall: 0.9003 - val_f1: 0.9004\n",
      "Epoch 51/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1411 - accuracy: 0.9451 - recall: 0.9451 - f1: 0.9451\n",
      "Epoch 00051: val_loss improved from 0.23002 to 0.19262, saving model to batch_relu_validation_200-051-0.944800-0.921800.h5\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1416 - accuracy: 0.9448 - recall: 0.9448 - f1: 0.9448 - val_loss: 0.1926 - val_accuracy: 0.9218 - val_recall: 0.9216 - val_f1: 0.9218\n",
      "Epoch 52/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1358 - accuracy: 0.9446 - recall: 0.9448 - f1: 0.9446\n",
      "Epoch 00052: val_loss did not improve from 0.19262\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1356 - accuracy: 0.9447 - recall: 0.9449 - f1: 0.9447 - val_loss: 0.2057 - val_accuracy: 0.9138 - val_recall: 0.9137 - val_f1: 0.9138\n",
      "Epoch 53/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1294 - accuracy: 0.9468 - recall: 0.9469 - f1: 0.9468\n",
      "Epoch 00053: val_loss did not improve from 0.19262\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1289 - accuracy: 0.9471 - recall: 0.9473 - f1: 0.9471 - val_loss: 0.1975 - val_accuracy: 0.9183 - val_recall: 0.9181 - val_f1: 0.9183\n",
      "Epoch 54/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9443 - recall: 0.9447 - f1: 0.9444\n",
      "Epoch 00054: val_loss did not improve from 0.19262\n",
      "150/150 [==============================] - 65s 437ms/step - loss: 0.1340 - accuracy: 0.9445 - recall: 0.9449 - f1: 0.9445 - val_loss: 0.2130 - val_accuracy: 0.9120 - val_recall: 0.9121 - val_f1: 0.9120\n",
      "Epoch 55/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9459 - recall: 0.9463 - f1: 0.9459\n",
      "Epoch 00055: val_loss did not improve from 0.19262\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1305 - accuracy: 0.9459 - recall: 0.9463 - f1: 0.9460 - val_loss: 0.2132 - val_accuracy: 0.9110 - val_recall: 0.9111 - val_f1: 0.9110\n",
      "Epoch 56/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1343 - accuracy: 0.9448 - recall: 0.9448 - f1: 0.9448\n",
      "Epoch 00056: val_loss did not improve from 0.19262\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1345 - accuracy: 0.9447 - recall: 0.9447 - f1: 0.9447 - val_loss: 0.2233 - val_accuracy: 0.9005 - val_recall: 0.9006 - val_f1: 0.9005\n",
      "Epoch 57/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1378 - accuracy: 0.9456 - recall: 0.9457 - f1: 0.9456\n",
      "Epoch 00057: val_loss did not improve from 0.19262\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1378 - accuracy: 0.9457 - recall: 0.9458 - f1: 0.9457 - val_loss: 0.2106 - val_accuracy: 0.9125 - val_recall: 0.9124 - val_f1: 0.9125\n",
      "Epoch 58/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1306 - accuracy: 0.9455 - recall: 0.9457 - f1: 0.9455\n",
      "Epoch 00058: val_loss did not improve from 0.19262\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1310 - accuracy: 0.9454 - recall: 0.9455 - f1: 0.9454 - val_loss: 0.2069 - val_accuracy: 0.9145 - val_recall: 0.9143 - val_f1: 0.9145\n",
      "Epoch 59/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.9469 - recall: 0.9471 - f1: 0.9469\n",
      "Epoch 00059: val_loss improved from 0.19262 to 0.18740, saving model to batch_relu_validation_200-059-0.946800-0.924667.h5\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1313 - accuracy: 0.9468 - recall: 0.9470 - f1: 0.9468 - val_loss: 0.1874 - val_accuracy: 0.9247 - val_recall: 0.9246 - val_f1: 0.9247\n",
      "Epoch 60/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9503 - recall: 0.9504 - f1: 0.9503\n",
      "Epoch 00060: val_loss did not improve from 0.18740\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1250 - accuracy: 0.9503 - recall: 0.9505 - f1: 0.9503 - val_loss: 0.2038 - val_accuracy: 0.9121 - val_recall: 0.9120 - val_f1: 0.9121\n",
      "Epoch 61/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9517 - recall: 0.9516 - f1: 0.9517\n",
      "Epoch 00061: val_loss did not improve from 0.18740\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1199 - accuracy: 0.9517 - recall: 0.9516 - f1: 0.9517 - val_loss: 0.1888 - val_accuracy: 0.9252 - val_recall: 0.9253 - val_f1: 0.9252\n",
      "Epoch 62/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1284 - accuracy: 0.9491 - recall: 0.9491 - f1: 0.9491\n",
      "Epoch 00062: val_loss did not improve from 0.18740\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1281 - accuracy: 0.9493 - recall: 0.9493 - f1: 0.9493 - val_loss: 0.2029 - val_accuracy: 0.9137 - val_recall: 0.9138 - val_f1: 0.9137\n",
      "Epoch 63/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9489 - recall: 0.9491 - f1: 0.9489\n",
      "Epoch 00063: val_loss did not improve from 0.18740\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1269 - accuracy: 0.9490 - recall: 0.9491 - f1: 0.9490 - val_loss: 0.2000 - val_accuracy: 0.9154 - val_recall: 0.9154 - val_f1: 0.9154\n",
      "Epoch 64/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9479 - recall: 0.9479 - f1: 0.9479\n",
      "Epoch 00064: val_loss did not improve from 0.18740\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1262 - accuracy: 0.9482 - recall: 0.9482 - f1: 0.9482 - val_loss: 0.2004 - val_accuracy: 0.9159 - val_recall: 0.9160 - val_f1: 0.9159\n",
      "Epoch 65/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488\n",
      "Epoch 00065: val_loss did not improve from 0.18740\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1199 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488 - val_loss: 0.1875 - val_accuracy: 0.9218 - val_recall: 0.9219 - val_f1: 0.9218\n",
      "Epoch 66/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9483 - recall: 0.9483 - f1: 0.9483\n",
      "Epoch 00066: val_loss did not improve from 0.18740\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1261 - accuracy: 0.9483 - recall: 0.9483 - f1: 0.9483 - val_loss: 0.1899 - val_accuracy: 0.9221 - val_recall: 0.9220 - val_f1: 0.9221\n",
      "Epoch 67/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9498 - recall: 0.9497 - f1: 0.9498\n",
      "Epoch 00067: val_loss improved from 0.18740 to 0.18255, saving model to batch_relu_validation_200-067-0.949700-0.924167.h5\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1241 - accuracy: 0.9497 - recall: 0.9496 - f1: 0.9497 - val_loss: 0.1826 - val_accuracy: 0.9242 - val_recall: 0.9243 - val_f1: 0.9242\n",
      "Epoch 68/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9511 - recall: 0.9511 - f1: 0.9511\n",
      "Epoch 00068: val_loss did not improve from 0.18255\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1188 - accuracy: 0.9510 - recall: 0.9509 - f1: 0.9510 - val_loss: 0.1869 - val_accuracy: 0.9206 - val_recall: 0.9205 - val_f1: 0.9206\n",
      "Epoch 69/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9492 - recall: 0.9493 - f1: 0.9492\n",
      "Epoch 00069: val_loss did not improve from 0.18255\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1230 - accuracy: 0.9492 - recall: 0.9493 - f1: 0.9492 - val_loss: 0.1842 - val_accuracy: 0.9250 - val_recall: 0.9250 - val_f1: 0.9250\n",
      "Epoch 70/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9506 - recall: 0.9504 - f1: 0.9506\n",
      "Epoch 00070: val_loss did not improve from 0.18255\n",
      "150/150 [==============================] - 65s 435ms/step - loss: 0.1207 - accuracy: 0.9506 - recall: 0.9504 - f1: 0.9506 - val_loss: 0.1908 - val_accuracy: 0.9214 - val_recall: 0.9213 - val_f1: 0.9214\n",
      "Epoch 71/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9510 - recall: 0.9511 - f1: 0.9510\n",
      "Epoch 00071: val_loss did not improve from 0.18255\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1214 - accuracy: 0.9509 - recall: 0.9510 - f1: 0.9509 - val_loss: 0.1847 - val_accuracy: 0.9229 - val_recall: 0.9231 - val_f1: 0.9229\n",
      "Epoch 72/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9541 - recall: 0.9542 - f1: 0.9541\n",
      "Epoch 00072: val_loss did not improve from 0.18255\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1217 - accuracy: 0.9543 - recall: 0.9545 - f1: 0.9543 - val_loss: 0.1829 - val_accuracy: 0.9234 - val_recall: 0.9236 - val_f1: 0.9234\n",
      "Epoch 73/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9484 - recall: 0.9486 - f1: 0.9484\n",
      "Epoch 00073: val_loss did not improve from 0.18255\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1239 - accuracy: 0.9485 - recall: 0.9487 - f1: 0.9485 - val_loss: 0.1879 - val_accuracy: 0.9227 - val_recall: 0.9227 - val_f1: 0.9227\n",
      "Epoch 74/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1196 - accuracy: 0.9507 - recall: 0.9509 - f1: 0.9507\n",
      "Epoch 00074: val_loss improved from 0.18255 to 0.17611, saving model to batch_relu_validation_200-074-0.950733-0.929300.h5\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1195 - accuracy: 0.9507 - recall: 0.9509 - f1: 0.9507 - val_loss: 0.1761 - val_accuracy: 0.9293 - val_recall: 0.9293 - val_f1: 0.9293\n",
      "Epoch 75/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9497 - recall: 0.9500 - f1: 0.9497\n",
      "Epoch 00075: val_loss did not improve from 0.17611\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1229 - accuracy: 0.9496 - recall: 0.9499 - f1: 0.9496 - val_loss: 0.1956 - val_accuracy: 0.9195 - val_recall: 0.9195 - val_f1: 0.9195\n",
      "Epoch 76/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9484 - recall: 0.9483 - f1: 0.9484\n",
      "Epoch 00076: val_loss did not improve from 0.17611\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1212 - accuracy: 0.9485 - recall: 0.9484 - f1: 0.9485 - val_loss: 0.1831 - val_accuracy: 0.9224 - val_recall: 0.9223 - val_f1: 0.9224\n",
      "Epoch 77/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1179 - accuracy: 0.9524 - recall: 0.9522 - f1: 0.9524\n",
      "Epoch 00077: val_loss did not improve from 0.17611\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1176 - accuracy: 0.9524 - recall: 0.9523 - f1: 0.9524 - val_loss: 0.1763 - val_accuracy: 0.9283 - val_recall: 0.9285 - val_f1: 0.9283\n",
      "Epoch 78/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9504 - recall: 0.9504 - f1: 0.9504\n",
      "Epoch 00078: val_loss did not improve from 0.17611\n",
      "150/150 [==============================] - 65s 437ms/step - loss: 0.1216 - accuracy: 0.9504 - recall: 0.9504 - f1: 0.9504 - val_loss: 0.1799 - val_accuracy: 0.9275 - val_recall: 0.9276 - val_f1: 0.9275\n",
      "Epoch 79/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1149 - accuracy: 0.9529 - recall: 0.9528 - f1: 0.9528\n",
      "Epoch 00079: val_loss did not improve from 0.17611\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1147 - accuracy: 0.9529 - recall: 0.9528 - f1: 0.9529 - val_loss: 0.1859 - val_accuracy: 0.9236 - val_recall: 0.9235 - val_f1: 0.9236\n",
      "Epoch 80/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 0.9516 - recall: 0.9515 - f1: 0.9516\n",
      "Epoch 00080: val_loss did not improve from 0.17611\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1173 - accuracy: 0.9517 - recall: 0.9517 - f1: 0.9517 - val_loss: 0.1814 - val_accuracy: 0.9248 - val_recall: 0.9249 - val_f1: 0.9248\n",
      "Epoch 81/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1257 - accuracy: 0.9485 - recall: 0.9485 - f1: 0.9485\n",
      "Epoch 00081: val_loss did not improve from 0.17611\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1258 - accuracy: 0.9485 - recall: 0.9485 - f1: 0.9485 - val_loss: 0.1864 - val_accuracy: 0.9236 - val_recall: 0.9236 - val_f1: 0.9236\n",
      "Epoch 82/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9497 - recall: 0.9497 - f1: 0.9497\n",
      "Epoch 00082: val_loss did not improve from 0.17611\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1212 - accuracy: 0.9498 - recall: 0.9498 - f1: 0.9498 - val_loss: 0.1879 - val_accuracy: 0.9233 - val_recall: 0.9232 - val_f1: 0.9233\n",
      "Epoch 83/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9533 - recall: 0.9533 - f1: 0.9533\n",
      "Epoch 00083: val_loss did not improve from 0.17611\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1213 - accuracy: 0.9533 - recall: 0.9533 - f1: 0.9533 - val_loss: 0.1785 - val_accuracy: 0.9283 - val_recall: 0.9285 - val_f1: 0.9283\n",
      "Epoch 84/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9474 - recall: 0.9474 - f1: 0.9474\n",
      "Epoch 00084: val_loss improved from 0.17611 to 0.17608, saving model to batch_relu_validation_200-084-0.947367-0.928833.h5\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1276 - accuracy: 0.9474 - recall: 0.9474 - f1: 0.9474 - val_loss: 0.1761 - val_accuracy: 0.9288 - val_recall: 0.9288 - val_f1: 0.9288\n",
      "Epoch 85/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9523 - recall: 0.9526 - f1: 0.9524\n",
      "Epoch 00085: val_loss did not improve from 0.17608\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1187 - accuracy: 0.9523 - recall: 0.9525 - f1: 0.9523 - val_loss: 0.1852 - val_accuracy: 0.9262 - val_recall: 0.9262 - val_f1: 0.9262\n",
      "Epoch 86/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9529 - recall: 0.9530 - f1: 0.9529\n",
      "Epoch 00086: val_loss did not improve from 0.17608\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1201 - accuracy: 0.9529 - recall: 0.9530 - f1: 0.9529 - val_loss: 0.1835 - val_accuracy: 0.9270 - val_recall: 0.9269 - val_f1: 0.9270\n",
      "Epoch 87/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9522 - recall: 0.9525 - f1: 0.9522\n",
      "Epoch 00087: val_loss did not improve from 0.17608\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1220 - accuracy: 0.9521 - recall: 0.9524 - f1: 0.9521 - val_loss: 0.1818 - val_accuracy: 0.9260 - val_recall: 0.9261 - val_f1: 0.9260\n",
      "Epoch 88/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 0.9498 - recall: 0.9497 - f1: 0.9498\n",
      "Epoch 00088: val_loss did not improve from 0.17608\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1226 - accuracy: 0.9498 - recall: 0.9497 - f1: 0.9498 - val_loss: 0.1836 - val_accuracy: 0.9246 - val_recall: 0.9246 - val_f1: 0.9246\n",
      "Epoch 89/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1181 - accuracy: 0.9512 - recall: 0.9511 - f1: 0.9512\n",
      "Epoch 00089: val_loss did not improve from 0.17608\n",
      "150/150 [==============================] - 65s 434ms/step - loss: 0.1180 - accuracy: 0.9513 - recall: 0.9512 - f1: 0.9513 - val_loss: 0.1824 - val_accuracy: 0.9235 - val_recall: 0.9234 - val_f1: 0.9235\n",
      "Epoch 90/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9515 - recall: 0.9515 - f1: 0.9515\n",
      "Epoch 00090: val_loss did not improve from 0.17608\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1219 - accuracy: 0.9516 - recall: 0.9516 - f1: 0.9516 - val_loss: 0.1817 - val_accuracy: 0.9264 - val_recall: 0.9264 - val_f1: 0.9264\n",
      "Epoch 91/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1181 - accuracy: 0.9528 - recall: 0.9528 - f1: 0.9528\n",
      "Epoch 00091: val_loss did not improve from 0.17608\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1184 - accuracy: 0.9528 - recall: 0.9527 - f1: 0.9528 - val_loss: 0.1776 - val_accuracy: 0.9251 - val_recall: 0.9251 - val_f1: 0.9251\n",
      "Epoch 92/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1233 - accuracy: 0.9510 - recall: 0.9511 - f1: 0.9510\n",
      "Epoch 00092: val_loss did not improve from 0.17608\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1229 - accuracy: 0.9512 - recall: 0.9513 - f1: 0.9512 - val_loss: 0.1862 - val_accuracy: 0.9265 - val_recall: 0.9265 - val_f1: 0.9265\n",
      "Epoch 93/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9486 - recall: 0.9486 - f1: 0.9486\n",
      "Epoch 00093: val_loss improved from 0.17608 to 0.17592, saving model to batch_relu_validation_200-093-0.948367-0.928700.h5\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1242 - accuracy: 0.9484 - recall: 0.9483 - f1: 0.9484 - val_loss: 0.1759 - val_accuracy: 0.9287 - val_recall: 0.9288 - val_f1: 0.9287\n",
      "Epoch 94/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9496 - recall: 0.9494 - f1: 0.9496\n",
      "Epoch 00094: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1242 - accuracy: 0.9495 - recall: 0.9493 - f1: 0.9495 - val_loss: 0.1832 - val_accuracy: 0.9241 - val_recall: 0.9240 - val_f1: 0.9241\n",
      "Epoch 95/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1184 - accuracy: 0.9506 - recall: 0.9507 - f1: 0.9506\n",
      "Epoch 00095: val_loss did not improve from 0.17592\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1182 - accuracy: 0.9507 - recall: 0.9508 - f1: 0.9507 - val_loss: 0.1808 - val_accuracy: 0.9282 - val_recall: 0.9283 - val_f1: 0.9282\n",
      "Epoch 96/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1182 - accuracy: 0.9522 - recall: 0.9519 - f1: 0.9522\n",
      "Epoch 00096: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1180 - accuracy: 0.9523 - recall: 0.9521 - f1: 0.9523 - val_loss: 0.1807 - val_accuracy: 0.9277 - val_recall: 0.9278 - val_f1: 0.9277\n",
      "Epoch 97/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9505 - recall: 0.9503 - f1: 0.9505\n",
      "Epoch 00097: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1198 - accuracy: 0.9506 - recall: 0.9505 - f1: 0.9506 - val_loss: 0.1889 - val_accuracy: 0.9230 - val_recall: 0.9229 - val_f1: 0.9230\n",
      "Epoch 98/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9488 - recall: 0.9490 - f1: 0.9488\n",
      "Epoch 00098: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 65s 433ms/step - loss: 0.1233 - accuracy: 0.9487 - recall: 0.9488 - f1: 0.9487 - val_loss: 0.1817 - val_accuracy: 0.9273 - val_recall: 0.9274 - val_f1: 0.9273\n",
      "Epoch 99/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9508 - recall: 0.9508 - f1: 0.9508\n",
      "Epoch 00099: val_loss did not improve from 0.17592\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1217 - accuracy: 0.9509 - recall: 0.9509 - f1: 0.9509 - val_loss: 0.1842 - val_accuracy: 0.9242 - val_recall: 0.9243 - val_f1: 0.9242\n",
      "Epoch 100/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 0.9488 - recall: 0.9487 - f1: 0.9488\n",
      "Epoch 00100: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 67s 448ms/step - loss: 0.1258 - accuracy: 0.9491 - recall: 0.9490 - f1: 0.9491 - val_loss: 0.1764 - val_accuracy: 0.9276 - val_recall: 0.9277 - val_f1: 0.9276\n",
      "Epoch 101/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9510 - recall: 0.9509 - f1: 0.9510\n",
      "Epoch 00101: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1218 - accuracy: 0.9509 - recall: 0.9509 - f1: 0.9509 - val_loss: 0.1852 - val_accuracy: 0.9244 - val_recall: 0.9245 - val_f1: 0.9244\n",
      "Epoch 102/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9505 - recall: 0.9506 - f1: 0.9505\n",
      "Epoch 00102: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1217 - accuracy: 0.9505 - recall: 0.9506 - f1: 0.9505 - val_loss: 0.1885 - val_accuracy: 0.9240 - val_recall: 0.9240 - val_f1: 0.9240\n",
      "Epoch 103/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9509 - recall: 0.9507 - f1: 0.9509\n",
      "Epoch 00103: val_loss did not improve from 0.17592\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1234 - accuracy: 0.9511 - recall: 0.9509 - f1: 0.9511 - val_loss: 0.1838 - val_accuracy: 0.9244 - val_recall: 0.9243 - val_f1: 0.9244\n",
      "Epoch 104/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1188 - accuracy: 0.9498 - recall: 0.9497 - f1: 0.9498\n",
      "Epoch 00104: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1190 - accuracy: 0.9497 - recall: 0.9496 - f1: 0.9497 - val_loss: 0.1820 - val_accuracy: 0.9268 - val_recall: 0.9267 - val_f1: 0.9268\n",
      "Epoch 105/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9512 - recall: 0.9512 - f1: 0.9512\n",
      "Epoch 00105: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1210 - accuracy: 0.9510 - recall: 0.9511 - f1: 0.9510 - val_loss: 0.1862 - val_accuracy: 0.9206 - val_recall: 0.9205 - val_f1: 0.9206\n",
      "Epoch 106/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9479 - recall: 0.9481 - f1: 0.9479\n",
      "Epoch 00106: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 65s 434ms/step - loss: 0.1236 - accuracy: 0.9480 - recall: 0.9481 - f1: 0.9480 - val_loss: 0.1833 - val_accuracy: 0.9258 - val_recall: 0.9259 - val_f1: 0.9258\n",
      "Epoch 107/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9493 - recall: 0.9493 - f1: 0.9493\n",
      "Epoch 00107: val_loss did not improve from 0.17592\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1210 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495 - val_loss: 0.1890 - val_accuracy: 0.9237 - val_recall: 0.9237 - val_f1: 0.9237\n",
      "Epoch 108/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9497 - recall: 0.9499 - f1: 0.9497\n",
      "Epoch 00108: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 65s 437ms/step - loss: 0.1220 - accuracy: 0.9499 - recall: 0.9501 - f1: 0.9499 - val_loss: 0.1827 - val_accuracy: 0.9220 - val_recall: 0.9219 - val_f1: 0.9220\n",
      "Epoch 109/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9514 - recall: 0.9515 - f1: 0.9514\n",
      "Epoch 00109: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1224 - accuracy: 0.9514 - recall: 0.9515 - f1: 0.9514 - val_loss: 0.1825 - val_accuracy: 0.9286 - val_recall: 0.9287 - val_f1: 0.9286\n",
      "Epoch 110/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9498 - recall: 0.9499 - f1: 0.9498\n",
      "Epoch 00110: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 65s 437ms/step - loss: 0.1240 - accuracy: 0.9494 - recall: 0.9495 - f1: 0.9494 - val_loss: 0.1768 - val_accuracy: 0.9274 - val_recall: 0.9273 - val_f1: 0.9274\n",
      "Epoch 111/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9528 - recall: 0.9529 - f1: 0.9528\n",
      "Epoch 00111: val_loss did not improve from 0.17592\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1187 - accuracy: 0.9529 - recall: 0.9531 - f1: 0.9529 - val_loss: 0.1843 - val_accuracy: 0.9251 - val_recall: 0.9251 - val_f1: 0.9251\n",
      "Epoch 112/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9497 - recall: 0.9499 - f1: 0.9497\n",
      "Epoch 00112: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1213 - accuracy: 0.9497 - recall: 0.9499 - f1: 0.9497 - val_loss: 0.1762 - val_accuracy: 0.9280 - val_recall: 0.9280 - val_f1: 0.9280\n",
      "Epoch 113/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9489 - recall: 0.9487 - f1: 0.9489\n",
      "Epoch 00113: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1208 - accuracy: 0.9489 - recall: 0.9487 - f1: 0.9489 - val_loss: 0.1793 - val_accuracy: 0.9272 - val_recall: 0.9273 - val_f1: 0.9272\n",
      "Epoch 114/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1159 - accuracy: 0.9522 - recall: 0.9522 - f1: 0.9522\n",
      "Epoch 00114: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 65s 433ms/step - loss: 0.1158 - accuracy: 0.9522 - recall: 0.9522 - f1: 0.9522 - val_loss: 0.1835 - val_accuracy: 0.9247 - val_recall: 0.9247 - val_f1: 0.9247\n",
      "Epoch 115/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9511 - recall: 0.9511 - f1: 0.9511\n",
      "Epoch 00115: val_loss improved from 0.17592 to 0.17498, saving model to batch_relu_validation_200-115-0.951267-0.929667.h5\n",
      "\n",
      "Epoch 00115: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1188 - accuracy: 0.9513 - recall: 0.9512 - f1: 0.9513 - val_loss: 0.1750 - val_accuracy: 0.9297 - val_recall: 0.9297 - val_f1: 0.9297\n",
      "Epoch 116/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9497 - recall: 0.9497 - f1: 0.9497\n",
      "Epoch 00116: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1191 - accuracy: 0.9498 - recall: 0.9497 - f1: 0.9498 - val_loss: 0.1791 - val_accuracy: 0.9268 - val_recall: 0.9268 - val_f1: 0.9268\n",
      "Epoch 117/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9527 - recall: 0.9526 - f1: 0.9527\n",
      "Epoch 00117: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1174 - accuracy: 0.9528 - recall: 0.9527 - f1: 0.9528 - val_loss: 0.1810 - val_accuracy: 0.9256 - val_recall: 0.9258 - val_f1: 0.9257\n",
      "Epoch 118/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1161 - accuracy: 0.9527 - recall: 0.9528 - f1: 0.9527\n",
      "Epoch 00118: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 65s 435ms/step - loss: 0.1163 - accuracy: 0.9526 - recall: 0.9527 - f1: 0.9526 - val_loss: 0.1775 - val_accuracy: 0.9278 - val_recall: 0.9281 - val_f1: 0.9279\n",
      "Epoch 119/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 0.9537 - recall: 0.9540 - f1: 0.9537\n",
      "Epoch 00119: val_loss did not improve from 0.17498\n",
      "\n",
      "Epoch 00119: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1170 - accuracy: 0.9538 - recall: 0.9541 - f1: 0.9538 - val_loss: 0.1807 - val_accuracy: 0.9257 - val_recall: 0.9257 - val_f1: 0.9257\n",
      "Epoch 120/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9508 - recall: 0.9511 - f1: 0.9508\n",
      "Epoch 00120: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1228 - accuracy: 0.9505 - recall: 0.9508 - f1: 0.9505 - val_loss: 0.1776 - val_accuracy: 0.9262 - val_recall: 0.9261 - val_f1: 0.9262\n",
      "Epoch 121/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1209 - accuracy: 0.9501 - recall: 0.9503 - f1: 0.9501\n",
      "Epoch 00121: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1205 - accuracy: 0.9504 - recall: 0.9505 - f1: 0.9504 - val_loss: 0.1941 - val_accuracy: 0.9202 - val_recall: 0.9202 - val_f1: 0.9202\n",
      "Epoch 122/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9501 - recall: 0.9500 - f1: 0.9501\n",
      "Epoch 00122: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 65s 434ms/step - loss: 0.1251 - accuracy: 0.9502 - recall: 0.9501 - f1: 0.9502 - val_loss: 0.1757 - val_accuracy: 0.9279 - val_recall: 0.9279 - val_f1: 0.9279\n",
      "Epoch 123/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9501 - recall: 0.9503 - f1: 0.9501\n",
      "Epoch 00123: val_loss did not improve from 0.17498\n",
      "\n",
      "Epoch 00123: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1194 - accuracy: 0.9498 - recall: 0.9500 - f1: 0.9498 - val_loss: 0.1799 - val_accuracy: 0.9246 - val_recall: 0.9247 - val_f1: 0.9246\n",
      "Epoch 124/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9515 - recall: 0.9515 - f1: 0.9515\n",
      "Epoch 00124: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 72s 477ms/step - loss: 0.1214 - accuracy: 0.9516 - recall: 0.9516 - f1: 0.9516 - val_loss: 0.1854 - val_accuracy: 0.9237 - val_recall: 0.9238 - val_f1: 0.9237\n",
      "Epoch 125/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9490 - recall: 0.9490 - f1: 0.9490\n",
      "Epoch 00125: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1227 - accuracy: 0.9489 - recall: 0.9489 - f1: 0.9489 - val_loss: 0.1808 - val_accuracy: 0.9260 - val_recall: 0.9261 - val_f1: 0.9260\n",
      "Epoch 126/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1194 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507\n",
      "Epoch 00126: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1193 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9508 - val_loss: 0.1789 - val_accuracy: 0.9264 - val_recall: 0.9264 - val_f1: 0.9264\n",
      "Epoch 127/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9506 - recall: 0.9506 - f1: 0.9506\n",
      "Epoch 00127: val_loss did not improve from 0.17498\n",
      "\n",
      "Epoch 00127: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.1210 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507 - val_loss: 0.1759 - val_accuracy: 0.9259 - val_recall: 0.9259 - val_f1: 0.9259\n",
      "Epoch 128/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9506 - recall: 0.9508 - f1: 0.9506\n",
      "Epoch 00128: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 67s 445ms/step - loss: 0.1211 - accuracy: 0.9504 - recall: 0.9506 - f1: 0.9504 - val_loss: 0.1851 - val_accuracy: 0.9238 - val_recall: 0.9238 - val_f1: 0.9238\n",
      "Epoch 129/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1271 - accuracy: 0.9470 - recall: 0.9468 - f1: 0.9470\n",
      "Epoch 00129: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1274 - accuracy: 0.9469 - recall: 0.9467 - f1: 0.9469 - val_loss: 0.1815 - val_accuracy: 0.9269 - val_recall: 0.9269 - val_f1: 0.9269\n",
      "Epoch 130/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9493 - recall: 0.9494 - f1: 0.9493\n",
      "Epoch 00130: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1211 - accuracy: 0.9491 - recall: 0.9492 - f1: 0.9491 - val_loss: 0.1771 - val_accuracy: 0.9276 - val_recall: 0.9277 - val_f1: 0.9276\n",
      "Epoch 131/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9489 - recall: 0.9489 - f1: 0.9489\n",
      "Epoch 00131: val_loss did not improve from 0.17498\n",
      "\n",
      "Epoch 00131: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1266 - accuracy: 0.9491 - recall: 0.9491 - f1: 0.9491 - val_loss: 0.1892 - val_accuracy: 0.9224 - val_recall: 0.9225 - val_f1: 0.9224\n",
      "Epoch 132/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9512 - recall: 0.9511 - f1: 0.9512\n",
      "Epoch 00132: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1206 - accuracy: 0.9513 - recall: 0.9511 - f1: 0.9513 - val_loss: 0.1923 - val_accuracy: 0.9202 - val_recall: 0.9203 - val_f1: 0.9202\n",
      "Epoch 133/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1172 - accuracy: 0.9542 - recall: 0.9542 - f1: 0.9542\n",
      "Epoch 00133: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1171 - accuracy: 0.9544 - recall: 0.9544 - f1: 0.9544 - val_loss: 0.1872 - val_accuracy: 0.9241 - val_recall: 0.9241 - val_f1: 0.9241\n",
      "Epoch 134/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1199 - accuracy: 0.9501 - recall: 0.9501 - f1: 0.9501\n",
      "Epoch 00134: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1196 - accuracy: 0.9502 - recall: 0.9501 - f1: 0.9502 - val_loss: 0.1907 - val_accuracy: 0.9223 - val_recall: 0.9222 - val_f1: 0.9223\n",
      "Epoch 135/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9490 - recall: 0.9489 - f1: 0.9490\n",
      "Epoch 00135: val_loss did not improve from 0.17498\n",
      "\n",
      "Epoch 00135: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1218 - accuracy: 0.9490 - recall: 0.9489 - f1: 0.9490 - val_loss: 0.1875 - val_accuracy: 0.9233 - val_recall: 0.9233 - val_f1: 0.9233\n",
      "Epoch 136/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1189 - accuracy: 0.9529 - recall: 0.9528 - f1: 0.9529\n",
      "Epoch 00136: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1187 - accuracy: 0.9530 - recall: 0.9529 - f1: 0.9530 - val_loss: 0.1877 - val_accuracy: 0.9214 - val_recall: 0.9215 - val_f1: 0.9214\n",
      "Epoch 137/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9509 - recall: 0.9509 - f1: 0.9509\n",
      "Epoch 00137: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1232 - accuracy: 0.9511 - recall: 0.9511 - f1: 0.9511 - val_loss: 0.1862 - val_accuracy: 0.9241 - val_recall: 0.9241 - val_f1: 0.9241\n",
      "Epoch 138/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1188 - accuracy: 0.9510 - recall: 0.9512 - f1: 0.9511\n",
      "Epoch 00138: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1191 - accuracy: 0.9509 - recall: 0.9511 - f1: 0.9509 - val_loss: 0.1846 - val_accuracy: 0.9231 - val_recall: 0.9231 - val_f1: 0.9231\n",
      "Epoch 139/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9503 - recall: 0.9506 - f1: 0.9503\n",
      "Epoch 00139: val_loss did not improve from 0.17498\n",
      "\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1209 - accuracy: 0.9505 - recall: 0.9507 - f1: 0.9505 - val_loss: 0.1780 - val_accuracy: 0.9253 - val_recall: 0.9253 - val_f1: 0.9253\n",
      "Epoch 140/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1172 - accuracy: 0.9519 - recall: 0.9518 - f1: 0.9519\n",
      "Epoch 00140: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1170 - accuracy: 0.9520 - recall: 0.9519 - f1: 0.9520 - val_loss: 0.1788 - val_accuracy: 0.9263 - val_recall: 0.9262 - val_f1: 0.9263\n",
      "Epoch 141/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9492 - recall: 0.9493 - f1: 0.9492\n",
      "Epoch 00141: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1236 - accuracy: 0.9493 - recall: 0.9493 - f1: 0.9493 - val_loss: 0.1870 - val_accuracy: 0.9228 - val_recall: 0.9229 - val_f1: 0.9228\n",
      "Epoch 142/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9490 - recall: 0.9493 - f1: 0.9490\n",
      "Epoch 00142: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1211 - accuracy: 0.9491 - recall: 0.9493 - f1: 0.9491 - val_loss: 0.1792 - val_accuracy: 0.9308 - val_recall: 0.9308 - val_f1: 0.9308\n",
      "Epoch 143/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9519 - recall: 0.9523 - f1: 0.9519\n",
      "Epoch 00143: val_loss did not improve from 0.17498\n",
      "\n",
      "Epoch 00143: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1199 - accuracy: 0.9520 - recall: 0.9523 - f1: 0.9520 - val_loss: 0.1852 - val_accuracy: 0.9235 - val_recall: 0.9236 - val_f1: 0.9235\n",
      "Epoch 144/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9521 - recall: 0.9520 - f1: 0.9521\n",
      "Epoch 00144: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1184 - accuracy: 0.9522 - recall: 0.9521 - f1: 0.9522 - val_loss: 0.1888 - val_accuracy: 0.9242 - val_recall: 0.9242 - val_f1: 0.9242\n",
      "Epoch 145/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9513 - recall: 0.9512 - f1: 0.9513\n",
      "Epoch 00145: val_loss improved from 0.17498 to 0.17405, saving model to batch_relu_validation_200-145-0.951267-0.930033.h5\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1195 - accuracy: 0.9513 - recall: 0.9512 - f1: 0.9513 - val_loss: 0.1741 - val_accuracy: 0.9300 - val_recall: 0.9302 - val_f1: 0.9300\n",
      "Epoch 146/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1146 - accuracy: 0.9547 - recall: 0.9547 - f1: 0.9547\n",
      "Epoch 00146: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1147 - accuracy: 0.9545 - recall: 0.9545 - f1: 0.9545 - val_loss: 0.1858 - val_accuracy: 0.9243 - val_recall: 0.9242 - val_f1: 0.9243\n",
      "Epoch 147/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9510 - recall: 0.9509 - f1: 0.9510\n",
      "Epoch 00147: val_loss did not improve from 0.17405\n",
      "\n",
      "Epoch 00147: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      "150/150 [==============================] - 65s 437ms/step - loss: 0.1200 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9508 - val_loss: 0.1803 - val_accuracy: 0.9255 - val_recall: 0.9255 - val_f1: 0.9255\n",
      "Epoch 148/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1175 - accuracy: 0.9523 - recall: 0.9523 - f1: 0.9523\n",
      "Epoch 00148: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1176 - accuracy: 0.9523 - recall: 0.9523 - f1: 0.9523 - val_loss: 0.1854 - val_accuracy: 0.9247 - val_recall: 0.9247 - val_f1: 0.9247\n",
      "Epoch 149/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1156 - accuracy: 0.9528 - recall: 0.9530 - f1: 0.9528\n",
      "Epoch 00149: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1158 - accuracy: 0.9526 - recall: 0.9528 - f1: 0.9526 - val_loss: 0.1824 - val_accuracy: 0.9264 - val_recall: 0.9267 - val_f1: 0.9265\n",
      "Epoch 150/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1171 - accuracy: 0.9524 - recall: 0.9523 - f1: 0.9524\n",
      "Epoch 00150: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1170 - accuracy: 0.9524 - recall: 0.9523 - f1: 0.9524 - val_loss: 0.1879 - val_accuracy: 0.9228 - val_recall: 0.9229 - val_f1: 0.9228\n",
      "Epoch 151/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9517 - recall: 0.9515 - f1: 0.9517\n",
      "Epoch 00151: val_loss did not improve from 0.17405\n",
      "\n",
      "Epoch 00151: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1184 - accuracy: 0.9518 - recall: 0.9517 - f1: 0.9518 - val_loss: 0.1752 - val_accuracy: 0.9269 - val_recall: 0.9269 - val_f1: 0.9269\n",
      "Epoch 152/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9485 - recall: 0.9485 - f1: 0.9485\n",
      "Epoch 00152: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1231 - accuracy: 0.9485 - recall: 0.9485 - f1: 0.9485 - val_loss: 0.1871 - val_accuracy: 0.9257 - val_recall: 0.9258 - val_f1: 0.9257\n",
      "Epoch 153/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9508 - recall: 0.9508 - f1: 0.9508\n",
      "Epoch 00153: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1213 - accuracy: 0.9505 - recall: 0.9505 - f1: 0.9505 - val_loss: 0.1857 - val_accuracy: 0.9239 - val_recall: 0.9239 - val_f1: 0.9239\n",
      "Epoch 154/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1141 - accuracy: 0.9542 - recall: 0.9544 - f1: 0.9542\n",
      "Epoch 00154: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1140 - accuracy: 0.9542 - recall: 0.9543 - f1: 0.9542 - val_loss: 0.1764 - val_accuracy: 0.9268 - val_recall: 0.9267 - val_f1: 0.9268\n",
      "Epoch 155/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1175 - accuracy: 0.9511 - recall: 0.9512 - f1: 0.9511\n",
      "Epoch 00155: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.1180 - accuracy: 0.9507 - recall: 0.9508 - f1: 0.9507 - val_loss: 0.1872 - val_accuracy: 0.9241 - val_recall: 0.9242 - val_f1: 0.9241\n",
      "Epoch 156/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1172 - accuracy: 0.9522 - recall: 0.9522 - f1: 0.9522\n",
      "Epoch 00156: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1172 - accuracy: 0.9523 - recall: 0.9523 - f1: 0.9523 - val_loss: 0.1850 - val_accuracy: 0.9239 - val_recall: 0.9240 - val_f1: 0.9239\n",
      "Epoch 157/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1144 - accuracy: 0.9535 - recall: 0.9535 - f1: 0.9535\n",
      "Epoch 00157: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1141 - accuracy: 0.9537 - recall: 0.9537 - f1: 0.9537 - val_loss: 0.1763 - val_accuracy: 0.9290 - val_recall: 0.9289 - val_f1: 0.9290\n",
      "Epoch 158/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1175 - accuracy: 0.9537 - recall: 0.9537 - f1: 0.9537\n",
      "Epoch 00158: val_loss improved from 0.17405 to 0.17313, saving model to batch_relu_validation_200-158-0.953833-0.928867.h5\n",
      "\n",
      "Epoch 00158: ReduceLROnPlateau reducing learning rate to 2.980232380322967e-11.\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1171 - accuracy: 0.9538 - recall: 0.9539 - f1: 0.9538 - val_loss: 0.1731 - val_accuracy: 0.9289 - val_recall: 0.9289 - val_f1: 0.9289\n",
      "Epoch 159/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9496 - recall: 0.9496 - f1: 0.9496\n",
      "Epoch 00159: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1220 - accuracy: 0.9494 - recall: 0.9494 - f1: 0.9494 - val_loss: 0.1790 - val_accuracy: 0.9281 - val_recall: 0.9283 - val_f1: 0.9281\n",
      "Epoch 160/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1094 - accuracy: 0.9557 - recall: 0.9556 - f1: 0.9557\n",
      "Epoch 00160: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1099 - accuracy: 0.9557 - recall: 0.9555 - f1: 0.9557 - val_loss: 0.1847 - val_accuracy: 0.9233 - val_recall: 0.9232 - val_f1: 0.9233\n",
      "Epoch 161/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9478 - recall: 0.9477 - f1: 0.9478\n",
      "Epoch 00161: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1276 - accuracy: 0.9476 - recall: 0.9476 - f1: 0.9476 - val_loss: 0.1789 - val_accuracy: 0.9270 - val_recall: 0.9269 - val_f1: 0.9270\n",
      "Epoch 162/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1180 - accuracy: 0.9519 - recall: 0.9517 - f1: 0.9519\n",
      "Epoch 00162: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 65s 434ms/step - loss: 0.1179 - accuracy: 0.9519 - recall: 0.9517 - f1: 0.9519 - val_loss: 0.1760 - val_accuracy: 0.9258 - val_recall: 0.9257 - val_f1: 0.9258\n",
      "Epoch 163/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9522 - recall: 0.9521 - f1: 0.9522\n",
      "Epoch 00163: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1225 - accuracy: 0.9523 - recall: 0.9522 - f1: 0.9523 - val_loss: 0.1775 - val_accuracy: 0.9289 - val_recall: 0.9290 - val_f1: 0.9289\n",
      "Epoch 164/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9499 - recall: 0.9503 - f1: 0.9500\n",
      "Epoch 00164: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00164: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1246 - accuracy: 0.9498 - recall: 0.9501 - f1: 0.9498 - val_loss: 0.1813 - val_accuracy: 0.9257 - val_recall: 0.9255 - val_f1: 0.9257\n",
      "Epoch 165/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9516 - recall: 0.9519 - f1: 0.9517\n",
      "Epoch 00165: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1201 - accuracy: 0.9517 - recall: 0.9519 - f1: 0.9517 - val_loss: 0.1837 - val_accuracy: 0.9222 - val_recall: 0.9222 - val_f1: 0.9222\n",
      "Epoch 166/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9514 - recall: 0.9513 - f1: 0.9514\n",
      "Epoch 00166: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1207 - accuracy: 0.9514 - recall: 0.9513 - f1: 0.9514 - val_loss: 0.1780 - val_accuracy: 0.9262 - val_recall: 0.9261 - val_f1: 0.9262\n",
      "Epoch 167/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9506 - recall: 0.9511 - f1: 0.9507\n",
      "Epoch 00167: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 69s 462ms/step - loss: 0.1242 - accuracy: 0.9506 - recall: 0.9511 - f1: 0.9507 - val_loss: 0.1843 - val_accuracy: 0.9231 - val_recall: 0.9233 - val_f1: 0.9231\n",
      "Epoch 168/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9489 - recall: 0.9490 - f1: 0.9489\n",
      "Epoch 00168: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00168: ReduceLROnPlateau reducing learning rate to 7.450580950807417e-12.\n",
      "150/150 [==============================] - 71s 475ms/step - loss: 0.1267 - accuracy: 0.9491 - recall: 0.9492 - f1: 0.9491 - val_loss: 0.1861 - val_accuracy: 0.9215 - val_recall: 0.9215 - val_f1: 0.9215\n",
      "Epoch 169/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9512 - recall: 0.9511 - f1: 0.9512\n",
      "Epoch 00169: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 73s 488ms/step - loss: 0.1208 - accuracy: 0.9510 - recall: 0.9510 - f1: 0.9510 - val_loss: 0.1843 - val_accuracy: 0.9257 - val_recall: 0.9258 - val_f1: 0.9257\n",
      "Epoch 170/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1269 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488\n",
      "Epoch 00170: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 68s 454ms/step - loss: 0.1265 - accuracy: 0.9490 - recall: 0.9491 - f1: 0.9490 - val_loss: 0.1811 - val_accuracy: 0.9262 - val_recall: 0.9263 - val_f1: 0.9262\n",
      "Epoch 171/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9500 - recall: 0.9501 - f1: 0.9500\n",
      "Epoch 00171: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1229 - accuracy: 0.9502 - recall: 0.9502 - f1: 0.9502 - val_loss: 0.1810 - val_accuracy: 0.9274 - val_recall: 0.9275 - val_f1: 0.9274\n",
      "Epoch 172/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9514 - recall: 0.9513 - f1: 0.9514\n",
      "Epoch 00172: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00172: ReduceLROnPlateau reducing learning rate to 3.725290475403709e-12.\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1212 - accuracy: 0.9517 - recall: 0.9516 - f1: 0.9517 - val_loss: 0.1880 - val_accuracy: 0.9231 - val_recall: 0.9232 - val_f1: 0.9231\n",
      "Epoch 173/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 0.9493 - recall: 0.9493 - f1: 0.9493\n",
      "Epoch 00173: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1249 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495 - val_loss: 0.1973 - val_accuracy: 0.9189 - val_recall: 0.9189 - val_f1: 0.9189\n",
      "Epoch 174/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1209 - accuracy: 0.9501 - recall: 0.9501 - f1: 0.9501\n",
      "Epoch 00174: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1206 - accuracy: 0.9501 - recall: 0.9501 - f1: 0.9501 - val_loss: 0.1835 - val_accuracy: 0.9232 - val_recall: 0.9232 - val_f1: 0.9232\n",
      "Epoch 175/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1163 - accuracy: 0.9520 - recall: 0.9521 - f1: 0.9520\n",
      "Epoch 00175: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1161 - accuracy: 0.9522 - recall: 0.9522 - f1: 0.9522 - val_loss: 0.1811 - val_accuracy: 0.9281 - val_recall: 0.9279 - val_f1: 0.9281\n",
      "Epoch 176/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9496 - recall: 0.9495 - f1: 0.9496\n",
      "Epoch 00176: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00176: ReduceLROnPlateau reducing learning rate to 1.8626452377018543e-12.\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1242 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495 - val_loss: 0.1861 - val_accuracy: 0.9219 - val_recall: 0.9219 - val_f1: 0.9219\n",
      "Epoch 177/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9498 - recall: 0.9501 - f1: 0.9498\n",
      "Epoch 00177: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1211 - accuracy: 0.9497 - recall: 0.9500 - f1: 0.9497 - val_loss: 0.1772 - val_accuracy: 0.9266 - val_recall: 0.9269 - val_f1: 0.9267\n",
      "Epoch 178/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1184 - accuracy: 0.9514 - recall: 0.9513 - f1: 0.9514\n",
      "Epoch 00178: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1192 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9508 - val_loss: 0.1821 - val_accuracy: 0.9251 - val_recall: 0.9252 - val_f1: 0.9251\n",
      "Epoch 179/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507\n",
      "Epoch 00179: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1223 - accuracy: 0.9509 - recall: 0.9509 - f1: 0.9509 - val_loss: 0.1776 - val_accuracy: 0.9262 - val_recall: 0.9261 - val_f1: 0.9262\n",
      "Epoch 180/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9505 - recall: 0.9505 - f1: 0.9505\n",
      "Epoch 00180: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00180: ReduceLROnPlateau reducing learning rate to 9.313226188509272e-13.\n",
      "150/150 [==============================] - 67s 445ms/step - loss: 0.1215 - accuracy: 0.9505 - recall: 0.9505 - f1: 0.9505 - val_loss: 0.1794 - val_accuracy: 0.9263 - val_recall: 0.9263 - val_f1: 0.9263\n",
      "Epoch 181/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9504 - recall: 0.9505 - f1: 0.9504\n",
      "Epoch 00181: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1210 - accuracy: 0.9505 - recall: 0.9506 - f1: 0.9505 - val_loss: 0.1864 - val_accuracy: 0.9236 - val_recall: 0.9235 - val_f1: 0.9236\n",
      "Epoch 182/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1194 - accuracy: 0.9517 - recall: 0.9520 - f1: 0.9518\n",
      "Epoch 00182: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1192 - accuracy: 0.9519 - recall: 0.9521 - f1: 0.9519 - val_loss: 0.1779 - val_accuracy: 0.9263 - val_recall: 0.9263 - val_f1: 0.9263\n",
      "Epoch 183/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1188 - accuracy: 0.9532 - recall: 0.9533 - f1: 0.9532\n",
      "Epoch 00183: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1196 - accuracy: 0.9529 - recall: 0.9529 - f1: 0.9529 - val_loss: 0.1751 - val_accuracy: 0.9291 - val_recall: 0.9291 - val_f1: 0.9291\n",
      "Epoch 184/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9475 - recall: 0.9475 - f1: 0.9475\n",
      "Epoch 00184: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00184: ReduceLROnPlateau reducing learning rate to 4.656613094254636e-13.\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1226 - accuracy: 0.9476 - recall: 0.9476 - f1: 0.9476 - val_loss: 0.1831 - val_accuracy: 0.9242 - val_recall: 0.9243 - val_f1: 0.9242\n",
      "Epoch 185/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9528 - recall: 0.9528 - f1: 0.9528\n",
      "Epoch 00185: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1175 - accuracy: 0.9526 - recall: 0.9525 - f1: 0.9526 - val_loss: 0.1802 - val_accuracy: 0.9269 - val_recall: 0.9270 - val_f1: 0.9269\n",
      "Epoch 186/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9480 - recall: 0.9479 - f1: 0.9480\n",
      "Epoch 00186: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 65s 434ms/step - loss: 0.1240 - accuracy: 0.9481 - recall: 0.9479 - f1: 0.9481 - val_loss: 0.1793 - val_accuracy: 0.9275 - val_recall: 0.9276 - val_f1: 0.9275\n",
      "Epoch 187/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9486 - recall: 0.9486 - f1: 0.9486\n",
      "Epoch 00187: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1228 - accuracy: 0.9487 - recall: 0.9487 - f1: 0.9487 - val_loss: 0.1831 - val_accuracy: 0.9233 - val_recall: 0.9233 - val_f1: 0.9233\n",
      "Epoch 188/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1188 - accuracy: 0.9512 - recall: 0.9511 - f1: 0.9512\n",
      "Epoch 00188: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00188: ReduceLROnPlateau reducing learning rate to 2.328306547127318e-13.\n",
      "150/150 [==============================] - 67s 445ms/step - loss: 0.1191 - accuracy: 0.9510 - recall: 0.9509 - f1: 0.9510 - val_loss: 0.1836 - val_accuracy: 0.9251 - val_recall: 0.9251 - val_f1: 0.9251\n",
      "Epoch 189/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9501 - recall: 0.9501 - f1: 0.9501\n",
      "Epoch 00189: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1230 - accuracy: 0.9502 - recall: 0.9502 - f1: 0.9502 - val_loss: 0.1791 - val_accuracy: 0.9261 - val_recall: 0.9261 - val_f1: 0.9261\n",
      "Epoch 190/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495\n",
      "Epoch 00190: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1215 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495 - val_loss: 0.1796 - val_accuracy: 0.9272 - val_recall: 0.9273 - val_f1: 0.9272\n",
      "Epoch 191/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 0.9483 - recall: 0.9483 - f1: 0.9483\n",
      "Epoch 00191: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1268 - accuracy: 0.9481 - recall: 0.9481 - f1: 0.9481 - val_loss: 0.1756 - val_accuracy: 0.9261 - val_recall: 0.9263 - val_f1: 0.9261\n",
      "Epoch 192/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507\n",
      "Epoch 00192: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00192: ReduceLROnPlateau reducing learning rate to 1.164153273563659e-13.\n",
      "150/150 [==============================] - 68s 452ms/step - loss: 0.1196 - accuracy: 0.9508 - recall: 0.9508 - f1: 0.9508 - val_loss: 0.1804 - val_accuracy: 0.9246 - val_recall: 0.9247 - val_f1: 0.9246\n",
      "Epoch 193/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9487 - recall: 0.9488 - f1: 0.9487\n",
      "Epoch 00193: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1228 - accuracy: 0.9485 - recall: 0.9486 - f1: 0.9485 - val_loss: 0.1831 - val_accuracy: 0.9242 - val_recall: 0.9241 - val_f1: 0.9242\n",
      "Epoch 194/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1167 - accuracy: 0.9513 - recall: 0.9513 - f1: 0.9513\n",
      "Epoch 00194: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1167 - accuracy: 0.9513 - recall: 0.9513 - f1: 0.9513 - val_loss: 0.1847 - val_accuracy: 0.9234 - val_recall: 0.9233 - val_f1: 0.9234\n",
      "Epoch 195/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9520 - recall: 0.9521 - f1: 0.9521\n",
      "Epoch 00195: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1219 - accuracy: 0.9520 - recall: 0.9521 - f1: 0.9520 - val_loss: 0.1862 - val_accuracy: 0.9250 - val_recall: 0.9251 - val_f1: 0.9250\n",
      "Epoch 196/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 0.9486 - recall: 0.9485 - f1: 0.9486\n",
      "Epoch 00196: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00196: ReduceLROnPlateau reducing learning rate to 5.820766367818295e-14.\n",
      "150/150 [==============================] - 67s 445ms/step - loss: 0.1265 - accuracy: 0.9486 - recall: 0.9485 - f1: 0.9486 - val_loss: 0.1796 - val_accuracy: 0.9247 - val_recall: 0.9247 - val_f1: 0.9247\n",
      "Epoch 197/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 0.9478 - recall: 0.9477 - f1: 0.9478\n",
      "Epoch 00197: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1269 - accuracy: 0.9478 - recall: 0.9477 - f1: 0.9478 - val_loss: 0.1844 - val_accuracy: 0.9248 - val_recall: 0.9248 - val_f1: 0.9248\n",
      "Epoch 198/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1289 - accuracy: 0.9480 - recall: 0.9479 - f1: 0.9480\n",
      "Epoch 00198: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1286 - accuracy: 0.9482 - recall: 0.9481 - f1: 0.9482 - val_loss: 0.1828 - val_accuracy: 0.9272 - val_recall: 0.9271 - val_f1: 0.9272\n",
      "Epoch 199/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9489 - recall: 0.9488 - f1: 0.9489\n",
      "Epoch 00199: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1225 - accuracy: 0.9492 - recall: 0.9491 - f1: 0.9492 - val_loss: 0.1799 - val_accuracy: 0.9246 - val_recall: 0.9247 - val_f1: 0.9246\n",
      "Epoch 200/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 0.9531 - recall: 0.9533 - f1: 0.9531\n",
      "Epoch 00200: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00200: ReduceLROnPlateau reducing learning rate to 2.9103831839091474e-14.\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1171 - accuracy: 0.9533 - recall: 0.9535 - f1: 0.9533 - val_loss: 0.1827 - val_accuracy: 0.9245 - val_recall: 0.9245 - val_f1: 0.9245\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "batch_normalization = True\n",
    "EPOCHS = 200\n",
    "STEPS_PER_EPOCH = 150\n",
    "activation = \"relu\"\n",
    "model = cnn_model(\n",
    "    shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "# Train the model with batch\n",
    "history = model.train()\n",
    "# model.save(\"batch_relu_validation_200.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "plt.savefig(\"batch_relu_validation_200.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,962\n",
      "Trainable params: 2,278,274\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,962\n",
      "Trainable params: 2,278,274\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_distributed_function_35057589 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_1/test_1.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_2/test_2.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_3/test_3.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_4/test_4.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_5/test_5.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_6/test_6.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_7/test_7.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_8/test_8.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_9/test_9.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_10/test_10.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_11/test_11.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_12/test_12.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_13/test_13.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_14/test_14.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_15/test_15.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_16/test_16.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_17/test_17.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_18/test_18.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_19/test_19.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_20/test_20.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_21/test_21.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_22/test_22.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_23/test_23.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_24/test_24.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_25/test_25.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_26/test_26.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_27/test_27.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_28/test_28.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_29/test_29.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_30/test_30.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_31/test_31.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_32/test_32.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_33/test_33.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_34/test_34.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_35/test_35.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_36/test_36.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_37/test_37.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_38/test_38.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_39/test_39.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_40/test_40.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_41/test_41.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_42/test_42.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_43/test_43.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_44/test_44.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_45/test_45.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_46/test_46.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_47/test_47.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_48/test_48.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_49/test_49.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_50/test_50.png\n"
     ]
    }
   ],
   "source": [
    "from helpers import *\n",
    "\n",
    "# from cnn_model import cnn_model\n",
    "\n",
    "# Instantiate the model\n",
    "batch_normalization = True\n",
    "activation = \"relu\"\n",
    "model = cnn_model(\n",
    "    shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model.load(\"batch_relu_validation_200-145-0.951267-0.930033.h5\")\n",
    "\n",
    "# Print a summary to make sure the correct model is used\n",
    "model.model.summary()\n",
    "\n",
    "# We add all test images to an array, used later for generating a submission\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "    image_filenames.append(image_filename)\n",
    "\n",
    "# Set-up submission filename\n",
    "submission_filename = \"batch_relu_validation_200-145-0.951267-0.930033.csv\"\n",
    "\n",
    "# Generates the submission\n",
    "generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,962\n",
      "Trainable params: 2,278,274\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/160\n",
      "Executing op LeakyRelu in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LeakyReluGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.5554 - accuracy: 0.7090 - recall: 0.6821 - f1: 0.6979\n",
      "Epoch 00001: val_loss improved from inf to 0.85737, saving model to batch_relu_validation_200-001-0.709167-0.259867.h5\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.5549 - accuracy: 0.7092 - recall: 0.6823 - f1: 0.6981 - val_loss: 0.8574 - val_accuracy: 0.2599 - val_recall: 0.2593 - val_f1: 0.2594\n",
      "Epoch 2/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.4262 - accuracy: 0.8009 - recall: 0.8000 - f1: 0.8007\n",
      "Epoch 00002: val_loss did not improve from 0.85737\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.4258 - accuracy: 0.8010 - recall: 0.8001 - f1: 0.8007 - val_loss: 0.9403 - val_accuracy: 0.4142 - val_recall: 0.4127 - val_f1: 0.4132\n",
      "Epoch 3/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3690 - accuracy: 0.8337 - recall: 0.8334 - f1: 0.8336\n",
      "Epoch 00003: val_loss improved from 0.85737 to 0.44966, saving model to batch_relu_validation_200-003-0.833967-0.789200.h5\n",
      "150/150 [==============================] - 68s 456ms/step - loss: 0.3685 - accuracy: 0.8340 - recall: 0.8335 - f1: 0.8339 - val_loss: 0.4497 - val_accuracy: 0.7892 - val_recall: 0.7810 - val_f1: 0.7875\n",
      "Epoch 4/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3478 - accuracy: 0.8426 - recall: 0.8421 - f1: 0.8425\n",
      "Epoch 00004: val_loss did not improve from 0.44966\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.3485 - accuracy: 0.8424 - recall: 0.8419 - f1: 0.8424 - val_loss: 0.6745 - val_accuracy: 0.6829 - val_recall: 0.6749 - val_f1: 0.6803\n",
      "Epoch 5/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3120 - accuracy: 0.8630 - recall: 0.8628 - f1: 0.8630\n",
      "Epoch 00005: val_loss improved from 0.44966 to 0.39337, saving model to batch_relu_validation_200-005-0.863133-0.811033.h5\n",
      "150/150 [==============================] - 67s 445ms/step - loss: 0.3116 - accuracy: 0.8631 - recall: 0.8629 - f1: 0.8631 - val_loss: 0.3934 - val_accuracy: 0.8110 - val_recall: 0.8060 - val_f1: 0.8101\n",
      "Epoch 6/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3003 - accuracy: 0.8704 - recall: 0.8699 - f1: 0.8703\n",
      "Epoch 00006: val_loss improved from 0.39337 to 0.37381, saving model to batch_relu_validation_200-006-0.870500-0.835467.h5\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.3001 - accuracy: 0.8705 - recall: 0.8700 - f1: 0.8704 - val_loss: 0.3738 - val_accuracy: 0.8355 - val_recall: 0.8409 - val_f1: 0.8364\n",
      "Epoch 7/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2713 - accuracy: 0.8837 - recall: 0.8837 - f1: 0.8837\n",
      "Epoch 00007: val_loss did not improve from 0.37381\n",
      "150/150 [==============================] - 68s 450ms/step - loss: 0.2708 - accuracy: 0.8841 - recall: 0.8841 - f1: 0.8841 - val_loss: 0.4487 - val_accuracy: 0.7863 - val_recall: 0.7887 - val_f1: 0.7868\n",
      "Epoch 8/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2511 - accuracy: 0.8944 - recall: 0.8944 - f1: 0.8944\n",
      "Epoch 00008: val_loss did not improve from 0.37381\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.2507 - accuracy: 0.8947 - recall: 0.8947 - f1: 0.8947 - val_loss: 0.4410 - val_accuracy: 0.7769 - val_recall: 0.7761 - val_f1: 0.7767\n",
      "Epoch 9/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2493 - accuracy: 0.8953 - recall: 0.8954 - f1: 0.8953\n",
      "Epoch 00009: val_loss improved from 0.37381 to 0.36026, saving model to batch_relu_validation_200-009-0.895367-0.821100.h5\n",
      "150/150 [==============================] - 68s 454ms/step - loss: 0.2491 - accuracy: 0.8954 - recall: 0.8955 - f1: 0.8954 - val_loss: 0.3603 - val_accuracy: 0.8211 - val_recall: 0.8223 - val_f1: 0.8213\n",
      "Epoch 10/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2449 - accuracy: 0.8965 - recall: 0.8954 - f1: 0.8964\n",
      "Epoch 00010: val_loss did not improve from 0.36026\n",
      "150/150 [==============================] - 67s 450ms/step - loss: 0.2448 - accuracy: 0.8964 - recall: 0.8953 - f1: 0.8963 - val_loss: 0.4443 - val_accuracy: 0.7939 - val_recall: 0.7914 - val_f1: 0.7934\n",
      "Epoch 11/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2319 - accuracy: 0.9028 - recall: 0.9021 - f1: 0.9027\n",
      "Epoch 00011: val_loss did not improve from 0.36026\n",
      "150/150 [==============================] - 68s 456ms/step - loss: 0.2318 - accuracy: 0.9030 - recall: 0.9023 - f1: 0.9030 - val_loss: 0.4460 - val_accuracy: 0.7894 - val_recall: 0.7901 - val_f1: 0.7895\n",
      "Epoch 12/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2256 - accuracy: 0.9075 - recall: 0.9066 - f1: 0.9074\n",
      "Epoch 00012: val_loss did not improve from 0.36026\n",
      "150/150 [==============================] - 68s 455ms/step - loss: 0.2254 - accuracy: 0.9077 - recall: 0.9067 - f1: 0.9076 - val_loss: 0.5510 - val_accuracy: 0.7635 - val_recall: 0.7634 - val_f1: 0.7635\n",
      "Epoch 13/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2270 - accuracy: 0.9051 - recall: 0.9046 - f1: 0.9051\n",
      "Epoch 00013: val_loss did not improve from 0.36026\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.2269 - accuracy: 0.9050 - recall: 0.9044 - f1: 0.9049 - val_loss: 0.4410 - val_accuracy: 0.7951 - val_recall: 0.7941 - val_f1: 0.7949\n",
      "Epoch 14/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2204 - accuracy: 0.9082 - recall: 0.9076 - f1: 0.9081\n",
      "Epoch 00014: val_loss improved from 0.36026 to 0.35490, saving model to batch_relu_validation_200-014-0.908000-0.835900.h5\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.2207 - accuracy: 0.9080 - recall: 0.9074 - f1: 0.9079 - val_loss: 0.3549 - val_accuracy: 0.8359 - val_recall: 0.8343 - val_f1: 0.8356\n",
      "Epoch 15/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2091 - accuracy: 0.9141 - recall: 0.9138 - f1: 0.9141\n",
      "Epoch 00015: val_loss did not improve from 0.35490\n",
      "150/150 [==============================] - 67s 450ms/step - loss: 0.2087 - accuracy: 0.9143 - recall: 0.9141 - f1: 0.9143 - val_loss: 0.5355 - val_accuracy: 0.7889 - val_recall: 0.7886 - val_f1: 0.7888\n",
      "Epoch 16/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2126 - accuracy: 0.9137 - recall: 0.9144 - f1: 0.9138\n",
      "Epoch 00016: val_loss did not improve from 0.35490\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.2119 - accuracy: 0.9141 - recall: 0.9148 - f1: 0.9141 - val_loss: 0.4850 - val_accuracy: 0.8035 - val_recall: 0.8043 - val_f1: 0.8037\n",
      "Epoch 17/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2060 - accuracy: 0.9180 - recall: 0.9182 - f1: 0.9180\n",
      "Epoch 00017: val_loss improved from 0.35490 to 0.31614, saving model to batch_relu_validation_200-017-0.918167-0.867033.h5\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.2056 - accuracy: 0.9182 - recall: 0.9183 - f1: 0.9182 - val_loss: 0.3161 - val_accuracy: 0.8670 - val_recall: 0.8684 - val_f1: 0.8672\n",
      "Epoch 18/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2014 - accuracy: 0.9162 - recall: 0.9163 - f1: 0.9162\n",
      "Epoch 00018: val_loss did not improve from 0.31614\n",
      "150/150 [==============================] - 67s 448ms/step - loss: 0.2014 - accuracy: 0.9162 - recall: 0.9163 - f1: 0.9162 - val_loss: 0.6109 - val_accuracy: 0.8019 - val_recall: 0.8021 - val_f1: 0.8019\n",
      "Epoch 19/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1981 - accuracy: 0.9209 - recall: 0.9207 - f1: 0.9209\n",
      "Epoch 00019: val_loss improved from 0.31614 to 0.26036, saving model to batch_relu_validation_200-019-0.920833-0.885067.h5\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1984 - accuracy: 0.9208 - recall: 0.9207 - f1: 0.9208 - val_loss: 0.2604 - val_accuracy: 0.8851 - val_recall: 0.8868 - val_f1: 0.8853\n",
      "Epoch 20/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1984 - accuracy: 0.9167 - recall: 0.9165 - f1: 0.9167\n",
      "Epoch 00020: val_loss did not improve from 0.26036\n",
      "150/150 [==============================] - 68s 456ms/step - loss: 0.1982 - accuracy: 0.9170 - recall: 0.9167 - f1: 0.9169 - val_loss: 0.3679 - val_accuracy: 0.8204 - val_recall: 0.8199 - val_f1: 0.8203\n",
      "Epoch 21/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1940 - accuracy: 0.9212 - recall: 0.9202 - f1: 0.9211\n",
      "Epoch 00021: val_loss did not improve from 0.26036\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1947 - accuracy: 0.9208 - recall: 0.9199 - f1: 0.9208 - val_loss: 0.7710 - val_accuracy: 0.7882 - val_recall: 0.7877 - val_f1: 0.7881\n",
      "Epoch 22/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1979 - accuracy: 0.9187 - recall: 0.9185 - f1: 0.9187\n",
      "Epoch 00022: val_loss did not improve from 0.26036\n",
      "150/150 [==============================] - 78s 522ms/step - loss: 0.1981 - accuracy: 0.9184 - recall: 0.9182 - f1: 0.9184 - val_loss: 0.3391 - val_accuracy: 0.8423 - val_recall: 0.8441 - val_f1: 0.8426\n",
      "Epoch 23/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1946 - accuracy: 0.9206 - recall: 0.9207 - f1: 0.9206\n",
      "Epoch 00023: val_loss did not improve from 0.26036\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.1946 - accuracy: 0.9204 - recall: 0.9206 - f1: 0.9204 - val_loss: 0.5076 - val_accuracy: 0.7750 - val_recall: 0.7753 - val_f1: 0.7751\n",
      "Epoch 24/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1922 - accuracy: 0.9227 - recall: 0.9227 - f1: 0.9226\n",
      "Epoch 00024: val_loss did not improve from 0.26036\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1927 - accuracy: 0.9226 - recall: 0.9226 - f1: 0.9226 - val_loss: 0.3248 - val_accuracy: 0.8459 - val_recall: 0.8459 - val_f1: 0.8459\n",
      "Epoch 25/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1889 - accuracy: 0.9193 - recall: 0.9190 - f1: 0.9193\n",
      "Epoch 00025: val_loss did not improve from 0.26036\n",
      "150/150 [==============================] - 67s 448ms/step - loss: 0.1891 - accuracy: 0.9192 - recall: 0.9189 - f1: 0.9192 - val_loss: 0.2984 - val_accuracy: 0.8669 - val_recall: 0.8678 - val_f1: 0.8670\n",
      "Epoch 26/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1856 - accuracy: 0.9258 - recall: 0.9258 - f1: 0.9258\n",
      "Epoch 00026: val_loss did not improve from 0.26036\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1859 - accuracy: 0.9256 - recall: 0.9257 - f1: 0.9256 - val_loss: 0.3464 - val_accuracy: 0.8355 - val_recall: 0.8365 - val_f1: 0.8357\n",
      "Epoch 27/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1873 - accuracy: 0.9263 - recall: 0.9262 - f1: 0.9263\n",
      "Epoch 00027: val_loss did not improve from 0.26036\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1868 - accuracy: 0.9265 - recall: 0.9264 - f1: 0.9265 - val_loss: 0.3248 - val_accuracy: 0.8483 - val_recall: 0.8489 - val_f1: 0.8484\n",
      "Epoch 28/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1805 - accuracy: 0.9274 - recall: 0.9275 - f1: 0.9274\n",
      "Epoch 00028: val_loss improved from 0.26036 to 0.25365, saving model to batch_relu_validation_200-028-0.927567-0.887833.h5\n",
      "150/150 [==============================] - 69s 458ms/step - loss: 0.1803 - accuracy: 0.9276 - recall: 0.9277 - f1: 0.9276 - val_loss: 0.2536 - val_accuracy: 0.8878 - val_recall: 0.8883 - val_f1: 0.8879\n",
      "Epoch 29/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1780 - accuracy: 0.9280 - recall: 0.9283 - f1: 0.9280\n",
      "Epoch 00029: val_loss did not improve from 0.25365\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1782 - accuracy: 0.9280 - recall: 0.9283 - f1: 0.9281 - val_loss: 0.4213 - val_accuracy: 0.8170 - val_recall: 0.8174 - val_f1: 0.8171\n",
      "Epoch 30/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1824 - accuracy: 0.9253 - recall: 0.9248 - f1: 0.9253\n",
      "Epoch 00030: val_loss did not improve from 0.25365\n",
      "150/150 [==============================] - 67s 448ms/step - loss: 0.1825 - accuracy: 0.9253 - recall: 0.9248 - f1: 0.9252 - val_loss: 0.3049 - val_accuracy: 0.8625 - val_recall: 0.8617 - val_f1: 0.8624\n",
      "Epoch 31/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1805 - accuracy: 0.9266 - recall: 0.9266 - f1: 0.9266\n",
      "Epoch 00031: val_loss did not improve from 0.25365\n",
      "150/150 [==============================] - 68s 455ms/step - loss: 0.1802 - accuracy: 0.9267 - recall: 0.9267 - f1: 0.9267 - val_loss: 0.2652 - val_accuracy: 0.8831 - val_recall: 0.8828 - val_f1: 0.8830\n",
      "Epoch 32/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1727 - accuracy: 0.9297 - recall: 0.9293 - f1: 0.9296\n",
      "Epoch 00032: val_loss improved from 0.25365 to 0.25281, saving model to batch_relu_validation_200-032-0.929600-0.897400.h5\n",
      "150/150 [==============================] - 69s 463ms/step - loss: 0.1727 - accuracy: 0.9296 - recall: 0.9292 - f1: 0.9296 - val_loss: 0.2528 - val_accuracy: 0.8974 - val_recall: 0.8979 - val_f1: 0.8975\n",
      "Epoch 33/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1671 - accuracy: 0.9328 - recall: 0.9328 - f1: 0.9328\n",
      "Epoch 00033: val_loss did not improve from 0.25281\n",
      "150/150 [==============================] - 70s 466ms/step - loss: 0.1677 - accuracy: 0.9325 - recall: 0.9324 - f1: 0.9325 - val_loss: 0.7300 - val_accuracy: 0.8027 - val_recall: 0.8026 - val_f1: 0.8026\n",
      "Epoch 34/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1701 - accuracy: 0.9303 - recall: 0.9302 - f1: 0.9303\n",
      "Epoch 00034: val_loss did not improve from 0.25281\n",
      "150/150 [==============================] - 67s 448ms/step - loss: 0.1700 - accuracy: 0.9303 - recall: 0.9302 - f1: 0.9303 - val_loss: 0.2873 - val_accuracy: 0.8718 - val_recall: 0.8722 - val_f1: 0.8719\n",
      "Epoch 35/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1691 - accuracy: 0.9310 - recall: 0.9310 - f1: 0.9310\n",
      "Epoch 00035: val_loss did not improve from 0.25281\n",
      "150/150 [==============================] - 68s 456ms/step - loss: 0.1689 - accuracy: 0.9310 - recall: 0.9311 - f1: 0.9310 - val_loss: 0.3470 - val_accuracy: 0.8608 - val_recall: 0.8606 - val_f1: 0.8608\n",
      "Epoch 36/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1685 - accuracy: 0.9309 - recall: 0.9308 - f1: 0.9309\n",
      "Epoch 00036: val_loss improved from 0.25281 to 0.25094, saving model to batch_relu_validation_200-036-0.931133-0.900033.h5\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1682 - accuracy: 0.9311 - recall: 0.9310 - f1: 0.9311 - val_loss: 0.2509 - val_accuracy: 0.9000 - val_recall: 0.9005 - val_f1: 0.9001\n",
      "Epoch 37/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1748 - accuracy: 0.9297 - recall: 0.9297 - f1: 0.9297\n",
      "Epoch 00037: val_loss improved from 0.25094 to 0.24714, saving model to batch_relu_validation_200-037-0.929433-0.891333.h5\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "150/150 [==============================] - 68s 451ms/step - loss: 0.1750 - accuracy: 0.9294 - recall: 0.9294 - f1: 0.9294 - val_loss: 0.2471 - val_accuracy: 0.8913 - val_recall: 0.8914 - val_f1: 0.8913\n",
      "Epoch 38/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9367 - recall: 0.9368 - f1: 0.9367\n",
      "Epoch 00038: val_loss improved from 0.24714 to 0.21298, saving model to batch_relu_validation_200-038-0.936967-0.908300.h5\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1563 - accuracy: 0.9370 - recall: 0.9371 - f1: 0.9370 - val_loss: 0.2130 - val_accuracy: 0.9083 - val_recall: 0.9084 - val_f1: 0.9083\n",
      "Epoch 39/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1509 - accuracy: 0.9397 - recall: 0.9396 - f1: 0.9397\n",
      "Epoch 00039: val_loss improved from 0.21298 to 0.20633, saving model to batch_relu_validation_200-039-0.939467-0.913300.h5\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1515 - accuracy: 0.9395 - recall: 0.9394 - f1: 0.9395 - val_loss: 0.2063 - val_accuracy: 0.9133 - val_recall: 0.9133 - val_f1: 0.9133\n",
      "Epoch 40/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1552 - accuracy: 0.9370 - recall: 0.9366 - f1: 0.9370\n",
      "Epoch 00040: val_loss did not improve from 0.20633\n",
      "150/150 [==============================] - 68s 455ms/step - loss: 0.1554 - accuracy: 0.9368 - recall: 0.9365 - f1: 0.9368 - val_loss: 0.2725 - val_accuracy: 0.8959 - val_recall: 0.8959 - val_f1: 0.8959\n",
      "Epoch 41/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1496 - accuracy: 0.9375 - recall: 0.9374 - f1: 0.9375\n",
      "Epoch 00041: val_loss did not improve from 0.20633\n",
      "150/150 [==============================] - 67s 448ms/step - loss: 0.1496 - accuracy: 0.9375 - recall: 0.9375 - f1: 0.9375 - val_loss: 0.2605 - val_accuracy: 0.8835 - val_recall: 0.8835 - val_f1: 0.8835\n",
      "Epoch 42/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1519 - accuracy: 0.9381 - recall: 0.9381 - f1: 0.9381\n",
      "Epoch 00042: val_loss did not improve from 0.20633\n",
      "150/150 [==============================] - 68s 451ms/step - loss: 0.1517 - accuracy: 0.9381 - recall: 0.9381 - f1: 0.9381 - val_loss: 0.2105 - val_accuracy: 0.9089 - val_recall: 0.9085 - val_f1: 0.9089\n",
      "Epoch 43/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1482 - accuracy: 0.9401 - recall: 0.9397 - f1: 0.9401\n",
      "Epoch 00043: val_loss did not improve from 0.20633\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1485 - accuracy: 0.9400 - recall: 0.9397 - f1: 0.9400 - val_loss: 0.2329 - val_accuracy: 0.9008 - val_recall: 0.9009 - val_f1: 0.9008\n",
      "Epoch 44/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1560 - accuracy: 0.9364 - recall: 0.9364 - f1: 0.9364\n",
      "Epoch 00044: val_loss did not improve from 0.20633\n",
      "150/150 [==============================] - 68s 457ms/step - loss: 0.1556 - accuracy: 0.9364 - recall: 0.9364 - f1: 0.9364 - val_loss: 0.2334 - val_accuracy: 0.9002 - val_recall: 0.8997 - val_f1: 0.9001\n",
      "Epoch 45/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1455 - accuracy: 0.9418 - recall: 0.9416 - f1: 0.9418\n",
      "Epoch 00045: val_loss improved from 0.20633 to 0.20597, saving model to batch_relu_validation_200-045-0.941767-0.913333.h5\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1454 - accuracy: 0.9418 - recall: 0.9416 - f1: 0.9418 - val_loss: 0.2060 - val_accuracy: 0.9133 - val_recall: 0.9133 - val_f1: 0.9133\n",
      "Epoch 46/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1474 - accuracy: 0.9402 - recall: 0.9401 - f1: 0.9402\n",
      "Epoch 00046: val_loss did not improve from 0.20597\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1471 - accuracy: 0.9403 - recall: 0.9402 - f1: 0.9403 - val_loss: 0.2141 - val_accuracy: 0.9074 - val_recall: 0.9074 - val_f1: 0.9074\n",
      "Epoch 47/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9404 - recall: 0.9403 - f1: 0.9404\n",
      "Epoch 00047: val_loss did not improve from 0.20597\n",
      "150/150 [==============================] - 68s 452ms/step - loss: 0.1488 - accuracy: 0.9405 - recall: 0.9405 - f1: 0.9405 - val_loss: 0.2138 - val_accuracy: 0.9088 - val_recall: 0.9089 - val_f1: 0.9088\n",
      "Epoch 48/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1468 - accuracy: 0.9411 - recall: 0.9411 - f1: 0.9411\n",
      "Epoch 00048: val_loss improved from 0.20597 to 0.20063, saving model to batch_relu_validation_200-048-0.940933-0.915300.h5\n",
      "150/150 [==============================] - 68s 454ms/step - loss: 0.1470 - accuracy: 0.9409 - recall: 0.9410 - f1: 0.9409 - val_loss: 0.2006 - val_accuracy: 0.9153 - val_recall: 0.9150 - val_f1: 0.9153\n",
      "Epoch 49/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1410 - accuracy: 0.9409 - recall: 0.9410 - f1: 0.9409\n",
      "Epoch 00049: val_loss did not improve from 0.20063\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.1407 - accuracy: 0.9409 - recall: 0.9410 - f1: 0.9409 - val_loss: 0.2050 - val_accuracy: 0.9142 - val_recall: 0.9142 - val_f1: 0.9142\n",
      "Epoch 50/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1451 - accuracy: 0.9384 - recall: 0.9385 - f1: 0.9384\n",
      "Epoch 00050: val_loss did not improve from 0.20063\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.1454 - accuracy: 0.9385 - recall: 0.9386 - f1: 0.9385 - val_loss: 0.2074 - val_accuracy: 0.9132 - val_recall: 0.9131 - val_f1: 0.9132\n",
      "Epoch 51/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1522 - accuracy: 0.9385 - recall: 0.9387 - f1: 0.9385\n",
      "Epoch 00051: val_loss did not improve from 0.20063\n",
      "150/150 [==============================] - 68s 455ms/step - loss: 0.1528 - accuracy: 0.9380 - recall: 0.9382 - f1: 0.9380 - val_loss: 0.2674 - val_accuracy: 0.8881 - val_recall: 0.8886 - val_f1: 0.8881\n",
      "Epoch 52/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1437 - accuracy: 0.9381 - recall: 0.9381 - f1: 0.9381\n",
      "Epoch 00052: val_loss did not improve from 0.20063\n",
      "150/150 [==============================] - 68s 452ms/step - loss: 0.1436 - accuracy: 0.9381 - recall: 0.9381 - f1: 0.9381 - val_loss: 0.2301 - val_accuracy: 0.9160 - val_recall: 0.9158 - val_f1: 0.9160\n",
      "Epoch 53/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1431 - accuracy: 0.9434 - recall: 0.9434 - f1: 0.9434\n",
      "Epoch 00053: val_loss did not improve from 0.20063\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1426 - accuracy: 0.9435 - recall: 0.9435 - f1: 0.9435 - val_loss: 0.2233 - val_accuracy: 0.9081 - val_recall: 0.9082 - val_f1: 0.9081\n",
      "Epoch 54/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1382 - accuracy: 0.9416 - recall: 0.9415 - f1: 0.9416\n",
      "Epoch 00054: val_loss did not improve from 0.20063\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1378 - accuracy: 0.9417 - recall: 0.9417 - f1: 0.9417 - val_loss: 0.2025 - val_accuracy: 0.9148 - val_recall: 0.9148 - val_f1: 0.9148\n",
      "Epoch 55/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1340 - accuracy: 0.9456 - recall: 0.9458 - f1: 0.9456\n",
      "Epoch 00055: val_loss did not improve from 0.20063\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1341 - accuracy: 0.9454 - recall: 0.9457 - f1: 0.9455 - val_loss: 0.2094 - val_accuracy: 0.9140 - val_recall: 0.9139 - val_f1: 0.9140\n",
      "Epoch 56/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1383 - accuracy: 0.9434 - recall: 0.9433 - f1: 0.9434\n",
      "Epoch 00056: val_loss improved from 0.20063 to 0.18905, saving model to batch_relu_validation_200-056-0.943367-0.921667.h5\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.1383 - accuracy: 0.9434 - recall: 0.9433 - f1: 0.9434 - val_loss: 0.1891 - val_accuracy: 0.9217 - val_recall: 0.9215 - val_f1: 0.9217\n",
      "Epoch 57/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1400 - accuracy: 0.9439 - recall: 0.9440 - f1: 0.9439\n",
      "Epoch 00057: val_loss did not improve from 0.18905\n",
      "150/150 [==============================] - 67s 445ms/step - loss: 0.1398 - accuracy: 0.9440 - recall: 0.9441 - f1: 0.9440 - val_loss: 0.2419 - val_accuracy: 0.8981 - val_recall: 0.8981 - val_f1: 0.8981\n",
      "Epoch 58/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1375 - accuracy: 0.9417 - recall: 0.9417 - f1: 0.9417\n",
      "Epoch 00058: val_loss did not improve from 0.18905\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1381 - accuracy: 0.9414 - recall: 0.9414 - f1: 0.9414 - val_loss: 0.3047 - val_accuracy: 0.8763 - val_recall: 0.8764 - val_f1: 0.8763\n",
      "Epoch 59/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1351 - accuracy: 0.9441 - recall: 0.9438 - f1: 0.9441\n",
      "Epoch 00059: val_loss did not improve from 0.18905\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.1351 - accuracy: 0.9439 - recall: 0.9437 - f1: 0.9439 - val_loss: 0.2165 - val_accuracy: 0.9181 - val_recall: 0.9179 - val_f1: 0.9181\n",
      "Epoch 60/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1311 - accuracy: 0.9475 - recall: 0.9474 - f1: 0.9475\n",
      "Epoch 00060: val_loss did not improve from 0.18905\n",
      "150/150 [==============================] - 68s 452ms/step - loss: 0.1311 - accuracy: 0.9475 - recall: 0.9473 - f1: 0.9475 - val_loss: 0.2034 - val_accuracy: 0.9129 - val_recall: 0.9126 - val_f1: 0.9128\n",
      "Epoch 61/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1305 - accuracy: 0.9453 - recall: 0.9454 - f1: 0.9453\n",
      "Epoch 00061: val_loss did not improve from 0.18905\n",
      "150/150 [==============================] - 67s 443ms/step - loss: 0.1305 - accuracy: 0.9450 - recall: 0.9451 - f1: 0.9450 - val_loss: 0.1944 - val_accuracy: 0.9192 - val_recall: 0.9188 - val_f1: 0.9192\n",
      "Epoch 62/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9487 - recall: 0.9484 - f1: 0.9487\n",
      "Epoch 00062: val_loss did not improve from 0.18905\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1301 - accuracy: 0.9487 - recall: 0.9483 - f1: 0.9486 - val_loss: 0.2199 - val_accuracy: 0.9049 - val_recall: 0.9047 - val_f1: 0.9048\n",
      "Epoch 63/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1287 - accuracy: 0.9460 - recall: 0.9460 - f1: 0.9460\n",
      "Epoch 00063: val_loss did not improve from 0.18905\n",
      "150/150 [==============================] - 68s 452ms/step - loss: 0.1286 - accuracy: 0.9461 - recall: 0.9461 - f1: 0.9461 - val_loss: 0.1920 - val_accuracy: 0.9171 - val_recall: 0.9169 - val_f1: 0.9171\n",
      "Epoch 64/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9488 - recall: 0.9487 - f1: 0.9488\n",
      "Epoch 00064: val_loss did not improve from 0.18905\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.1254 - accuracy: 0.9488 - recall: 0.9487 - f1: 0.9488 - val_loss: 0.1953 - val_accuracy: 0.9160 - val_recall: 0.9157 - val_f1: 0.9160\n",
      "Epoch 65/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9502 - recall: 0.9503 - f1: 0.9502\n",
      "Epoch 00065: val_loss improved from 0.18905 to 0.17861, saving model to batch_relu_validation_200-065-0.950100-0.924933.h5\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1214 - accuracy: 0.9501 - recall: 0.9503 - f1: 0.9501 - val_loss: 0.1786 - val_accuracy: 0.9249 - val_recall: 0.9248 - val_f1: 0.9249\n",
      "Epoch 66/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1290 - accuracy: 0.9485 - recall: 0.9483 - f1: 0.9485\n",
      "Epoch 00066: val_loss did not improve from 0.17861\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.1287 - accuracy: 0.9487 - recall: 0.9485 - f1: 0.9487 - val_loss: 0.1907 - val_accuracy: 0.9209 - val_recall: 0.9208 - val_f1: 0.9209\n",
      "Epoch 67/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1296 - accuracy: 0.9468 - recall: 0.9467 - f1: 0.9468\n",
      "Epoch 00067: val_loss did not improve from 0.17861\n",
      "150/150 [==============================] - 68s 455ms/step - loss: 0.1296 - accuracy: 0.9466 - recall: 0.9465 - f1: 0.9466 - val_loss: 0.1827 - val_accuracy: 0.9263 - val_recall: 0.9261 - val_f1: 0.9263\n",
      "Epoch 68/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9505 - recall: 0.9505 - f1: 0.9505\n",
      "Epoch 00068: val_loss did not improve from 0.17861\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1249 - accuracy: 0.9502 - recall: 0.9502 - f1: 0.9502 - val_loss: 0.1965 - val_accuracy: 0.9164 - val_recall: 0.9162 - val_f1: 0.9164\n",
      "Epoch 69/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9469 - recall: 0.9469 - f1: 0.9469\n",
      "Epoch 00069: val_loss did not improve from 0.17861\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1277 - accuracy: 0.9468 - recall: 0.9468 - f1: 0.9468 - val_loss: 0.1817 - val_accuracy: 0.9265 - val_recall: 0.9264 - val_f1: 0.9265\n",
      "Epoch 70/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9483 - recall: 0.9481 - f1: 0.9483\n",
      "Epoch 00070: val_loss improved from 0.17861 to 0.17550, saving model to batch_relu_validation_200-070-0.948267-0.928433.h5\n",
      "150/150 [==============================] - 67s 450ms/step - loss: 0.1237 - accuracy: 0.9483 - recall: 0.9480 - f1: 0.9483 - val_loss: 0.1755 - val_accuracy: 0.9284 - val_recall: 0.9283 - val_f1: 0.9284\n",
      "Epoch 71/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9465 - recall: 0.9463 - f1: 0.9465\n",
      "Epoch 00071: val_loss did not improve from 0.17550\n",
      "150/150 [==============================] - 68s 455ms/step - loss: 0.1269 - accuracy: 0.9466 - recall: 0.9463 - f1: 0.9466 - val_loss: 0.1813 - val_accuracy: 0.9240 - val_recall: 0.9238 - val_f1: 0.9240\n",
      "Epoch 72/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9497 - recall: 0.9497 - f1: 0.9497\n",
      "Epoch 00072: val_loss did not improve from 0.17550\n",
      "150/150 [==============================] - 68s 452ms/step - loss: 0.1263 - accuracy: 0.9499 - recall: 0.9499 - f1: 0.9499 - val_loss: 0.1866 - val_accuracy: 0.9224 - val_recall: 0.9221 - val_f1: 0.9224\n",
      "Epoch 73/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9486 - recall: 0.9486 - f1: 0.9486\n",
      "Epoch 00073: val_loss did not improve from 0.17550\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1256 - accuracy: 0.9486 - recall: 0.9485 - f1: 0.9486 - val_loss: 0.1881 - val_accuracy: 0.9251 - val_recall: 0.9251 - val_f1: 0.9251\n",
      "Epoch 74/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1269 - accuracy: 0.9461 - recall: 0.9459 - f1: 0.9461\n",
      "Epoch 00074: val_loss improved from 0.17550 to 0.17388, saving model to batch_relu_validation_200-074-0.946167-0.927667.h5\n",
      "150/150 [==============================] - 68s 450ms/step - loss: 0.1269 - accuracy: 0.9462 - recall: 0.9459 - f1: 0.9461 - val_loss: 0.1739 - val_accuracy: 0.9277 - val_recall: 0.9277 - val_f1: 0.9277\n",
      "Epoch 75/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9485 - recall: 0.9486 - f1: 0.9485\n",
      "Epoch 00075: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 466ms/step - loss: 0.1228 - accuracy: 0.9487 - recall: 0.9488 - f1: 0.9487 - val_loss: 0.1921 - val_accuracy: 0.9201 - val_recall: 0.9201 - val_f1: 0.9201\n",
      "Epoch 76/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9482 - recall: 0.9481 - f1: 0.9482\n",
      "Epoch 00076: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 68s 454ms/step - loss: 0.1272 - accuracy: 0.9483 - recall: 0.9483 - f1: 0.9483 - val_loss: 0.1887 - val_accuracy: 0.9195 - val_recall: 0.9193 - val_f1: 0.9195\n",
      "Epoch 77/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9496 - recall: 0.9496 - f1: 0.9496\n",
      "Epoch 00077: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "150/150 [==============================] - 68s 450ms/step - loss: 0.1249 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495 - val_loss: 0.1750 - val_accuracy: 0.9266 - val_recall: 0.9265 - val_f1: 0.9266\n",
      "Epoch 78/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9493 - recall: 0.9495 - f1: 0.9493\n",
      "Epoch 00078: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.1238 - accuracy: 0.9491 - recall: 0.9493 - f1: 0.9491 - val_loss: 0.1765 - val_accuracy: 0.9249 - val_recall: 0.9246 - val_f1: 0.9249\n",
      "Epoch 79/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9526 - recall: 0.9528 - f1: 0.9526\n",
      "Epoch 00079: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 467ms/step - loss: 0.1201 - accuracy: 0.9524 - recall: 0.9526 - f1: 0.9524 - val_loss: 0.1861 - val_accuracy: 0.9240 - val_recall: 0.9238 - val_f1: 0.9240\n",
      "Epoch 80/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9484 - recall: 0.9483 - f1: 0.9484\n",
      "Epoch 00080: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 461ms/step - loss: 0.1212 - accuracy: 0.9484 - recall: 0.9483 - f1: 0.9484 - val_loss: 0.1762 - val_accuracy: 0.9259 - val_recall: 0.9258 - val_f1: 0.9259\n",
      "Epoch 81/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9483 - recall: 0.9483 - f1: 0.9483\n",
      "Epoch 00081: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 463ms/step - loss: 0.1264 - accuracy: 0.9484 - recall: 0.9484 - f1: 0.9484 - val_loss: 0.1847 - val_accuracy: 0.9250 - val_recall: 0.9249 - val_f1: 0.9250\n",
      "Epoch 82/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507\n",
      "Epoch 00082: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 72s 481ms/step - loss: 0.1200 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507 - val_loss: 0.1851 - val_accuracy: 0.9212 - val_recall: 0.9211 - val_f1: 0.9212\n",
      "Epoch 83/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1212 - accuracy: 0.9511 - recall: 0.9509 - f1: 0.9511\n",
      "Epoch 00083: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "150/150 [==============================] - 72s 478ms/step - loss: 0.1209 - accuracy: 0.9512 - recall: 0.9510 - f1: 0.9512 - val_loss: 0.1753 - val_accuracy: 0.9279 - val_recall: 0.9279 - val_f1: 0.9279\n",
      "Epoch 84/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1315 - accuracy: 0.9471 - recall: 0.9472 - f1: 0.9472\n",
      "Epoch 00084: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 77s 514ms/step - loss: 0.1315 - accuracy: 0.9470 - recall: 0.9471 - f1: 0.9470 - val_loss: 0.1751 - val_accuracy: 0.9291 - val_recall: 0.9291 - val_f1: 0.9291\n",
      "Epoch 85/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9512 - recall: 0.9512 - f1: 0.9512\n",
      "Epoch 00085: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 76s 508ms/step - loss: 0.1216 - accuracy: 0.9513 - recall: 0.9513 - f1: 0.9513 - val_loss: 0.1831 - val_accuracy: 0.9268 - val_recall: 0.9267 - val_f1: 0.9268\n",
      "Epoch 86/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9512 - recall: 0.9513 - f1: 0.9512\n",
      "Epoch 00086: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 76s 509ms/step - loss: 0.1226 - accuracy: 0.9513 - recall: 0.9514 - f1: 0.9513 - val_loss: 0.1834 - val_accuracy: 0.9253 - val_recall: 0.9251 - val_f1: 0.9253\n",
      "Epoch 87/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9481 - recall: 0.9479 - f1: 0.9480\n",
      "Epoch 00087: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "150/150 [==============================] - 87s 582ms/step - loss: 0.1254 - accuracy: 0.9480 - recall: 0.9479 - f1: 0.9480 - val_loss: 0.1813 - val_accuracy: 0.9240 - val_recall: 0.9240 - val_f1: 0.9240\n",
      "Epoch 88/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9472 - recall: 0.9472 - f1: 0.9472\n",
      "Epoch 00088: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 83s 555ms/step - loss: 0.1273 - accuracy: 0.9473 - recall: 0.9473 - f1: 0.9473 - val_loss: 0.1796 - val_accuracy: 0.9274 - val_recall: 0.9273 - val_f1: 0.9274\n",
      "Epoch 89/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1196 - accuracy: 0.9520 - recall: 0.9521 - f1: 0.9520\n",
      "Epoch 00089: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 81s 542ms/step - loss: 0.1200 - accuracy: 0.9520 - recall: 0.9521 - f1: 0.9520 - val_loss: 0.1832 - val_accuracy: 0.9221 - val_recall: 0.9221 - val_f1: 0.9221\n",
      "Epoch 90/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9512 - recall: 0.9510 - f1: 0.9512\n",
      "Epoch 00090: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 78s 517ms/step - loss: 0.1218 - accuracy: 0.9510 - recall: 0.9509 - f1: 0.9510 - val_loss: 0.1848 - val_accuracy: 0.9227 - val_recall: 0.9228 - val_f1: 0.9227\n",
      "Epoch 91/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9522 - recall: 0.9523 - f1: 0.9522\n",
      "Epoch 00091: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 76s 504ms/step - loss: 0.1223 - accuracy: 0.9520 - recall: 0.9521 - f1: 0.9520 - val_loss: 0.1797 - val_accuracy: 0.9254 - val_recall: 0.9253 - val_f1: 0.9254\n",
      "Epoch 92/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 0.9484 - recall: 0.9482 - f1: 0.9484\n",
      "Epoch 00092: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 75s 501ms/step - loss: 0.1267 - accuracy: 0.9485 - recall: 0.9483 - f1: 0.9485 - val_loss: 0.1894 - val_accuracy: 0.9227 - val_recall: 0.9226 - val_f1: 0.9227\n",
      "Epoch 93/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9497 - recall: 0.9497 - f1: 0.9497\n",
      "Epoch 00093: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "150/150 [==============================] - 75s 502ms/step - loss: 0.1266 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495 - val_loss: 0.1789 - val_accuracy: 0.9251 - val_recall: 0.9249 - val_f1: 0.9251\n",
      "Epoch 94/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9491 - recall: 0.9489 - f1: 0.9491\n",
      "Epoch 00094: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 75s 500ms/step - loss: 0.1259 - accuracy: 0.9490 - recall: 0.9488 - f1: 0.9490 - val_loss: 0.1839 - val_accuracy: 0.9239 - val_recall: 0.9236 - val_f1: 0.9239\n",
      "Epoch 95/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9521 - recall: 0.9523 - f1: 0.9521\n",
      "Epoch 00095: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 76s 508ms/step - loss: 0.1190 - accuracy: 0.9524 - recall: 0.9526 - f1: 0.9524 - val_loss: 0.1810 - val_accuracy: 0.9256 - val_recall: 0.9255 - val_f1: 0.9256\n",
      "Epoch 96/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9504 - recall: 0.9504 - f1: 0.9504\n",
      "Epoch 00096: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 72s 481ms/step - loss: 0.1219 - accuracy: 0.9504 - recall: 0.9504 - f1: 0.9504 - val_loss: 0.1777 - val_accuracy: 0.9271 - val_recall: 0.9271 - val_f1: 0.9271\n",
      "Epoch 97/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488\n",
      "Epoch 00097: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 73s 486ms/step - loss: 0.1251 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488 - val_loss: 0.1876 - val_accuracy: 0.9224 - val_recall: 0.9224 - val_f1: 0.9224\n",
      "Epoch 98/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9493 - recall: 0.9491 - f1: 0.9493\n",
      "Epoch 00098: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 78s 518ms/step - loss: 0.1261 - accuracy: 0.9491 - recall: 0.9489 - f1: 0.9491 - val_loss: 0.1856 - val_accuracy: 0.9240 - val_recall: 0.9239 - val_f1: 0.9240\n",
      "Epoch 99/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9492 - recall: 0.9492 - f1: 0.9492\n",
      "Epoch 00099: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "150/150 [==============================] - 77s 516ms/step - loss: 0.1229 - accuracy: 0.9494 - recall: 0.9493 - f1: 0.9494 - val_loss: 0.1835 - val_accuracy: 0.9228 - val_recall: 0.9226 - val_f1: 0.9228\n",
      "Epoch 100/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1283 - accuracy: 0.9466 - recall: 0.9466 - f1: 0.9466\n",
      "Epoch 00100: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 79s 530ms/step - loss: 0.1281 - accuracy: 0.9467 - recall: 0.9467 - f1: 0.9467 - val_loss: 0.1781 - val_accuracy: 0.9268 - val_recall: 0.9268 - val_f1: 0.9268\n",
      "Epoch 101/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9500 - recall: 0.9499 - f1: 0.9500\n",
      "Epoch 00101: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 469ms/step - loss: 0.1237 - accuracy: 0.9499 - recall: 0.9498 - f1: 0.9499 - val_loss: 0.1856 - val_accuracy: 0.9229 - val_recall: 0.9227 - val_f1: 0.9228\n",
      "Epoch 102/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9481 - recall: 0.9482 - f1: 0.9481\n",
      "Epoch 00102: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 75s 502ms/step - loss: 0.1229 - accuracy: 0.9480 - recall: 0.9481 - f1: 0.9480 - val_loss: 0.1868 - val_accuracy: 0.9236 - val_recall: 0.9234 - val_f1: 0.9236\n",
      "Epoch 103/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9508\n",
      "Epoch 00103: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "150/150 [==============================] - 76s 504ms/step - loss: 0.1234 - accuracy: 0.9509 - recall: 0.9508 - f1: 0.9509 - val_loss: 0.1868 - val_accuracy: 0.9238 - val_recall: 0.9238 - val_f1: 0.9238\n",
      "Epoch 104/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1212 - accuracy: 0.9496 - recall: 0.9496 - f1: 0.9496\n",
      "Epoch 00104: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 76s 510ms/step - loss: 0.1216 - accuracy: 0.9496 - recall: 0.9496 - f1: 0.9496 - val_loss: 0.1813 - val_accuracy: 0.9270 - val_recall: 0.9269 - val_f1: 0.9270\n",
      "Epoch 105/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9509 - recall: 0.9508 - f1: 0.9509\n",
      "Epoch 00105: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 75s 498ms/step - loss: 0.1189 - accuracy: 0.9510 - recall: 0.9509 - f1: 0.9510 - val_loss: 0.1838 - val_accuracy: 0.9219 - val_recall: 0.9218 - val_f1: 0.9219\n",
      "Epoch 106/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9500 - recall: 0.9500 - f1: 0.9500\n",
      "Epoch 00106: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 75s 502ms/step - loss: 0.1234 - accuracy: 0.9501 - recall: 0.9501 - f1: 0.9501 - val_loss: 0.1833 - val_accuracy: 0.9265 - val_recall: 0.9265 - val_f1: 0.9265\n",
      "Epoch 107/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9482 - recall: 0.9485 - f1: 0.9482\n",
      "Epoch 00107: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "150/150 [==============================] - 77s 515ms/step - loss: 0.1241 - accuracy: 0.9484 - recall: 0.9486 - f1: 0.9484 - val_loss: 0.1889 - val_accuracy: 0.9211 - val_recall: 0.9210 - val_f1: 0.9211\n",
      "Epoch 108/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9493 - recall: 0.9492 - f1: 0.9493\n",
      "Epoch 00108: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 76s 506ms/step - loss: 0.1246 - accuracy: 0.9492 - recall: 0.9491 - f1: 0.9492 - val_loss: 0.1825 - val_accuracy: 0.9229 - val_recall: 0.9227 - val_f1: 0.9229\n",
      "Epoch 109/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9508\n",
      "Epoch 00109: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 76s 505ms/step - loss: 0.1223 - accuracy: 0.9507 - recall: 0.9506 - f1: 0.9507 - val_loss: 0.1822 - val_accuracy: 0.9259 - val_recall: 0.9258 - val_f1: 0.9259\n",
      "Epoch 110/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9491 - recall: 0.9490 - f1: 0.9491\n",
      "Epoch 00110: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 76s 508ms/step - loss: 0.1261 - accuracy: 0.9486 - recall: 0.9485 - f1: 0.9486 - val_loss: 0.1786 - val_accuracy: 0.9251 - val_recall: 0.9251 - val_f1: 0.9251\n",
      "Epoch 111/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9508\n",
      "Epoch 00111: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "150/150 [==============================] - 76s 509ms/step - loss: 0.1258 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9508 - val_loss: 0.1849 - val_accuracy: 0.9239 - val_recall: 0.9237 - val_f1: 0.9239\n",
      "Epoch 112/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 0.9453 - recall: 0.9452 - f1: 0.9453\n",
      "Epoch 00112: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 463ms/step - loss: 0.1261 - accuracy: 0.9452 - recall: 0.9451 - f1: 0.9452 - val_loss: 0.1762 - val_accuracy: 0.9272 - val_recall: 0.9271 - val_f1: 0.9272\n",
      "Epoch 113/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 0.9484 - recall: 0.9483 - f1: 0.9484\n",
      "Epoch 00113: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 68s 457ms/step - loss: 0.1253 - accuracy: 0.9483 - recall: 0.9482 - f1: 0.9483 - val_loss: 0.1781 - val_accuracy: 0.9265 - val_recall: 0.9265 - val_f1: 0.9265\n",
      "Epoch 114/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9495 - recall: 0.9494 - f1: 0.9495\n",
      "Epoch 00114: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 460ms/step - loss: 0.1200 - accuracy: 0.9495 - recall: 0.9494 - f1: 0.9495 - val_loss: 0.1853 - val_accuracy: 0.9225 - val_recall: 0.9224 - val_f1: 0.9225\n",
      "Epoch 115/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9486 - recall: 0.9485 - f1: 0.9486\n",
      "Epoch 00115: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00115: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "150/150 [==============================] - 78s 517ms/step - loss: 0.1220 - accuracy: 0.9485 - recall: 0.9485 - f1: 0.9485 - val_loss: 0.1787 - val_accuracy: 0.9256 - val_recall: 0.9255 - val_f1: 0.9256\n",
      "Epoch 116/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9494 - recall: 0.9494 - f1: 0.9494\n",
      "Epoch 00116: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 81s 540ms/step - loss: 0.1226 - accuracy: 0.9494 - recall: 0.9495 - f1: 0.9494 - val_loss: 0.1774 - val_accuracy: 0.9251 - val_recall: 0.9249 - val_f1: 0.9251\n",
      "Epoch 117/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9505 - recall: 0.9505 - f1: 0.9505\n",
      "Epoch 00117: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 76s 506ms/step - loss: 0.1225 - accuracy: 0.9506 - recall: 0.9506 - f1: 0.9506 - val_loss: 0.1780 - val_accuracy: 0.9264 - val_recall: 0.9264 - val_f1: 0.9264\n",
      "Epoch 118/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1171 - accuracy: 0.9517 - recall: 0.9517 - f1: 0.9517\n",
      "Epoch 00118: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 458ms/step - loss: 0.1171 - accuracy: 0.9518 - recall: 0.9517 - f1: 0.9518 - val_loss: 0.1783 - val_accuracy: 0.9262 - val_recall: 0.9261 - val_f1: 0.9262\n",
      "Epoch 119/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1209 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507\n",
      "Epoch 00119: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 466ms/step - loss: 0.1207 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507 - val_loss: 0.1834 - val_accuracy: 0.9239 - val_recall: 0.9238 - val_f1: 0.9239\n",
      "Epoch 120/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9509 - recall: 0.9509 - f1: 0.9509\n",
      "Epoch 00120: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 468ms/step - loss: 0.1279 - accuracy: 0.9508 - recall: 0.9509 - f1: 0.9508 - val_loss: 0.1788 - val_accuracy: 0.9246 - val_recall: 0.9245 - val_f1: 0.9246\n",
      "Epoch 121/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9508 - recall: 0.9505 - f1: 0.9508\n",
      "Epoch 00121: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 71s 474ms/step - loss: 0.1203 - accuracy: 0.9507 - recall: 0.9505 - f1: 0.9507 - val_loss: 0.1940 - val_accuracy: 0.9194 - val_recall: 0.9191 - val_f1: 0.9194\n",
      "Epoch 122/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1278 - accuracy: 0.9508 - recall: 0.9509 - f1: 0.9508\n",
      "Epoch 00122: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00122: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "150/150 [==============================] - 73s 489ms/step - loss: 0.1278 - accuracy: 0.9507 - recall: 0.9509 - f1: 0.9507 - val_loss: 0.1770 - val_accuracy: 0.9253 - val_recall: 0.9253 - val_f1: 0.9253\n",
      "Epoch 123/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9517 - recall: 0.9517 - f1: 0.9517\n",
      "Epoch 00123: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 468ms/step - loss: 0.1235 - accuracy: 0.9518 - recall: 0.9518 - f1: 0.9518 - val_loss: 0.1783 - val_accuracy: 0.9240 - val_recall: 0.9238 - val_f1: 0.9240\n",
      "Epoch 124/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9480 - recall: 0.9480 - f1: 0.9480\n",
      "Epoch 00124: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 72s 477ms/step - loss: 0.1230 - accuracy: 0.9481 - recall: 0.9481 - f1: 0.9481 - val_loss: 0.1832 - val_accuracy: 0.9227 - val_recall: 0.9226 - val_f1: 0.9227\n",
      "Epoch 125/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9517 - recall: 0.9517 - f1: 0.9517\n",
      "Epoch 00125: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 465ms/step - loss: 0.1239 - accuracy: 0.9518 - recall: 0.9518 - f1: 0.9518 - val_loss: 0.1801 - val_accuracy: 0.9242 - val_recall: 0.9242 - val_f1: 0.9242\n",
      "Epoch 126/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 0.9489 - recall: 0.9487 - f1: 0.9488\n",
      "Epoch 00126: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00126: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "150/150 [==============================] - 69s 460ms/step - loss: 0.1222 - accuracy: 0.9491 - recall: 0.9489 - f1: 0.9491 - val_loss: 0.1810 - val_accuracy: 0.9240 - val_recall: 0.9239 - val_f1: 0.9240\n",
      "Epoch 127/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9494 - recall: 0.9493 - f1: 0.9494\n",
      "Epoch 00127: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 464ms/step - loss: 0.1274 - accuracy: 0.9494 - recall: 0.9493 - f1: 0.9494 - val_loss: 0.1750 - val_accuracy: 0.9269 - val_recall: 0.9268 - val_f1: 0.9269\n",
      "Epoch 128/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1284 - accuracy: 0.9484 - recall: 0.9484 - f1: 0.9484\n",
      "Epoch 00128: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 71s 471ms/step - loss: 0.1284 - accuracy: 0.9484 - recall: 0.9483 - f1: 0.9484 - val_loss: 0.1867 - val_accuracy: 0.9227 - val_recall: 0.9226 - val_f1: 0.9227\n",
      "Epoch 129/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1300 - accuracy: 0.9468 - recall: 0.9469 - f1: 0.9468\n",
      "Epoch 00129: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 465ms/step - loss: 0.1306 - accuracy: 0.9467 - recall: 0.9468 - f1: 0.9467 - val_loss: 0.1824 - val_accuracy: 0.9232 - val_recall: 0.9232 - val_f1: 0.9232\n",
      "Epoch 130/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488\n",
      "Epoch 00130: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00130: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "150/150 [==============================] - 70s 469ms/step - loss: 0.1232 - accuracy: 0.9485 - recall: 0.9486 - f1: 0.9485 - val_loss: 0.1792 - val_accuracy: 0.9250 - val_recall: 0.9249 - val_f1: 0.9250\n",
      "Epoch 131/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9498 - recall: 0.9497 - f1: 0.9498\n",
      "Epoch 00131: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 464ms/step - loss: 0.1258 - accuracy: 0.9497 - recall: 0.9497 - f1: 0.9497 - val_loss: 0.1891 - val_accuracy: 0.9226 - val_recall: 0.9224 - val_f1: 0.9226\n",
      "Epoch 132/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9504 - recall: 0.9503 - f1: 0.9504\n",
      "Epoch 00132: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 470ms/step - loss: 0.1234 - accuracy: 0.9505 - recall: 0.9503 - f1: 0.9505 - val_loss: 0.1910 - val_accuracy: 0.9186 - val_recall: 0.9185 - val_f1: 0.9186\n",
      "Epoch 133/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9498 - recall: 0.9498 - f1: 0.9498\n",
      "Epoch 00133: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 466ms/step - loss: 0.1219 - accuracy: 0.9499 - recall: 0.9499 - f1: 0.9499 - val_loss: 0.1866 - val_accuracy: 0.9217 - val_recall: 0.9217 - val_f1: 0.9217\n",
      "Epoch 134/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9497 - recall: 0.9497 - f1: 0.9497\n",
      "Epoch 00134: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00134: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "150/150 [==============================] - 71s 477ms/step - loss: 0.1226 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495 - val_loss: 0.1887 - val_accuracy: 0.9229 - val_recall: 0.9227 - val_f1: 0.9229\n",
      "Epoch 135/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9478 - recall: 0.9479 - f1: 0.9478\n",
      "Epoch 00135: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 469ms/step - loss: 0.1241 - accuracy: 0.9478 - recall: 0.9479 - f1: 0.9478 - val_loss: 0.1840 - val_accuracy: 0.9223 - val_recall: 0.9221 - val_f1: 0.9223\n",
      "Epoch 136/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9519 - recall: 0.9519 - f1: 0.9519\n",
      "Epoch 00136: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 469ms/step - loss: 0.1214 - accuracy: 0.9520 - recall: 0.9520 - f1: 0.9520 - val_loss: 0.1876 - val_accuracy: 0.9201 - val_recall: 0.9201 - val_f1: 0.9201\n",
      "Epoch 137/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9496 - recall: 0.9498 - f1: 0.9496\n",
      "Epoch 00137: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 464ms/step - loss: 0.1269 - accuracy: 0.9499 - recall: 0.9501 - f1: 0.9499 - val_loss: 0.1848 - val_accuracy: 0.9256 - val_recall: 0.9254 - val_f1: 0.9256\n",
      "Epoch 138/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9498 - recall: 0.9498 - f1: 0.9498\n",
      "Epoch 00138: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "150/150 [==============================] - 70s 465ms/step - loss: 0.1262 - accuracy: 0.9498 - recall: 0.9498 - f1: 0.9498 - val_loss: 0.1835 - val_accuracy: 0.9229 - val_recall: 0.9228 - val_f1: 0.9229\n",
      "Epoch 139/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9505 - recall: 0.9505 - f1: 0.9505\n",
      "Epoch 00139: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 71s 470ms/step - loss: 0.1215 - accuracy: 0.9503 - recall: 0.9503 - f1: 0.9503 - val_loss: 0.1809 - val_accuracy: 0.9248 - val_recall: 0.9248 - val_f1: 0.9248\n",
      "Epoch 140/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9500 - recall: 0.9499 - f1: 0.9500\n",
      "Epoch 00140: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 73s 484ms/step - loss: 0.1204 - accuracy: 0.9498 - recall: 0.9497 - f1: 0.9498 - val_loss: 0.1777 - val_accuracy: 0.9253 - val_recall: 0.9254 - val_f1: 0.9253\n",
      "Epoch 141/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9477 - recall: 0.9478 - f1: 0.9477\n",
      "Epoch 00141: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 468ms/step - loss: 0.1273 - accuracy: 0.9476 - recall: 0.9477 - f1: 0.9476 - val_loss: 0.1902 - val_accuracy: 0.9216 - val_recall: 0.9215 - val_f1: 0.9216\n",
      "Epoch 142/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1285 - accuracy: 0.9471 - recall: 0.9469 - f1: 0.9471\n",
      "Epoch 00142: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00142: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "150/150 [==============================] - 70s 466ms/step - loss: 0.1281 - accuracy: 0.9474 - recall: 0.9472 - f1: 0.9474 - val_loss: 0.1778 - val_accuracy: 0.9292 - val_recall: 0.9291 - val_f1: 0.9292\n",
      "Epoch 143/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9513 - recall: 0.9513 - f1: 0.9513\n",
      "Epoch 00143: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 72s 478ms/step - loss: 0.1218 - accuracy: 0.9513 - recall: 0.9513 - f1: 0.9513 - val_loss: 0.1850 - val_accuracy: 0.9242 - val_recall: 0.9239 - val_f1: 0.9241\n",
      "Epoch 144/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1196 - accuracy: 0.9509 - recall: 0.9507 - f1: 0.9509\n",
      "Epoch 00144: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 71s 476ms/step - loss: 0.1197 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9508 - val_loss: 0.1885 - val_accuracy: 0.9215 - val_recall: 0.9214 - val_f1: 0.9215\n",
      "Epoch 145/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9477 - recall: 0.9476 - f1: 0.9476\n",
      "Epoch 00145: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 68s 456ms/step - loss: 0.1265 - accuracy: 0.9477 - recall: 0.9477 - f1: 0.9477 - val_loss: 0.1750 - val_accuracy: 0.9267 - val_recall: 0.9268 - val_f1: 0.9267\n",
      "Epoch 146/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9528 - recall: 0.9523 - f1: 0.9527\n",
      "Epoch 00146: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00146: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "150/150 [==============================] - 67s 450ms/step - loss: 0.1178 - accuracy: 0.9526 - recall: 0.9522 - f1: 0.9526 - val_loss: 0.1846 - val_accuracy: 0.9221 - val_recall: 0.9219 - val_f1: 0.9221\n",
      "Epoch 147/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9490 - recall: 0.9489 - f1: 0.9490\n",
      "Epoch 00147: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 464ms/step - loss: 0.1252 - accuracy: 0.9491 - recall: 0.9489 - f1: 0.9491 - val_loss: 0.1806 - val_accuracy: 0.9264 - val_recall: 0.9263 - val_f1: 0.9264\n",
      "Epoch 148/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 0.9493 - recall: 0.9493 - f1: 0.9493\n",
      "Epoch 00148: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 458ms/step - loss: 0.1224 - accuracy: 0.9494 - recall: 0.9493 - f1: 0.9494 - val_loss: 0.1852 - val_accuracy: 0.9243 - val_recall: 0.9243 - val_f1: 0.9243\n",
      "Epoch 149/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1168 - accuracy: 0.9526 - recall: 0.9527 - f1: 0.9526\n",
      "Epoch 00149: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 68s 451ms/step - loss: 0.1168 - accuracy: 0.9525 - recall: 0.9525 - f1: 0.9525 - val_loss: 0.1813 - val_accuracy: 0.9253 - val_recall: 0.9251 - val_f1: 0.9253\n",
      "Epoch 150/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9514 - recall: 0.9513 - f1: 0.9514\n",
      "Epoch 00150: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 459ms/step - loss: 0.1176 - accuracy: 0.9514 - recall: 0.9513 - f1: 0.9514 - val_loss: 0.1852 - val_accuracy: 0.9228 - val_recall: 0.9227 - val_f1: 0.9228\n",
      "Epoch 151/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9508 - recall: 0.9510 - f1: 0.9508\n",
      "Epoch 00151: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 68s 457ms/step - loss: 0.1228 - accuracy: 0.9510 - recall: 0.9511 - f1: 0.9510 - val_loss: 0.1762 - val_accuracy: 0.9259 - val_recall: 0.9257 - val_f1: 0.9258\n",
      "Epoch 152/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9476 - recall: 0.9476 - f1: 0.9476\n",
      "Epoch 00152: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 461ms/step - loss: 0.1246 - accuracy: 0.9476 - recall: 0.9476 - f1: 0.9476 - val_loss: 0.1871 - val_accuracy: 0.9232 - val_recall: 0.9229 - val_f1: 0.9232\n",
      "Epoch 153/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9492 - recall: 0.9493 - f1: 0.9492\n",
      "Epoch 00153: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00153: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.1240 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488 - val_loss: 0.1841 - val_accuracy: 0.9249 - val_recall: 0.9247 - val_f1: 0.9249\n",
      "Epoch 154/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9530 - recall: 0.9529 - f1: 0.9530\n",
      "Epoch 00154: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1175 - accuracy: 0.9529 - recall: 0.9529 - f1: 0.9529 - val_loss: 0.1780 - val_accuracy: 0.9250 - val_recall: 0.9252 - val_f1: 0.9251\n",
      "Epoch 155/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9501 - recall: 0.9500 - f1: 0.9501\n",
      "Epoch 00155: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 75s 503ms/step - loss: 0.1216 - accuracy: 0.9499 - recall: 0.9498 - f1: 0.9499 - val_loss: 0.1857 - val_accuracy: 0.9241 - val_recall: 0.9241 - val_f1: 0.9241\n",
      "Epoch 156/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9503 - recall: 0.9505 - f1: 0.9503\n",
      "Epoch 00156: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 462ms/step - loss: 0.1177 - accuracy: 0.9502 - recall: 0.9504 - f1: 0.9502 - val_loss: 0.1867 - val_accuracy: 0.9228 - val_recall: 0.9227 - val_f1: 0.9228\n",
      "Epoch 157/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9507 - recall: 0.9505 - f1: 0.9507\n",
      "Epoch 00157: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00157: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      "150/150 [==============================] - 68s 454ms/step - loss: 0.1198 - accuracy: 0.9507 - recall: 0.9505 - f1: 0.9507 - val_loss: 0.1775 - val_accuracy: 0.9274 - val_recall: 0.9272 - val_f1: 0.9274\n",
      "Epoch 158/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9537 - recall: 0.9536 - f1: 0.9537\n",
      "Epoch 00158: val_loss improved from 0.17388 to 0.17187, saving model to batch_relu_validation_200-158-0.953700-0.927400.h5\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.1174 - accuracy: 0.9537 - recall: 0.9536 - f1: 0.9537 - val_loss: 0.1719 - val_accuracy: 0.9274 - val_recall: 0.9274 - val_f1: 0.9274\n",
      "Epoch 159/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9502 - recall: 0.9505 - f1: 0.9503\n",
      "Epoch 00159: val_loss did not improve from 0.17187\n",
      "150/150 [==============================] - 69s 458ms/step - loss: 0.1251 - accuracy: 0.9500 - recall: 0.9502 - f1: 0.9500 - val_loss: 0.1801 - val_accuracy: 0.9276 - val_recall: 0.9277 - val_f1: 0.9276\n",
      "Epoch 160/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1120 - accuracy: 0.9548 - recall: 0.9545 - f1: 0.9548\n",
      "Epoch 00160: val_loss did not improve from 0.17187\n",
      "150/150 [==============================] - 69s 463ms/step - loss: 0.1126 - accuracy: 0.9548 - recall: 0.9545 - f1: 0.9548 - val_loss: 0.1837 - val_accuracy: 0.9241 - val_recall: 0.9240 - val_f1: 0.9241\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "batch_normalization = True\n",
    "EPOCHS = 160\n",
    "STEPS_PER_EPOCH = 150\n",
    "activation = \"LeakyReLU\"\n",
    "model = cnn_model(\n",
    "    shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "# Train the model with batch\n",
    "history = model.train()\n",
    "# model.save(\"batch_relu_validation_200.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "plt.savefig(\"batch_relu_validation_200.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,962\n",
      "Trainable params: 2,278,274\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,962\n",
      "Trainable params: 2,278,274\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_distributed_function_63110756 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Processing image => data/test_set_images/test_1/test_1.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_2/test_2.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_3/test_3.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_4/test_4.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_5/test_5.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_6/test_6.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_7/test_7.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_8/test_8.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_9/test_9.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_10/test_10.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_11/test_11.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_12/test_12.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_13/test_13.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_14/test_14.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_15/test_15.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_16/test_16.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_17/test_17.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_18/test_18.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_19/test_19.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_20/test_20.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_21/test_21.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_22/test_22.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_23/test_23.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_24/test_24.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_25/test_25.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_26/test_26.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_27/test_27.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_28/test_28.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_29/test_29.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_30/test_30.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_31/test_31.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_32/test_32.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_33/test_33.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_34/test_34.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_35/test_35.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_36/test_36.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_37/test_37.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_38/test_38.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_39/test_39.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_40/test_40.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_41/test_41.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_42/test_42.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_43/test_43.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_44/test_44.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_45/test_45.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_46/test_46.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_47/test_47.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_48/test_48.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_49/test_49.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_50/test_50.png\n"
     ]
    }
   ],
   "source": [
    "from helpers import *\n",
    "\n",
    "# from cnn_model import cnn_model\n",
    "\n",
    "# Instantiate the model\n",
    "batch_normalization = True\n",
    "activation = \"LeakyReLU\"\n",
    "model = cnn_model(\n",
    "    shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model.load(\"batch_relu_validation_200-074-0.946167-0.927667.h5\")\n",
    "\n",
    "# Print a summary to make sure the correct model is used\n",
    "model.model.summary()\n",
    "\n",
    "# We add all test images to an array, used later for generating a submission\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "    image_filenames.append(image_filename)\n",
    "\n",
    "# Set-up submission filename\n",
    "submission_filename = \"batch_relu_validation_200-074-0.946167-0.927667.csv\"\n",
    "\n",
    "# Generates the submission\n",
    "generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
