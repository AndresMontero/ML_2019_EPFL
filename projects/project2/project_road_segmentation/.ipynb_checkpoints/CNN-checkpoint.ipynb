{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext lab_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from utils import *\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.compat.v2.keras.layers import BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2588520503679708072\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4937233203\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2336674795719986523\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(8)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training images, images loaded: 100 \n",
      "Loading groundtruth images, images loaded: 100 \n"
     ]
    }
   ],
   "source": [
    "image_dir_train = \"data/training/images/\"\n",
    "files = os.listdir(image_dir_train)\n",
    "n_train = len(files)\n",
    "print(f\"Loading training images, images loaded: {n_train} \")\n",
    "imgs_train = np.asarray(\n",
    "    [load_image(image_dir_train + files[i]) for i in range(n_train)]\n",
    ")\n",
    "gt_dir_train = \"data/training/groundtruth/\"\n",
    "print(f\"Loading groundtruth images, images loaded: {n_train} \")\n",
    "gt_imgs_train = np.asarray(\n",
    "    [load_image(gt_dir_train + files[i]) for i in range(n_train)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 400, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 400)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_imgs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 400\n",
    "# Patches for training\n",
    "img_patches_train = [\n",
    "    crop_image(imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "gt_patches_train = [\n",
    "    crop_image(gt_imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = np.asarray(\n",
    "    [\n",
    "        img_patches_train[i][j]\n",
    "        for i in range(len(img_patches_train))\n",
    "        for j in range(len(img_patches_train[i]))\n",
    "    ]\n",
    ")\n",
    "Y_train = np.asarray(\n",
    "    [\n",
    "        gt_patches_train[i][j]\n",
    "        for i in range(len(gt_patches_train))\n",
    "        for j in range(len(gt_patches_train[i]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 400, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 400)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_dir_val = \"data/validating/images/\"\n",
    "# files = os.listdir(image_dir_val)\n",
    "# n_val = len(files)\n",
    "# print(f\"Loading validating images, images loaded: {n_val} \")\n",
    "# imgs_val = np.asarray([load_image(image_dir_val + files[i]) for i in range(n_val)])\n",
    "# gt_dir_val = \"data/validating/groundtruth/\"\n",
    "# print(f\"Loading validating groundtruth, images loaded: {n_val} \")\n",
    "# gt_imgs_val = np.asarray([load_image(gt_dir_val + files[i]) for i in range(n_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_size = 400\n",
    "# # Patches for validating\n",
    "# img_patches_val = [\n",
    "#     crop_image(imgs_val[i], image_size, image_size) for i in range(n_val)\n",
    "# ]\n",
    "# gt_patches_val = [\n",
    "#     crop_image(gt_imgs_val[i], image_size, image_size) for i in range(n_val)\n",
    "# ]\n",
    "\n",
    "# # Separate features and labels\n",
    "# X_val = np.asarray(\n",
    "#     [\n",
    "#         img_patches_val[i][j]\n",
    "#         for i in range(len(img_patches_val))\n",
    "#         for j in range(len(img_patches_val[i]))\n",
    "#     ]\n",
    "# )\n",
    "# Y_val = np.asarray(\n",
    "#     [\n",
    "#         gt_patches_val[i][j]\n",
    "#         for i in range(len(gt_patches_val))\n",
    "#         for j in range(len(gt_patches_val[i]))\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = imag_rotation_aug(imgs_train, gt_imgs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "Y_train = np.asarray(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 456, 456, 3)\n",
      "(900, 456, 456)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "n_train = Y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val, Y_val = imag_rotation_aug(imgs_val, gt_imgs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val = np.asarray(X_val)\n",
    "# Y_val = np.asarray(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_val.shape)\n",
    "# print(Y_val.shape)\n",
    "# n_val = Y_val.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create functions to calcualte precision, recall and F-1 in the training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Compute the Precision for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "        Precision (numpy.float64): the Precision of the batch \n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Compute the Recall for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "       Recall (numpy.float64): the Recal of the batch \n",
    "    \"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    \"\"\"Compute the F-1 for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "       F-1 (numpy.float64): the F-1 of the batch \n",
    "    \"\"\"\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erick architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        self.model = self.initialize_U_NET(shape)\n",
    "\n",
    "    def initialize_U_NET(self, shape):\n",
    "        \"\"\"Create Network Architecture.\n",
    "        Args:\n",
    "            shape (triplet): Size of the input layer height x width x colors (64 x 64 x 3)\n",
    "        Returns:\n",
    "            model (Neural Network): Architecture of the model\n",
    "        \"\"\"\n",
    "        # INPUT\n",
    "        # shape     - Size of the input images\n",
    "        # OUTPUT\n",
    "        # model    - Compiled CNN\n",
    "\n",
    "        # Define hyperparamters\n",
    "        KERNEL3 = (3, 3)\n",
    "        KERNEL5 = (5, 5)\n",
    "\n",
    "        # Define a model\n",
    "        model = Sequential()\n",
    "\n",
    "        # Add the layers\n",
    "        # Selection of the model is described in the report\n",
    "        # We use padding = 'same' to avoid issues with the matrix sizes\n",
    "        model.add(Conv2D(64, KERNEL5, input_shape=shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Conv2D(128, KERNEL3, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Conv2D(512, KERNEL3, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        # Flatten it and use regularizers to avoid overfitting\n",
    "        # The parameters have been chosen empirically\n",
    "        model.add(Flatten())\n",
    "        model.add(\n",
    "            Dense(\n",
    "                128, kernel_regularizer=l2(0.000001), activity_regularizer=l2(0.000001)\n",
    "            )\n",
    "        )\n",
    "        # model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # Add output layer\n",
    "        model.add(\n",
    "            Dense(2, kernel_regularizer=l2(0.000001), activity_regularizer=l2(0.000001))\n",
    "        )\n",
    "        model.add(Activation(\"sigmoid\"))\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            metrics=[\"accuracy\", recall, f1],\n",
    "        )\n",
    "\n",
    "        # Print a summary of the model to see what has been generated\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the Model.\n",
    "\n",
    "        Returns:\n",
    "            History (History_Keras): History of the training\n",
    "        \"\"\"\n",
    "        # Early stopping callback after 10 steps\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"loss\", patience=10, verbose=1, restore_best_weights=True,\n",
    "        )\n",
    "        # Reduce learning rate on plateau after 5 steps\n",
    "        lr_callback = ReduceLROnPlateau(\n",
    "            monitor=\"loss\", factor=0.8, patience=4, verbose=1, cooldown=1,\n",
    "        )\n",
    "        save_best = ModelCheckpoint(\n",
    "            \"Erick_dropout_0.2-{epoch:03d}-{f1:03f}.h5\",\n",
    "            save_best_only=True,\n",
    "            monitor=\"loss\",\n",
    "            verbose=1,\n",
    "        )\n",
    "        callbacks = [lr_callback, save_best, early_stopping]\n",
    "\n",
    "        # Train the model using the previously defined functions and callbacks\n",
    "        history = self.model.fit_generator(\n",
    "            create_minibatch(\n",
    "                X_train, Y_train, n_train, WINDOW_SIZE, BATCH_SIZE, PATCH_SIZE, WIDTH\n",
    "            ),\n",
    "            steps_per_epoch=STEPS_PER_EPOCH,\n",
    "            epochs=EPOCHS,\n",
    "            use_multiprocessing=False,\n",
    "            workers=1,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "            #             validation_data=create_minibatch(\n",
    "            #                 X_val, Y_val, n_val, WINDOW_SIZE, BATCH_SIZE, PATCH_SIZE, WIDTH\n",
    "            #             ),\n",
    "            #             validation_steps=STEPS_PER_EPOCH / 3,\n",
    "        )\n",
    "        return history\n",
    "\n",
    "    def classify(self, X):\n",
    "        \"\"\"Classify Image as either road or not.\n",
    "        Args:\n",
    "            X (image): part of the image to classify\n",
    "        Returns:\n",
    "            Predictions : Predictions for each patch\n",
    "        \"\"\"\n",
    "        # Subdivide the images into blocks with a stride and patch_size of 16\n",
    "        img_patches = create_patches(X, 16, 16, padding=24)\n",
    "\n",
    "        # Predict\n",
    "        predictions = self.model.predict(img_patches)\n",
    "        predictions = (predictions[:, 0] < predictions[:, 1]) * 1\n",
    "\n",
    "        # Regroup patches into images\n",
    "        return predictions.reshape(X.shape[0], -1)\n",
    "\n",
    "    #         return group_patches(predictions, X.shape[0])\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"Loads Saved Model.\n",
    "        Args:\n",
    "           filename (string): name of the model\n",
    "           \n",
    "        \"\"\"\n",
    "        # Load the model (used for submission)\n",
    "        dependencies = {\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "        self.model = load_model(filename, custom_objects=dependencies)\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Saves trained model.\n",
    "        Args:\n",
    "           filename (string): name of the model\n",
    "           \n",
    "        \"\"\"\n",
    "        self.model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 64)        4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1048704   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,603,010\n",
      "Trainable params: 2,603,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We define parameters of the model\n",
    "BATCH_SIZE = 300\n",
    "WINDOW_SIZE = 64\n",
    "PATCH_SIZE = 16\n",
    "EPOCHS = 300\n",
    "STEPS_PER_EPOCH = 100\n",
    "WIDTH = 448\n",
    "model = CNN(shape=(WINDOW_SIZE, WINDOW_SIZE, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.5494 - accuracy: 0.7576 - recall: 0.7588 - f1: 0.7580\n",
      "Epoch 00001: loss improved from inf to 0.54927, saving model to Erick_dropout-001-0.757924.h5\n",
      "100/100 [==============================] - 37s 374ms/step - loss: 0.5493 - accuracy: 0.7575 - recall: 0.7587 - f1: 0.7579\n",
      "Epoch 2/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.5008 - accuracy: 0.7499 - recall: 0.7492 - f1: 0.7498\n",
      "Epoch 00002: loss improved from 0.54927 to 0.50086, saving model to Erick_dropout-002-0.749423.h5\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.5009 - accuracy: 0.7496 - recall: 0.7489 - f1: 0.7494\n",
      "Epoch 3/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.4731 - accuracy: 0.7609 - recall: 0.7594 - f1: 0.7605\n",
      "Epoch 00003: loss improved from 0.50086 to 0.47233, saving model to Erick_dropout-003-0.761112.h5\n",
      "100/100 [==============================] - 32s 318ms/step - loss: 0.4723 - accuracy: 0.7614 - recall: 0.7603 - f1: 0.7611\n",
      "Epoch 4/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.4702 - accuracy: 0.7740 - recall: 0.7745 - f1: 0.7741\n",
      "Epoch 00004: loss improved from 0.47233 to 0.46972, saving model to Erick_dropout-004-0.774242.h5\n",
      "100/100 [==============================] - 32s 316ms/step - loss: 0.4697 - accuracy: 0.7742 - recall: 0.7746 - f1: 0.7742\n",
      "Epoch 5/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.4323 - accuracy: 0.7952 - recall: 0.7974 - f1: 0.7956\n",
      "Epoch 00005: loss improved from 0.46972 to 0.43184, saving model to Erick_dropout-005-0.795921.h5\n",
      "100/100 [==============================] - 32s 316ms/step - loss: 0.4318 - accuracy: 0.7955 - recall: 0.7978 - f1: 0.7959\n",
      "Epoch 6/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3894 - accuracy: 0.8190 - recall: 0.8202 - f1: 0.8192\n",
      "Epoch 00006: loss improved from 0.43184 to 0.38892, saving model to Erick_dropout-006-0.819471.h5\n",
      "100/100 [==============================] - 32s 315ms/step - loss: 0.3889 - accuracy: 0.8193 - recall: 0.8204 - f1: 0.8195\n",
      "Epoch 7/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3449 - accuracy: 0.8430 - recall: 0.8424 - f1: 0.8429\n",
      "Epoch 00007: loss improved from 0.38892 to 0.34476, saving model to Erick_dropout-007-0.843140.h5\n",
      "100/100 [==============================] - 32s 324ms/step - loss: 0.3448 - accuracy: 0.8432 - recall: 0.8426 - f1: 0.8431\n",
      "Epoch 8/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3314 - accuracy: 0.8520 - recall: 0.8524 - f1: 0.8520\n",
      "Epoch 00008: loss improved from 0.34476 to 0.33059, saving model to Erick_dropout-008-0.852497.h5\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 0.3306 - accuracy: 0.8524 - recall: 0.8529 - f1: 0.8525\n",
      "Epoch 9/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3014 - accuracy: 0.8655 - recall: 0.8655 - f1: 0.8655\n",
      "Epoch 00009: loss improved from 0.33059 to 0.30175, saving model to Erick_dropout-009-0.865496.h5\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 0.3017 - accuracy: 0.8655 - recall: 0.8655 - f1: 0.8655\n",
      "Epoch 10/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.8746 - recall: 0.8748 - f1: 0.8746\n",
      "Epoch 00010: loss improved from 0.30175 to 0.28993, saving model to Erick_dropout-010-0.874566.h5\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 0.2899 - accuracy: 0.8746 - recall: 0.8748 - f1: 0.8746\n",
      "Epoch 11/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.8740 - recall: 0.8740 - f1: 0.8740\n",
      "Epoch 00011: loss improved from 0.28993 to 0.28660, saving model to Erick_dropout-011-0.873804.h5\n",
      "100/100 [==============================] - 34s 336ms/step - loss: 0.2866 - accuracy: 0.8738 - recall: 0.8739 - f1: 0.8738\n",
      "Epoch 12/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2678 - accuracy: 0.8849 - recall: 0.8852 - f1: 0.8849\n",
      "Epoch 00012: loss improved from 0.28660 to 0.26816, saving model to Erick_dropout-012-0.884646.h5\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.2682 - accuracy: 0.8846 - recall: 0.8849 - f1: 0.8846\n",
      "Epoch 13/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2569 - accuracy: 0.8877 - recall: 0.8883 - f1: 0.8878\n",
      "Epoch 00013: loss improved from 0.26816 to 0.25665, saving model to Erick_dropout-013-0.888105.h5\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.2567 - accuracy: 0.8880 - recall: 0.8887 - f1: 0.8881\n",
      "Epoch 14/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2524 - accuracy: 0.8913 - recall: 0.8912 - f1: 0.8913\n",
      "Epoch 00014: loss improved from 0.25665 to 0.25234, saving model to Erick_dropout-014-0.891125.h5\n",
      "100/100 [==============================] - 32s 324ms/step - loss: 0.2523 - accuracy: 0.8911 - recall: 0.8911 - f1: 0.8911\n",
      "Epoch 15/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2482 - accuracy: 0.8918 - recall: 0.8920 - f1: 0.8918\n",
      "Epoch 00015: loss improved from 0.25234 to 0.24769, saving model to Erick_dropout-015-0.891938.h5\n",
      "100/100 [==============================] - 32s 315ms/step - loss: 0.2477 - accuracy: 0.8919 - recall: 0.8921 - f1: 0.8919\n",
      "Epoch 16/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2362 - accuracy: 0.8989 - recall: 0.8990 - f1: 0.8989\n",
      "Epoch 00016: loss improved from 0.24769 to 0.23609, saving model to Erick_dropout-016-0.898836.h5\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.2361 - accuracy: 0.8988 - recall: 0.8990 - f1: 0.8988\n",
      "Epoch 17/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2375 - accuracy: 0.8982 - recall: 0.8987 - f1: 0.8983\n",
      "Epoch 00017: loss did not improve from 0.23609\n",
      "100/100 [==============================] - 31s 312ms/step - loss: 0.2376 - accuracy: 0.8982 - recall: 0.8987 - f1: 0.8983\n",
      "Epoch 18/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2326 - accuracy: 0.9014 - recall: 0.9016 - f1: 0.9014\n",
      "Epoch 00018: loss improved from 0.23609 to 0.23272, saving model to Erick_dropout-018-0.901402.h5\n",
      "100/100 [==============================] - 32s 317ms/step - loss: 0.2327 - accuracy: 0.9014 - recall: 0.9015 - f1: 0.9014\n",
      "Epoch 19/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2285 - accuracy: 0.9034 - recall: 0.9035 - f1: 0.9034\n",
      "Epoch 00019: loss improved from 0.23272 to 0.22910, saving model to Erick_dropout-019-0.902893.h5\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 0.2291 - accuracy: 0.9029 - recall: 0.9030 - f1: 0.9029\n",
      "Epoch 20/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2213 - accuracy: 0.9066 - recall: 0.9069 - f1: 0.9066\n",
      "Epoch 00020: loss improved from 0.22910 to 0.22082, saving model to Erick_dropout-020-0.906962.h5\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.2208 - accuracy: 0.9069 - recall: 0.9072 - f1: 0.9070\n",
      "Epoch 21/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2092 - accuracy: 0.9126 - recall: 0.9118 - f1: 0.9125\n",
      "Epoch 00021: loss improved from 0.22082 to 0.20922, saving model to Erick_dropout-021-0.912524.h5\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.2092 - accuracy: 0.9126 - recall: 0.9118 - f1: 0.9125\n",
      "Epoch 22/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2130 - accuracy: 0.9106 - recall: 0.9103 - f1: 0.9106\n",
      "Epoch 00022: loss did not improve from 0.20922\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.2125 - accuracy: 0.9108 - recall: 0.9105 - f1: 0.9108\n",
      "Epoch 23/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2087 - accuracy: 0.9122 - recall: 0.9118 - f1: 0.9122\n",
      "Epoch 00023: loss improved from 0.20922 to 0.20870, saving model to Erick_dropout-023-0.912263.h5\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.2087 - accuracy: 0.9123 - recall: 0.9119 - f1: 0.9123\n",
      "Epoch 24/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2073 - accuracy: 0.9158 - recall: 0.9155 - f1: 0.9158\n",
      "Epoch 00024: loss improved from 0.20870 to 0.20730, saving model to Erick_dropout-024-0.915725.h5\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 0.2073 - accuracy: 0.9158 - recall: 0.9154 - f1: 0.9157\n",
      "Epoch 25/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1993 - accuracy: 0.9156 - recall: 0.9152 - f1: 0.9156\n",
      "Epoch 00025: loss improved from 0.20730 to 0.19935, saving model to Erick_dropout-025-0.915561.h5\n",
      "100/100 [==============================] - 33s 328ms/step - loss: 0.1994 - accuracy: 0.9156 - recall: 0.9151 - f1: 0.9156\n",
      "Epoch 26/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2021 - accuracy: 0.9162 - recall: 0.9157 - f1: 0.9161\n",
      "Epoch 00026: loss did not improve from 0.19935\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.2023 - accuracy: 0.9163 - recall: 0.9158 - f1: 0.9162\n",
      "Epoch 27/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2039 - accuracy: 0.9147 - recall: 0.9143 - f1: 0.9147\n",
      "Epoch 00027: loss did not improve from 0.19935\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 0.2037 - accuracy: 0.9149 - recall: 0.9144 - f1: 0.9149\n",
      "Epoch 28/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1976 - accuracy: 0.9158 - recall: 0.9154 - f1: 0.9157\n",
      "Epoch 00028: loss improved from 0.19935 to 0.19742, saving model to Erick_dropout-028-0.915875.h5\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.1974 - accuracy: 0.9159 - recall: 0.9155 - f1: 0.9159\n",
      "Epoch 29/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1952 - accuracy: 0.9196 - recall: 0.9200 - f1: 0.9197\n",
      "Epoch 00029: loss improved from 0.19742 to 0.19498, saving model to Erick_dropout-029-0.919846.h5\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 0.1950 - accuracy: 0.9198 - recall: 0.9201 - f1: 0.9198\n",
      "Epoch 30/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1948 - accuracy: 0.9201 - recall: 0.9200 - f1: 0.9201\n",
      "Epoch 00030: loss improved from 0.19498 to 0.19457, saving model to Erick_dropout-030-0.920202.h5\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 0.1946 - accuracy: 0.9202 - recall: 0.9201 - f1: 0.9202\n",
      "Epoch 31/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1996 - accuracy: 0.9169 - recall: 0.9168 - f1: 0.9169\n",
      "Epoch 00031: loss did not improve from 0.19457\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.2000 - accuracy: 0.9168 - recall: 0.9166 - f1: 0.9167\n",
      "Epoch 32/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1962 - accuracy: 0.9190 - recall: 0.9188 - f1: 0.9190\n",
      "Epoch 00032: loss did not improve from 0.19457\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 0.1961 - accuracy: 0.9191 - recall: 0.9189 - f1: 0.9191\n",
      "Epoch 33/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1880 - accuracy: 0.9243 - recall: 0.9242 - f1: 0.9243\n",
      "Epoch 00033: loss improved from 0.19457 to 0.18750, saving model to Erick_dropout-033-0.924460.h5\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1875 - accuracy: 0.9245 - recall: 0.9244 - f1: 0.9245\n",
      "Epoch 34/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1899 - accuracy: 0.9231 - recall: 0.9232 - f1: 0.9231\n",
      "Epoch 00034: loss did not improve from 0.18750\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1899 - accuracy: 0.9232 - recall: 0.9232 - f1: 0.9232\n",
      "Epoch 35/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1820 - accuracy: 0.9257 - recall: 0.9253 - f1: 0.9257\n",
      "Epoch 00035: loss improved from 0.18750 to 0.18193, saving model to Erick_dropout-035-0.925723.h5\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1819 - accuracy: 0.9258 - recall: 0.9254 - f1: 0.9257\n",
      "Epoch 36/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1763 - accuracy: 0.9279 - recall: 0.9278 - f1: 0.9279\n",
      "Epoch 00036: loss improved from 0.18193 to 0.17627, saving model to Erick_dropout-036-0.928013.h5\n",
      "100/100 [==============================] - 31s 312ms/step - loss: 0.1763 - accuracy: 0.9280 - recall: 0.9280 - f1: 0.9280\n",
      "Epoch 37/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1834 - accuracy: 0.9276 - recall: 0.9273 - f1: 0.9276\n",
      "Epoch 00037: loss did not improve from 0.17627\n",
      "100/100 [==============================] - 32s 316ms/step - loss: 0.1830 - accuracy: 0.9277 - recall: 0.9274 - f1: 0.9277\n",
      "Epoch 38/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1787 - accuracy: 0.9276 - recall: 0.9275 - f1: 0.9276\n",
      "Epoch 00038: loss did not improve from 0.17627\n",
      "100/100 [==============================] - 31s 312ms/step - loss: 0.1787 - accuracy: 0.9277 - recall: 0.9276 - f1: 0.9277\n",
      "Epoch 39/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1848 - accuracy: 0.9236 - recall: 0.9238 - f1: 0.9236\n",
      "Epoch 00039: loss did not improve from 0.17627\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 0.1851 - accuracy: 0.9235 - recall: 0.9237 - f1: 0.9235\n",
      "Epoch 40/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1790 - accuracy: 0.9299 - recall: 0.9298 - f1: 0.9299\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.17627\n",
      "100/100 [==============================] - 31s 312ms/step - loss: 0.1788 - accuracy: 0.9299 - recall: 0.9298 - f1: 0.9299\n",
      "Epoch 41/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1807 - accuracy: 0.9271 - recall: 0.9268 - f1: 0.9271\n",
      "Epoch 00041: loss did not improve from 0.17627\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 0.1806 - accuracy: 0.9273 - recall: 0.9269 - f1: 0.9272\n",
      "Epoch 42/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1694 - accuracy: 0.9332 - recall: 0.9329 - f1: 0.9332\n",
      "Epoch 00042: loss improved from 0.17627 to 0.16884, saving model to Erick_dropout-042-0.933394.h5\n",
      "100/100 [==============================] - 33s 333ms/step - loss: 0.1688 - accuracy: 0.9334 - recall: 0.9331 - f1: 0.9334\n",
      "Epoch 43/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1642 - accuracy: 0.9347 - recall: 0.9348 - f1: 0.9348\n",
      "Epoch 00043: loss improved from 0.16884 to 0.16472, saving model to Erick_dropout-043-0.934458.h5\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.1647 - accuracy: 0.9344 - recall: 0.9345 - f1: 0.9345\n",
      "Epoch 44/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1694 - accuracy: 0.9343 - recall: 0.9343 - f1: 0.9343\n",
      "Epoch 00044: loss did not improve from 0.16472\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1698 - accuracy: 0.9341 - recall: 0.9341 - f1: 0.9341\n",
      "Epoch 45/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1678 - accuracy: 0.9325 - recall: 0.9324 - f1: 0.9325\n",
      "Epoch 00045: loss did not improve from 0.16472\n",
      "100/100 [==============================] - 33s 327ms/step - loss: 0.1677 - accuracy: 0.9325 - recall: 0.9324 - f1: 0.9325\n",
      "Epoch 46/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1602 - accuracy: 0.9364 - recall: 0.9364 - f1: 0.9364\n",
      "Epoch 00046: loss improved from 0.16472 to 0.16000, saving model to Erick_dropout-046-0.936604.h5\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 0.1600 - accuracy: 0.9366 - recall: 0.9367 - f1: 0.9366\n",
      "Epoch 47/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9354 - recall: 0.9354 - f1: 0.9354\n",
      "Epoch 00047: loss improved from 0.16000 to 0.15670, saving model to Erick_dropout-047-0.935347.h5\n",
      "100/100 [==============================] - 32s 325ms/step - loss: 0.1567 - accuracy: 0.9354 - recall: 0.9353 - f1: 0.9353\n",
      "Epoch 48/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1644 - accuracy: 0.9345 - recall: 0.9343 - f1: 0.9345\n",
      "Epoch 00048: loss did not improve from 0.15670\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1645 - accuracy: 0.9345 - recall: 0.9343 - f1: 0.9345\n",
      "Epoch 49/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1600 - accuracy: 0.9372 - recall: 0.9372 - f1: 0.9372\n",
      "Epoch 00049: loss did not improve from 0.15670\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 0.1601 - accuracy: 0.9371 - recall: 0.9372 - f1: 0.9371\n",
      "Epoch 50/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1619 - accuracy: 0.9364 - recall: 0.9363 - f1: 0.9364\n",
      "Epoch 00050: loss did not improve from 0.15670\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1618 - accuracy: 0.9365 - recall: 0.9363 - f1: 0.9365\n",
      "Epoch 51/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1571 - accuracy: 0.9388 - recall: 0.9389 - f1: 0.9388\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "\n",
      "Epoch 00051: loss did not improve from 0.15670\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 0.1572 - accuracy: 0.9386 - recall: 0.9387 - f1: 0.9386\n",
      "Epoch 52/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1492 - accuracy: 0.9399 - recall: 0.9400 - f1: 0.9399\n",
      "Epoch 00052: loss improved from 0.15670 to 0.14925, saving model to Erick_dropout-052-0.939937.h5\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.1492 - accuracy: 0.9399 - recall: 0.9400 - f1: 0.9399\n",
      "Epoch 53/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1510 - accuracy: 0.9398 - recall: 0.9400 - f1: 0.9398\n",
      "Epoch 00053: loss did not improve from 0.14925\n",
      "100/100 [==============================] - 32s 318ms/step - loss: 0.1510 - accuracy: 0.9398 - recall: 0.9400 - f1: 0.9399\n",
      "Epoch 54/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1487 - accuracy: 0.9409 - recall: 0.9407 - f1: 0.9409\n",
      "Epoch 00054: loss improved from 0.14925 to 0.14863, saving model to Erick_dropout-054-0.941006.h5\n",
      "100/100 [==============================] - 32s 324ms/step - loss: 0.1486 - accuracy: 0.9410 - recall: 0.9408 - f1: 0.9410\n",
      "Epoch 55/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1482 - accuracy: 0.9402 - recall: 0.9402 - f1: 0.9402\n",
      "Epoch 00055: loss improved from 0.14863 to 0.14819, saving model to Erick_dropout-055-0.940301.h5\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.1482 - accuracy: 0.9403 - recall: 0.9403 - f1: 0.9403\n",
      "Epoch 56/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1506 - accuracy: 0.9400 - recall: 0.9401 - f1: 0.9400\n",
      "Epoch 00056: loss did not improve from 0.14819\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1506 - accuracy: 0.9401 - recall: 0.9402 - f1: 0.9401\n",
      "Epoch 57/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1485 - accuracy: 0.9409 - recall: 0.9410 - f1: 0.9409\n",
      "Epoch 00057: loss did not improve from 0.14819\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 0.1487 - accuracy: 0.9408 - recall: 0.9409 - f1: 0.9408\n",
      "Epoch 58/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1469 - accuracy: 0.9420 - recall: 0.9422 - f1: 0.9420\n",
      "Epoch 00058: loss improved from 0.14819 to 0.14700, saving model to Erick_dropout-058-0.941976.h5\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 0.1470 - accuracy: 0.9420 - recall: 0.9421 - f1: 0.9420\n",
      "Epoch 59/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1467 - accuracy: 0.9425 - recall: 0.9425 - f1: 0.9425\n",
      "Epoch 00059: loss improved from 0.14700 to 0.14668, saving model to Erick_dropout-059-0.942547.h5\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1467 - accuracy: 0.9426 - recall: 0.9425 - f1: 0.9425\n",
      "Epoch 60/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1450 - accuracy: 0.9427 - recall: 0.9427 - f1: 0.9427\n",
      "Epoch 00060: loss improved from 0.14668 to 0.14511, saving model to Erick_dropout-060-0.942680.h5\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 0.1451 - accuracy: 0.9427 - recall: 0.9426 - f1: 0.9427\n",
      "Epoch 61/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1546 - accuracy: 0.9388 - recall: 0.9390 - f1: 0.9388\n",
      "Epoch 00061: loss did not improve from 0.14511\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1555 - accuracy: 0.9387 - recall: 0.9388 - f1: 0.9387\n",
      "Epoch 62/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1451 - accuracy: 0.9434 - recall: 0.9435 - f1: 0.9434\n",
      "Epoch 00062: loss improved from 0.14511 to 0.14480, saving model to Erick_dropout-062-0.943472.h5\n",
      "100/100 [==============================] - 32s 325ms/step - loss: 0.1448 - accuracy: 0.9435 - recall: 0.9436 - f1: 0.9435\n",
      "Epoch 63/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1431 - accuracy: 0.9437 - recall: 0.9438 - f1: 0.9437\n",
      "Epoch 00063: loss improved from 0.14480 to 0.14327, saving model to Erick_dropout-063-0.943594.h5\n",
      "100/100 [==============================] - 34s 339ms/step - loss: 0.1433 - accuracy: 0.9436 - recall: 0.9437 - f1: 0.9436\n",
      "Epoch 64/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1382 - accuracy: 0.9453 - recall: 0.9454 - f1: 0.9453\n",
      "Epoch 00064: loss improved from 0.14327 to 0.13836, saving model to Erick_dropout-064-0.945238.h5\n",
      "100/100 [==============================] - 33s 329ms/step - loss: 0.1384 - accuracy: 0.9452 - recall: 0.9453 - f1: 0.9452\n",
      "Epoch 65/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1440 - accuracy: 0.9436 - recall: 0.9434 - f1: 0.9436\n",
      "Epoch 00065: loss did not improve from 0.13836\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 0.1439 - accuracy: 0.9435 - recall: 0.9434 - f1: 0.9435\n",
      "Epoch 66/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1446 - accuracy: 0.9423 - recall: 0.9424 - f1: 0.9423\n",
      "Epoch 00066: loss did not improve from 0.13836\n",
      "100/100 [==============================] - 36s 355ms/step - loss: 0.1445 - accuracy: 0.9422 - recall: 0.9423 - f1: 0.9422\n",
      "Epoch 67/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1451 - accuracy: 0.9424 - recall: 0.9426 - f1: 0.9424\n",
      "Epoch 00067: loss did not improve from 0.13836\n",
      "100/100 [==============================] - 32s 315ms/step - loss: 0.1448 - accuracy: 0.9425 - recall: 0.9428 - f1: 0.9425\n",
      "Epoch 68/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1364 - accuracy: 0.9472 - recall: 0.9472 - f1: 0.9472\n",
      "Epoch 00068: loss improved from 0.13836 to 0.13648, saving model to Erick_dropout-068-0.947303.h5\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1365 - accuracy: 0.9473 - recall: 0.9473 - f1: 0.9473\n",
      "Epoch 69/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1400 - accuracy: 0.9434 - recall: 0.9435 - f1: 0.9434\n",
      "Epoch 00069: loss did not improve from 0.13648\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 0.1395 - accuracy: 0.9436 - recall: 0.9437 - f1: 0.9436\n",
      "Epoch 70/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1400 - accuracy: 0.9456 - recall: 0.9456 - f1: 0.9456\n",
      "Epoch 00070: loss did not improve from 0.13648\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 0.1397 - accuracy: 0.9456 - recall: 0.9456 - f1: 0.9456\n",
      "Epoch 71/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1404 - accuracy: 0.9440 - recall: 0.9441 - f1: 0.9440\n",
      "Epoch 00071: loss did not improve from 0.13648\n",
      "100/100 [==============================] - 32s 315ms/step - loss: 0.1406 - accuracy: 0.9440 - recall: 0.9441 - f1: 0.9440\n",
      "Epoch 72/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1360 - accuracy: 0.9479 - recall: 0.9479 - f1: 0.9479\n",
      "Epoch 00072: loss improved from 0.13648 to 0.13638, saving model to Erick_dropout-072-0.947721.h5\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 0.1364 - accuracy: 0.9477 - recall: 0.9478 - f1: 0.9477\n",
      "Epoch 73/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1408 - accuracy: 0.9449 - recall: 0.9449 - f1: 0.9449\n",
      "Epoch 00073: loss did not improve from 0.13638\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1407 - accuracy: 0.9449 - recall: 0.9449 - f1: 0.9449\n",
      "Epoch 74/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1340 - accuracy: 0.9479 - recall: 0.9480 - f1: 0.9479\n",
      "Epoch 00074: loss improved from 0.13638 to 0.13440, saving model to Erick_dropout-074-0.947578.h5\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 0.1344 - accuracy: 0.9476 - recall: 0.9477 - f1: 0.9476\n",
      "Epoch 75/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1403 - accuracy: 0.9454 - recall: 0.9453 - f1: 0.9454\n",
      "Epoch 00075: loss did not improve from 0.13440\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1400 - accuracy: 0.9456 - recall: 0.9454 - f1: 0.9456\n",
      "Epoch 76/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1368 - accuracy: 0.9465 - recall: 0.9466 - f1: 0.9465\n",
      "Epoch 00076: loss did not improve from 0.13440\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.1367 - accuracy: 0.9465 - recall: 0.9466 - f1: 0.9465\n",
      "Epoch 77/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1375 - accuracy: 0.9452 - recall: 0.9453 - f1: 0.9452\n",
      "Epoch 00077: loss did not improve from 0.13440\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 0.1374 - accuracy: 0.9453 - recall: 0.9454 - f1: 0.9454\n",
      "Epoch 78/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1360 - accuracy: 0.9462 - recall: 0.9458 - f1: 0.9461\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "\n",
      "Epoch 00078: loss did not improve from 0.13440\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 0.1359 - accuracy: 0.9463 - recall: 0.9459 - f1: 0.9463\n",
      "Epoch 79/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1280 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495\n",
      "Epoch 00079: loss improved from 0.13440 to 0.12796, saving model to Erick_dropout-079-0.949531.h5\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1280 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495\n",
      "Epoch 80/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1340 - accuracy: 0.9471 - recall: 0.9471 - f1: 0.9471\n",
      "Epoch 00080: loss did not improve from 0.12796\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1340 - accuracy: 0.9472 - recall: 0.9471 - f1: 0.9472\n",
      "Epoch 81/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1310 - accuracy: 0.9472 - recall: 0.9472 - f1: 0.9472\n",
      "Epoch 00081: loss did not improve from 0.12796\n",
      "100/100 [==============================] - 33s 325ms/step - loss: 0.1308 - accuracy: 0.9472 - recall: 0.9472 - f1: 0.9472\n",
      "Epoch 82/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 0.9497 - recall: 0.9497 - f1: 0.9497\n",
      "Epoch 00082: loss improved from 0.12796 to 0.12541, saving model to Erick_dropout-082-0.949665.h5\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.1254 - accuracy: 0.9497 - recall: 0.9496 - f1: 0.9497\n",
      "Epoch 83/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9521 - recall: 0.9520 - f1: 0.9521\n",
      "Epoch 00083: loss improved from 0.12541 to 0.12361, saving model to Erick_dropout-083-0.952065.h5\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.1236 - accuracy: 0.9521 - recall: 0.9520 - f1: 0.9521\n",
      "Epoch 84/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9499 - recall: 0.9498 - f1: 0.9499\n",
      "Epoch 00084: loss did not improve from 0.12361\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.1259 - accuracy: 0.9499 - recall: 0.9499 - f1: 0.9499\n",
      "Epoch 85/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9522 - recall: 0.9521 - f1: 0.9522\n",
      "Epoch 00085: loss improved from 0.12361 to 0.12273, saving model to Erick_dropout-085-0.952093.h5\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.1227 - accuracy: 0.9521 - recall: 0.9520 - f1: 0.9521\n",
      "Epoch 86/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9524 - recall: 0.9523 - f1: 0.9524\n",
      "Epoch 00086: loss improved from 0.12273 to 0.12202, saving model to Erick_dropout-086-0.952446.h5\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.1220 - accuracy: 0.9524 - recall: 0.9524 - f1: 0.9524\n",
      "Epoch 87/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9519 - recall: 0.9519 - f1: 0.9519\n",
      "Epoch 00087: loss did not improve from 0.12202\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1245 - accuracy: 0.9517 - recall: 0.9518 - f1: 0.9518\n",
      "Epoch 88/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9529 - recall: 0.9530 - f1: 0.9529\n",
      "Epoch 00088: loss improved from 0.12202 to 0.11927, saving model to Erick_dropout-088-0.952739.h5\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.1193 - accuracy: 0.9527 - recall: 0.9528 - f1: 0.9527\n",
      "Epoch 89/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9499 - recall: 0.9501 - f1: 0.9499\n",
      "Epoch 00089: loss did not improve from 0.11927\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1242 - accuracy: 0.9499 - recall: 0.9501 - f1: 0.9499\n",
      "Epoch 90/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9507 - recall: 0.9508 - f1: 0.9507\n",
      "Epoch 00090: loss did not improve from 0.11927\n",
      "100/100 [==============================] - 36s 356ms/step - loss: 0.1233 - accuracy: 0.9506 - recall: 0.9507 - f1: 0.9506\n",
      "Epoch 91/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9528 - recall: 0.9527 - f1: 0.9528\n",
      "Epoch 00091: loss did not improve from 0.11927\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1212 - accuracy: 0.9530 - recall: 0.9529 - f1: 0.9530\n",
      "Epoch 92/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9527 - recall: 0.9529 - f1: 0.9527\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "\n",
      "Epoch 00092: loss did not improve from 0.11927\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 0.1239 - accuracy: 0.9527 - recall: 0.9528 - f1: 0.9527\n",
      "Epoch 93/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1164 - accuracy: 0.9546 - recall: 0.9545 - f1: 0.9546\n",
      "Epoch 00093: loss improved from 0.11927 to 0.11624, saving model to Erick_dropout-093-0.954666.h5\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.1162 - accuracy: 0.9547 - recall: 0.9546 - f1: 0.9547\n",
      "Epoch 94/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1164 - accuracy: 0.9553 - recall: 0.9556 - f1: 0.9553\n",
      "Epoch 00094: loss did not improve from 0.11624\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1166 - accuracy: 0.9552 - recall: 0.9555 - f1: 0.9552\n",
      "Epoch 95/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1172 - accuracy: 0.9550 - recall: 0.9551 - f1: 0.9550\n",
      "Epoch 00095: loss did not improve from 0.11624\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1172 - accuracy: 0.9550 - recall: 0.9551 - f1: 0.9550\n",
      "Epoch 96/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9534 - recall: 0.9534 - f1: 0.9534\n",
      "Epoch 00096: loss did not improve from 0.11624\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.1210 - accuracy: 0.9535 - recall: 0.9535 - f1: 0.9535\n",
      "Epoch 97/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1166 - accuracy: 0.9531 - recall: 0.9531 - f1: 0.9531\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "\n",
      "Epoch 00097: loss did not improve from 0.11624\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1163 - accuracy: 0.9531 - recall: 0.9531 - f1: 0.9531\n",
      "Epoch 98/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1141 - accuracy: 0.9544 - recall: 0.9546 - f1: 0.9545\n",
      "Epoch 00098: loss improved from 0.11624 to 0.11444, saving model to Erick_dropout-098-0.954208.h5\n",
      "100/100 [==============================] - 32s 324ms/step - loss: 0.1144 - accuracy: 0.9542 - recall: 0.9544 - f1: 0.9542\n",
      "Epoch 99/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1154 - accuracy: 0.9559 - recall: 0.9559 - f1: 0.9559\n",
      "Epoch 00099: loss did not improve from 0.11444\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.1152 - accuracy: 0.9559 - recall: 0.9559 - f1: 0.9559\n",
      "Epoch 100/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1118 - accuracy: 0.9561 - recall: 0.9562 - f1: 0.9561\n",
      "Epoch 00100: loss improved from 0.11444 to 0.11171, saving model to Erick_dropout-100-0.956174.h5\n",
      "100/100 [==============================] - 33s 332ms/step - loss: 0.1117 - accuracy: 0.9562 - recall: 0.9563 - f1: 0.9562\n",
      "Epoch 101/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1120 - accuracy: 0.9582 - recall: 0.9581 - f1: 0.9582\n",
      "Epoch 00101: loss did not improve from 0.11171\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1119 - accuracy: 0.9582 - recall: 0.9581 - f1: 0.9582\n",
      "Epoch 102/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1151 - accuracy: 0.9539 - recall: 0.9538 - f1: 0.9539\n",
      "Epoch 00102: loss did not improve from 0.11171\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1151 - accuracy: 0.9540 - recall: 0.9539 - f1: 0.9540\n",
      "Epoch 103/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1127 - accuracy: 0.9552 - recall: 0.9552 - f1: 0.9552\n",
      "Epoch 00103: loss did not improve from 0.11171\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 0.1127 - accuracy: 0.9552 - recall: 0.9553 - f1: 0.9552\n",
      "Epoch 104/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1115 - accuracy: 0.9566 - recall: 0.9566 - f1: 0.9566\n",
      "Epoch 00104: loss improved from 0.11171 to 0.11145, saving model to Erick_dropout-104-0.956665.h5\n",
      "100/100 [==============================] - 32s 324ms/step - loss: 0.1115 - accuracy: 0.9567 - recall: 0.9566 - f1: 0.9567\n",
      "Epoch 105/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1084 - accuracy: 0.9584 - recall: 0.9585 - f1: 0.9584\n",
      "Epoch 00105: loss improved from 0.11145 to 0.10896, saving model to Erick_dropout-105-0.958272.h5\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.1090 - accuracy: 0.9583 - recall: 0.9584 - f1: 0.9583\n",
      "Epoch 106/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1098 - accuracy: 0.9575 - recall: 0.9575 - f1: 0.9575\n",
      "Epoch 00106: loss did not improve from 0.10896\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1098 - accuracy: 0.9575 - recall: 0.9575 - f1: 0.9575\n",
      "Epoch 107/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1123 - accuracy: 0.9556 - recall: 0.9555 - f1: 0.9556\n",
      "Epoch 00107: loss did not improve from 0.10896\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1120 - accuracy: 0.9557 - recall: 0.9556 - f1: 0.9557\n",
      "Epoch 108/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1119 - accuracy: 0.9561 - recall: 0.9562 - f1: 0.9561\n",
      "Epoch 00108: loss did not improve from 0.10896\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1117 - accuracy: 0.9563 - recall: 0.9563 - f1: 0.9563\n",
      "Epoch 109/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1103 - accuracy: 0.9563 - recall: 0.9564 - f1: 0.9563\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "\n",
      "Epoch 00109: loss did not improve from 0.10896\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1103 - accuracy: 0.9564 - recall: 0.9565 - f1: 0.9564\n",
      "Epoch 110/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1099 - accuracy: 0.9578 - recall: 0.9579 - f1: 0.9578\n",
      "Epoch 00110: loss did not improve from 0.10896\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 0.1094 - accuracy: 0.9580 - recall: 0.9581 - f1: 0.9580\n",
      "Epoch 111/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9595 - recall: 0.9596 - f1: 0.9595\n",
      "Epoch 00111: loss improved from 0.10896 to 0.10602, saving model to Erick_dropout-111-0.959338.h5\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.1060 - accuracy: 0.9593 - recall: 0.9594 - f1: 0.9593\n",
      "Epoch 112/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1029 - accuracy: 0.9602 - recall: 0.9603 - f1: 0.9602\n",
      "Epoch 00112: loss improved from 0.10602 to 0.10310, saving model to Erick_dropout-112-0.960220.h5\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.1031 - accuracy: 0.9602 - recall: 0.9603 - f1: 0.9602\n",
      "Epoch 113/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1063 - accuracy: 0.9576 - recall: 0.9577 - f1: 0.9576\n",
      "Epoch 00113: loss did not improve from 0.10310\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1060 - accuracy: 0.9579 - recall: 0.9580 - f1: 0.9579\n",
      "Epoch 114/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1061 - accuracy: 0.9586 - recall: 0.9586 - f1: 0.9586\n",
      "Epoch 00114: loss did not improve from 0.10310\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.1065 - accuracy: 0.9585 - recall: 0.9585 - f1: 0.9585\n",
      "Epoch 115/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1026 - accuracy: 0.9594 - recall: 0.9596 - f1: 0.9594\n",
      "Epoch 00115: loss improved from 0.10310 to 0.10253, saving model to Erick_dropout-115-0.959486.h5\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1025 - accuracy: 0.9595 - recall: 0.9596 - f1: 0.9595\n",
      "Epoch 116/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1008 - accuracy: 0.9600 - recall: 0.9600 - f1: 0.9600\n",
      "Epoch 00116: loss improved from 0.10253 to 0.10054, saving model to Erick_dropout-116-0.960066.h5\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.1005 - accuracy: 0.9601 - recall: 0.9601 - f1: 0.9601\n",
      "Epoch 117/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1063 - accuracy: 0.9580 - recall: 0.9582 - f1: 0.9580\n",
      "Epoch 00117: loss did not improve from 0.10054\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1064 - accuracy: 0.9580 - recall: 0.9581 - f1: 0.9580\n",
      "Epoch 118/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1049 - accuracy: 0.9589 - recall: 0.9590 - f1: 0.9589\n",
      "Epoch 00118: loss did not improve from 0.10054\n",
      "100/100 [==============================] - 33s 325ms/step - loss: 0.1047 - accuracy: 0.9590 - recall: 0.9592 - f1: 0.9590\n",
      "Epoch 119/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1032 - accuracy: 0.9600 - recall: 0.9601 - f1: 0.9600\n",
      "Epoch 00119: loss did not improve from 0.10054\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.1028 - accuracy: 0.9602 - recall: 0.9602 - f1: 0.9602\n",
      "Epoch 120/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1009 - accuracy: 0.9610 - recall: 0.9611 - f1: 0.9610\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "\n",
      "Epoch 00120: loss did not improve from 0.10054\n",
      "100/100 [==============================] - 33s 329ms/step - loss: 0.1007 - accuracy: 0.9611 - recall: 0.9612 - f1: 0.9611\n",
      "Epoch 121/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0994 - accuracy: 0.9615 - recall: 0.9615 - f1: 0.9615\n",
      "Epoch 00121: loss improved from 0.10054 to 0.09924, saving model to Erick_dropout-121-0.961531.h5\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0992 - accuracy: 0.9615 - recall: 0.9615 - f1: 0.9615\n",
      "Epoch 122/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1024 - accuracy: 0.9604 - recall: 0.9606 - f1: 0.9604\n",
      "Epoch 00122: loss did not improve from 0.09924\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.1026 - accuracy: 0.9603 - recall: 0.9606 - f1: 0.9604\n",
      "Epoch 123/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0995 - accuracy: 0.9616 - recall: 0.9617 - f1: 0.9616\n",
      "Epoch 00123: loss did not improve from 0.09924\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0995 - accuracy: 0.9615 - recall: 0.9616 - f1: 0.9615\n",
      "Epoch 124/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0991 - accuracy: 0.9602 - recall: 0.9603 - f1: 0.9602\n",
      "Epoch 00124: loss did not improve from 0.09924\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0995 - accuracy: 0.9602 - recall: 0.9603 - f1: 0.9602\n",
      "Epoch 125/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1036 - accuracy: 0.9603 - recall: 0.9604 - f1: 0.9603\n",
      "Epoch 00125: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "\n",
      "Epoch 00125: loss did not improve from 0.09924\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.1034 - accuracy: 0.9604 - recall: 0.9605 - f1: 0.9604\n",
      "Epoch 126/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1002 - accuracy: 0.9621 - recall: 0.9621 - f1: 0.9621\n",
      "Epoch 00126: loss did not improve from 0.09924\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.1000 - accuracy: 0.9621 - recall: 0.9622 - f1: 0.9621\n",
      "Epoch 127/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1032 - accuracy: 0.9592 - recall: 0.9593 - f1: 0.9592\n",
      "Epoch 00127: loss did not improve from 0.09924\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.1032 - accuracy: 0.9593 - recall: 0.9595 - f1: 0.9593\n",
      "Epoch 128/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0974 - accuracy: 0.9612 - recall: 0.9613 - f1: 0.9612\n",
      "Epoch 00128: loss improved from 0.09924 to 0.09731, saving model to Erick_dropout-128-0.961287.h5\n",
      "100/100 [==============================] - 33s 325ms/step - loss: 0.0973 - accuracy: 0.9613 - recall: 0.9614 - f1: 0.9613\n",
      "Epoch 129/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0956 - accuracy: 0.9623 - recall: 0.9623 - f1: 0.9623\n",
      "Epoch 00129: loss improved from 0.09731 to 0.09547, saving model to Erick_dropout-129-0.962416.h5\n",
      "100/100 [==============================] - 33s 329ms/step - loss: 0.0955 - accuracy: 0.9624 - recall: 0.9624 - f1: 0.9624\n",
      "Epoch 130/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0945 - accuracy: 0.9629 - recall: 0.9630 - f1: 0.9629\n",
      "Epoch 00130: loss improved from 0.09547 to 0.09424, saving model to Erick_dropout-130-0.963070.h5\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.0942 - accuracy: 0.9631 - recall: 0.9632 - f1: 0.9631\n",
      "Epoch 131/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0958 - accuracy: 0.9621 - recall: 0.9622 - f1: 0.9621\n",
      "Epoch 00131: loss did not improve from 0.09424\n",
      "100/100 [==============================] - 34s 337ms/step - loss: 0.0959 - accuracy: 0.9620 - recall: 0.9621 - f1: 0.9620\n",
      "Epoch 132/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0989 - accuracy: 0.9615 - recall: 0.9614 - f1: 0.9615\n",
      "Epoch 00132: loss did not improve from 0.09424\n",
      "100/100 [==============================] - 34s 337ms/step - loss: 0.0993 - accuracy: 0.9613 - recall: 0.9612 - f1: 0.9613\n",
      "Epoch 133/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0938 - accuracy: 0.9647 - recall: 0.9648 - f1: 0.9647\n",
      "Epoch 00133: loss improved from 0.09424 to 0.09396, saving model to Erick_dropout-133-0.964685.h5\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.0940 - accuracy: 0.9647 - recall: 0.9647 - f1: 0.9647\n",
      "Epoch 134/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0940 - accuracy: 0.9630 - recall: 0.9631 - f1: 0.9630\n",
      "Epoch 00134: loss did not improve from 0.09396\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 0.0945 - accuracy: 0.9629 - recall: 0.9629 - f1: 0.9629\n",
      "Epoch 135/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0942 - accuracy: 0.9639 - recall: 0.9638 - f1: 0.9639\n",
      "Epoch 00135: loss did not improve from 0.09396\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0942 - accuracy: 0.9640 - recall: 0.9638 - f1: 0.9640\n",
      "Epoch 136/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0965 - accuracy: 0.9618 - recall: 0.9618 - f1: 0.9618\n",
      "Epoch 00136: loss did not improve from 0.09396\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0967 - accuracy: 0.9618 - recall: 0.9618 - f1: 0.9618\n",
      "Epoch 137/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0955 - accuracy: 0.9624 - recall: 0.9624 - f1: 0.9624\n",
      "Epoch 00137: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "\n",
      "Epoch 00137: loss did not improve from 0.09396\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.0952 - accuracy: 0.9626 - recall: 0.9625 - f1: 0.9626\n",
      "Epoch 138/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0957 - accuracy: 0.9625 - recall: 0.9624 - f1: 0.9625\n",
      "Epoch 00138: loss did not improve from 0.09396\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0956 - accuracy: 0.9627 - recall: 0.9625 - f1: 0.9626\n",
      "Epoch 139/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0927 - accuracy: 0.9641 - recall: 0.9641 - f1: 0.9641\n",
      "Epoch 00139: loss improved from 0.09396 to 0.09272, saving model to Erick_dropout-139-0.963982.h5\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0927 - accuracy: 0.9640 - recall: 0.9640 - f1: 0.9640\n",
      "Epoch 140/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0922 - accuracy: 0.9649 - recall: 0.9648 - f1: 0.9649\n",
      "Epoch 00140: loss improved from 0.09272 to 0.09246, saving model to Erick_dropout-140-0.964880.h5\n",
      "100/100 [==============================] - 33s 331ms/step - loss: 0.0925 - accuracy: 0.9649 - recall: 0.9648 - f1: 0.9649\n",
      "Epoch 141/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0956 - accuracy: 0.9632 - recall: 0.9631 - f1: 0.9632\n",
      "Epoch 00141: loss did not improve from 0.09246\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0954 - accuracy: 0.9632 - recall: 0.9632 - f1: 0.9632\n",
      "Epoch 142/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0934 - accuracy: 0.9626 - recall: 0.9627 - f1: 0.9626\n",
      "Epoch 00142: loss did not improve from 0.09246\n",
      "100/100 [==============================] - 33s 330ms/step - loss: 0.0933 - accuracy: 0.9627 - recall: 0.9628 - f1: 0.9627\n",
      "Epoch 143/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0941 - accuracy: 0.9641 - recall: 0.9642 - f1: 0.9641\n",
      "Epoch 00143: loss did not improve from 0.09246\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.0940 - accuracy: 0.9642 - recall: 0.9642 - f1: 0.9642\n",
      "Epoch 144/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0930 - accuracy: 0.9634 - recall: 0.9635 - f1: 0.9634\n",
      "Epoch 00144: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "\n",
      "Epoch 00144: loss did not improve from 0.09246\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0929 - accuracy: 0.9636 - recall: 0.9636 - f1: 0.9636\n",
      "Epoch 145/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0891 - accuracy: 0.9652 - recall: 0.9653 - f1: 0.9652\n",
      "Epoch 00145: loss improved from 0.09246 to 0.08872, saving model to Erick_dropout-145-0.965420.h5\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0887 - accuracy: 0.9654 - recall: 0.9655 - f1: 0.9654\n",
      "Epoch 146/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0922 - accuracy: 0.9636 - recall: 0.9636 - f1: 0.9636\n",
      "Epoch 00146: loss did not improve from 0.08872\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 0.0922 - accuracy: 0.9636 - recall: 0.9636 - f1: 0.9636\n",
      "Epoch 147/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0900 - accuracy: 0.9641 - recall: 0.9640 - f1: 0.9641\n",
      "Epoch 00147: loss did not improve from 0.08872\n",
      "100/100 [==============================] - 33s 325ms/step - loss: 0.0899 - accuracy: 0.9642 - recall: 0.9641 - f1: 0.9642\n",
      "Epoch 148/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0923 - accuracy: 0.9641 - recall: 0.9641 - f1: 0.9641\n",
      "Epoch 00148: loss did not improve from 0.08872\n",
      "100/100 [==============================] - 36s 363ms/step - loss: 0.0929 - accuracy: 0.9638 - recall: 0.9638 - f1: 0.9638\n",
      "Epoch 149/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0908 - accuracy: 0.9636 - recall: 0.9637 - f1: 0.9636\n",
      "Epoch 00149: ReduceLROnPlateau reducing learning rate to 8.589935605414213e-05.\n",
      "\n",
      "Epoch 00149: loss did not improve from 0.08872\n",
      "100/100 [==============================] - 42s 422ms/step - loss: 0.0908 - accuracy: 0.9637 - recall: 0.9638 - f1: 0.9637\n",
      "Epoch 150/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0916 - accuracy: 0.9652 - recall: 0.9652 - f1: 0.9652\n",
      "Epoch 00150: loss did not improve from 0.08872\n",
      "100/100 [==============================] - 40s 404ms/step - loss: 0.0916 - accuracy: 0.9652 - recall: 0.9652 - f1: 0.9652\n",
      "Epoch 151/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0904 - accuracy: 0.9646 - recall: 0.9646 - f1: 0.9646\n",
      "Epoch 00151: loss did not improve from 0.08872\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0902 - accuracy: 0.9647 - recall: 0.9647 - f1: 0.9647\n",
      "Epoch 152/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0879 - accuracy: 0.9666 - recall: 0.9667 - f1: 0.9667\n",
      "Epoch 00152: loss improved from 0.08872 to 0.08809, saving model to Erick_dropout-152-0.966700.h5\n",
      "100/100 [==============================] - 43s 435ms/step - loss: 0.0881 - accuracy: 0.9667 - recall: 0.9667 - f1: 0.9667\n",
      "Epoch 153/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0916 - accuracy: 0.9636 - recall: 0.9636 - f1: 0.9636\n",
      "Epoch 00153: loss did not improve from 0.08809\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0914 - accuracy: 0.9637 - recall: 0.9638 - f1: 0.9637\n",
      "Epoch 154/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0900 - accuracy: 0.9653 - recall: 0.9653 - f1: 0.9653\n",
      "Epoch 00154: loss did not improve from 0.08809\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0901 - accuracy: 0.9653 - recall: 0.9652 - f1: 0.9653\n",
      "Epoch 155/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0892 - accuracy: 0.9649 - recall: 0.9649 - f1: 0.9649\n",
      "Epoch 00155: loss did not improve from 0.08809\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.0895 - accuracy: 0.9649 - recall: 0.9649 - f1: 0.9649\n",
      "Epoch 156/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0891 - accuracy: 0.9647 - recall: 0.9648 - f1: 0.9647\n",
      "Epoch 00156: ReduceLROnPlateau reducing learning rate to 6.871948717162013e-05.\n",
      "\n",
      "Epoch 00156: loss did not improve from 0.08809\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0894 - accuracy: 0.9647 - recall: 0.9648 - f1: 0.9647\n",
      "Epoch 157/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0868 - accuracy: 0.9669 - recall: 0.9669 - f1: 0.9669\n",
      "Epoch 00157: loss improved from 0.08809 to 0.08703, saving model to Erick_dropout-157-0.966818.h5\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 0.0870 - accuracy: 0.9668 - recall: 0.9668 - f1: 0.9668\n",
      "Epoch 158/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0889 - accuracy: 0.9658 - recall: 0.9659 - f1: 0.9658\n",
      "Epoch 00158: loss did not improve from 0.08703\n",
      "100/100 [==============================] - 32s 324ms/step - loss: 0.0890 - accuracy: 0.9656 - recall: 0.9657 - f1: 0.9656\n",
      "Epoch 159/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0870 - accuracy: 0.9656 - recall: 0.9656 - f1: 0.9656\n",
      "Epoch 00159: loss improved from 0.08703 to 0.08663, saving model to Erick_dropout-159-0.965851.h5\n",
      "100/100 [==============================] - 33s 326ms/step - loss: 0.0866 - accuracy: 0.9658 - recall: 0.9659 - f1: 0.9659\n",
      "Epoch 160/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0867 - accuracy: 0.9662 - recall: 0.9662 - f1: 0.9662\n",
      "Epoch 00160: loss did not improve from 0.08663\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.0868 - accuracy: 0.9661 - recall: 0.9662 - f1: 0.9662\n",
      "Epoch 161/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0895 - accuracy: 0.9655 - recall: 0.9655 - f1: 0.9655\n",
      "Epoch 00161: loss did not improve from 0.08663\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0896 - accuracy: 0.9655 - recall: 0.9655 - f1: 0.9655\n",
      "Epoch 162/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0878 - accuracy: 0.9657 - recall: 0.9658 - f1: 0.9657\n",
      "Epoch 00162: loss did not improve from 0.08663\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0876 - accuracy: 0.9659 - recall: 0.9659 - f1: 0.9659\n",
      "Epoch 163/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0870 - accuracy: 0.9665 - recall: 0.9666 - f1: 0.9665\n",
      "Epoch 00163: ReduceLROnPlateau reducing learning rate to 5.497558740898967e-05.\n",
      "\n",
      "Epoch 00163: loss did not improve from 0.08663\n",
      "100/100 [==============================] - 33s 327ms/step - loss: 0.0870 - accuracy: 0.9666 - recall: 0.9666 - f1: 0.9666\n",
      "Epoch 164/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0877 - accuracy: 0.9654 - recall: 0.9655 - f1: 0.9654\n",
      "Epoch 00164: loss did not improve from 0.08663\n",
      "100/100 [==============================] - 32s 324ms/step - loss: 0.0876 - accuracy: 0.9654 - recall: 0.9655 - f1: 0.9655\n",
      "Epoch 165/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0913 - accuracy: 0.9652 - recall: 0.9652 - f1: 0.9652\n",
      "Epoch 00165: loss did not improve from 0.08663\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.0911 - accuracy: 0.9652 - recall: 0.9652 - f1: 0.9652\n",
      "Epoch 166/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0845 - accuracy: 0.9682 - recall: 0.9682 - f1: 0.9682\n",
      "Epoch 00166: loss improved from 0.08663 to 0.08456, saving model to Erick_dropout-166-0.968102.h5\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.0846 - accuracy: 0.9681 - recall: 0.9682 - f1: 0.9681\n",
      "Epoch 167/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0879 - accuracy: 0.9657 - recall: 0.9657 - f1: 0.9657\n",
      "Epoch 00167: loss did not improve from 0.08456\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0876 - accuracy: 0.9658 - recall: 0.9659 - f1: 0.9658\n",
      "Epoch 168/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0875 - accuracy: 0.9659 - recall: 0.9658 - f1: 0.9659\n",
      "Epoch 00168: loss did not improve from 0.08456\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.0874 - accuracy: 0.9658 - recall: 0.9657 - f1: 0.9658\n",
      "Epoch 169/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9666 - recall: 0.9666 - f1: 0.9666\n",
      "Epoch 00169: loss did not improve from 0.08456\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.0869 - accuracy: 0.9666 - recall: 0.9666 - f1: 0.9666\n",
      "Epoch 170/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0861 - accuracy: 0.9679 - recall: 0.9679 - f1: 0.9679\n",
      "Epoch 00170: ReduceLROnPlateau reducing learning rate to 4.398046876303852e-05.\n",
      "\n",
      "Epoch 00170: loss did not improve from 0.08456\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0859 - accuracy: 0.9679 - recall: 0.9680 - f1: 0.9680\n",
      "Epoch 171/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0883 - accuracy: 0.9661 - recall: 0.9662 - f1: 0.9661\n",
      "Epoch 00171: loss did not improve from 0.08456\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 0.0884 - accuracy: 0.9660 - recall: 0.9660 - f1: 0.9660\n",
      "Epoch 172/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0888 - accuracy: 0.9648 - recall: 0.9648 - f1: 0.9648\n",
      "Epoch 00172: loss did not improve from 0.08456\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0888 - accuracy: 0.9649 - recall: 0.9648 - f1: 0.9648\n",
      "Epoch 173/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0881 - accuracy: 0.9654 - recall: 0.9654 - f1: 0.9654\n",
      "Epoch 00173: loss did not improve from 0.08456\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.0883 - accuracy: 0.9653 - recall: 0.9653 - f1: 0.9653\n",
      "Epoch 174/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0858 - accuracy: 0.9660 - recall: 0.9659 - f1: 0.9660\n",
      "Epoch 00174: ReduceLROnPlateau reducing learning rate to 3.518437442835421e-05.\n",
      "\n",
      "Epoch 00174: loss did not improve from 0.08456\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 0.0857 - accuracy: 0.9661 - recall: 0.9660 - f1: 0.9661\n",
      "Epoch 175/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0857 - accuracy: 0.9672 - recall: 0.9672 - f1: 0.9672\n",
      "Epoch 00175: loss did not improve from 0.08456\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 0.0855 - accuracy: 0.9673 - recall: 0.9674 - f1: 0.9673\n",
      "Epoch 176/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0831 - accuracy: 0.9677 - recall: 0.9677 - f1: 0.9677\n",
      "Epoch 00176: loss improved from 0.08456 to 0.08331, saving model to Erick_dropout-176-0.967619.h5\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 0.0833 - accuracy: 0.9676 - recall: 0.9677 - f1: 0.9676\n",
      "Epoch 177/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0864 - accuracy: 0.9651 - recall: 0.9650 - f1: 0.9651\n",
      "Epoch 00177: loss did not improve from 0.08331\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0867 - accuracy: 0.9651 - recall: 0.9650 - f1: 0.9651\n",
      "Epoch 178/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0855 - accuracy: 0.9662 - recall: 0.9662 - f1: 0.9662\n",
      "Epoch 00178: loss did not improve from 0.08331\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.0853 - accuracy: 0.9663 - recall: 0.9663 - f1: 0.9663\n",
      "Epoch 179/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9665 - recall: 0.9666 - f1: 0.9665\n",
      "Epoch 00179: loss did not improve from 0.08331\n",
      "100/100 [==============================] - 34s 338ms/step - loss: 0.0850 - accuracy: 0.9664 - recall: 0.9665 - f1: 0.9664\n",
      "Epoch 180/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0829 - accuracy: 0.9679 - recall: 0.9679 - f1: 0.9679\n",
      "Epoch 00180: loss improved from 0.08331 to 0.08299, saving model to Erick_dropout-180-0.967783.h5\n",
      "100/100 [==============================] - 33s 333ms/step - loss: 0.0830 - accuracy: 0.9678 - recall: 0.9678 - f1: 0.9678\n",
      "Epoch 181/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0850 - accuracy: 0.9661 - recall: 0.9660 - f1: 0.9661\n",
      "Epoch 00181: loss did not improve from 0.08299\n",
      "100/100 [==============================] - 36s 361ms/step - loss: 0.0853 - accuracy: 0.9660 - recall: 0.9660 - f1: 0.9660\n",
      "Epoch 182/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0845 - accuracy: 0.9668 - recall: 0.9667 - f1: 0.9668\n",
      "Epoch 00182: loss did not improve from 0.08299\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0841 - accuracy: 0.9670 - recall: 0.9670 - f1: 0.9670\n",
      "Epoch 183/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0857 - accuracy: 0.9667 - recall: 0.9667 - f1: 0.9667\n",
      "Epoch 00183: loss did not improve from 0.08299\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.0859 - accuracy: 0.9666 - recall: 0.9666 - f1: 0.9666\n",
      "Epoch 184/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0853 - accuracy: 0.9657 - recall: 0.9657 - f1: 0.9657\n",
      "Epoch 00184: ReduceLROnPlateau reducing learning rate to 2.8147498960606756e-05.\n",
      "\n",
      "Epoch 00184: loss did not improve from 0.08299\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0852 - accuracy: 0.9657 - recall: 0.9658 - f1: 0.9657\n",
      "Epoch 185/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0848 - accuracy: 0.9679 - recall: 0.9679 - f1: 0.9679\n",
      "Epoch 00185: loss did not improve from 0.08299\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.0848 - accuracy: 0.9678 - recall: 0.9679 - f1: 0.9678\n",
      "Epoch 186/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0901 - accuracy: 0.9656 - recall: 0.9656 - f1: 0.9656\n",
      "Epoch 00186: loss did not improve from 0.08299\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0900 - accuracy: 0.9657 - recall: 0.9656 - f1: 0.9657\n",
      "Epoch 187/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0839 - accuracy: 0.9669 - recall: 0.9667 - f1: 0.9669\n",
      "Epoch 00187: loss did not improve from 0.08299\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.0839 - accuracy: 0.9669 - recall: 0.9667 - f1: 0.9669\n",
      "Epoch 188/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0801 - accuracy: 0.9695 - recall: 0.9694 - f1: 0.9695\n",
      "Epoch 00188: loss improved from 0.08299 to 0.08042, saving model to Erick_dropout-188-0.969283.h5\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.0804 - accuracy: 0.9693 - recall: 0.9693 - f1: 0.9693\n",
      "Epoch 189/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0863 - accuracy: 0.9671 - recall: 0.9670 - f1: 0.9671\n",
      "Epoch 00189: loss did not improve from 0.08042\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.0863 - accuracy: 0.9670 - recall: 0.9670 - f1: 0.9670\n",
      "Epoch 190/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0858 - accuracy: 0.9666 - recall: 0.9668 - f1: 0.9666\n",
      "Epoch 00190: loss did not improve from 0.08042\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0858 - accuracy: 0.9666 - recall: 0.9668 - f1: 0.9666\n",
      "Epoch 191/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0848 - accuracy: 0.9675 - recall: 0.9675 - f1: 0.9675\n",
      "Epoch 00191: loss did not improve from 0.08042\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0849 - accuracy: 0.9675 - recall: 0.9675 - f1: 0.9675\n",
      "Epoch 192/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0840 - accuracy: 0.9665 - recall: 0.9665 - f1: 0.9665\n",
      "Epoch 00192: ReduceLROnPlateau reducing learning rate to 2.25179988774471e-05.\n",
      "\n",
      "Epoch 00192: loss did not improve from 0.08042\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.0841 - accuracy: 0.9665 - recall: 0.9665 - f1: 0.9665\n",
      "Epoch 193/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0818 - accuracy: 0.9673 - recall: 0.9673 - f1: 0.9673\n",
      "Epoch 00193: loss did not improve from 0.08042\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.0817 - accuracy: 0.9674 - recall: 0.9674 - f1: 0.9674\n",
      "Epoch 194/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0860 - accuracy: 0.9653 - recall: 0.9653 - f1: 0.9653\n",
      "Epoch 00194: loss did not improve from 0.08042\n",
      "100/100 [==============================] - 33s 326ms/step - loss: 0.0860 - accuracy: 0.9653 - recall: 0.9653 - f1: 0.9653\n",
      "Epoch 195/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0839 - accuracy: 0.9664 - recall: 0.9664 - f1: 0.9664\n",
      "Epoch 00195: loss did not improve from 0.08042\n",
      "100/100 [==============================] - 32s 315ms/step - loss: 0.0837 - accuracy: 0.9665 - recall: 0.9665 - f1: 0.9665\n",
      "Epoch 196/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9690 - recall: 0.9689 - f1: 0.9690\n",
      "Epoch 00196: ReduceLROnPlateau reducing learning rate to 1.8014399392995985e-05.\n",
      "\n",
      "Epoch 00196: loss did not improve from 0.08042\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.0812 - accuracy: 0.9689 - recall: 0.9688 - f1: 0.9689\n",
      "Epoch 197/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0827 - accuracy: 0.9686 - recall: 0.9686 - f1: 0.9686\n",
      "Epoch 00197: loss did not improve from 0.08042\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.0826 - accuracy: 0.9688 - recall: 0.9687 - f1: 0.9687\n",
      "Epoch 198/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9680 - recall: 0.9680 - f1: 0.9680\n",
      "Epoch 00198: loss did not improve from 0.08042\n",
      "Restoring model weights from the end of the best epoch.\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.0808 - accuracy: 0.9681 - recall: 0.9681 - f1: 0.9681\n",
      "Epoch 00198: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(history.history[\"loss\"][1:], label=\"train_loss\")\n",
    "# plt.plot(history.history[\"val_loss\"][1:], label=\"val_loss\")\n",
    "plt.plot(history.history[\"accuracy\"][1:], label=\"train_acc\")\n",
    "# plt.plot(history.history[\"val_accuracy\"][1:], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"plots/Erick_dropout_0.2.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 64)        4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               1048704   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,603,010\n",
      "Trainable params: 2,603,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 64)        4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1048704   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,603,010\n",
      "Trainable params: 2,603,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Processing image => data/test_set_images/test_1/test_1.png\n",
      "Processing image => data/test_set_images/test_2/test_2.png\n",
      "Processing image => data/test_set_images/test_3/test_3.png\n",
      "Processing image => data/test_set_images/test_4/test_4.png\n",
      "Processing image => data/test_set_images/test_5/test_5.png\n",
      "Processing image => data/test_set_images/test_6/test_6.png\n",
      "Processing image => data/test_set_images/test_7/test_7.png\n",
      "Processing image => data/test_set_images/test_8/test_8.png\n",
      "Processing image => data/test_set_images/test_9/test_9.png\n",
      "Processing image => data/test_set_images/test_10/test_10.png\n",
      "Processing image => data/test_set_images/test_11/test_11.png\n",
      "Processing image => data/test_set_images/test_12/test_12.png\n",
      "Processing image => data/test_set_images/test_13/test_13.png\n",
      "Processing image => data/test_set_images/test_14/test_14.png\n",
      "Processing image => data/test_set_images/test_15/test_15.png\n",
      "Processing image => data/test_set_images/test_16/test_16.png\n",
      "Processing image => data/test_set_images/test_17/test_17.png\n",
      "Processing image => data/test_set_images/test_18/test_18.png\n",
      "Processing image => data/test_set_images/test_19/test_19.png\n",
      "Processing image => data/test_set_images/test_20/test_20.png\n",
      "Processing image => data/test_set_images/test_21/test_21.png\n",
      "Processing image => data/test_set_images/test_22/test_22.png\n",
      "Processing image => data/test_set_images/test_23/test_23.png\n",
      "Processing image => data/test_set_images/test_24/test_24.png\n",
      "Processing image => data/test_set_images/test_25/test_25.png\n",
      "Processing image => data/test_set_images/test_26/test_26.png\n",
      "Processing image => data/test_set_images/test_27/test_27.png\n",
      "Processing image => data/test_set_images/test_28/test_28.png\n",
      "Processing image => data/test_set_images/test_29/test_29.png\n",
      "Processing image => data/test_set_images/test_30/test_30.png\n",
      "Processing image => data/test_set_images/test_31/test_31.png\n",
      "Processing image => data/test_set_images/test_32/test_32.png\n",
      "Processing image => data/test_set_images/test_33/test_33.png\n",
      "Processing image => data/test_set_images/test_34/test_34.png\n",
      "Processing image => data/test_set_images/test_35/test_35.png\n",
      "Processing image => data/test_set_images/test_36/test_36.png\n",
      "Processing image => data/test_set_images/test_37/test_37.png\n",
      "Processing image => data/test_set_images/test_38/test_38.png\n",
      "Processing image => data/test_set_images/test_39/test_39.png\n",
      "Processing image => data/test_set_images/test_40/test_40.png\n",
      "Processing image => data/test_set_images/test_41/test_41.png\n",
      "Processing image => data/test_set_images/test_42/test_42.png\n",
      "Processing image => data/test_set_images/test_43/test_43.png\n",
      "Processing image => data/test_set_images/test_44/test_44.png\n",
      "Processing image => data/test_set_images/test_45/test_45.png\n",
      "Processing image => data/test_set_images/test_46/test_46.png\n",
      "Processing image => data/test_set_images/test_47/test_47.png\n",
      "Processing image => data/test_set_images/test_48/test_48.png\n",
      "Processing image => data/test_set_images/test_49/test_49.png\n",
      "Processing image => data/test_set_images/test_50/test_50.png\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = CNN(shape=(WINDOW_SIZE, WINDOW_SIZE, 3))\n",
    "\n",
    "# Load the model\n",
    "model.load(\"Erick_dropout-188-0.969283.h5\")\n",
    "\n",
    "model.model.summary()\n",
    "\n",
    "# We add all test images to an array, used later for generating a submission\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "    image_filenames.append(image_filename)\n",
    "\n",
    "# Set-up submission filename\n",
    "submission_filename = \"Erick_dropout-188-0.969283.csv\"\n",
    "\n",
    "# Generates the submission\n",
    "generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
