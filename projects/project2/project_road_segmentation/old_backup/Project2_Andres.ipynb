{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext lab_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from helpers import *\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.compat.v2.keras.layers import BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14422392345796079315\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4937233203\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10895966127995085386\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(8)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training images, images loaded: 80 \n",
      "Loading groundtruth images, images loaded: 80 \n"
     ]
    }
   ],
   "source": [
    "image_dir_train = \"data/training/images/\"\n",
    "files = os.listdir(image_dir_train)\n",
    "n_train = len(files)\n",
    "print(f\"Loading training images, images loaded: {n_train} \")\n",
    "imgs_train = np.asarray(\n",
    "    [load_image(image_dir_train + files[i]) for i in range(n_train)]\n",
    ")\n",
    "gt_dir_train = \"data/training/groundtruth/\"\n",
    "print(f\"Loading groundtruth images, images loaded: {n_train} \")\n",
    "gt_imgs_train = np.asarray(\n",
    "    [load_image(gt_dir_train + files[i]) for i in range(n_train)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 400, 400, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 400, 400)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_imgs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 400\n",
    "# Patches for training\n",
    "img_patches_train = [\n",
    "    crop_image(imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "gt_patches_train = [\n",
    "    crop_image(gt_imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = np.asarray(\n",
    "    [\n",
    "        img_patches_train[i][j]\n",
    "        for i in range(len(img_patches_train))\n",
    "        for j in range(len(img_patches_train[i]))\n",
    "    ]\n",
    ")\n",
    "Y_train = np.asarray(\n",
    "    [\n",
    "        gt_patches_train[i][j]\n",
    "        for i in range(len(gt_patches_train))\n",
    "        for j in range(len(gt_patches_train[i]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 400, 400, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 400, 400)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validating images, images loaded: 20 \n",
      "Loading validating groundtruth, images loaded: 20 \n"
     ]
    }
   ],
   "source": [
    "image_dir_val = \"data/validating/images/\"\n",
    "files = os.listdir(image_dir_val)\n",
    "n_val = len(files)\n",
    "print(f\"Loading validating images, images loaded: {n_val} \")\n",
    "imgs_val = np.asarray([load_image(image_dir_val + files[i]) for i in range(n_val)])\n",
    "gt_dir_val = \"data/validating/groundtruth/\"\n",
    "print(f\"Loading validating groundtruth, images loaded: {n_val} \")\n",
    "gt_imgs_val = np.asarray([load_image(gt_dir_val + files[i]) for i in range(n_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 400, 400, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 400, 400)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 400\n",
    "# Patches for validating\n",
    "img_patches_val = [\n",
    "    crop_image(imgs_val[i], image_size, image_size) for i in range(n_val)\n",
    "]\n",
    "gt_patches_val = [\n",
    "    crop_image(gt_imgs_val[i], image_size, image_size) for i in range(n_val)\n",
    "]\n",
    "\n",
    "# Separate features and labels\n",
    "X_val = np.asarray(\n",
    "    [\n",
    "        img_patches_val[i][j]\n",
    "        for i in range(len(img_patches_val))\n",
    "        for j in range(len(img_patches_val[i]))\n",
    "    ]\n",
    ")\n",
    "Y_val = np.asarray(\n",
    "    [\n",
    "        gt_patches_val[i][j]\n",
    "        for i in range(len(gt_patches_val))\n",
    "        for j in range(len(gt_patches_val[i]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 400, 400, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 400, 400)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = imag_rotation_aug(imgs_train, gt_imgs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "Y_train = np.asarray(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 456, 456, 3)\n",
      "(720, 456, 456)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "n_train = Y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = imag_rotation_aug(imgs_val, gt_imgs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.asarray(X_val)\n",
    "Y_val = np.asarray(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 456, 456, 3)\n",
      "(180, 456, 456)\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "n_val = Y_val.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create functions to calcualte precision, recall and F-1 in the training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Compute the Precision for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "        Precision (numpy.float64): the Precision of the batch \n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Compute the Recall for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "       Recall (numpy.float64): the Recal of the batch \n",
    "    \"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    \"\"\"Compute the F-1 for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "       F-1 (numpy.float64): the F-1 of the batch \n",
    "    \"\"\"\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down(input_layer, filters, pool=True):\n",
    "    \"\"\"Create convloutional and residual layers to reduce dimensions.\n",
    "    Args:\n",
    "        input_layer (layer): input layer before convolution\n",
    "        filters (numpy.int64): number of filters\n",
    "    Returns:\n",
    "        max_pool (layer): layer after max-pooling\n",
    "        residual (connection ): connection to connect with next layers\n",
    "    \"\"\"\n",
    "    batchnorm = BatchNormalization()(input_layer)\n",
    "    conv1 = Conv2D(filters, (5, 5), padding=\"same\", activation=\"relu\")(batchnorm)\n",
    "    residual = Conv2D(filters, (3, 3), padding=\"same\", activation=\"relu\")(conv1)\n",
    "    if pool:\n",
    "        max_pool = MaxPool2D()(residual)\n",
    "        return max_pool, residual\n",
    "    else:\n",
    "        return residual\n",
    "\n",
    "\n",
    "def up(input_layer, residual, filters):\n",
    "    \"\"\"Create convloutional and residual layers to increase dimensions.\n",
    "    Args:\n",
    "        input_layer (layer): input layer before convolution\\\n",
    "        residual (connection ): connection to connect with next layers\n",
    "        filters (numpy.int64): number of filters\n",
    "    Returns:\n",
    "        conv2 (layer): convolutional layer\n",
    "    \"\"\"\n",
    "    filters = int(filters)\n",
    "    batchnorm = BatchNormalization()(input_layer)\n",
    "    upsample = UpSampling2D()(batchnorm)\n",
    "    upconv = Conv2D(filters, kernel_size=(2, 2), padding=\"same\")(upsample)\n",
    "    concat = Concatenate(axis=3)([residual, upconv])\n",
    "    conv1 = Conv2D(filters, (5, 5), padding=\"same\", activation=\"relu\")(concat)\n",
    "    conv2 = Conv2D(filters, (3, 3), padding=\"same\", activation=\"relu\")(conv1)\n",
    "    return conv2\n",
    "\n",
    "\n",
    "class U_NET:\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        self.model = self.initialize_U_NET(shape)\n",
    "\n",
    "    def initialize_U_NET(self, shape):\n",
    "        \"\"\"Create Network Architecture.\n",
    "        Args:\n",
    "            shape (triplet): Size of the input layer height x width x colors (64 x 64 x 3)\n",
    "        Returns:\n",
    "            model (Neural Network): Architecture of the model\n",
    "        \"\"\"\n",
    "        # Make a custom U-nets implementation.\n",
    "        filters = 64\n",
    "        input_layer = Input(shape=shape)\n",
    "        layers = [input_layer]\n",
    "        residuals = []\n",
    "\n",
    "        # Down 1, 64\n",
    "        d1, res1 = down(input_layer, filters)\n",
    "        residuals.append(res1)\n",
    "        filters *= 2\n",
    "\n",
    "        # Down 2, 32\n",
    "        d2, res2 = down(d1, filters)\n",
    "        residuals.append(res2)\n",
    "        filters *= 2\n",
    "\n",
    "        # Down 3, 16\n",
    "        d3, res3 = down(d2, filters)\n",
    "        residuals.append(res3)\n",
    "        filters *= 2\n",
    "\n",
    "        # Down 4, 8\n",
    "        d4, res4 = down(d3, filters)\n",
    "        residuals.append(res4)\n",
    "        filters *= 2\n",
    "\n",
    "        # Up 1, 8\n",
    "        up1 = up(d4, residual=residuals[-1], filters=filters / 2)\n",
    "        filters /= 2\n",
    "\n",
    "        # Up 2,  16\n",
    "        up2 = up(up1, residual=residuals[-2], filters=filters / 2)\n",
    "        filters /= 2\n",
    "\n",
    "        # Up 3, 32\n",
    "        up3 = up(up2, residual=residuals[-3], filters=filters / 2)\n",
    "        filters /= 2\n",
    "\n",
    "        # Up 4, 64\n",
    "        up4 = up(up3, residual=residuals[-4], filters=filters / 2)\n",
    "\n",
    "        conv_1 = Conv2D(1, 1, activation=\"relu\")(up4)\n",
    "        flaten = Flatten()(conv_1)\n",
    "        batch_1 = BatchNormalization()(flaten)\n",
    "        out = Dense(\n",
    "            2,\n",
    "            activation=\"sigmoid\",\n",
    "            kernel_regularizer=l2(0.00001),\n",
    "            activity_regularizer=l2(0.00001),\n",
    "        )(batch_1)\n",
    "        #         flaten = Flatten()(up4)\n",
    "        #         batch_1 = BatchNormalization()(flaten)\n",
    "        #         dense1 = Dense(\n",
    "        #             64,\n",
    "        #             activation=\"relu\",\n",
    "        #             kernel_regularizer=l2(0.00001),\n",
    "        #             activity_regularizer=l2(0.00001),\n",
    "        #         )(flaten)\n",
    "        #         batch_2 = BatchNormalization()(dense1)\n",
    "        #         out = Dense(\n",
    "        #             2,\n",
    "        #             activation=\"sigmoid\",\n",
    "        #             kernel_regularizer=l2(0.00001),\n",
    "        #             activity_regularizer=l2(0.00001),\n",
    "        #         )(batch_2)\n",
    "\n",
    "        model = Model(input_layer, out)\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            metrics=[\"accuracy\", recall, f1],\n",
    "        )\n",
    "\n",
    "        # Print a summary of the model to see what has been generated\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the Model.\n",
    "\n",
    "        Returns:\n",
    "            History (History_Keras): History of the training\n",
    "        \"\"\"\n",
    "        # Early stopping callback after 10 steps\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_accuracy\", patience=10, verbose=1, restore_best_weights=True,\n",
    "        )\n",
    "        # Reduce learning rate on plateau after 4 steps\n",
    "        lr_callback = ReduceLROnPlateau(\n",
    "            monitor=\"val_accuracy\",\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            verbose=1,\n",
    "            mode=\"max\",\n",
    "            cooldown=1,\n",
    "        )\n",
    "        save_best = ModelCheckpoint(\n",
    "            \"Unet_batchnorm-{epoch:03d}-{f1:03f}-{val_f1:03f}.h5\",\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            verbose=1,\n",
    "        )\n",
    "        callbacks = [lr_callback, save_best, early_stopping]\n",
    "\n",
    "        # Train the model using the previously defined functions and callbacks\n",
    "        history = self.model.fit_generator(\n",
    "            create_minibatch(\n",
    "                X_train, Y_train, n_train, WINDOW_SIZE, BATCH_SIZE, PATCH_SIZE\n",
    "            ),\n",
    "            steps_per_epoch=STEPS_PER_EPOCH,\n",
    "            epochs=EPOCHS,\n",
    "            use_multiprocessing=False,\n",
    "            workers=1,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "            validation_data=create_minibatch(\n",
    "                X_val, Y_val, n_val, WINDOW_SIZE, BATCH_SIZE, PATCH_SIZE\n",
    "            ),\n",
    "            validation_steps=STEPS_PER_EPOCH / 3,\n",
    "        )\n",
    "        return history\n",
    "\n",
    "    def classify(self, X):\n",
    "        \"\"\"Classify Image as either road or not.\n",
    "        Args:\n",
    "            X (image): part of the image to classify\n",
    "        Returns:\n",
    "            Predictions : Predictions for each patch\n",
    "        \"\"\"\n",
    "        # Subdivide the images into blocks with a stride and patch_size of 16\n",
    "        img_patches = create_patches(X, 16, 16, padding=24)\n",
    "\n",
    "        # Predict\n",
    "        predictions = self.model.predict(img_patches)\n",
    "        predictions = (predictions[:, 0] < predictions[:, 1]) * 1\n",
    "\n",
    "        # Regroup patches into images\n",
    "        return predictions.reshape(X.shape[0], -1)\n",
    "\n",
    "    #         return group_patches(predictions, X.shape[0])\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"Loads Saved Model.\n",
    "        Args:\n",
    "           filename (string): name of the model\n",
    "           \n",
    "        \"\"\"\n",
    "        # Load the model (used for submission)\n",
    "        dependencies = {\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "        self.model = load_model(filename, custom_objects=dependencies)\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Saves trained model.\n",
    "        Args:\n",
    "           filename (string): name of the model\n",
    "           \n",
    "        \"\"\"\n",
    "        self.model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64, 64, 3)    12          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 64)   4864        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 64)   36928       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 32, 32, 64)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 128)  204928      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 128)  147584      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 128)  0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 128)  512         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 256)  819456      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 256)  590080      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 256)    0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 8, 8, 256)    1024        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 8, 8, 512)    3277312     batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 8, 8, 512)    2359808     conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 512)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 4, 4, 512)    2048        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 8, 8, 512)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 512)    1049088     up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 8, 8, 1024)   0           conv2d_7[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 512)    13107712    concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 8, 512)    2359808     conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 8, 8, 512)    2048        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 512)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 256)  524544      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16, 16, 512)  0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 256)  3277056     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 256)  590080      conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 256)  1024        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 128)  131200      up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 256)  0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 128)  819328      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 128)  147584      conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 64, 64, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 64, 64, 64)   32832       up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 64, 64, 128)  0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 64, 64, 64)   204864      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 64, 64)   36928       conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, 64, 1)    65          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4096)         0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 4096)         16384       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            8194        batch_normalization_8[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 29,754,063\n",
      "Trainable params: 29,742,153\n",
      "Non-trainable params: 11,910\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We define parameters of the model\n",
    "BATCH_SIZE = 150\n",
    "WINDOW_SIZE = 64\n",
    "PATCH_SIZE = 16\n",
    "EPOCHS = 300\n",
    "STEPS_PER_EPOCH = 100\n",
    "model = U_NET(shape=(WINDOW_SIZE, WINDOW_SIZE, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.5229 - accuracy: 0.7352 - recall: 0.7312 - f1: 0.7318\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.74588, saving model to Unet_batchnorm-001-0.732424-0.745883.h5\n",
      "100/100 [==============================] - 165s 2s/step - loss: 0.5223 - accuracy: 0.7357 - recall: 0.7317 - f1: 0.7324 - val_loss: 3.1219 - val_accuracy: 0.7459 - val_recall: 0.7459 - val_f1: 0.7459\n",
      "Epoch 2/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.3940 - accuracy: 0.8070 - recall: 0.8056 - f1: 0.8067\n",
      "Epoch 00002: val_accuracy did not improve from 0.74588\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.3935 - accuracy: 0.8072 - recall: 0.8057 - f1: 0.8068 - val_loss: 1.0955 - val_accuracy: 0.7390 - val_recall: 0.7390 - val_f1: 0.7390\n",
      "Epoch 3/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.3812 - accuracy: 0.8168 - recall: 0.8169 - f1: 0.8168\n",
      "Epoch 00003: val_accuracy did not improve from 0.74588\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.3803 - accuracy: 0.8174 - recall: 0.8175 - f1: 0.8174 - val_loss: 1.4642 - val_accuracy: 0.7107 - val_recall: 0.7104 - val_f1: 0.7106\n",
      "Epoch 4/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.3359 - accuracy: 0.8447 - recall: 0.8453 - f1: 0.8448\n",
      "Epoch 00004: val_accuracy did not improve from 0.74588\n",
      "100/100 [==============================] - 152s 2s/step - loss: 0.3356 - accuracy: 0.8444 - recall: 0.8449 - f1: 0.8445 - val_loss: 0.8547 - val_accuracy: 0.7254 - val_recall: 0.7237 - val_f1: 0.7249\n",
      "Epoch 5/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.3284 - accuracy: 0.8486 - recall: 0.8482 - f1: 0.8485\n",
      "Epoch 00005: val_accuracy improved from 0.74588 to 0.80657, saving model to Unet_batchnorm-005-0.848301-0.806287.h5\n",
      "100/100 [==============================] - 154s 2s/step - loss: 0.3284 - accuracy: 0.8484 - recall: 0.8480 - f1: 0.8483 - val_loss: 0.5393 - val_accuracy: 0.8066 - val_recall: 0.8051 - val_f1: 0.8063\n",
      "Epoch 6/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.3049 - accuracy: 0.8635 - recall: 0.8639 - f1: 0.8636\n",
      "Epoch 00006: val_accuracy did not improve from 0.80657\n",
      "100/100 [==============================] - 152s 2s/step - loss: 0.3045 - accuracy: 0.8638 - recall: 0.8641 - f1: 0.8638 - val_loss: 1.0956 - val_accuracy: 0.7219 - val_recall: 0.7237 - val_f1: 0.7224\n",
      "Epoch 7/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2748 - accuracy: 0.8761 - recall: 0.8768 - f1: 0.8762\n",
      "Epoch 00007: val_accuracy did not improve from 0.80657\n",
      "100/100 [==============================] - 152s 2s/step - loss: 0.2746 - accuracy: 0.8759 - recall: 0.8766 - f1: 0.8760 - val_loss: 1.2114 - val_accuracy: 0.6681 - val_recall: 0.6720 - val_f1: 0.6694\n",
      "Epoch 8/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2573 - accuracy: 0.8881 - recall: 0.8881 - f1: 0.8881\n",
      "Epoch 00008: val_accuracy improved from 0.80657 to 0.82608, saving model to Unet_batchnorm-008-0.888032-0.826286.h5\n",
      "100/100 [==============================] - 155s 2s/step - loss: 0.2577 - accuracy: 0.8880 - recall: 0.8881 - f1: 0.8880 - val_loss: 0.4651 - val_accuracy: 0.8261 - val_recall: 0.8273 - val_f1: 0.8263\n",
      "Epoch 9/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2578 - accuracy: 0.8873 - recall: 0.8867 - f1: 0.8872\n",
      "Epoch 00009: val_accuracy improved from 0.82608 to 0.86716, saving model to Unet_batchnorm-009-0.887361-0.867399.h5\n",
      "100/100 [==============================] - 154s 2s/step - loss: 0.2576 - accuracy: 0.8874 - recall: 0.8868 - f1: 0.8874 - val_loss: 0.3285 - val_accuracy: 0.8672 - val_recall: 0.8688 - val_f1: 0.8674\n",
      "Epoch 10/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2325 - accuracy: 0.8997 - recall: 0.8997 - f1: 0.8997\n",
      "Epoch 00010: val_accuracy did not improve from 0.86716\n",
      "100/100 [==============================] - 153s 2s/step - loss: 0.2320 - accuracy: 0.8999 - recall: 0.8999 - f1: 0.8999 - val_loss: 0.4797 - val_accuracy: 0.8340 - val_recall: 0.8329 - val_f1: 0.8338\n",
      "Epoch 11/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2423 - accuracy: 0.8963 - recall: 0.8966 - f1: 0.8964\n",
      "Epoch 00011: val_accuracy did not improve from 0.86716\n",
      "100/100 [==============================] - 152s 2s/step - loss: 0.2419 - accuracy: 0.8966 - recall: 0.8969 - f1: 0.8967 - val_loss: 0.4885 - val_accuracy: 0.8344 - val_recall: 0.8310 - val_f1: 0.8338\n",
      "Epoch 12/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2177 - accuracy: 0.9039 - recall: 0.9038 - f1: 0.9039\n",
      "Epoch 00012: val_accuracy improved from 0.86716 to 0.87765, saving model to Unet_batchnorm-012-0.903651-0.878145.h5\n",
      "100/100 [==============================] - 156s 2s/step - loss: 0.2188 - accuracy: 0.9037 - recall: 0.9035 - f1: 0.9037 - val_loss: 0.3304 - val_accuracy: 0.8776 - val_recall: 0.8818 - val_f1: 0.8781\n",
      "Epoch 13/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2269 - accuracy: 0.9003 - recall: 0.8999 - f1: 0.9003\n",
      "Epoch 00013: val_accuracy did not improve from 0.87765\n",
      "100/100 [==============================] - 161s 2s/step - loss: 0.2268 - accuracy: 0.9001 - recall: 0.8997 - f1: 0.9000 - val_loss: 0.6934 - val_accuracy: 0.7857 - val_recall: 0.7837 - val_f1: 0.7853\n",
      "Epoch 14/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2052 - accuracy: 0.9137 - recall: 0.9137 - f1: 0.9137\n",
      "Epoch 00014: val_accuracy did not improve from 0.87765\n",
      "100/100 [==============================] - 164s 2s/step - loss: 0.2048 - accuracy: 0.9138 - recall: 0.9137 - f1: 0.9138 - val_loss: 0.7408 - val_accuracy: 0.7541 - val_recall: 0.7555 - val_f1: 0.7545\n",
      "Epoch 15/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2131 - accuracy: 0.9079 - recall: 0.9078 - f1: 0.9079\n",
      "Epoch 00015: val_accuracy improved from 0.87765 to 0.88020, saving model to Unet_batchnorm-015-0.908291-0.880196.h5\n",
      "100/100 [==============================] - 167s 2s/step - loss: 0.2123 - accuracy: 0.9083 - recall: 0.9082 - f1: 0.9083 - val_loss: 0.3089 - val_accuracy: 0.8802 - val_recall: 0.8802 - val_f1: 0.8802\n",
      "Epoch 16/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2040 - accuracy: 0.9142 - recall: 0.9140 - f1: 0.9142\n",
      "Epoch 00016: val_accuracy did not improve from 0.88020\n",
      "100/100 [==============================] - 163s 2s/step - loss: 0.2039 - accuracy: 0.9143 - recall: 0.9141 - f1: 0.9143 - val_loss: 0.4766 - val_accuracy: 0.8243 - val_recall: 0.8243 - val_f1: 0.8243\n",
      "Epoch 17/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2018 - accuracy: 0.9154 - recall: 0.9145 - f1: 0.9153\n",
      "Epoch 00017: val_accuracy improved from 0.88020 to 0.88196, saving model to Unet_batchnorm-017-0.914942-0.880861.h5\n",
      "100/100 [==============================] - 168s 2s/step - loss: 0.2035 - accuracy: 0.9150 - recall: 0.9139 - f1: 0.9149 - val_loss: 0.3526 - val_accuracy: 0.8820 - val_recall: 0.8729 - val_f1: 0.8809\n",
      "Epoch 18/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2175 - accuracy: 0.9079 - recall: 0.9073 - f1: 0.9079\n",
      "Epoch 00018: val_accuracy did not improve from 0.88196\n",
      "100/100 [==============================] - 166s 2s/step - loss: 0.2180 - accuracy: 0.9077 - recall: 0.9070 - f1: 0.9076 - val_loss: 1.4453 - val_accuracy: 0.8416 - val_recall: 0.8380 - val_f1: 0.8410\n",
      "Epoch 19/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2513 - accuracy: 0.8962 - recall: 0.8971 - f1: 0.8963\n",
      "Epoch 00019: val_accuracy did not improve from 0.88196\n",
      "100/100 [==============================] - 178s 2s/step - loss: 0.2513 - accuracy: 0.8960 - recall: 0.8970 - f1: 0.8961 - val_loss: 2.5747 - val_accuracy: 0.5172 - val_recall: 0.5063 - val_f1: 0.5118\n",
      "Epoch 20/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2145 - accuracy: 0.9089 - recall: 0.9085 - f1: 0.9089\n",
      "Epoch 00020: val_accuracy did not improve from 0.88196\n",
      "100/100 [==============================] - 162s 2s/step - loss: 0.2150 - accuracy: 0.9087 - recall: 0.9083 - f1: 0.9086 - val_loss: 0.3988 - val_accuracy: 0.8437 - val_recall: 0.8433 - val_f1: 0.8437\n",
      "Epoch 21/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1955 - accuracy: 0.9156 - recall: 0.9155 - f1: 0.9156\n",
      "Epoch 00021: val_accuracy did not improve from 0.88196\n",
      "100/100 [==============================] - 161s 2s/step - loss: 0.1958 - accuracy: 0.9154 - recall: 0.9153 - f1: 0.9154 - val_loss: 0.3098 - val_accuracy: 0.8713 - val_recall: 0.8696 - val_f1: 0.8710\n",
      "Epoch 22/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1933 - accuracy: 0.9156 - recall: 0.9154 - f1: 0.9156\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.88196\n",
      "100/100 [==============================] - 161s 2s/step - loss: 0.1933 - accuracy: 0.9157 - recall: 0.9155 - f1: 0.9157 - val_loss: 0.3002 - val_accuracy: 0.8798 - val_recall: 0.8812 - val_f1: 0.8800\n",
      "Epoch 23/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1715 - accuracy: 0.9258 - recall: 0.9259 - f1: 0.9258\n",
      "Epoch 00023: val_accuracy improved from 0.88196 to 0.89824, saving model to Unet_batchnorm-023-0.925975-0.898252.h5\n",
      "100/100 [==============================] - 157s 2s/step - loss: 0.1711 - accuracy: 0.9260 - recall: 0.9261 - f1: 0.9260 - val_loss: 0.2561 - val_accuracy: 0.8982 - val_recall: 0.8984 - val_f1: 0.8983\n",
      "Epoch 24/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1577 - accuracy: 0.9344 - recall: 0.9345 - f1: 0.9344\n",
      "Epoch 00024: val_accuracy improved from 0.89824 to 0.90706, saving model to Unet_batchnorm-024-0.934442-0.907089.h5\n",
      "100/100 [==============================] - 156s 2s/step - loss: 0.1575 - accuracy: 0.9344 - recall: 0.9345 - f1: 0.9344 - val_loss: 0.2437 - val_accuracy: 0.9071 - val_recall: 0.9073 - val_f1: 0.9071\n",
      "Epoch 25/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1561 - accuracy: 0.9363 - recall: 0.9362 - f1: 0.9363\n",
      "Epoch 00025: val_accuracy did not improve from 0.90706\n",
      "100/100 [==============================] - 158s 2s/step - loss: 0.1566 - accuracy: 0.9361 - recall: 0.9361 - f1: 0.9361 - val_loss: 0.2626 - val_accuracy: 0.8933 - val_recall: 0.8935 - val_f1: 0.8934\n",
      "Epoch 26/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1531 - accuracy: 0.9371 - recall: 0.9372 - f1: 0.9371\n",
      "Epoch 00026: val_accuracy did not improve from 0.90706\n",
      "100/100 [==============================] - 153s 2s/step - loss: 0.1527 - accuracy: 0.9374 - recall: 0.9374 - f1: 0.9374 - val_loss: 0.2468 - val_accuracy: 0.9029 - val_recall: 0.9033 - val_f1: 0.9030\n",
      "Epoch 27/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1534 - accuracy: 0.9355 - recall: 0.9354 - f1: 0.9354\n",
      "Epoch 00027: val_accuracy did not improve from 0.90706\n",
      "100/100 [==============================] - 152s 2s/step - loss: 0.1534 - accuracy: 0.9352 - recall: 0.9351 - f1: 0.9352 - val_loss: 0.2574 - val_accuracy: 0.9029 - val_recall: 0.9027 - val_f1: 0.9029\n",
      "Epoch 28/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1489 - accuracy: 0.9370 - recall: 0.9370 - f1: 0.9370\n",
      "Epoch 00028: val_accuracy did not improve from 0.90706\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1494 - accuracy: 0.9367 - recall: 0.9367 - f1: 0.9367 - val_loss: 0.2556 - val_accuracy: 0.9026 - val_recall: 0.9029 - val_f1: 0.9027\n",
      "Epoch 29/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1496 - accuracy: 0.9369 - recall: 0.9370 - f1: 0.9369\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.90706\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1495 - accuracy: 0.9369 - recall: 0.9369 - f1: 0.9369 - val_loss: 0.2335 - val_accuracy: 0.9071 - val_recall: 0.9073 - val_f1: 0.9071\n",
      "Epoch 30/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1456 - accuracy: 0.9387 - recall: 0.9388 - f1: 0.9387\n",
      "Epoch 00030: val_accuracy improved from 0.90706 to 0.90873, saving model to Unet_batchnorm-030-0.938874-0.908714.h5\n",
      "100/100 [==============================] - 154s 2s/step - loss: 0.1450 - accuracy: 0.9389 - recall: 0.9390 - f1: 0.9389 - val_loss: 0.2416 - val_accuracy: 0.9087 - val_recall: 0.9086 - val_f1: 0.9087\n",
      "Epoch 31/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1449 - accuracy: 0.9401 - recall: 0.9401 - f1: 0.9401\n",
      "Epoch 00031: val_accuracy did not improve from 0.90873\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1449 - accuracy: 0.9400 - recall: 0.9399 - f1: 0.9400 - val_loss: 0.2420 - val_accuracy: 0.9060 - val_recall: 0.9061 - val_f1: 0.9060\n",
      "Epoch 32/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1395 - accuracy: 0.9397 - recall: 0.9398 - f1: 0.9397\n",
      "Epoch 00032: val_accuracy did not improve from 0.90873\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1397 - accuracy: 0.9397 - recall: 0.9398 - f1: 0.9397 - val_loss: 0.2462 - val_accuracy: 0.9078 - val_recall: 0.9080 - val_f1: 0.9079\n",
      "Epoch 33/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1395 - accuracy: 0.9425 - recall: 0.9426 - f1: 0.9425\n",
      "Epoch 00033: val_accuracy did not improve from 0.90873\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1394 - accuracy: 0.9426 - recall: 0.9426 - f1: 0.9426 - val_loss: 0.2650 - val_accuracy: 0.8960 - val_recall: 0.8959 - val_f1: 0.8960\n",
      "Epoch 34/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1346 - accuracy: 0.9455 - recall: 0.9455 - f1: 0.9455\n",
      "Epoch 00034: val_accuracy did not improve from 0.90873\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1343 - accuracy: 0.9457 - recall: 0.9457 - f1: 0.9457 - val_loss: 0.2659 - val_accuracy: 0.9033 - val_recall: 0.9035 - val_f1: 0.9033\n",
      "Epoch 35/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1383 - accuracy: 0.9443 - recall: 0.9443 - f1: 0.9443\n",
      "Epoch 00035: val_accuracy improved from 0.90873 to 0.91186, saving model to Unet_batchnorm-035-0.944299-0.911853.h5\n",
      "100/100 [==============================] - 153s 2s/step - loss: 0.1383 - accuracy: 0.9443 - recall: 0.9443 - f1: 0.9443 - val_loss: 0.2330 - val_accuracy: 0.9119 - val_recall: 0.9118 - val_f1: 0.9119\n",
      "Epoch 36/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1334 - accuracy: 0.9445 - recall: 0.9445 - f1: 0.9445\n",
      "Epoch 00036: val_accuracy improved from 0.91186 to 0.91373, saving model to Unet_batchnorm-036-0.944867-0.913723.h5\n",
      "100/100 [==============================] - 153s 2s/step - loss: 0.1329 - accuracy: 0.9449 - recall: 0.9449 - f1: 0.9449 - val_loss: 0.2361 - val_accuracy: 0.9137 - val_recall: 0.9137 - val_f1: 0.9137\n",
      "Epoch 37/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1323 - accuracy: 0.9455 - recall: 0.9454 - f1: 0.9455\n",
      "Epoch 00037: val_accuracy did not improve from 0.91373\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1325 - accuracy: 0.9454 - recall: 0.9453 - f1: 0.9454 - val_loss: 0.2484 - val_accuracy: 0.9069 - val_recall: 0.9071 - val_f1: 0.9069\n",
      "Epoch 38/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1312 - accuracy: 0.9474 - recall: 0.9473 - f1: 0.9474\n",
      "Epoch 00038: val_accuracy did not improve from 0.91373\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1310 - accuracy: 0.9476 - recall: 0.9475 - f1: 0.9476 - val_loss: 0.2504 - val_accuracy: 0.9024 - val_recall: 0.9024 - val_f1: 0.9024\n",
      "Epoch 39/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1302 - accuracy: 0.9457 - recall: 0.9457 - f1: 0.9457\n",
      "Epoch 00039: val_accuracy did not improve from 0.91373\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1296 - accuracy: 0.9460 - recall: 0.9460 - f1: 0.9460 - val_loss: 0.2684 - val_accuracy: 0.8946 - val_recall: 0.8945 - val_f1: 0.8946\n",
      "Epoch 40/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1313 - accuracy: 0.9459 - recall: 0.9459 - f1: 0.9459\n",
      "Epoch 00040: val_accuracy did not improve from 0.91373\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1307 - accuracy: 0.9462 - recall: 0.9462 - f1: 0.9462 - val_loss: 0.2468 - val_accuracy: 0.9016 - val_recall: 0.9018 - val_f1: 0.9016\n",
      "Epoch 41/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1256 - accuracy: 0.9478 - recall: 0.9479 - f1: 0.9478\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.91373\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1258 - accuracy: 0.9478 - recall: 0.9479 - f1: 0.9478 - val_loss: 0.2577 - val_accuracy: 0.9036 - val_recall: 0.9037 - val_f1: 0.9036\n",
      "Epoch 42/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1409 - accuracy: 0.9425 - recall: 0.9426 - f1: 0.9425\n",
      "Epoch 00042: val_accuracy did not improve from 0.91373\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1404 - accuracy: 0.9427 - recall: 0.9428 - f1: 0.9427 - val_loss: 0.2573 - val_accuracy: 0.9005 - val_recall: 0.9004 - val_f1: 0.9005\n",
      "Epoch 43/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1313 - accuracy: 0.9465 - recall: 0.9465 - f1: 0.9465\n",
      "Epoch 00043: val_accuracy did not improve from 0.91373\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1312 - accuracy: 0.9466 - recall: 0.9466 - f1: 0.9466 - val_loss: 0.2598 - val_accuracy: 0.9053 - val_recall: 0.9055 - val_f1: 0.9053\n",
      "Epoch 44/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1263 - accuracy: 0.9488 - recall: 0.9488 - f1: 0.9488\n",
      "Epoch 00044: val_accuracy did not improve from 0.91373\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1264 - accuracy: 0.9487 - recall: 0.9487 - f1: 0.9487 - val_loss: 0.2542 - val_accuracy: 0.9051 - val_recall: 0.9051 - val_f1: 0.9051\n",
      "Epoch 45/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1281 - accuracy: 0.9457 - recall: 0.9456 - f1: 0.9457\n",
      "Epoch 00045: val_accuracy did not improve from 0.91373\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1275 - accuracy: 0.9461 - recall: 0.9460 - f1: 0.9461 - val_loss: 0.2826 - val_accuracy: 0.8941 - val_recall: 0.8941 - val_f1: 0.8941\n",
      "Epoch 46/300\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1304 - accuracy: 0.9454 - recall: 0.9453 - f1: 0.9454\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.91373\n",
      "Restoring model weights from the end of the best epoch.\n",
      "100/100 [==============================] - 151s 2s/step - loss: 0.1304 - accuracy: 0.9454 - recall: 0.9453 - f1: 0.9454 - val_loss: 0.2428 - val_accuracy: 0.9049 - val_recall: 0.9049 - val_f1: 0.9049\n",
      "Epoch 00046: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(history.history[\"loss\"][1:], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"][1:], label=\"val_loss\")\n",
    "plt.plot(history.history[\"accuracy\"][1:], label=\"train_acc\")\n",
    "plt.plot(history.history[\"val_accuracy\"][1:], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"Unet_batchnorm.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 3)    12          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 64, 64, 64)   4864        batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 64, 64, 64)   36928       conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 64)   256         max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 128)  204928      batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 128)  147584      conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 128)  0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 256)  819456      batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 256)  590080      conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 8, 8, 256)    0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 8, 8, 256)    1024        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 512)    3277312     batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 512)    2359808     conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 4, 4, 512)    0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 4, 4, 512)    2048        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 8, 8, 512)    0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 512)    1049088     up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 8, 8, 1024)   0           conv2d_28[0][0]                  \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 512)    13107712    concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 512)    2359808     conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 512)    2048        conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 16, 16, 512)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 16, 16, 256)  524544      up_sampling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 16, 16, 512)  0           conv2d_26[0][0]                  \n",
      "                                                                 conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 16, 16, 256)  3277056     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 256)  590080      conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 256)  1024        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 32, 32, 256)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 32, 32, 128)  131200      up_sampling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 256)  0           conv2d_24[0][0]                  \n",
      "                                                                 conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 32, 128)  819328      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 32, 128)  147584      conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 128)  512         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 64, 64, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 64, 64, 64)   32832       up_sampling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64, 64, 128)  0           conv2d_22[0][0]                  \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 64, 64, 64)   204864      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 64, 64, 64)   36928       conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 64, 64, 1)    65          conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4096)         0           conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 4096)         16384       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            8194        batch_normalization_17[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 29,754,063\n",
      "Trainable params: 29,742,153\n",
      "Non-trainable params: 11,910\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64, 64, 3)    12          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 64)   4864        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 64)   36928       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 32, 32, 64)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 128)  204928      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 128)  147584      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 128)  0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 128)  512         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 256)  819456      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 256)  590080      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 256)    0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 8, 8, 256)    1024        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 8, 8, 512)    3277312     batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 8, 8, 512)    2359808     conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 512)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 4, 4, 512)    2048        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 8, 8, 512)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 512)    1049088     up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 8, 8, 1024)   0           conv2d_7[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 512)    13107712    concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 8, 512)    2359808     conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 8, 8, 512)    2048        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 512)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 256)  524544      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16, 16, 512)  0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 256)  3277056     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 256)  590080      conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 256)  1024        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 128)  131200      up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 256)  0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 128)  819328      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 128)  147584      conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 64, 64, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 64, 64, 64)   32832       up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 64, 64, 128)  0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 64, 64, 64)   204864      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 64, 64)   36928       conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, 64, 1)    65          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4096)         0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 4096)         16384       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            8194        batch_normalization_8[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 29,754,063\n",
      "Trainable params: 29,742,153\n",
      "Non-trainable params: 11,910\n",
      "__________________________________________________________________________________________________\n",
      "Processing image => data/test_set_images/test_1/test_1.png\n",
      "Processing image => data/test_set_images/test_2/test_2.png\n",
      "Processing image => data/test_set_images/test_3/test_3.png\n",
      "Processing image => data/test_set_images/test_4/test_4.png\n",
      "Processing image => data/test_set_images/test_5/test_5.png\n",
      "Processing image => data/test_set_images/test_6/test_6.png\n",
      "Processing image => data/test_set_images/test_7/test_7.png\n",
      "Processing image => data/test_set_images/test_8/test_8.png\n",
      "Processing image => data/test_set_images/test_9/test_9.png\n",
      "Processing image => data/test_set_images/test_10/test_10.png\n",
      "Processing image => data/test_set_images/test_11/test_11.png\n",
      "Processing image => data/test_set_images/test_12/test_12.png\n",
      "Processing image => data/test_set_images/test_13/test_13.png\n",
      "Processing image => data/test_set_images/test_14/test_14.png\n",
      "Processing image => data/test_set_images/test_15/test_15.png\n",
      "Processing image => data/test_set_images/test_16/test_16.png\n",
      "Processing image => data/test_set_images/test_17/test_17.png\n",
      "Processing image => data/test_set_images/test_18/test_18.png\n",
      "Processing image => data/test_set_images/test_19/test_19.png\n",
      "Processing image => data/test_set_images/test_20/test_20.png\n",
      "Processing image => data/test_set_images/test_21/test_21.png\n",
      "Processing image => data/test_set_images/test_22/test_22.png\n",
      "Processing image => data/test_set_images/test_23/test_23.png\n",
      "Processing image => data/test_set_images/test_24/test_24.png\n",
      "Processing image => data/test_set_images/test_25/test_25.png\n",
      "Processing image => data/test_set_images/test_26/test_26.png\n",
      "Processing image => data/test_set_images/test_27/test_27.png\n",
      "Processing image => data/test_set_images/test_28/test_28.png\n",
      "Processing image => data/test_set_images/test_29/test_29.png\n",
      "Processing image => data/test_set_images/test_30/test_30.png\n",
      "Processing image => data/test_set_images/test_31/test_31.png\n",
      "Processing image => data/test_set_images/test_32/test_32.png\n",
      "Processing image => data/test_set_images/test_33/test_33.png\n",
      "Processing image => data/test_set_images/test_34/test_34.png\n",
      "Processing image => data/test_set_images/test_35/test_35.png\n",
      "Processing image => data/test_set_images/test_36/test_36.png\n",
      "Processing image => data/test_set_images/test_37/test_37.png\n",
      "Processing image => data/test_set_images/test_38/test_38.png\n",
      "Processing image => data/test_set_images/test_39/test_39.png\n",
      "Processing image => data/test_set_images/test_40/test_40.png\n",
      "Processing image => data/test_set_images/test_41/test_41.png\n",
      "Processing image => data/test_set_images/test_42/test_42.png\n",
      "Processing image => data/test_set_images/test_43/test_43.png\n",
      "Processing image => data/test_set_images/test_44/test_44.png\n",
      "Processing image => data/test_set_images/test_45/test_45.png\n",
      "Processing image => data/test_set_images/test_46/test_46.png\n",
      "Processing image => data/test_set_images/test_47/test_47.png\n",
      "Processing image => data/test_set_images/test_48/test_48.png\n",
      "Processing image => data/test_set_images/test_49/test_49.png\n",
      "Processing image => data/test_set_images/test_50/test_50.png\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = U_NET(shape=(WINDOW_SIZE, WINDOW_SIZE, 3))\n",
    "\n",
    "# Load the model\n",
    "model.load(\"Unet_batchnorm-036-0.944867-0.913723.h5\")\n",
    "\n",
    "model.model.summary()\n",
    "\n",
    "# We add all test images to an array, used later for generating a submission\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "    image_filenames.append(image_filename)\n",
    "\n",
    "# Set-up submission filename\n",
    "submission_filename = \"Unet_batchnorm-036-0.944867-0.913723.csv\"\n",
    "\n",
    "# Generates the submission\n",
    "generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a set of images\n",
    "# root_dir = \"data/training/\"\n",
    "\n",
    "# # Select the directory for the images and load them\n",
    "# image_dir = root_dir + \"images/\"\n",
    "# files = os.listdir(image_dir)\n",
    "# n = len(files)\n",
    "\n",
    "# print(\"Loading \" + str(n) + \" images\")\n",
    "# imgs = np.asarray([load_image(image_dir + files[i]) for i in range(n)])\n",
    "\n",
    "# # Select the directory for groundtruth images and load them\n",
    "# gt_dir = root_dir + \"groundtruth/\"\n",
    "# print(\"Loading \" + str(n) + \" groundtruth images\")\n",
    "# gt_imgs = np.asarray([load_image(gt_dir + files[i]) for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_size = 400\n",
    "\n",
    "# # We separate the images from the groundtruth images\n",
    "# img_patches = [img_crop(imgs[i], image_size, image_size) for i in range(n)]\n",
    "# gt_patches = [img_crop(gt_imgs[i], image_size, image_size) for i in range(n)]\n",
    "\n",
    "# # Linearize the list and labeling them X and Y\n",
    "# X = np.asarray(\n",
    "#     [\n",
    "#         img_patches[i][j]\n",
    "#         for i in range(len(img_patches))\n",
    "#         for j in range(len(img_patches[i]))\n",
    "#     ]\n",
    "# )\n",
    "# Y = np.asarray(\n",
    "#     [\n",
    "#         gt_patches[i][j]\n",
    "#         for i in range(len(gt_patches))\n",
    "#         for j in range(len(gt_patches[i]))\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_minibatch():\n",
    "\n",
    "#     # Fix the seed\n",
    "#     np.random.seed(1)\n",
    "\n",
    "#     # We define the window size of 72, batch size of 100 (empirically chosen)\n",
    "#     # and patch size should correspond to 16\n",
    "#     w_size = 72\n",
    "#     batch_size = 100\n",
    "#     patch_size = 16\n",
    "#     num_images = 100\n",
    "\n",
    "#     while True:\n",
    "#         # Generate one minibatch\n",
    "#         batch_image = np.empty((batch_size, w_size, w_size, 3))\n",
    "#         batch_label = np.empty((batch_size, 2))\n",
    "\n",
    "#         for i in range(batch_size):\n",
    "\n",
    "#             # Select a random index represnting an image\n",
    "#             random_index = np.random.choice(num_images)\n",
    "\n",
    "#             # Width of original image\n",
    "#             width = 400\n",
    "\n",
    "#             # Sample a random window from the image\n",
    "#             random_sample = np.random.randint(w_size // 2, width - w_size // 2, 2)\n",
    "\n",
    "#             # Create a sub image of size 72x72\n",
    "#             sampled_image = X[random_index][\n",
    "#                 random_sample[0] - w_size // 2 : random_sample[0] + w_size // 2,\n",
    "#                 random_sample[1] - w_size // 2 : random_sample[1] + w_size // 2,\n",
    "#             ]\n",
    "\n",
    "#             # Take its corresponding ground-truth image\n",
    "#             correspond_ground_truth = Y[random_index][\n",
    "#                 random_sample[0] - patch_size // 2 : random_sample[0] + patch_size // 2,\n",
    "#                 random_sample[1] - patch_size // 2 : random_sample[1] + patch_size // 2,\n",
    "#             ]\n",
    "\n",
    "#             # We set in the label depending on the threshold of 0.2\n",
    "#             # The label becomes either 0 or 1 by applying to_categorical with parameter 2\n",
    "#             label = to_categorical(\n",
    "#                 (np.array([np.mean(correspond_ground_truth)]) > 0.2) * 1, 2\n",
    "#             )\n",
    "\n",
    "#             # The image augmentation is based on both flipping and rotating (randomly in steps of 45°)\n",
    "#             # Random vertical and horizontal flip\n",
    "#             if np.random.choice(2) == 1:\n",
    "#                 sampled_image = np.flipud(sampled_image)\n",
    "\n",
    "#             if np.random.choice(2) == 1:\n",
    "#                 sampled_image = np.fliplr(sampled_image)\n",
    "\n",
    "#             # Random rotation in steps of 45°\n",
    "#             rotations = [0, 45, 90, 135, 180, 225, 270, 315, 350]\n",
    "\n",
    "#             # We select a rotation degree randomly\n",
    "#             rotation_choice = np.random.choice(len(rotations))\n",
    "\n",
    "#             # Rotate it using the random value (uses the scipy library)\n",
    "#             sampled_image = scipy.ndimage.rotate(\n",
    "#                 sampled_image,\n",
    "#                 rotations[rotation_choice],\n",
    "#                 order=1,\n",
    "#                 reshape=False,\n",
    "#                 mode=\"reflect\",\n",
    "#             )\n",
    "\n",
    "#             # We put in the sub image and its corresponding label before yielding it\n",
    "#             batch_image[i] = sampled_image\n",
    "#             batch_label[i] = label\n",
    "\n",
    "#         # Yield the mini_batch to the model\n",
    "#         yield (batch_image, batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def down(input_layer, filters, pool=True):\n",
    "#     conv1 = Conv2D(filters, (3, 3), padding=\"same\", activation=\"relu\")(input_layer)\n",
    "#     residual = Conv2D(filters, (3, 3), padding=\"same\", activation=\"relu\")(conv1)\n",
    "#     if pool:\n",
    "#         max_pool = MaxPool2D()(residual)\n",
    "#         return max_pool, residual\n",
    "#     else:\n",
    "#         return residual\n",
    "\n",
    "\n",
    "# def up(input_layer, residual, filters):\n",
    "#     filters = int(filters)\n",
    "#     upsample = UpSampling2D()(input_layer)\n",
    "#     upconv = Conv2D(filters, kernel_size=(2, 2), padding=\"same\")(upsample)\n",
    "#     concat = Concatenate(axis=3)([residual, upconv])\n",
    "#     conv1 = Conv2D(filters, (3, 3), padding=\"same\", activation=\"relu\")(concat)\n",
    "#     conv2 = Conv2D(filters, (3, 3), padding=\"same\", activation=\"relu\")(conv1)\n",
    "#     return conv2\n",
    "\n",
    "\n",
    "# class cnn_model:\n",
    "\n",
    "#     # Initialize the class\n",
    "#     def __init__(self, shape, batch_normalization, activation):\n",
    "#         self.shape = shape\n",
    "#         self.batch_normalization = batch_normalization\n",
    "#         self.activation = activation\n",
    "#         self.model = self.initialize_cnn_model(shape, batch_normalization, activation)\n",
    "\n",
    "#     def initialize_cnn_model(self, shape, batch_normalization, activation):\n",
    "#         # Make a custom U-nets implementation.\n",
    "#         filters = 64\n",
    "#         input_layer = Input(shape=[128, 128, 3])\n",
    "#         layers = [input_layer]\n",
    "#         residuals = []\n",
    "\n",
    "#         # Down 1, 128\n",
    "#         d1, res1 = down(input_layer, filters)\n",
    "#         residuals.append(res1)\n",
    "#         filters *= 2\n",
    "\n",
    "#         # Down 2, 64\n",
    "#         d2, res2 = down(d1, filters)\n",
    "#         residuals.append(res2)\n",
    "#         filters *= 2\n",
    "\n",
    "#         # Down 3, 32\n",
    "#         d3, res3 = down(d2, filters)\n",
    "#         residuals.append(res3)\n",
    "#         filters *= 2\n",
    "\n",
    "#         # Down 4, 16\n",
    "#         d4, res4 = down(d3, filters)\n",
    "#         residuals.append(res4)\n",
    "#         filters *= 2\n",
    "\n",
    "#         # Down 5, 8\n",
    "#         d5 = down(d4, filters, pool=False)\n",
    "\n",
    "#         # Up 1, 16\n",
    "#         up1 = up(d5, residual=residuals[-1], filters=filters / 2)\n",
    "#         filters /= 2\n",
    "\n",
    "#         # Up 2,  32\n",
    "#         up2 = up(up1, residual=residuals[-2], filters=filters / 2)\n",
    "#         filters /= 2\n",
    "\n",
    "#         # Up 3, 64\n",
    "#         up3 = up(up2, residual=residuals[-3], filters=filters / 2)\n",
    "#         filters /= 2\n",
    "\n",
    "#         # Up 4, 128\n",
    "#         up4 = up(up3, residual=residuals[-4], filters=filters / 2)\n",
    "#         out = Conv2D(filters=1, kernel_size=(1, 1), activation=\"sigmoid\")(up4)\n",
    "\n",
    "#         model = Model(input_layer, out)\n",
    "#         model.compile(\n",
    "#             loss=\"binary_crossentropy\",\n",
    "#             optimizer=Adam(lr=0.001),\n",
    "#             metrics=[\"accuracy\", recall, f1],\n",
    "#         )\n",
    "\n",
    "#         # Print a summary of the model to see what has been generated\n",
    "#         model.summary()\n",
    "\n",
    "#         return model\n",
    "\n",
    "#     def train(self):\n",
    "\n",
    "#         # Early stopping callback after 10 steps\n",
    "#         early_stopping = EarlyStopping(\n",
    "#             monitor=\"f1\", min_delta=0.5, patience=20, verbose=0, mode=\"max\"\n",
    "#         )\n",
    "\n",
    "#         # Reduce learning rate on plateau after 4 steps\n",
    "#         lr_callback = ReduceLROnPlateau(\n",
    "#             monitor=\"f1\", factor=0.5, patience=4, verbose=0, mode=\"max\"\n",
    "#         )\n",
    "\n",
    "#         # Place the callbacks in a list to be used when training\n",
    "#         #         callbacks = [cb, early_stopping, lr_callback]\n",
    "#         save_best = ModelCheckpoint(\n",
    "#             \"batch_relu_OLD_Ali_amsgrad_noise-{epoch:03d}-{f1:03f}.h5\",\n",
    "#             save_best_only=True,\n",
    "#             monitor=\"f1\",\n",
    "#             mode=\"max\",\n",
    "#             verbose=1,\n",
    "#         )\n",
    "\n",
    "#         callbacks = [lr_callback, save_best]\n",
    "\n",
    "#         # Train the model using the previously defined functions and callbacks\n",
    "#         history = self.model.fit_generator(\n",
    "#             create_minibatch(),\n",
    "#             steps_per_epoch=STEPS_PER_EPOCH,\n",
    "#             epochs=EPOCHS,\n",
    "#             use_multiprocessing=False,\n",
    "#             workers=1,\n",
    "#             callbacks=callbacks,\n",
    "#             verbose=1,\n",
    "#         )\n",
    "\n",
    "#         return history\n",
    "\n",
    "#     def classify(self, X):\n",
    "#         # Subdivide the images into blocks with a stride and patch_size of 16\n",
    "#         img_patches = create_patches(X, 16, 16, padding=28)\n",
    "\n",
    "#         # Predict\n",
    "#         predictions = self.model.predict(img_patches)\n",
    "#         predictions = (predictions[:, 0] < predictions[:, 1]) * 1\n",
    "\n",
    "#         # Regroup patches into images\n",
    "#         return group_patches(predictions, X.shape[0])\n",
    "\n",
    "#     def load(self, filename):\n",
    "#         # Load the model (used for submission)\n",
    "#         dependencies = {\n",
    "#             \"recall\": recall,\n",
    "#             \"f1\": f1,\n",
    "#         }\n",
    "#         self.model = load_model(filename, custom_objects=dependencies)\n",
    "\n",
    "#     def save(self, filename):\n",
    "#         # Save the model (used to then load to submit)\n",
    "#         self.model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "# EPOCHS = 120\n",
    "# STEPS_PER_EPOCH = 150\n",
    "# batch_normalization = True\n",
    "# activation = \"relu\"\n",
    "# model = cnn_model(\n",
    "#     shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    "# )\n",
    "# # Train the model with batch\n",
    "# history = model.train()\n",
    "# model.save(\"batch_relu_OLD_Ali_amsgrad_noise.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the training loss and accuracy\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "# # plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "# plt.plot(history.history[\"accuracy\"], label=\"train_acc\")\n",
    "# # plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "# plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Loss/Accuracy\")\n",
    "# plt.legend(loc=\"lower left\")\n",
    "# plt.savefig(\"batch_relu_OLD_Ali_amsgrad_noise.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helpers import *\n",
    "\n",
    "# # from cnn_model import cnn_model\n",
    "\n",
    "# # Instantiate the model\n",
    "# batch_normalization = True\n",
    "# activation = \"relu\"\n",
    "# model = cnn_model(\n",
    "#     shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    "# )\n",
    "\n",
    "# # Load the model\n",
    "# model.load(\"batch_relu_OLD_Ali_amsgrad_noise-092-0.952039.h5\")\n",
    "\n",
    "# # Print a summary to make sure the correct model is used\n",
    "# model.model.summary()\n",
    "\n",
    "# # We add all test images to an array, used later for generating a submission\n",
    "# image_filenames = []\n",
    "# for i in range(1, 51):\n",
    "#     image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "#     image_filenames.append(image_filename)\n",
    "\n",
    "# # Set-up submission filename\n",
    "# submission_filename = \"batch_relu_OLD_Ali_amsgrad_noise-092-0.952039.csv\"\n",
    "\n",
    "# # Generates the submission\n",
    "# generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "# # We define the number of epochs and steps per epochs\n",
    "# EPOCHS = 160\n",
    "# STEPS_PER_EPOCH = 150\n",
    "# batch_normalization = True\n",
    "# activation = \"LeakyReLU\"\n",
    "# model = cnn_model(\n",
    "#     shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    "# )\n",
    "# # Train the model\n",
    "# history = model.train()\n",
    "# # model.save(\"no_batch_LeakyRelu_validation_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the training loss and accuracy\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "# plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "# plt.plot(history.history[\"accuracy\"], label=\"train_acc\")\n",
    "# plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "# plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Loss/Accuracy\")\n",
    "# plt.legend(loc=\"lower left\")\n",
    "# plt.savefig(\"batch_LeakyReLU_validation_160_dropout-0.2.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helpers import *\n",
    "\n",
    "# # from cnn_model import cnn_model\n",
    "\n",
    "# # Instantiate the model\n",
    "# batch_normalization = True\n",
    "# activation = \"LeakyReLU\"\n",
    "# model = cnn_model(\n",
    "#     shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    "# )\n",
    "\n",
    "# # Load the model\n",
    "# model.load(\n",
    "#     \"saved_models/batch_LeakyReLU_validation_160_dropout-0.2-070-0.948567-0.928000.h5\"\n",
    "# )\n",
    "\n",
    "# # Print a summary to make sure the correct model is used\n",
    "# model.model.summary()\n",
    "\n",
    "# # We add all test images to an array, used later for generating a submission\n",
    "# image_filenames = []\n",
    "# for i in range(1, 51):\n",
    "#     image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "#     image_filenames.append(image_filename)\n",
    "\n",
    "# # Set-up submission filename\n",
    "# submission_filename = \"to_submit_csv/batch_LeakyReLU_validation_160_dropout-0.2-070-0.948567-0.928000.csv\"\n",
    "\n",
    "# # Generates the submission\n",
    "# generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # history.history[\"loss\"]\n",
    "# print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history.history[\"accuracy\"])\n",
    "# plt.plot(history.history[\"val_accuracy\"])\n",
    "# plt.title(\"model accuracy\")\n",
    "# plt.ylabel(\"accuracy\")\n",
    "# plt.xlabel(\"epoch\")\n",
    "# plt.legend([\"train\", \"val\"], loc=\"upper left\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the training loss and accuracy\n",
    "# N = EPOCHS\n",
    "# print(N)\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure()\n",
    "# plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "# plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Loss/Accuracy\")\n",
    "# plt.legend(loc=\"lower left\")\n",
    "# plt.show()\n",
    "# plt.savefig(\"model1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "# batch_normalization = False\n",
    "# EPOCHS = 200\n",
    "\n",
    "# activation = \"relu\"\n",
    "# model = cnn_model(\n",
    "#     shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    "# )\n",
    "# # Train the model with batch\n",
    "# model.train()\n",
    "# model.save(\"no_batch_relu_validation.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "# batch_normalization = True\n",
    "# EPOCHS = 200\n",
    "\n",
    "# activation = \"LeakyReLU\"\n",
    "# model = cnn_model(\n",
    "#     shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    "# )\n",
    "# # Train the model with batch\n",
    "# model.train()\n",
    "# model.save(\"batch_LeakyReLU_validation.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "# batch_normalization = True\n",
    "# EPOCHS = 200\n",
    "# STEPS_PER_EPOCH = 150\n",
    "# activation = \"relu\"\n",
    "# model = cnn_model(\n",
    "#     shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    "# )\n",
    "# # Train the model with batch\n",
    "# history = model.train()\n",
    "# # model.save(\"batch_relu_validation_200.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the training loss and accuracy\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "# plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "# plt.plot(history.history[\"accuracy\"], label=\"train_acc\")\n",
    "# plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "# plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Loss/Accuracy\")\n",
    "# plt.legend(loc=\"lower left\")\n",
    "# plt.savefig(\"batch_relu_validation_200.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helpers import *\n",
    "\n",
    "# # from cnn_model import cnn_model\n",
    "\n",
    "# # Instantiate the model\n",
    "# batch_normalization = True\n",
    "# activation = \"relu\"\n",
    "# model = cnn_model(\n",
    "#     shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    "# )\n",
    "\n",
    "# # Load the model\n",
    "# model.load(\"batch_relu_validation_200-145-0.951267-0.930033.h5\")\n",
    "\n",
    "# # Print a summary to make sure the correct model is used\n",
    "# model.model.summary()\n",
    "\n",
    "# # We add all test images to an array, used later for generating a submission\n",
    "# image_filenames = []\n",
    "# for i in range(1, 51):\n",
    "#     image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "#     image_filenames.append(image_filename)\n",
    "\n",
    "# # Set-up submission filename\n",
    "# submission_filename = \"batch_relu_validation_200-145-0.951267-0.930033.csv\"\n",
    "\n",
    "# # Generates the submission\n",
    "# generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "# batch_normalization = True\n",
    "# EPOCHS = 160\n",
    "# STEPS_PER_EPOCH = 150\n",
    "# activation = \"LeakyReLU\"\n",
    "# model = cnn_model(\n",
    "#     shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    "# )\n",
    "# # Train the model with batch\n",
    "# history = model.train()\n",
    "# # model.save(\"batch_relu_validation_200.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the training loss and accuracy\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "# plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "# plt.plot(history.history[\"accuracy\"], label=\"train_acc\")\n",
    "# plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "# plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Loss/Accuracy\")\n",
    "# plt.legend(loc=\"lower left\")\n",
    "# plt.savefig(\"batch_LeakyReLU_validation_160.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helpers import *\n",
    "\n",
    "# # from cnn_model import cnn_model\n",
    "\n",
    "# # Instantiate the model\n",
    "# batch_normalization = True\n",
    "# activation = \"LeakyReLU\"\n",
    "# model = cnn_model(\n",
    "#     shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    "# )\n",
    "\n",
    "# # Load the model\n",
    "# model.load(\"batch_relu_validation_200-074-0.946167-0.927667.h5\")\n",
    "\n",
    "# # Print a summary to make sure the correct model is used\n",
    "# model.model.summary()\n",
    "\n",
    "# # We add all test images to an array, used later for generating a submission\n",
    "# image_filenames = []\n",
    "# for i in range(1, 51):\n",
    "#     image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "#     image_filenames.append(image_filename)\n",
    "\n",
    "# # Set-up submission filename\n",
    "# submission_filename = \"batch_relu_validation_200-074-0.946167-0.927667.csv\"\n",
    "\n",
    "# # Generates the submission\n",
    "# generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW BATCH ERICK V7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy as scipy\n",
    "\n",
    "\n",
    "# def pad_matrix(mat, h_pad, w_pad, val=0):\n",
    "#     h_pad = int(h_pad)\n",
    "#     w_pad = int(w_pad)\n",
    "#     if len(mat.shape) == 3:\n",
    "#         padded_mat = np.pad(\n",
    "#             mat,\n",
    "#             ((h_pad, h_pad), (w_pad, w_pad), (0, 0)),\n",
    "#             mode=\"constant\",\n",
    "#             constant_values=((val, val), (val, val), (0, 0)),\n",
    "#         )\n",
    "#     elif len(mat.shape) == 2:\n",
    "#         padded_mat = np.pad(\n",
    "#             mat,\n",
    "#             ((h_pad, h_pad), (w_pad, w_pad)),\n",
    "#             mode=\"constant\",\n",
    "#             constant_values=((val, val), (val, val)),\n",
    "#         )\n",
    "#     else:\n",
    "#         raise ValueError(\"This method can only handle 2d or 3d arrays\")\n",
    "#     return padded_mat\n",
    "\n",
    "\n",
    "# def imag_rotation(X, Y, number_rotations=8):\n",
    "\n",
    "#     w = X.shape[1]\n",
    "#     w_2 = w // 2  # half of the width\n",
    "#     padding = 82\n",
    "#     Xrs = X\n",
    "#     Yrs = Y\n",
    "#     Xrs = np.expand_dims(Xrs, 0)\n",
    "#     Yrs = np.expand_dims(Yrs, 0)\n",
    "#     thetas = np.random.randint(0, high=360, size=number_rotations)\n",
    "#     for theta in thetas:\n",
    "#         Xr = pad_matrix(\n",
    "#             X, padding, padding\n",
    "#         )  # Selected for the specific case of images of (400,400)\n",
    "#         Yr = pad_matrix(\n",
    "#             Y, padding, padding\n",
    "#         )  # Selected for the specific case of images of (400,400)\n",
    "#         Xr = scipy.ndimage.rotate(Xr, theta, reshape=False)\n",
    "#         Yr = scipy.ndimage.rotate(Yr, theta, reshape=False)\n",
    "#         theta = theta * np.pi / 180\n",
    "#         a = int(\n",
    "#             w_2 / (np.sqrt(2) * np.cos(np.pi / 4 - np.mod(theta, np.pi / 2)))\n",
    "#         )  # width and height of the biggest square inside the rotated square\n",
    "#         w_p = w_2 + padding\n",
    "#         Xr = Xr[w_p - a : w_p + a, w_p - a : w_p + a, :]\n",
    "#         Yr = Yr[w_p - a : w_p + a, w_p - a : w_p + a]\n",
    "\n",
    "#         Xr = cv2.resize(Xr, dsize=(w_2 * 2, w_2 * 2), interpolation=cv2.INTER_CUBIC)\n",
    "#         Yr = cv2.resize(Yr, dsize=(w_2 * 2, w_2 * 2), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "#         if np.random.choice(2) == 1:\n",
    "#             Xr = np.flipud(Xr)\n",
    "#             Yr = np.flipud(Yr)\n",
    "\n",
    "#         if np.random.choice(2) == 1:\n",
    "#             Xr = np.fliplr(Xr)\n",
    "#             Yr = np.fliplr(Yr)\n",
    "\n",
    "#         Xr = np.expand_dims(Xr, 0)\n",
    "#         Yr = np.expand_dims(Yr, 0)\n",
    "#         Xrs = np.append(Xrs, Xr, axis=0)\n",
    "#         Yrs = np.append(Yrs, Yr, axis=0)\n",
    "\n",
    "#     return Xrs, Yrs\n",
    "\n",
    "\n",
    "# def imag_rotation_aug(Xr, Yr, number_rotations=8):\n",
    "\n",
    "#     Xrs, Yrs = imag_rotation(Xr[0], Yr[0])\n",
    "#     for i in range(1, len(Xr)):\n",
    "#         Xrr, Yrr = imag_rotation(Xr[i], Yr[i])\n",
    "#         Xrs = np.append(Xrs, Xrr, axis=0)\n",
    "#         Yrs = np.append(Yrs, Yrr, axis=0)\n",
    "\n",
    "#     Xrs_shuf = []\n",
    "#     Yrs_shuf = []\n",
    "#     index_shuf = list(range(len(Xrs)))\n",
    "#     np.random.shuffle(index_shuf)\n",
    "#     for i in index_shuf:\n",
    "#         Xrs_shuf.append(Xrs[i])\n",
    "#         Yrs_shuf.append(Yrs[i])\n",
    "\n",
    "#     return Xrs_shuf, Yrs_shuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a set of images\n",
    "# root_dir = \"data/\"\n",
    "\n",
    "# # Select the directory for the images and load them\n",
    "# image_dir_train = root_dir + \"training/images/\"\n",
    "# files = os.listdir(image_dir_train)\n",
    "# n_train = len(files)\n",
    "\n",
    "# print(\"Loading \" + str(n_train) + \" images\")\n",
    "# imgs_train = np.asarray(\n",
    "#     [load_image(image_dir_train + files[i]) for i in range(n_train)]\n",
    "# )\n",
    "\n",
    "# # Select the directory for groundtruth images and load them\n",
    "# gt_dir_train = root_dir + \"training/groundtruth/\"\n",
    "# print(\"Loading \" + str(n_train) + \" groundtruth images\")\n",
    "# gt_imgs_train = np.asarray(\n",
    "#     [load_image(gt_dir_train + files[i]) for i in range(n_train)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select the directory for the images and load them\n",
    "# image_dir_val = root_dir + \"validating/images/\"\n",
    "# files = os.listdir(image_dir_val)\n",
    "# n_val = len(files)\n",
    "\n",
    "# print(\"Loading \" + str(n_val) + \" images\")\n",
    "# imgs_val = np.asarray([load_image(image_dir_val + files[i]) for i in range(n_val)])\n",
    "\n",
    "# # Select the directory for groundtruth images and load them\n",
    "# gt_dir_val = root_dir + \"validating/groundtruth/\"\n",
    "# print(\"Loading \" + str(n_val) + \" groundtruth images\")\n",
    "# gt_imgs_val = np.asarray([load_image(gt_dir_val + files[i]) for i in range(n_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, Y_train = imag_rotation_aug(imgs_train, gt_imgs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val, Y_val = imag_rotation_aug(imgs_val, gt_imgs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_minibatch(X, Y, n):\n",
    "\n",
    "#     # Fix the seed\n",
    "#     np.random.seed(1)\n",
    "\n",
    "#     # We define the window size of 72, batch size of 100 (empirically chosen)\n",
    "#     # and patch size should correspond to 16\n",
    "#     w_size = 72\n",
    "#     batch_size = 80\n",
    "#     patch_size = 72\n",
    "#     num_images = n\n",
    "\n",
    "#     while True:\n",
    "#         # Generate one minibatch\n",
    "#         batch_image = np.empty((batch_size, w_size, w_size, 3))\n",
    "#         batch_label = np.empty((batch_size, 2))\n",
    "\n",
    "#         for i in range(batch_size):\n",
    "\n",
    "#             # Select a random index representing an image\n",
    "#             random_index = np.random.choice(num_images)\n",
    "\n",
    "#             # Width of original image\n",
    "#             width = 400\n",
    "\n",
    "#             # Sample a random window from the image\n",
    "#             random_sample = np.random.randint(w_size // 2, width - w_size // 2, 2)\n",
    "\n",
    "#             # Create a sub image of size 72x72\n",
    "#             sampled_image = X[random_index][\n",
    "#                 random_sample[0] - w_size // 2 : random_sample[0] + w_size // 2,\n",
    "#                 random_sample[1] - w_size // 2 : random_sample[1] + w_size // 2,\n",
    "#             ]\n",
    "\n",
    "#             # Take its corresponding ground-truth image\n",
    "#             correspond_ground_truth = Y[random_index][\n",
    "#                 random_sample[0] - patch_size // 2 : random_sample[0] + patch_size // 2,\n",
    "#                 random_sample[1] - patch_size // 2 : random_sample[1] + patch_size // 2,\n",
    "#             ]\n",
    "\n",
    "#             # We set in the label depending on the threshold of 0.2\n",
    "#             # The label becomes either 0 or 1 by applying to_categorical with parameter 2\n",
    "#             label = to_categorical(\n",
    "#                 (np.array([np.mean(correspond_ground_truth)]) > 0.2) * 1, 2\n",
    "#             )\n",
    "\n",
    "#             # We put in the sub image and its corresponding label before yielding it\n",
    "#             batch_image[i] = sampled_image\n",
    "#             batch_label[i] = label\n",
    "\n",
    "#         # Yield the mini_batch to the model\n",
    "#         yield (batch_image, batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class cnn_model:\n",
    "\n",
    "#     # Initialize the class\n",
    "#     def __init__(self, shape, batch_normalization, activation):\n",
    "#         self.shape = shape\n",
    "#         self.batch_normalization = batch_normalization\n",
    "#         self.activation = activation\n",
    "#         self.model = self.initialize_cnn_model(shape, batch_normalization, activation)\n",
    "\n",
    "#     def initialize_cnn_model(self, shape, batch_normalization, activation):\n",
    "#         #         print(activation)\n",
    "\n",
    "#         # INPUT\n",
    "#         # shape     - Size of the input images\n",
    "#         # OUTPUT\n",
    "#         # model    - Compiled CNN\n",
    "\n",
    "#         # Define hyperparamters\n",
    "#         KERNEL3 = (3, 3)\n",
    "#         KERNEL5 = (5, 5)\n",
    "\n",
    "#         # Define a model\n",
    "#         model = Sequential()\n",
    "\n",
    "#         # Add the layers\n",
    "#         # Selection of the model is described in the report\n",
    "#         # We use padding = 'same' to avoid issues with the matrix sizes\n",
    "#         model.add(Conv2D(64, KERNEL5, input_shape=shape, padding=\"same\"))\n",
    "#         if batch_normalization:\n",
    "#             model.add(BatchNormalization())\n",
    "#         model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "#             Activation(activation)\n",
    "#         )\n",
    "#         model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "#         model.add(Dropout(0.2))\n",
    "\n",
    "#         model.add(Conv2D(128, KERNEL3, padding=\"same\"))\n",
    "#         if batch_normalization:\n",
    "#             model.add(BatchNormalization())\n",
    "#         model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "#             Activation(activation)\n",
    "#         )\n",
    "#         model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "#         model.add(Dropout(0.2))\n",
    "\n",
    "#         model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "#         if batch_normalization:\n",
    "#             model.add(BatchNormalization())\n",
    "#         model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "#             Activation(activation)\n",
    "#         )\n",
    "#         model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "#         model.add(Dropout(0.2))\n",
    "\n",
    "#         model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "#         if batch_normalization:\n",
    "#             model.add(BatchNormalization())\n",
    "#         model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "#             Activation(activation)\n",
    "#         )\n",
    "#         model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "#         model.add(Dropout(0.2))\n",
    "\n",
    "#         model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "#         if batch_normalization:\n",
    "#             model.add(BatchNormalization())\n",
    "#         model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "#             Activation(activation)\n",
    "#         )\n",
    "#         model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "#         model.add(Dropout(0.2))\n",
    "\n",
    "#         model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "#         if batch_normalization:\n",
    "#             model.add(BatchNormalization())\n",
    "#         model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "#             Activation(activation)\n",
    "#         )\n",
    "#         model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "#         model.add(Dropout(0.2))\n",
    "\n",
    "#         # Flatten it and use regularizers to avoid overfitting\n",
    "#         # The parameters have been chosen empirically\n",
    "#         model.add(Flatten())\n",
    "#         model.add(\n",
    "#             Dense(128, kernel_regularizer=l2(0.0001), activity_regularizer=l2(0.0001))\n",
    "#         )\n",
    "#         #         if batch_normalization:\n",
    "#         #             model.add(BatchNormalization())\n",
    "#         model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "#             Activation(activation)\n",
    "#         )\n",
    "#         model.add(Dropout(0.5))\n",
    "\n",
    "#         # Add output layer\n",
    "#         model.add(\n",
    "#             Dense(2, kernel_regularizer=l2(0.0001), activity_regularizer=l2(0.0001))\n",
    "#         )\n",
    "#         model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "#         # Compile the model using the binary crossentropy loss and the Adam optimizer for it\n",
    "#         # We used the accuracy as a metric, but F1 score is also a plausible choice\n",
    "#         model.compile(\n",
    "#             loss=\"binary_crossentropy\",\n",
    "#             optimizer=Adam(lr=0.001),\n",
    "#             metrics=[\"accuracy\", recall, f1],\n",
    "#         )\n",
    "\n",
    "#         # Print a summary of the model to see what has been generated\n",
    "#         model.summary()\n",
    "\n",
    "#         return model\n",
    "\n",
    "#     def train(self):\n",
    "\n",
    "#         # Early stopping callback after 10 steps\n",
    "#         early_stopping = EarlyStopping(\n",
    "#             monitor=\"val_loss\",\n",
    "#             min_delta=0,\n",
    "#             patience=15,\n",
    "#             verbose=1,\n",
    "#             mode=\"auto\",\n",
    "#             restore_best_weights=True,\n",
    "#         )\n",
    "\n",
    "#         # Reduce learning rate on plateau after 4 steps\n",
    "#         lr_callback = ReduceLROnPlateau(\n",
    "#             monitor=\"loss\", factor=0.5, patience=4, verbose=1, mode=\"auto\"\n",
    "#         )\n",
    "#         save_best = ModelCheckpoint(\n",
    "#             \"batch_LeakyReLU_validation_160_dropout-0.2_erickAugv7-{epoch:03d}-{accuracy:03f}-{val_accuracy:03f}.h5\",\n",
    "#             save_best_only=True,\n",
    "#             monitor=\"val_loss\",\n",
    "#             mode=\"auto\",\n",
    "#             verbose=1,\n",
    "#         )\n",
    "\n",
    "#         # Place the callbacks in a list to be used when training\n",
    "#         #         callbacks = [cb, early_stopping, lr_callback]\n",
    "#         callbacks = [save_best, lr_callback]\n",
    "\n",
    "#         # Train the model using the previously defined functions and callbacks\n",
    "#         history = self.model.fit_generator(\n",
    "#             create_minibatch(X_train, Y_train, n_train * 9),\n",
    "#             steps_per_epoch=STEPS_PER_EPOCH,\n",
    "#             epochs=EPOCHS,\n",
    "#             use_multiprocessing=False,\n",
    "#             workers=1,\n",
    "#             callbacks=callbacks,\n",
    "#             verbose=1,\n",
    "#             validation_data=create_minibatch(X_val, Y_val, n_val * 9),\n",
    "#             validation_steps=STEPS_PER_EPOCH,\n",
    "#         )\n",
    "#         #         to_plot = self.model.fit_generator(\n",
    "#         #             create_minibatch(X_train, Y_train, n_train),\n",
    "#         #             steps_per_epoch=100,\n",
    "#         #             epochs=EPOCHS,\n",
    "#         #             use_multiprocessing=False,\n",
    "#         #             workers=1,\n",
    "#         #             callbacks=callbacks,\n",
    "#         #             verbose=1,\n",
    "#         #             validation_data=create_minibatch(X_val, Y_val, n_val),\n",
    "#         #             validation_steps=100,\n",
    "#         #         )\n",
    "#         return history\n",
    "\n",
    "#     def classify(self, X):\n",
    "#         # Subdivide the images into blocks with a stride and patch_size of 16\n",
    "#         img_patches = create_patches(X, 16, 16, padding=28)\n",
    "\n",
    "#         # Predict\n",
    "#         predictions = self.model.predict(img_patches)\n",
    "#         predictions = (predictions[:, 0] < predictions[:, 1]) * 1\n",
    "\n",
    "#         # Regroup patches into images\n",
    "#         return group_patches(predictions, X.shape[0])\n",
    "\n",
    "#     def load(self, filename):\n",
    "#         # Load the model (used for submission)\n",
    "#         dependencies = {\n",
    "#             \"recall\": recall,\n",
    "#             \"f1\": f1,\n",
    "#         }\n",
    "#         self.model = load_model(filename, custom_objects=dependencies)\n",
    "\n",
    "\n",
    "# #     def save(self, filename):\n",
    "# #         # Save the model (used to then load to submit)\n",
    "# #         self.model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "# batch_normalization = True\n",
    "# EPOCHS = 200\n",
    "# # EPOCHS = 2\n",
    "# STEPS_PER_EPOCH = 150\n",
    "# activation = \"LeakyReLU\"\n",
    "# model = cnn_model(\n",
    "#     shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    "# )\n",
    "# # Train the model with batch\n",
    "# history = model.train()\n",
    "# # model.save(\"batch_relu_validation_200.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the training loss and accuracy\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "# plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "# plt.plot(history.history[\"accuracy\"], label=\"train_acc\")\n",
    "# plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "# plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Loss/Accuracy\")\n",
    "# plt.legend(loc=\"lower left\")\n",
    "# plt.savefig(\"batch_LeakyReLU_validation_erickv7.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helpers import *\n",
    "\n",
    "# # from cnn_model import cnn_model\n",
    "\n",
    "# # Instantiate the model\n",
    "# batch_normalization = True\n",
    "# activation = \"LeakyReLU\"\n",
    "# model = cnn_model(\n",
    "#     shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    "# )\n",
    "\n",
    "# # Load the model\n",
    "# model.load(\n",
    "#     \"batch_LeakyReLU_validation_160_dropout-0.2_erickAugv7-099-0.956417-0.907208.h5\"\n",
    "# )\n",
    "\n",
    "# # Print a summary to make sure the correct model is used\n",
    "# model.model.summary()\n",
    "\n",
    "# # We add all test images to an array, used later for generating a submission\n",
    "# image_filenames = []\n",
    "# for i in range(1, 51):\n",
    "#     image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "#     image_filenames.append(image_filename)\n",
    "\n",
    "# # Set-up submission filename\n",
    "# submission_filename = (\n",
    "#     \"batch_LeakyReLU_validation_160_dropout-0.2_erickAugv7-099-0.956417-0.907208.csv\"\n",
    "# )\n",
    "\n",
    "# # Generates the submission\n",
    "# generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class cnn_model:\n",
    "\n",
    "#     # Initialize the class\n",
    "#     def __init__(self, shape, batch_normalization, activation):\n",
    "#         self.shape = shape\n",
    "#         self.batch_normalization = batch_normalization\n",
    "#         self.activation = activation\n",
    "#         self.model = self.initialize_cnn_model(shape, batch_normalization, activation)\n",
    "\n",
    "#     def initialize_cnn_model(self, shape, batch_normalization, activation):\n",
    "#         #         print(activation)\n",
    "\n",
    "#         # INPUT\n",
    "#         # shape     - Size of the input images\n",
    "#         # OUTPUT\n",
    "#         # model    - Compiled CNN\n",
    "\n",
    "#         # Define hyperparamters\n",
    "#         KERNEL3 = (3, 3)\n",
    "#         KERNEL5 = (5, 5)\n",
    "\n",
    "#         # Define a model\n",
    "#         model = Sequential()\n",
    "\n",
    "#         # Add the layers\n",
    "#         # Selection of the model is described in the report\n",
    "#         # We use padding = 'same' to avoid issues with the matrix sizes\n",
    "#         model.add(Conv2D(64, KERNEL5, input_shape=shape, padding=\"same\"))\n",
    "#         if batch_normalization:\n",
    "#             model.add(BatchNormalization())\n",
    "#         model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "#             Activation(activation)\n",
    "#         )\n",
    "#         model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "#         model.add(Dropout(0.5))\n",
    "\n",
    "#         model.add(Conv2D(128, KERNEL3, padding=\"same\"))\n",
    "#         if batch_normalization:\n",
    "#             model.add(BatchNormalization())\n",
    "#         model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "#             Activation(activation)\n",
    "#         )\n",
    "#         model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "#         model.add(Dropout(0.5))\n",
    "\n",
    "#         model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "#         if batch_normalization:\n",
    "#             model.add(BatchNormalization())\n",
    "#         model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "#             Activation(activation)\n",
    "#         )\n",
    "#         model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "#         model.add(Dropout(0.5))\n",
    "\n",
    "#         model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "#         if batch_normalization:\n",
    "#             model.add(BatchNormalization())\n",
    "#         model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "#             Activation(activation)\n",
    "#         )\n",
    "#         model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "#         model.add(Dropout(0.5))\n",
    "\n",
    "#         model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "#         if batch_normalization:\n",
    "#             model.add(BatchNormalization())\n",
    "#         model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "#             Activation(activation)\n",
    "#         )\n",
    "#         model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "#         model.add(Dropout(0.5))\n",
    "\n",
    "#         model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "#         if batch_normalization:\n",
    "#             model.add(BatchNormalization())\n",
    "#         model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "#             Activation(activation)\n",
    "#         )\n",
    "#         model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "#         model.add(Dropout(0.5))\n",
    "\n",
    "#         # Flatten it and use regularizers to avoid overfitting\n",
    "#         # The parameters have been chosen empirically\n",
    "#         model.add(Flatten())\n",
    "#         model.add(\n",
    "#             Dense(128, kernel_regularizer=l2(0.001), activity_regularizer=l2(0.001))\n",
    "#         )\n",
    "#         #         if batch_normalization:\n",
    "#         #             model.add(BatchNormalization())\n",
    "#         model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "#             Activation(activation)\n",
    "#         )\n",
    "#         model.add(Dropout(0.5))\n",
    "\n",
    "#         # Add output layer\n",
    "#         model.add(\n",
    "#             Dense(2, kernel_regularizer=l2(0.001), activity_regularizer=l2(0.001))\n",
    "#         )\n",
    "#         model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "#         # Compile the model using the binary crossentropy loss and the Adam optimizer for it\n",
    "#         # We used the accuracy as a metric, but F1 score is also a plausible choice\n",
    "#         model.compile(\n",
    "#             loss=\"binary_crossentropy\",\n",
    "#             optimizer=Adam(lr=0.0005, amsgrad=True),\n",
    "#             metrics=[\"accuracy\", recall, f1],\n",
    "#         )\n",
    "\n",
    "#         # Print a summary of the model to see what has been generated\n",
    "#         model.summary()\n",
    "\n",
    "#         return model\n",
    "\n",
    "#     def train(self):\n",
    "\n",
    "#         # Early stopping callback after 10 steps\n",
    "#         early_stopping = EarlyStopping(\n",
    "#             monitor=\"val_loss\",\n",
    "#             min_delta=0,\n",
    "#             patience=15,\n",
    "#             verbose=1,\n",
    "#             mode=\"auto\",\n",
    "#             restore_best_weights=True,\n",
    "#         )\n",
    "\n",
    "#         # Reduce learning rate on plateau after 4 steps\n",
    "#         lr_callback = ReduceLROnPlateau(\n",
    "#             monitor=\"loss\", factor=0.5, patience=4, verbose=1, mode=\"auto\"\n",
    "#         )\n",
    "#         save_best = ModelCheckpoint(\n",
    "#             \"batch_LeakyReLU_validation_200_dropout-0.5_erickAugv7-{epoch:03d}-{accuracy:03f}-{val_accuracy:03f}.h5\",\n",
    "#             save_best_only=True,\n",
    "#             monitor=\"val_f1\",\n",
    "#             mode=\"max\",\n",
    "#             verbose=1,\n",
    "#         )\n",
    "\n",
    "#         # Place the callbacks in a list to be used when training\n",
    "#         #         callbacks = [cb, early_stopping, lr_callback]\n",
    "#         callbacks = [save_best, lr_callback]\n",
    "\n",
    "#         # Train the model using the previously defined functions and callbacks\n",
    "#         history = self.model.fit_generator(\n",
    "#             create_minibatch(X_train, Y_train, n_train * 9),\n",
    "#             steps_per_epoch=STEPS_PER_EPOCH,\n",
    "#             epochs=EPOCHS,\n",
    "#             use_multiprocessing=False,\n",
    "#             workers=1,\n",
    "#             callbacks=callbacks,\n",
    "#             verbose=1,\n",
    "#             validation_data=create_minibatch(X_val, Y_val, n_val * 9),\n",
    "#             validation_steps=STEPS_PER_EPOCH,\n",
    "#         )\n",
    "#         #         to_plot = self.model.fit_generator(\n",
    "#         #             create_minibatch(X_train, Y_train, n_train),\n",
    "#         #             steps_per_epoch=100,\n",
    "#         #             epochs=EPOCHS,\n",
    "#         #             use_multiprocessing=False,\n",
    "#         #             workers=1,\n",
    "#         #             callbacks=callbacks,\n",
    "#         #             verbose=1,\n",
    "#         #             validation_data=create_minibatch(X_val, Y_val, n_val),\n",
    "#         #             validation_steps=100,\n",
    "#         #         )\n",
    "#         return history\n",
    "\n",
    "#     def classify(self, X):\n",
    "#         # Subdivide the images into blocks with a stride and patch_size of 16\n",
    "#         img_patches = create_patches(X, 16, 16, padding=28)\n",
    "\n",
    "#         # Predict\n",
    "#         predictions = self.model.predict(img_patches)\n",
    "#         predictions = (predictions[:, 0] < predictions[:, 1]) * 1\n",
    "\n",
    "#         # Regroup patches into images\n",
    "#         return group_patches(predictions, X.shape[0])\n",
    "\n",
    "#     def load(self, filename):\n",
    "#         # Load the model (used for submission)\n",
    "#         dependencies = {\n",
    "#             \"recall\": recall,\n",
    "#             \"f1\": f1,\n",
    "#         }\n",
    "#         self.model = load_model(filename, custom_objects=dependencies)\n",
    "\n",
    "\n",
    "# #     def save(self, filename):\n",
    "# #         # Save the model (used to then load to submit)\n",
    "# #         self.model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "# batch_normalization = True\n",
    "# EPOCHS = 150\n",
    "# # EPOCHS = 2\n",
    "# STEPS_PER_EPOCH = 150\n",
    "# activation = \"LeakyReLU\"\n",
    "# model = cnn_model(\n",
    "#     shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    "# )\n",
    "# # Train the model with batch\n",
    "# history = model.train()\n",
    "# # model.save(\"batch_relu_validation_200.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the training loss and accuracy\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "# plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "# plt.plot(history.history[\"f1\"], label=\"train_f1\")\n",
    "# plt.plot(history.history[\"val_f1\"], label=\"val_f1\")\n",
    "# plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Loss/Accuracy\")\n",
    "# plt.legend(loc=\"lower left\")\n",
    "# plt.savefig(\"batch_LeakyReLU_validation_erickv7_dropout0.5_adagram.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helpers import *\n",
    "\n",
    "# # from cnn_model import cnn_model\n",
    "\n",
    "# # Instantiate the model\n",
    "# batch_normalization = True\n",
    "# activation = \"LeakyReLU\"\n",
    "# model = cnn_model(\n",
    "#     shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    "# )\n",
    "\n",
    "# # Load the model\n",
    "# model.load(\n",
    "#     \"batch_LeakyReLU_validation_200_dropout-0.5_erickAugv7-074-0.912500-0.858625.h5\"\n",
    "# )\n",
    "\n",
    "# # Print a summary to make sure the correct model is used\n",
    "# model.model.summary()\n",
    "\n",
    "# # We add all test images to an array, used later for generating a submission\n",
    "# image_filenames = []\n",
    "# for i in range(1, 51):\n",
    "#     image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "#     image_filenames.append(image_filename)\n",
    "\n",
    "# # Set-up submission filename\n",
    "# submission_filename = (\n",
    "#     \"batch_LeakyReLU_validation_200_dropout-0.5_erickAugv7-074-0.912500-0.858625.csv\"\n",
    "# )\n",
    "\n",
    "# # Generates the submission\n",
    "# generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
