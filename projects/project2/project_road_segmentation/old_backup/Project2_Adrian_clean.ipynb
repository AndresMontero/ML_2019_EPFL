{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext lab_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15146164481257805684\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3080278016\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17862336242555355020\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 980M, pci bus id: 0000:01:00.0, compute capability: 5.2\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "Use tensorflow.keras or Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as scipy\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.applications import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "from datetime import datetime\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import all the necessary for our model\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    LeakyReLU,\n",
    "    GaussianNoise,\n",
    ")\n",
    "from tensorflow.keras import backend as K\n",
    "from kerassurgeon.operations import delete_layer, insert_layer, replace_layer\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from tensorflow.compat.v2.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training images, images loaded: 80 \n",
      "Loading groundtruth images, images loaded: 80 \n"
     ]
    }
   ],
   "source": [
    "image_dir_train = \"data/training/images/\"\n",
    "files = os.listdir(image_dir_train)\n",
    "n_train = len(files)\n",
    "print(f\"Loading training images, images loaded: {n_train} \")\n",
    "imgs_train = np.asarray(\n",
    "    [load_image(image_dir_train + files[i]) for i in range(n_train)]\n",
    ")\n",
    "gt_dir_train = \"data/training/groundtruth/\"\n",
    "print(f\"Loading groundtruth images, images loaded: {n_train} \")\n",
    "gt_imgs_train = np.asarray(\n",
    "    [load_image(gt_dir_train + files[i]) for i in range(n_train)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 400\n",
    "# Patches for training\n",
    "img_patches_train = [\n",
    "    crop_image(imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "gt_patches_train = [\n",
    "    crop_image(gt_imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = np.asarray(\n",
    "    [\n",
    "        img_patches_train[i][j]\n",
    "        for i in range(len(img_patches_train))\n",
    "        for j in range(len(img_patches_train[i]))\n",
    "    ]\n",
    ")\n",
    "Y_train = np.asarray(\n",
    "    [\n",
    "        gt_patches_train[i][j]\n",
    "        for i in range(len(gt_patches_train))\n",
    "        for j in range(len(gt_patches_train[i]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validating images, images loaded: 20 \n",
      "Loading validating groundtruth, images loaded: 20 \n"
     ]
    }
   ],
   "source": [
    "image_dir_val = \"data/validating/images/\"\n",
    "files = os.listdir(image_dir_val)\n",
    "n_val = len(files)\n",
    "print(f\"Loading validating images, images loaded: {n_val} \")\n",
    "imgs_val = np.asarray([load_image(image_dir_val + files[i]) for i in range(n_val)])\n",
    "gt_dir_val = \"data/validating/groundtruth/\"\n",
    "print(f\"Loading validating groundtruth, images loaded: {n_val} \")\n",
    "gt_imgs_val = np.asarray([load_image(gt_dir_val + files[i]) for i in range(n_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 400\n",
    "# Patches for validating\n",
    "img_patches_val = [\n",
    "    crop_image(imgs_val[i], image_size, image_size) for i in range(n_val)\n",
    "]\n",
    "gt_patches_val = [\n",
    "    crop_image(gt_imgs_val[i], image_size, image_size) for i in range(n_val)\n",
    "]\n",
    "\n",
    "# Separate features and labels\n",
    "X_val = np.asarray(\n",
    "    [\n",
    "        img_patches_val[i][j]\n",
    "        for i in range(len(img_patches_val))\n",
    "        for j in range(len(img_patches_val[i]))\n",
    "    ]\n",
    ")\n",
    "Y_val = np.asarray(\n",
    "    [\n",
    "        gt_patches_val[i][j]\n",
    "        for i in range(len(gt_patches_val))\n",
    "        for j in range(len(gt_patches_val[i]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select data augmentation\n",
    "Method 1 or 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 1\n",
    "data_aug_factor = 1\n",
    "if method == 2:\n",
    "    data_aug_factor = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == 2:\n",
    "\n",
    "    def pad_matrix(mat, h_pad, w_pad, val=0):\n",
    "        h_pad = int(h_pad)\n",
    "        w_pad = int(w_pad)\n",
    "        if len(mat.shape) == 3:\n",
    "            padded_mat = np.pad(\n",
    "                mat,\n",
    "                ((h_pad, h_pad), (w_pad, w_pad), (0, 0)),\n",
    "                mode=\"constant\",\n",
    "                constant_values=((val, val), (val, val), (0, 0)),\n",
    "            )\n",
    "        elif len(mat.shape) == 2:\n",
    "            padded_mat = np.pad(\n",
    "                mat,\n",
    "                ((h_pad, h_pad), (w_pad, w_pad)),\n",
    "                mode=\"constant\",\n",
    "                constant_values=((val, val), (val, val)),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"This method can only handle 2d or 3d arrays\")\n",
    "        return padded_mat\n",
    "\n",
    "    def imag_rotation(X, Y, number_rotations=8):\n",
    "\n",
    "        w = X.shape[1]\n",
    "        w_2 = w // 2  # half of the width\n",
    "        padding = 82\n",
    "        Xrs = X\n",
    "        Yrs = Y\n",
    "\n",
    "        Xrs = np.expand_dims(Xrs, 0)\n",
    "        Yrs = np.expand_dims(Yrs, 0)\n",
    "\n",
    "        thetas = np.random.randint(0, high=360, size=number_rotations)\n",
    "        for theta in thetas:\n",
    "            Xr = pad_matrix(\n",
    "                X, padding, padding\n",
    "            )  # Selected for the specific case of images of (400,400)\n",
    "            Yr = pad_matrix(\n",
    "                Y, padding, padding\n",
    "            )  # Selected for the specific case of images of (400,400)\n",
    "            Xr = scipy.ndimage.rotate(Xr, theta, reshape=False)\n",
    "            Yr = scipy.ndimage.rotate(Yr, theta, reshape=False)\n",
    "            theta = theta * np.pi / 180\n",
    "            a = int(\n",
    "                w_2 / (np.sqrt(2) * np.cos(np.pi / 4 - np.mod(theta, np.pi / 2)))\n",
    "            )  # width and height of the biggest square inside the rotated square\n",
    "            w_p = w_2 + padding\n",
    "            Xr = Xr[w_p - a : w_p + a, w_p - a : w_p + a, :]\n",
    "            Yr = Yr[w_p - a : w_p + a, w_p - a : w_p + a]\n",
    "\n",
    "            Xr = cv2.resize(Xr, dsize=(w_2 * 2, w_2 * 2), interpolation=cv2.INTER_CUBIC)\n",
    "            Yr = cv2.resize(Yr, dsize=(w_2 * 2, w_2 * 2), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "            if np.random.choice(2) == 1:\n",
    "                Xr = np.flipud(Xr)\n",
    "                Yr = np.flipud(Yr)\n",
    "\n",
    "            if np.random.choice(2) == 1:\n",
    "                Xr = np.fliplr(Xr)\n",
    "                Yr = np.fliplr(Yr)\n",
    "\n",
    "            Xr = np.expand_dims(Xr, 0)\n",
    "            Yr = np.expand_dims(Yr, 0)\n",
    "            Xrs = np.append(Xrs, Xr, axis=0)\n",
    "            Yrs = np.append(Yrs, Yr, axis=0)\n",
    "\n",
    "        return Xrs, Yrs\n",
    "\n",
    "    def imag_rotation_aug(Xr, Yr, number_rotations=8):\n",
    "\n",
    "        Xrs, Yrs = imag_rotation(Xr[0], Yr[0])\n",
    "        for i in range(1, len(Xr)):\n",
    "            Xrr, Yrr = imag_rotation(Xr[i], Yr[i])\n",
    "            Xrs = np.append(Xrs, Xrr, axis=0)\n",
    "            Yrs = np.append(Yrs, Yrr, axis=0)\n",
    "\n",
    "        Xrs_shuf = []\n",
    "        Yrs_shuf = []\n",
    "        index_shuf = list(range(len(Xrs)))\n",
    "        np.random.shuffle(index_shuf)\n",
    "        for i in index_shuf:\n",
    "            Xrs_shuf.append(Xrs[i])\n",
    "            Yrs_shuf.append(Yrs[i])\n",
    "\n",
    "        return Xrs_shuf, Yrs_shuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == 2:\n",
    "    X_train, Y_train = imag_rotation_aug(imgs_train, gt_imgs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == 2:\n",
    "    X_val, Y_val = imag_rotation_aug(imgs_val, gt_imgs_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating mini-batch and running data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == 2:\n",
    "\n",
    "    def create_minibatch(X, Y, n):\n",
    "\n",
    "        # Fix the seed\n",
    "        # np.random.seed(1)\n",
    "\n",
    "        # We define the window size of 72, batch size of 100 (empirically chosen)\n",
    "        # and patch size should correspond to 16\n",
    "        w_size = 64\n",
    "        batch_size = 100\n",
    "        patch_size = 16\n",
    "        num_images = n\n",
    "\n",
    "        while True:\n",
    "            # Generate one minibatch\n",
    "            batch_image = np.empty((batch_size, w_size, w_size, 3))\n",
    "            batch_label = np.empty((batch_size, 2))\n",
    "\n",
    "            for i in range(batch_size):\n",
    "\n",
    "                # Select a random index representing an image\n",
    "                random_index = np.random.choice(num_images)\n",
    "\n",
    "                # Width of original image\n",
    "                width = 400\n",
    "\n",
    "                # Sample a random window from the image\n",
    "                random_sample = np.random.randint(w_size // 2, width - w_size // 2, 2)\n",
    "\n",
    "                # Create a sub image of size 72x72\n",
    "                sampled_image = X[random_index][\n",
    "                    random_sample[0] - w_size // 2 : random_sample[0] + w_size // 2,\n",
    "                    random_sample[1] - w_size // 2 : random_sample[1] + w_size // 2,\n",
    "                ]\n",
    "\n",
    "                # Take its corresponding ground-truth image\n",
    "                correspond_ground_truth = Y[random_index][\n",
    "                    random_sample[0]\n",
    "                    - patch_size // 2 : random_sample[0]\n",
    "                    + patch_size // 2,\n",
    "                    random_sample[1]\n",
    "                    - patch_size // 2 : random_sample[1]\n",
    "                    + patch_size // 2,\n",
    "                ]\n",
    "\n",
    "                # We set in the label depending on the threshold of 0.25\n",
    "                # The label becomes either 0 or 1 by applying to_categorical with parameter 2\n",
    "                label = to_categorical(\n",
    "                    (np.array([np.mean(correspond_ground_truth)]) > 0.25) * 1, 2\n",
    "                )\n",
    "\n",
    "                # We put in the sub image and its corresponding label before yielding it\n",
    "                batch_image[i] = sampled_image\n",
    "                batch_label[i] = label\n",
    "\n",
    "            # Yield the mini_batch to the model\n",
    "            yield (batch_image, batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == 1:\n",
    "\n",
    "    def create_minibatch(X, Y, n, batch_size=100):\n",
    "\n",
    "        # Fix the seed\n",
    "        np.random.seed(1)\n",
    "\n",
    "        # We define the window size of 72, batch size of 100 (empirically chosen)\n",
    "        # and patch size should correspond to 16\n",
    "        w_size = 64\n",
    "        batch_size = batch_size\n",
    "        patch_size = 16\n",
    "        num_images = n\n",
    "\n",
    "        while True:\n",
    "            # Generate one minibatch\n",
    "            batch_image = np.empty((batch_size, w_size, w_size, 3))\n",
    "            batch_label = np.empty((batch_size, 2))\n",
    "\n",
    "            for i in range(batch_size):\n",
    "\n",
    "                # Select a random index represnting an image\n",
    "                random_index = np.random.choice(num_images)\n",
    "\n",
    "                # Width of original image\n",
    "                width = 400\n",
    "\n",
    "                # Sample a random window from the image\n",
    "                random_sample = np.random.randint(w_size // 2, width - w_size // 2, 2)\n",
    "\n",
    "                # Create a sub image of size 72x72\n",
    "                sampled_image = X[random_index][\n",
    "                    random_sample[0] - w_size // 2 : random_sample[0] + w_size // 2,\n",
    "                    random_sample[1] - w_size // 2 : random_sample[1] + w_size // 2,\n",
    "                ]\n",
    "\n",
    "                # Take its corresponding ground-truth image\n",
    "                correspond_ground_truth = Y[random_index][\n",
    "                    random_sample[0]\n",
    "                    - patch_size // 2 : random_sample[0]\n",
    "                    + patch_size // 2,\n",
    "                    random_sample[1]\n",
    "                    - patch_size // 2 : random_sample[1]\n",
    "                    + patch_size // 2,\n",
    "                ]\n",
    "\n",
    "                # We set in the label depending on the threshold of 0.25\n",
    "                # The label becomes either 0 or 1 by applying to_categorical with parameter 2\n",
    "                label = to_categorical(\n",
    "                    (np.array([np.mean(correspond_ground_truth)]) > 0.25) * 1, 2\n",
    "                )\n",
    "\n",
    "                # The image augmentation is based on both flipping and rotating (randomly in steps of 45°)\n",
    "                # Random vertical and horizontal flip\n",
    "                if np.random.choice(2) == 1:\n",
    "                    sampled_image = np.flipud(sampled_image)\n",
    "\n",
    "                if np.random.choice(2) == 1:\n",
    "                    sampled_image = np.fliplr(sampled_image)\n",
    "\n",
    "                # Random rotation in steps of 45°\n",
    "                rotations = [0, 45, 90, 135, 180, 225, 270, 315]\n",
    "\n",
    "                # We select a rotation degree randomly\n",
    "                rotation_choice = np.random.choice(len(rotations))\n",
    "\n",
    "                # Rotate it using the random value (uses the scipy library)\n",
    "                sampled_image = scipy.ndimage.rotate(\n",
    "                    sampled_image,\n",
    "                    rotations[rotation_choice],\n",
    "                    order=1,\n",
    "                    reshape=False,\n",
    "                    mode=\"reflect\",\n",
    "                )\n",
    "\n",
    "                # We put in the sub image and its corresponding label before yielding it\n",
    "                batch_image[i] = sampled_image\n",
    "                batch_label[i] = label\n",
    "\n",
    "            # Yield the mini_batch to the model\n",
    "            yield (batch_image, batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    return 2 * ((prec * rec) / (prec + rec + K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet-Unet model Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet_unet_model:\n",
    "\n",
    "    # Initialize the class\n",
    "    def __init__(\n",
    "        self,\n",
    "        shape,\n",
    "        batch_normalization,\n",
    "        activation,\n",
    "        dropout,\n",
    "        amsgrad=False,\n",
    "        lr=1e-4,\n",
    "        noise=0.0,\n",
    "    ):\n",
    "        self.shape = shape\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.amsgrad = amsgrad\n",
    "        self.lr = lr\n",
    "        self.noise = noise\n",
    "        self.model = self.initialize_resnet_unet_model()\n",
    "\n",
    "    def conv_act(self, inputs, out_filters, activation=\"relu\"):\n",
    "        return Conv2D(\n",
    "            filters=out_filters,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "        )(inputs)\n",
    "\n",
    "    def decoder(\n",
    "        self,\n",
    "        inputs,\n",
    "        mid_filters=512,\n",
    "        out_filters=256,\n",
    "        activation=\"relu\",\n",
    "        block_name=\"decoder\",\n",
    "    ):\n",
    "        with K.name_scope(block_name):\n",
    "            if activation == \"leaky_relu\":\n",
    "                activation = None\n",
    "                conv = LeakyReLU(alpha=0.3)(\n",
    "                    self.conv_act(inputs, mid_filters, activation)\n",
    "                )\n",
    "            else:\n",
    "                conv = self.conv_act(inputs, mid_filters, activation)\n",
    "            conv_tr = Conv2DTranspose(\n",
    "                filters=out_filters,\n",
    "                activation=activation,\n",
    "                kernel_size=4,\n",
    "                strides=2,\n",
    "                padding=\"same\",\n",
    "            )(conv)\n",
    "        return conv_tr\n",
    "\n",
    "    def initialize_resnet_unet_model(self):\n",
    "        #         print(activation)\n",
    "\n",
    "        # INPUT\n",
    "        # shape     - Size of the input images\n",
    "        # OUTPUT\n",
    "        # model    - Compiled CNN\n",
    "\n",
    "        # Define parameters\n",
    "        max_pooling_size = 2\n",
    "        max_pooling_strd = 2\n",
    "\n",
    "        # Load a pretrained ResNet\n",
    "        num_classes = 2\n",
    "        resnet50 = ResNet50(\n",
    "            include_top=False,\n",
    "            weights=\"imagenet\",\n",
    "            classes=num_classes,\n",
    "            input_shape=self.shape,\n",
    "        )\n",
    "        resnet50.compile(\n",
    "            optimizer=Adam(lr=self.lr, amsgrad=self.amsgrad), loss=\"binary_crossentropy\"\n",
    "        )\n",
    "\n",
    "        # ResNet convolution outputs\n",
    "        conv5_out = resnet50.get_layer(\"conv5_block3_out\").output\n",
    "        conv4_out = resnet50.get_layer(\"conv4_block6_out\").output\n",
    "        conv3_out = resnet50.get_layer(\"conv3_block4_out\").output\n",
    "        conv2_out = resnet50.get_layer(\"conv2_block3_out\").output\n",
    "\n",
    "        pool = MaxPooling2D(max_pooling_size, strides=max_pooling_strd, padding=\"same\")(\n",
    "            resnet50.get_output_at(0)\n",
    "        )\n",
    "        dec_center = self.decoder(\n",
    "            pool,\n",
    "            mid_filters=self.shape[0] * 2,\n",
    "            out_filters=self.shape[0],\n",
    "            activation=self.activation,\n",
    "            block_name=\"decoder_center\",\n",
    "        )\n",
    "\n",
    "        if self.batch_normalization:\n",
    "            dec_center = BatchNormalization()(dec_center)\n",
    "        if self.dropout > 0:\n",
    "            dec_center = Dropout(dropout)(dec_center)\n",
    "\n",
    "        cat1 = Concatenate()([dec_center, conv5_out])\n",
    "        dec5 = self.decoder(\n",
    "            cat1,\n",
    "            mid_filters=int(self.shape[0] * 2),\n",
    "            out_filters=int(self.shape[0]),\n",
    "            activation=self.activation,\n",
    "            block_name=\"decoder5\",\n",
    "        )\n",
    "        if self.batch_normalization:\n",
    "            dec5 = BatchNormalization()(dec5)\n",
    "        if self.dropout > 0:\n",
    "            dec5 = Dropout(self.dropout)(dec5)\n",
    "\n",
    "        cat2 = Concatenate()([dec5, conv4_out])\n",
    "        dec4 = self.decoder(\n",
    "            cat2,\n",
    "            mid_filters=int(self.shape[0] * 2),\n",
    "            out_filters=int(self.shape[0]),\n",
    "            activation=self.activation,\n",
    "            block_name=\"decoder4\",\n",
    "        )\n",
    "        if self.batch_normalization:\n",
    "            dec4 = BatchNormalization()(dec4)\n",
    "        if self.dropout > 0:\n",
    "            dec4 = Dropout(self.dropout)(dec4)\n",
    "\n",
    "        cat3 = Concatenate()([dec4, conv3_out])\n",
    "        dec3 = self.decoder(\n",
    "            cat3,\n",
    "            mid_filters=int(self.shape[0]),\n",
    "            out_filters=int(self.shape[0] // 4),\n",
    "            activation=self.activation,\n",
    "            block_name=\"decoder3\",\n",
    "        )\n",
    "        if self.batch_normalization:\n",
    "            dec3 = BatchNormalization()(dec3)\n",
    "        if self.dropout > 0:\n",
    "            dec3 = Dropout(self.dropout)(dec3)\n",
    "\n",
    "        cat2 = Concatenate()([dec3, conv2_out])\n",
    "        dec2 = self.decoder(\n",
    "            cat2,\n",
    "            mid_filters=int(self.shape[0] // 2),\n",
    "            out_filters=int(self.shape[0] // 2),\n",
    "            activation=self.activation,\n",
    "            block_name=\"decoder2\",\n",
    "        )\n",
    "        if self.batch_normalization:\n",
    "            dec2 = BatchNormalization()(dec2)\n",
    "        if dropout > 0:\n",
    "            dec2 = Dropout(self.dropout)(dec2)\n",
    "\n",
    "        dec1 = self.decoder(\n",
    "            dec2,\n",
    "            mid_filters=int(self.shape[0] // 2),\n",
    "            out_filters=int(self.shape[0] // 8),\n",
    "            activation=self.activation,\n",
    "            block_name=\"decoder1\",\n",
    "        )\n",
    "        if self.batch_normalization:\n",
    "            dec1 = BatchNormalization()(dec1)\n",
    "        if self.dropout > 0:\n",
    "            dec1 = Dropout(self.dropout)(dec1)\n",
    "\n",
    "        dec0 = self.conv_act(dec1, out_filters=int(self.shape[0] // 8))\n",
    "        conv_f = Conv2D(1, 1, activation=\"sigmoid\", padding=\"same\")(dec0)\n",
    "        flatten_0 = Flatten()(conv_f)\n",
    "        dense_0 = Dense(\n",
    "            self.shape[0] / 2,\n",
    "            kernel_regularizer=l2(1e-6),\n",
    "            activity_regularizer=l2(1e-6),\n",
    "        )(flatten_0)\n",
    "        dropout_0 = Dropout(0.5)(dense_0)\n",
    "        lk_relu_0 = LeakyReLU(alpha=0.1)(dropout_0)\n",
    "        dense_1 = Dense(2, kernel_regularizer=l2(1e-6), activity_regularizer=l2(1e-6))(\n",
    "            lk_relu_0\n",
    "        )\n",
    "        dropout_1 = Dropout(0.2)(dense_1)\n",
    "        output = Activation(\"sigmoid\")(dropout_1)\n",
    "        model = Model(inputs=resnet50.get_input_at(0), outputs=output)\n",
    "\n",
    "        #           Compile the model using the binary crossentropy loss and the Adam optimizer for it\n",
    "        #           We used the accuracy as a metric, but F1 score is also a plausible choice\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(lr=lr, amsgrad=self.amsgrad),\n",
    "            metrics=[\"accuracy\", recall, f1],\n",
    "        )\n",
    "\n",
    "        # Print a summary of the model to see what has been generated\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        epochs,\n",
    "        steps_per_epoch,\n",
    "        n_train=85,\n",
    "        n_val=15,\n",
    "        batch_size=100,\n",
    "        data_aug_factor=1,\n",
    "    ):\n",
    "\n",
    "        # Early stopping callback after 10 steps\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\", min_delta=0, patience=10, verbose=1, mode=\"auto\"\n",
    "        )\n",
    "\n",
    "        # Reduce learning rate on plateau after 4 steps\n",
    "        lr_callback = ReduceLROnPlateau(\n",
    "            monitor=\"loss\", factor=0.5, patience=4, verbose=1, mode=\"auto\"\n",
    "        )\n",
    "\n",
    "        # Save best model\n",
    "        weights_filename = \"model_\"\n",
    "        if self.batch_normalization:\n",
    "            weights_filename = weights_filename + \"batch_\"\n",
    "        weights_filename = (\n",
    "            weights_filename\n",
    "            + self.activation\n",
    "            + \"_\"\n",
    "            + str(epochs)\n",
    "            + \"_\"\n",
    "            + \"dropout_\"\n",
    "            + str(self.dropout)\n",
    "            + \"_\"\n",
    "            + \"{epoch:03d}_\"\n",
    "            + \"{f1:03f}_\"\n",
    "            + \"{val_accuracy:03f}.h5\"\n",
    "        )\n",
    "        save_best = ModelCheckpoint(\n",
    "            weights_filename,\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"auto\",\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        # Place the callbacks in a list to be used when training\n",
    "        #         callbacks = [cb, early_stopping, lr_callback]\n",
    "        callbacks = [save_best, lr_callback]\n",
    "\n",
    "        # Train the model using the previously defined functions and callbacks\n",
    "        history = self.model.fit_generator(\n",
    "            create_minibatch(\n",
    "                X_train, Y_train, data_aug_factor * n_train, batch_size=batch_size\n",
    "            ),\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            epochs=epochs,\n",
    "            use_multiprocessing=False,\n",
    "            workers=1,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "            validation_data=create_minibatch(\n",
    "                X_val, Y_val, data_aug_factor * n_val, batch_size=batch_size\n",
    "            ),\n",
    "            validation_steps=steps_per_epoch,\n",
    "        )\n",
    "\n",
    "        return history\n",
    "\n",
    "    def classify(self, X):\n",
    "        # Subdivide the images into blocks with a stride and patch_size of 16\n",
    "        img_patches = create_patches(X, 16, 16, padding=24)\n",
    "\n",
    "        # Predict\n",
    "        predictions = self.model.predict(img_patches)\n",
    "        predictions = (predictions[:, 0] < predictions[:, 1]) * 1\n",
    "\n",
    "        # Regroup patches into images\n",
    "        #         return group_patches(predictions, X.self.shape[0])\n",
    "        return group_patches(predictions, X.shape[0])\n",
    "\n",
    "    def load(self, filename):\n",
    "        # Load the model (used for submission)\n",
    "        dependencies = {\"recall\": recall, \"f1\": f1}\n",
    "        self.model = load_model(filename, custom_objects=dependencies)\n",
    "\n",
    "    def save(self, filename):\n",
    "        # Save the model (used to then load to submit)\n",
    "        self.model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_normalization = True\n",
    "activation = \"relu\"\n",
    "dropout = 0\n",
    "amsgrad = False\n",
    "lr = 1e-4\n",
    "noise = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 70, 70, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 32, 32, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 32, 32, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 32, 32, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 34, 34, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 16, 16, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 16, 16, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 16, 16, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 16, 16, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 16, 16, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 16, 16, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 16, 16, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 16, 16, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 16, 16, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 16, 16, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 16, 16, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 16, 16, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 16, 16, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 16, 16, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 16, 16, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 8, 8, 128)    32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 8, 8, 128)    0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 8, 8, 128)    0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 8, 8, 512)    131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 8, 8, 512)    0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 8, 8, 512)    0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 8, 8, 128)    0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 8, 8, 128)    0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 8, 8, 512)    0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 8, 8, 512)    0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 8, 8, 128)    0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 8, 8, 128)    0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 8, 8, 512)    0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 8, 8, 512)    0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 8, 8, 128)    0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 8, 8, 128)    0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 8, 8, 512)    0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 8, 8, 512)    0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 4, 4, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 4, 4, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 4, 4, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 4, 4, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 4, 4, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 4, 4, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 4, 4, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 4, 4, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 4, 4, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 4, 4, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 4, 4, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 4, 4, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 4, 4, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 4, 4, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 4, 4, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 4, 4, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 4, 4, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 4, 4, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 4, 4, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 4, 4, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 4, 4, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 4, 4, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 4, 4, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 4, 4, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 2, 2, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 2, 2, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 2, 2, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 2, 2, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 2, 2, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 2, 2, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 2, 2, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 2, 2, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 2, 2, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 2, 2, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 2, 2, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 2, 2, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 2048)   0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 1, 1, 128)    2359424     max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 2, 2, 64)     131136      conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 2, 2, 64)     256         conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2, 2, 2112)   0           batch_normalization[0][0]        \n",
      "                                                                 conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 2, 2, 128)    2433152     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 4, 4, 64)     131136      conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 4, 4, 64)     256         conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 4, 1088)   0           batch_normalization_1[0][0]      \n",
      "                                                                 conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 4, 4, 128)    1253504     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 8, 8, 64)     131136      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 8, 8, 64)     256         conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8, 8, 576)    0           batch_normalization_2[0][0]      \n",
      "                                                                 conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 64)     331840      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 16, 16, 16)   16400       conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 16)   64          conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 16, 16, 272)  0           batch_normalization_3[0][0]      \n",
      "                                                                 conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 32)   78368       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 32, 32, 32)   16416       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 64, 64, 8)    4104        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 64, 8)    32          conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 8)    584         batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 1)    9           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4096)         0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           131104      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 32)           0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            66          leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2)            0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 2)            0           dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 30,616,331\n",
      "Trainable params: 30,562,715\n",
      "Non-trainable params: 53,616\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6179 - accuracy: 0.6806 - recall: 0.5781 - f1: 0.6429\n",
      "Epoch 00001: val_loss improved from inf to 0.58232, saving model to model_batch_relu_80_dropout_0_001_0.642919_0.732100.h5\n",
      "100/100 [==============================] - 86s 865ms/step - loss: 0.6177 - accuracy: 0.6805 - recall: 0.5781 - f1: 0.6429 - val_loss: 0.5823 - val_accuracy: 0.7321 - val_recall: 0.7321 - val_f1: 0.7321\n",
      "Epoch 2/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.5014 - accuracy: 0.7171 - recall: 0.6254 - f1: 0.6877\n",
      "Epoch 00002: val_loss did not improve from 0.58232\n",
      "100/100 [==============================] - 76s 757ms/step - loss: 0.5007 - accuracy: 0.7175 - recall: 0.6260 - f1: 0.6882 - val_loss: 1.1936 - val_accuracy: 0.7308 - val_recall: 0.7308 - val_f1: 0.7308\n",
      "Epoch 3/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3822 - accuracy: 0.8033 - recall: 0.7114 - f1: 0.7830\n",
      "Epoch 00003: val_loss did not improve from 0.58232\n",
      "100/100 [==============================] - 75s 754ms/step - loss: 0.3831 - accuracy: 0.8033 - recall: 0.7118 - f1: 0.7831 - val_loss: 1.4715 - val_accuracy: 0.7221 - val_recall: 0.7221 - val_f1: 0.7221\n",
      "Epoch 4/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3371 - accuracy: 0.8211 - recall: 0.7217 - f1: 0.8008\n",
      "Epoch 00004: val_loss did not improve from 0.58232\n",
      "100/100 [==============================] - 76s 763ms/step - loss: 0.3378 - accuracy: 0.8209 - recall: 0.7219 - f1: 0.8007 - val_loss: 0.6773 - val_accuracy: 0.7312 - val_recall: 0.7312 - val_f1: 0.7312\n",
      "Epoch 5/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3184 - accuracy: 0.8296 - recall: 0.7314 - f1: 0.8105\n",
      "Epoch 00005: val_loss did not improve from 0.58232\n",
      "100/100 [==============================] - 79s 795ms/step - loss: 0.3179 - accuracy: 0.8299 - recall: 0.7318 - f1: 0.8109 - val_loss: 1.3306 - val_accuracy: 0.7305 - val_recall: 0.7305 - val_f1: 0.7305\n",
      "Epoch 6/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3020 - accuracy: 0.8354 - recall: 0.7317 - f1: 0.8157\n",
      "Epoch 00006: val_loss did not improve from 0.58232\n",
      "100/100 [==============================] - 80s 799ms/step - loss: 0.3018 - accuracy: 0.8354 - recall: 0.7317 - f1: 0.8158 - val_loss: 0.7597 - val_accuracy: 0.7238 - val_recall: 0.7237 - val_f1: 0.7238\n",
      "Epoch 7/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2935 - accuracy: 0.8440 - recall: 0.7431 - f1: 0.8260\n",
      "Epoch 00007: val_loss did not improve from 0.58232\n",
      "100/100 [==============================] - 79s 790ms/step - loss: 0.2938 - accuracy: 0.8436 - recall: 0.7427 - f1: 0.8255 - val_loss: 0.6523 - val_accuracy: 0.7158 - val_recall: 0.7164 - val_f1: 0.7160\n",
      "Epoch 8/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2859 - accuracy: 0.8454 - recall: 0.7519 - f1: 0.8290\n",
      "Epoch 00008: val_loss did not improve from 0.58232\n",
      "100/100 [==============================] - 79s 786ms/step - loss: 0.2860 - accuracy: 0.8454 - recall: 0.7515 - f1: 0.8289 - val_loss: 1.2314 - val_accuracy: 0.7257 - val_recall: 0.7259 - val_f1: 0.7257\n",
      "Epoch 9/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2824 - accuracy: 0.8440 - recall: 0.7444 - f1: 0.8262\n",
      "Epoch 00009: val_loss did not improve from 0.58232\n",
      "100/100 [==============================] - 79s 794ms/step - loss: 0.2826 - accuracy: 0.8439 - recall: 0.7444 - f1: 0.8261 - val_loss: 1.2784 - val_accuracy: 0.7408 - val_recall: 0.7407 - val_f1: 0.7407\n",
      "Epoch 10/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2717 - accuracy: 0.8518 - recall: 0.7568 - f1: 0.8357\n",
      "Epoch 00010: val_loss improved from 0.58232 to 0.37938, saving model to model_batch_relu_80_dropout_0_010_0.836175_0.829500.h5\n",
      "100/100 [==============================] - 80s 799ms/step - loss: 0.2708 - accuracy: 0.8522 - recall: 0.7572 - f1: 0.8362 - val_loss: 0.3794 - val_accuracy: 0.8295 - val_recall: 0.8430 - val_f1: 0.8318\n",
      "Epoch 11/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2748 - accuracy: 0.8485 - recall: 0.7504 - f1: 0.8316\n",
      "Epoch 00011: val_loss did not improve from 0.37938\n",
      "100/100 [==============================] - 76s 759ms/step - loss: 0.2751 - accuracy: 0.8481 - recall: 0.7497 - f1: 0.8311 - val_loss: 0.3840 - val_accuracy: 0.8501 - val_recall: 0.8486 - val_f1: 0.8499\n",
      "Epoch 12/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2674 - accuracy: 0.8531 - recall: 0.7572 - f1: 0.8370\n",
      "Epoch 00012: val_loss improved from 0.37938 to 0.26727, saving model to model_batch_relu_80_dropout_0_012_0.836815_0.889950.h5\n",
      "100/100 [==============================] - 78s 775ms/step - loss: 0.2678 - accuracy: 0.8530 - recall: 0.7568 - f1: 0.8368 - val_loss: 0.2673 - val_accuracy: 0.8899 - val_recall: 0.8987 - val_f1: 0.8909\n",
      "Epoch 13/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2688 - accuracy: 0.8494 - recall: 0.7511 - f1: 0.8325\n",
      "Epoch 00013: val_loss improved from 0.26727 to 0.25495, saving model to model_batch_relu_80_dropout_0_013_0.832667_0.905550.h5\n",
      "100/100 [==============================] - 78s 781ms/step - loss: 0.2684 - accuracy: 0.8495 - recall: 0.7513 - f1: 0.8327 - val_loss: 0.2549 - val_accuracy: 0.9056 - val_recall: 0.9158 - val_f1: 0.9065\n",
      "Epoch 14/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2596 - accuracy: 0.8537 - recall: 0.7590 - f1: 0.8377\n",
      "Epoch 00014: val_loss did not improve from 0.25495\n",
      "100/100 [==============================] - 76s 758ms/step - loss: 0.2596 - accuracy: 0.8540 - recall: 0.7593 - f1: 0.8380 - val_loss: 0.2648 - val_accuracy: 0.8947 - val_recall: 0.9007 - val_f1: 0.8954\n",
      "Epoch 15/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2595 - accuracy: 0.8546 - recall: 0.7593 - f1: 0.8389\n",
      "Epoch 00015: val_loss improved from 0.25495 to 0.21451, saving model to model_batch_relu_80_dropout_0_015_0.838279_0.916800.h5\n",
      "100/100 [==============================] - 80s 802ms/step - loss: 0.2598 - accuracy: 0.8541 - recall: 0.7585 - f1: 0.8383 - val_loss: 0.2145 - val_accuracy: 0.9168 - val_recall: 0.9179 - val_f1: 0.9169\n",
      "Epoch 16/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2643 - accuracy: 0.8506 - recall: 0.7490 - f1: 0.8331\n",
      "Epoch 00016: val_loss did not improve from 0.21451\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.2641 - accuracy: 0.8508 - recall: 0.7494 - f1: 0.8335 - val_loss: 0.2394 - val_accuracy: 0.9106 - val_recall: 0.9138 - val_f1: 0.9109\n",
      "Epoch 17/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2656 - accuracy: 0.8522 - recall: 0.7553 - f1: 0.8359\n",
      "Epoch 00017: val_loss improved from 0.21451 to 0.20948, saving model to model_batch_relu_80_dropout_0_017_0.836113_0.919450.h5\n",
      "100/100 [==============================] - 77s 773ms/step - loss: 0.2654 - accuracy: 0.8524 - recall: 0.7555 - f1: 0.8361 - val_loss: 0.2095 - val_accuracy: 0.9194 - val_recall: 0.9226 - val_f1: 0.9197\n",
      "Epoch 18/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2535 - accuracy: 0.8525 - recall: 0.7514 - f1: 0.8356\n",
      "Epoch 00018: val_loss did not improve from 0.20948\n",
      "100/100 [==============================] - 78s 776ms/step - loss: 0.2533 - accuracy: 0.8526 - recall: 0.7517 - f1: 0.8357 - val_loss: 0.2314 - val_accuracy: 0.9195 - val_recall: 0.9210 - val_f1: 0.9196\n",
      "Epoch 19/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2507 - accuracy: 0.8586 - recall: 0.7594 - f1: 0.8424\n",
      "Epoch 00019: val_loss improved from 0.20948 to 0.19710, saving model to model_batch_relu_80_dropout_0_019_0.842048_0.929850.h5\n",
      "100/100 [==============================] - 78s 776ms/step - loss: 0.2510 - accuracy: 0.8583 - recall: 0.7590 - f1: 0.8420 - val_loss: 0.1971 - val_accuracy: 0.9298 - val_recall: 0.9296 - val_f1: 0.9298\n",
      "Epoch 20/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2616 - accuracy: 0.8541 - recall: 0.7552 - f1: 0.8377\n",
      "Epoch 00020: val_loss did not improve from 0.19710\n",
      "100/100 [==============================] - 76s 757ms/step - loss: 0.2612 - accuracy: 0.8540 - recall: 0.7547 - f1: 0.8375 - val_loss: 0.2265 - val_accuracy: 0.9134 - val_recall: 0.9187 - val_f1: 0.9139\n",
      "Epoch 21/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2449 - accuracy: 0.8591 - recall: 0.7616 - f1: 0.8434\n",
      "Epoch 00021: val_loss did not improve from 0.19710\n",
      "100/100 [==============================] - 76s 762ms/step - loss: 0.2448 - accuracy: 0.8590 - recall: 0.7617 - f1: 0.8433 - val_loss: 0.2275 - val_accuracy: 0.9112 - val_recall: 0.9174 - val_f1: 0.9118\n",
      "Epoch 22/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2551 - accuracy: 0.8508 - recall: 0.7478 - f1: 0.8329\n",
      "Epoch 00022: val_loss did not improve from 0.19710\n",
      "100/100 [==============================] - 76s 760ms/step - loss: 0.2550 - accuracy: 0.8508 - recall: 0.7481 - f1: 0.8331 - val_loss: 0.2585 - val_accuracy: 0.9103 - val_recall: 0.9085 - val_f1: 0.9101\n",
      "Epoch 23/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2523 - accuracy: 0.8548 - recall: 0.7539 - f1: 0.8379\n",
      "Epoch 00023: val_loss did not improve from 0.19710\n",
      "100/100 [==============================] - 76s 757ms/step - loss: 0.2524 - accuracy: 0.8546 - recall: 0.7537 - f1: 0.8377 - val_loss: 0.2035 - val_accuracy: 0.9218 - val_recall: 0.9211 - val_f1: 0.9217\n",
      "Epoch 24/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2507 - accuracy: 0.8580 - recall: 0.7614 - f1: 0.8424\n",
      "Epoch 00024: val_loss did not improve from 0.19710\n",
      "100/100 [==============================] - 76s 758ms/step - loss: 0.2505 - accuracy: 0.8579 - recall: 0.7613 - f1: 0.8423 - val_loss: 0.1983 - val_accuracy: 0.9215 - val_recall: 0.9270 - val_f1: 0.9220\n",
      "Epoch 25/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2468 - accuracy: 0.8584 - recall: 0.7598 - f1: 0.8423\n",
      "Epoch 00025: val_loss did not improve from 0.19710\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "100/100 [==============================] - 76s 760ms/step - loss: 0.2475 - accuracy: 0.8583 - recall: 0.7598 - f1: 0.8422 - val_loss: 0.2077 - val_accuracy: 0.9193 - val_recall: 0.9188 - val_f1: 0.9192\n",
      "Epoch 26/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2435 - accuracy: 0.8624 - recall: 0.7651 - f1: 0.8471\n",
      "Epoch 00026: val_loss improved from 0.19710 to 0.19033, saving model to model_batch_relu_80_dropout_0_026_0.847459_0.926800.h5\n",
      "100/100 [==============================] - 78s 779ms/step - loss: 0.2431 - accuracy: 0.8627 - recall: 0.7655 - f1: 0.8475 - val_loss: 0.1903 - val_accuracy: 0.9268 - val_recall: 0.9304 - val_f1: 0.9271\n",
      "Epoch 27/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2324 - accuracy: 0.8655 - recall: 0.7692 - f1: 0.8506\n",
      "Epoch 00027: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.2318 - accuracy: 0.8656 - recall: 0.7693 - f1: 0.8508 - val_loss: 0.2096 - val_accuracy: 0.9190 - val_recall: 0.9204 - val_f1: 0.9191\n",
      "Epoch 28/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2296 - accuracy: 0.8644 - recall: 0.7686 - f1: 0.8496\n",
      "Epoch 00028: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 76s 763ms/step - loss: 0.2294 - accuracy: 0.8646 - recall: 0.7688 - f1: 0.8498 - val_loss: 0.2079 - val_accuracy: 0.9265 - val_recall: 0.9311 - val_f1: 0.9268\n",
      "Epoch 29/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2329 - accuracy: 0.8571 - recall: 0.7552 - f1: 0.8404\n",
      "Epoch 00029: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 76s 762ms/step - loss: 0.2326 - accuracy: 0.8571 - recall: 0.7549 - f1: 0.8403 - val_loss: 0.2060 - val_accuracy: 0.9204 - val_recall: 0.9215 - val_f1: 0.9205\n",
      "Epoch 30/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2319 - accuracy: 0.8651 - recall: 0.7681 - f1: 0.8501\n",
      "Epoch 00030: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 76s 763ms/step - loss: 0.2322 - accuracy: 0.8651 - recall: 0.7681 - f1: 0.8501 - val_loss: 0.2222 - val_accuracy: 0.9233 - val_recall: 0.9265 - val_f1: 0.9236\n",
      "Epoch 31/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2299 - accuracy: 0.8657 - recall: 0.7668 - f1: 0.8504\n",
      "Epoch 00031: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 76s 761ms/step - loss: 0.2301 - accuracy: 0.8654 - recall: 0.7661 - f1: 0.8499 - val_loss: 0.2133 - val_accuracy: 0.9250 - val_recall: 0.9289 - val_f1: 0.9253\n",
      "Epoch 32/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2258 - accuracy: 0.8655 - recall: 0.7676 - f1: 0.8502\n",
      "Epoch 00032: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 76s 758ms/step - loss: 0.2254 - accuracy: 0.8658 - recall: 0.7682 - f1: 0.8506 - val_loss: 0.2254 - val_accuracy: 0.9195 - val_recall: 0.9208 - val_f1: 0.9196\n",
      "Epoch 33/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2274 - accuracy: 0.8635 - recall: 0.7632 - f1: 0.8478\n",
      "Epoch 00033: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 76s 759ms/step - loss: 0.2276 - accuracy: 0.8633 - recall: 0.7628 - f1: 0.8476 - val_loss: 0.2151 - val_accuracy: 0.9136 - val_recall: 0.9159 - val_f1: 0.9139\n",
      "Epoch 34/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2197 - accuracy: 0.8680 - recall: 0.7725 - f1: 0.8535\n",
      "Epoch 00034: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 76s 762ms/step - loss: 0.2201 - accuracy: 0.8680 - recall: 0.7726 - f1: 0.8536 - val_loss: 0.2021 - val_accuracy: 0.9245 - val_recall: 0.9281 - val_f1: 0.9248\n",
      "Epoch 35/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2283 - accuracy: 0.8622 - recall: 0.7632 - f1: 0.8467\n",
      "Epoch 00035: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 76s 763ms/step - loss: 0.2285 - accuracy: 0.8620 - recall: 0.7630 - f1: 0.8465 - val_loss: 0.2284 - val_accuracy: 0.9250 - val_recall: 0.9279 - val_f1: 0.9252\n",
      "Epoch 36/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2296 - accuracy: 0.8634 - recall: 0.7627 - f1: 0.8475\n",
      "Epoch 00036: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 76s 760ms/step - loss: 0.2295 - accuracy: 0.8633 - recall: 0.7627 - f1: 0.8474 - val_loss: 0.1946 - val_accuracy: 0.9301 - val_recall: 0.9328 - val_f1: 0.9303\n",
      "Epoch 37/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2228 - accuracy: 0.8649 - recall: 0.7645 - f1: 0.8492\n",
      "Epoch 00037: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 76s 759ms/step - loss: 0.2229 - accuracy: 0.8648 - recall: 0.7642 - f1: 0.8491 - val_loss: 0.2290 - val_accuracy: 0.9240 - val_recall: 0.9257 - val_f1: 0.9242\n",
      "Epoch 38/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2224 - accuracy: 0.8704 - recall: 0.7765 - f1: 0.8564\n",
      "Epoch 00038: val_loss did not improve from 0.19033\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "100/100 [==============================] - 76s 756ms/step - loss: 0.2221 - accuracy: 0.8705 - recall: 0.7766 - f1: 0.8565 - val_loss: 0.1941 - val_accuracy: 0.9301 - val_recall: 0.9329 - val_f1: 0.9303\n",
      "Epoch 39/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2233 - accuracy: 0.8678 - recall: 0.7707 - f1: 0.8530\n",
      "Epoch 00039: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 76s 760ms/step - loss: 0.2232 - accuracy: 0.8681 - recall: 0.7712 - f1: 0.8534 - val_loss: 0.2077 - val_accuracy: 0.9280 - val_recall: 0.9295 - val_f1: 0.9282\n",
      "Epoch 40/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2181 - accuracy: 0.8649 - recall: 0.7664 - f1: 0.8496\n",
      "Epoch 00040: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 76s 760ms/step - loss: 0.2177 - accuracy: 0.8648 - recall: 0.7663 - f1: 0.8495 - val_loss: 0.2123 - val_accuracy: 0.9309 - val_recall: 0.9316 - val_f1: 0.9309\n",
      "Epoch 41/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2180 - accuracy: 0.8627 - recall: 0.7595 - f1: 0.8465\n",
      "Epoch 00041: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 76s 758ms/step - loss: 0.2182 - accuracy: 0.8629 - recall: 0.7597 - f1: 0.8467 - val_loss: 0.2114 - val_accuracy: 0.9278 - val_recall: 0.9300 - val_f1: 0.9280\n",
      "Epoch 42/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2188 - accuracy: 0.8667 - recall: 0.7637 - f1: 0.8510\n",
      "Epoch 00042: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 77s 765ms/step - loss: 0.2185 - accuracy: 0.8667 - recall: 0.7637 - f1: 0.8510 - val_loss: 0.1969 - val_accuracy: 0.9312 - val_recall: 0.9329 - val_f1: 0.9314\n",
      "Epoch 43/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2138 - accuracy: 0.8676 - recall: 0.7673 - f1: 0.8522\n",
      "Epoch 00043: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 77s 765ms/step - loss: 0.2134 - accuracy: 0.8680 - recall: 0.7680 - f1: 0.8528 - val_loss: 0.1970 - val_accuracy: 0.9283 - val_recall: 0.9304 - val_f1: 0.9285\n",
      "Epoch 44/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2161 - accuracy: 0.8677 - recall: 0.7709 - f1: 0.8531\n",
      "Epoch 00044: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.2160 - accuracy: 0.8678 - recall: 0.7709 - f1: 0.8532 - val_loss: 0.1983 - val_accuracy: 0.9304 - val_recall: 0.9322 - val_f1: 0.9305\n",
      "Epoch 45/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2131 - accuracy: 0.8734 - recall: 0.7784 - f1: 0.8596\n",
      "Epoch 00045: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 77s 775ms/step - loss: 0.2136 - accuracy: 0.8735 - recall: 0.7787 - f1: 0.8598 - val_loss: 0.1923 - val_accuracy: 0.9338 - val_recall: 0.9368 - val_f1: 0.9341\n",
      "Epoch 46/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2192 - accuracy: 0.8647 - recall: 0.7622 - f1: 0.8488\n",
      "Epoch 00046: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 78s 780ms/step - loss: 0.2191 - accuracy: 0.8650 - recall: 0.7625 - f1: 0.8490 - val_loss: 0.2050 - val_accuracy: 0.9299 - val_recall: 0.9311 - val_f1: 0.9300\n",
      "Epoch 47/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2138 - accuracy: 0.8686 - recall: 0.7664 - f1: 0.8533\n",
      "Epoch 00047: val_loss did not improve from 0.19033\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "100/100 [==============================] - 76s 762ms/step - loss: 0.2137 - accuracy: 0.8686 - recall: 0.7662 - f1: 0.8532 - val_loss: 0.1937 - val_accuracy: 0.9291 - val_recall: 0.9293 - val_f1: 0.9292\n",
      "Epoch 48/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2147 - accuracy: 0.8718 - recall: 0.7741 - f1: 0.8574\n",
      "Epoch 00048: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 76s 761ms/step - loss: 0.2148 - accuracy: 0.8717 - recall: 0.7741 - f1: 0.8573 - val_loss: 0.1955 - val_accuracy: 0.9339 - val_recall: 0.9346 - val_f1: 0.9339\n",
      "Epoch 49/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2109 - accuracy: 0.8712 - recall: 0.7720 - f1: 0.8565\n",
      "Epoch 00049: val_loss did not improve from 0.19033\n",
      "100/100 [==============================] - 76s 758ms/step - loss: 0.2109 - accuracy: 0.8712 - recall: 0.7718 - f1: 0.8564 - val_loss: 0.1939 - val_accuracy: 0.9343 - val_recall: 0.9349 - val_f1: 0.9343\n",
      "Epoch 50/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2125 - accuracy: 0.8713 - recall: 0.7714 - f1: 0.8563\n",
      "Epoch 00050: val_loss improved from 0.19033 to 0.18961, saving model to model_batch_relu_80_dropout_0_050_0.856467_0.937150.h5\n",
      "100/100 [==============================] - 78s 784ms/step - loss: 0.2123 - accuracy: 0.8713 - recall: 0.7717 - f1: 0.8565 - val_loss: 0.1896 - val_accuracy: 0.9372 - val_recall: 0.9383 - val_f1: 0.9372\n",
      "Epoch 51/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2069 - accuracy: 0.8705 - recall: 0.7714 - f1: 0.8557\n",
      "Epoch 00051: val_loss did not improve from 0.18961\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.2071 - accuracy: 0.8702 - recall: 0.7708 - f1: 0.8553 - val_loss: 0.2158 - val_accuracy: 0.9257 - val_recall: 0.9275 - val_f1: 0.9258\n",
      "Epoch 52/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2152 - accuracy: 0.8656 - recall: 0.7635 - f1: 0.8499\n",
      "Epoch 00052: val_loss improved from 0.18961 to 0.18446, saving model to model_batch_relu_80_dropout_0_052_0.849586_0.933300.h5\n",
      "100/100 [==============================] - 78s 777ms/step - loss: 0.2161 - accuracy: 0.8654 - recall: 0.7633 - f1: 0.8496 - val_loss: 0.1845 - val_accuracy: 0.9333 - val_recall: 0.9348 - val_f1: 0.9334\n",
      "Epoch 53/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2116 - accuracy: 0.8725 - recall: 0.7756 - f1: 0.8584\n",
      "Epoch 00053: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 759ms/step - loss: 0.2117 - accuracy: 0.8723 - recall: 0.7752 - f1: 0.8582 - val_loss: 0.1980 - val_accuracy: 0.9315 - val_recall: 0.9332 - val_f1: 0.9316\n",
      "Epoch 54/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2082 - accuracy: 0.8711 - recall: 0.7676 - f1: 0.8556\n",
      "Epoch 00054: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.2092 - accuracy: 0.8712 - recall: 0.7679 - f1: 0.8558 - val_loss: 0.1899 - val_accuracy: 0.9336 - val_recall: 0.9360 - val_f1: 0.9337\n",
      "Epoch 55/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2063 - accuracy: 0.8724 - recall: 0.7737 - f1: 0.8579\n",
      "Epoch 00055: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 759ms/step - loss: 0.2066 - accuracy: 0.8722 - recall: 0.7732 - f1: 0.8577 - val_loss: 0.2076 - val_accuracy: 0.9293 - val_recall: 0.9315 - val_f1: 0.9294\n",
      "Epoch 56/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2055 - accuracy: 0.8759 - recall: 0.7795 - f1: 0.8621\n",
      "Epoch 00056: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 761ms/step - loss: 0.2055 - accuracy: 0.8757 - recall: 0.7794 - f1: 0.8619 - val_loss: 0.2022 - val_accuracy: 0.9358 - val_recall: 0.9372 - val_f1: 0.9358\n",
      "Epoch 57/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2078 - accuracy: 0.8741 - recall: 0.7772 - f1: 0.8600\n",
      "Epoch 00057: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 759ms/step - loss: 0.2082 - accuracy: 0.8742 - recall: 0.7774 - f1: 0.8601 - val_loss: 0.2299 - val_accuracy: 0.9240 - val_recall: 0.9266 - val_f1: 0.9242\n",
      "Epoch 58/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2066 - accuracy: 0.8755 - recall: 0.7785 - f1: 0.8616\n",
      "Epoch 00058: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 763ms/step - loss: 0.2064 - accuracy: 0.8755 - recall: 0.7784 - f1: 0.8616 - val_loss: 0.1929 - val_accuracy: 0.9327 - val_recall: 0.9369 - val_f1: 0.9331\n",
      "Epoch 59/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2019 - accuracy: 0.8769 - recall: 0.7824 - f1: 0.8636\n",
      "Epoch 00059: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 77s 771ms/step - loss: 0.2025 - accuracy: 0.8767 - recall: 0.7821 - f1: 0.8635 - val_loss: 0.2146 - val_accuracy: 0.9302 - val_recall: 0.9340 - val_f1: 0.9305\n",
      "Epoch 60/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2036 - accuracy: 0.8751 - recall: 0.7789 - f1: 0.8611\n",
      "Epoch 00060: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.2037 - accuracy: 0.8750 - recall: 0.7790 - f1: 0.8611 - val_loss: 0.2121 - val_accuracy: 0.9294 - val_recall: 0.9309 - val_f1: 0.9295\n",
      "Epoch 61/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2099 - accuracy: 0.8691 - recall: 0.7668 - f1: 0.8535\n",
      "Epoch 00061: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 761ms/step - loss: 0.2099 - accuracy: 0.8691 - recall: 0.7667 - f1: 0.8535 - val_loss: 0.1971 - val_accuracy: 0.9344 - val_recall: 0.9362 - val_f1: 0.9345\n",
      "Epoch 62/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2008 - accuracy: 0.8763 - recall: 0.7787 - f1: 0.8624\n",
      "Epoch 00062: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 760ms/step - loss: 0.2006 - accuracy: 0.8764 - recall: 0.7788 - f1: 0.8625 - val_loss: 0.2223 - val_accuracy: 0.9316 - val_recall: 0.9321 - val_f1: 0.9316\n",
      "Epoch 63/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2079 - accuracy: 0.8706 - recall: 0.7678 - f1: 0.8553\n",
      "Epoch 00063: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 759ms/step - loss: 0.2076 - accuracy: 0.8703 - recall: 0.7673 - f1: 0.8549 - val_loss: 0.2130 - val_accuracy: 0.9305 - val_recall: 0.9317 - val_f1: 0.9306\n",
      "Epoch 64/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2001 - accuracy: 0.8755 - recall: 0.7773 - f1: 0.8613\n",
      "Epoch 00064: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.2003 - accuracy: 0.8755 - recall: 0.7771 - f1: 0.8613 - val_loss: 0.1964 - val_accuracy: 0.9342 - val_recall: 0.9337 - val_f1: 0.9342\n",
      "Epoch 65/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2052 - accuracy: 0.8735 - recall: 0.7727 - f1: 0.8588\n",
      "Epoch 00065: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.2054 - accuracy: 0.8735 - recall: 0.7728 - f1: 0.8588 - val_loss: 0.1970 - val_accuracy: 0.9337 - val_recall: 0.9345 - val_f1: 0.9337\n",
      "Epoch 66/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2060 - accuracy: 0.8750 - recall: 0.7744 - f1: 0.8605\n",
      "Epoch 00066: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.2054 - accuracy: 0.8753 - recall: 0.7751 - f1: 0.8609 - val_loss: 0.2115 - val_accuracy: 0.9304 - val_recall: 0.9303 - val_f1: 0.9304\n",
      "Epoch 67/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2094 - accuracy: 0.8692 - recall: 0.7663 - f1: 0.8537\n",
      "Epoch 00067: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 755ms/step - loss: 0.2097 - accuracy: 0.8691 - recall: 0.7661 - f1: 0.8535 - val_loss: 0.2009 - val_accuracy: 0.9299 - val_recall: 0.9319 - val_f1: 0.9301\n",
      "Epoch 68/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2052 - accuracy: 0.8733 - recall: 0.7772 - f1: 0.8594\n",
      "Epoch 00068: val_loss did not improve from 0.18446\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "100/100 [==============================] - 76s 759ms/step - loss: 0.2046 - accuracy: 0.8737 - recall: 0.7778 - f1: 0.8598 - val_loss: 0.1991 - val_accuracy: 0.9320 - val_recall: 0.9334 - val_f1: 0.9321\n",
      "Epoch 69/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2058 - accuracy: 0.8736 - recall: 0.7758 - f1: 0.8593\n",
      "Epoch 00069: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 760ms/step - loss: 0.2055 - accuracy: 0.8739 - recall: 0.7762 - f1: 0.8597 - val_loss: 0.2092 - val_accuracy: 0.9320 - val_recall: 0.9329 - val_f1: 0.9321\n",
      "Epoch 70/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2077 - accuracy: 0.8678 - recall: 0.7636 - f1: 0.8518\n",
      "Epoch 00070: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.2074 - accuracy: 0.8680 - recall: 0.7639 - f1: 0.8520 - val_loss: 0.1984 - val_accuracy: 0.9365 - val_recall: 0.9379 - val_f1: 0.9366\n",
      "Epoch 71/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1997 - accuracy: 0.8756 - recall: 0.7772 - f1: 0.8615\n",
      "Epoch 00071: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.2000 - accuracy: 0.8755 - recall: 0.7772 - f1: 0.8614 - val_loss: 0.2055 - val_accuracy: 0.9333 - val_recall: 0.9350 - val_f1: 0.9334\n",
      "Epoch 72/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2067 - accuracy: 0.8716 - recall: 0.7719 - f1: 0.8570\n",
      "Epoch 00072: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 756ms/step - loss: 0.2066 - accuracy: 0.8716 - recall: 0.7719 - f1: 0.8570 - val_loss: 0.2080 - val_accuracy: 0.9340 - val_recall: 0.9350 - val_f1: 0.9340\n",
      "Epoch 73/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2058 - accuracy: 0.8743 - recall: 0.7758 - f1: 0.8602\n",
      "Epoch 00073: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 756ms/step - loss: 0.2060 - accuracy: 0.8739 - recall: 0.7748 - f1: 0.8596 - val_loss: 0.2116 - val_accuracy: 0.9286 - val_recall: 0.9296 - val_f1: 0.9286\n",
      "Epoch 74/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2009 - accuracy: 0.8762 - recall: 0.7788 - f1: 0.8623\n",
      "Epoch 00074: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 759ms/step - loss: 0.2008 - accuracy: 0.8761 - recall: 0.7787 - f1: 0.8622 - val_loss: 0.2116 - val_accuracy: 0.9282 - val_recall: 0.9303 - val_f1: 0.9284\n",
      "Epoch 75/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1987 - accuracy: 0.8776 - recall: 0.7805 - f1: 0.8639\n",
      "Epoch 00075: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 757ms/step - loss: 0.1991 - accuracy: 0.8773 - recall: 0.7799 - f1: 0.8635 - val_loss: 0.2077 - val_accuracy: 0.9307 - val_recall: 0.9320 - val_f1: 0.9308\n",
      "Epoch 76/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2077 - accuracy: 0.8703 - recall: 0.7700 - f1: 0.8554\n",
      "Epoch 00076: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 78s 777ms/step - loss: 0.2074 - accuracy: 0.8704 - recall: 0.7699 - f1: 0.8555 - val_loss: 0.2006 - val_accuracy: 0.9329 - val_recall: 0.9347 - val_f1: 0.9330\n",
      "Epoch 77/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2074 - accuracy: 0.8724 - recall: 0.7728 - f1: 0.8576\n",
      "Epoch 00077: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.2067 - accuracy: 0.8728 - recall: 0.7734 - f1: 0.8581 - val_loss: 0.2090 - val_accuracy: 0.9316 - val_recall: 0.9327 - val_f1: 0.9317\n",
      "Epoch 78/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2012 - accuracy: 0.8754 - recall: 0.7754 - f1: 0.8610\n",
      "Epoch 00078: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 76s 757ms/step - loss: 0.2013 - accuracy: 0.8752 - recall: 0.7751 - f1: 0.8608 - val_loss: 0.2102 - val_accuracy: 0.9316 - val_recall: 0.9324 - val_f1: 0.9317\n",
      "Epoch 79/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2027 - accuracy: 0.8761 - recall: 0.7803 - f1: 0.8624\n",
      "Epoch 00079: val_loss did not improve from 0.18446\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "100/100 [==============================] - 76s 757ms/step - loss: 0.2027 - accuracy: 0.8760 - recall: 0.7801 - f1: 0.8624 - val_loss: 0.2060 - val_accuracy: 0.9384 - val_recall: 0.9393 - val_f1: 0.9384\n",
      "Epoch 80/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1989 - accuracy: 0.8746 - recall: 0.7748 - f1: 0.8602\n",
      "Epoch 00080: val_loss did not improve from 0.18446\n",
      "100/100 [==============================] - 78s 782ms/step - loss: 0.1989 - accuracy: 0.8747 - recall: 0.7749 - f1: 0.8603 - val_loss: 0.2020 - val_accuracy: 0.9329 - val_recall: 0.9340 - val_f1: 0.9329\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 80\n",
    "STEPS_PER_EPOCH = 100\n",
    "batch_size = 100\n",
    "\n",
    "model = resnet_unet_model(\n",
    "    shape=(64, 64, 3),\n",
    "    batch_normalization=batch_normalization,\n",
    "    activation=activation,\n",
    "    dropout=dropout,\n",
    "    amsgrad=amsgrad,\n",
    "    lr=lr,\n",
    "    noise=noise,\n",
    ")\n",
    "# Train the model\n",
    "history = model.train(\n",
    "    EPOCHS, STEPS_PER_EPOCH, n_train, n_val, batch_size, data_aug_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-90ab1b6361d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/trainHistoryDict_best_model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile_pi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_pi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "with open('/trainHistoryDict_best_model', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_str = str(datetime.now()).replace(\" \", \"_\").replace(\".\", \"_\").replace(\":\", \"_\")\n",
    "if batch_normalization:\n",
    "    specific_name = now_str + \"_batchnorm\" + \"_dropout{:2.2f}\".format(dropout)\n",
    "else:\n",
    "    specific_name = now_str + \"_dropout{:2.2f}\".format(dropout)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(history.history[\"f1\"])\n",
    "plt.plot(history.history[\"val_f1\"])\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"ResNet-UNet: training vs validation f1 and loss\")\n",
    "plt.ylabel(\"value\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend(\n",
    "    [\"train_f1\", \"val_f1\", \"loss\", \"val_loss\"],\n",
    "    loc=\"lower left\",\n",
    "    bbox_to_anchor=(0.0, -0.07),\n",
    "    ncol=4,\n",
    "    borderaxespad=0,\n",
    "    frameon=False,\n",
    ")\n",
    "plt.savefig(specific_name + \"_resnet_unet_metrics_f1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(history.history[\"loss\"][1:], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"][1:], label=\"val_loss\")\n",
    "plt.plot(history.history[\"accuracy\"][1:], label=\"train_acc\")\n",
    "plt.plot(history.history[\"val_accuracy\"][1:], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"Unet_batchnorm.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 70, 70, 3)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 32, 32, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 32, 32, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 32, 32, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 34, 34, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 16, 16, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 16, 16, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 16, 16, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 16, 16, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 16, 16, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 16, 16, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 16, 16, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 16, 16, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 16, 16, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 16, 16, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 16, 16, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 16, 16, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 16, 16, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 16, 16, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 16, 16, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 8, 8, 128)    32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 8, 8, 128)    0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 8, 8, 128)    0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 8, 8, 512)    131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 8, 8, 512)    0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 8, 8, 512)    0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 8, 8, 128)    0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 8, 8, 128)    0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 8, 8, 512)    0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 8, 8, 512)    0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 8, 8, 128)    0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 8, 8, 128)    0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 8, 8, 512)    0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 8, 8, 512)    0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 8, 8, 128)    0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 8, 8, 128)    0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 8, 8, 512)    0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 8, 8, 512)    0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 4, 4, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 4, 4, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 4, 4, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 4, 4, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 4, 4, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 4, 4, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 4, 4, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 4, 4, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 4, 4, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 4, 4, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 4, 4, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 4, 4, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 4, 4, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 4, 4, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 4, 4, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 4, 4, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 4, 4, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 4, 4, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 4, 4, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 4, 4, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 4, 4, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 4, 4, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 4, 4, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 4, 4, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 2, 2, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 2, 2, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 2, 2, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 2, 2, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 2, 2, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 2, 2, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 2, 2, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 2, 2, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 2, 2, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 2, 2, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 2, 2, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 2, 2, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 2048)   0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 1, 1, 128)    2359424     max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 2, 2, 64)     131136      conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 2, 2, 64)     256         conv2d_transpose_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 2, 2, 2112)   0           batch_normalization_6[0][0]      \n",
      "                                                                 conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 2, 2, 128)    2433152     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 4, 4, 64)     131136      conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 4, 4, 64)     256         conv2d_transpose_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 4, 4, 1088)   0           batch_normalization_7[0][0]      \n",
      "                                                                 conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 4, 4, 128)    1253504     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTrans (None, 8, 8, 64)     131136      conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 64)     256         conv2d_transpose_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 8, 8, 576)    0           batch_normalization_8[0][0]      \n",
      "                                                                 conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 8, 8, 64)     331840      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTrans (None, 16, 16, 16)   16400       conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 16)   64          conv2d_transpose_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 16, 16, 272)  0           batch_normalization_9[0][0]      \n",
      "                                                                 conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   78368       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_10 (Conv2DTran (None, 32, 32, 32)   16416       conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_transpose_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 32)   9248        batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_11 (Conv2DTran (None, 64, 64, 8)    4104        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 64, 64, 8)    32          conv2d_transpose_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 8)    584         batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 1)    9           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4096)         0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           131104      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32)           0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            66          leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2)            0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 2)            0           dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 30,616,331\n",
      "Trainable params: 30,562,715\n",
      "Non-trainable params: 53,616\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from helpers import *\n",
    "\n",
    "# from resnet_unet_model import resnet_unet_model\n",
    "\n",
    "# Instantiate the model\n",
    "model = resnet_unet_model(\n",
    "    shape=(64, 64, 3),\n",
    "    batch_normalization=batch_normalization,\n",
    "    activation=activation,\n",
    "    dropout=dropout,\n",
    "    amsgrad=amsgrad,\n",
    "    lr=lr,\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model.load(\"model_batch_relu_80_dropout_0_052_0.849586_0.933300.h5\")\n",
    "\n",
    "# Print a summary to make sure the correct model is used\n",
    "model.model.summary()\n",
    "\n",
    "# We add all test images to an array, used later for generating a submission\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = (\n",
    "        \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "    )\n",
    "    image_filenames.append(image_filename)\n",
    "\n",
    "# Set-up submission filename\n",
    "submission_filename = \"resnet_unet.csv\"\n",
    "# Generates the submission\n",
    "generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
