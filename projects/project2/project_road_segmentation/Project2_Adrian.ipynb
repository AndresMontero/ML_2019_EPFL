{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext lab_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2516659968480773110\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3156787200\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13616420986963603714\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 980M, pci bus id: 0000:01:00.0, compute capability: 5.2\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Create some tensors\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as scipy\n",
    "import os\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.applications import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend as K\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary for our model\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    LeakyReLU,\n",
    ")\n",
    "from tensorflow.compat.v2.keras.layers import BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "# keras, model definition...\n",
    "cb = TQDMNotebookCallback()\n",
    "setattr(cb, \"on_train_batch_begin\", lambda x, y: None)\n",
    "setattr(cb, \"on_train_batch_end\", lambda x, y: None)\n",
    "\n",
    "# model.fit(X_train, Y_train, verbose=0, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 85 images\n",
      "Loading 85 groundtruth images\n"
     ]
    }
   ],
   "source": [
    "# Load a set of images\n",
    "root_dir = \"data/\"\n",
    "\n",
    "# Select the directory for the images and load them\n",
    "image_dir_train = root_dir + \"training/images/\"\n",
    "files = os.listdir(image_dir_train)\n",
    "n_train = len(files)\n",
    "\n",
    "print(\"Loading \" + str(n_train) + \" images\")\n",
    "imgs_train = np.asarray(\n",
    "    [load_image(image_dir_train + files[i]) for i in range(n_train)]\n",
    ")\n",
    "\n",
    "# Select the directory for groundtruth images and load them\n",
    "gt_dir_train = root_dir + \"training/groundtruth/\"\n",
    "print(\"Loading \" + str(n_train) + \" groundtruth images\")\n",
    "gt_imgs_train = np.asarray(\n",
    "    [load_image(gt_dir_train + files[i]) for i in range(n_train)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 400, 400, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 400, 400)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_imgs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 400\n",
    "\n",
    "# We separate the images from the groundtruth images\n",
    "img_patches_train = [\n",
    "    img_crop(imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "gt_patches_train = [\n",
    "    img_crop(gt_imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "\n",
    "# Linearize the list and labeling them X and Y\n",
    "X_train = np.asarray(\n",
    "    [\n",
    "        img_patches_train[i][j]\n",
    "        for i in range(len(img_patches_train))\n",
    "        for j in range(len(img_patches_train[i]))\n",
    "    ]\n",
    ")\n",
    "Y_train = np.asarray(\n",
    "    [\n",
    "        gt_patches_train[i][j]\n",
    "        for i in range(len(gt_patches_train))\n",
    "        for j in range(len(gt_patches_train[i]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 400, 400, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 400, 400)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 15 images\n",
      "Loading 15 groundtruth images\n"
     ]
    }
   ],
   "source": [
    "# Select the directory for the images and load them\n",
    "image_dir_val = root_dir + \"validating/images/\"\n",
    "files = os.listdir(image_dir_val)\n",
    "n_val = len(files)\n",
    "\n",
    "print(\"Loading \" + str(n_val) + \" images\")\n",
    "imgs_val = np.asarray([load_image(image_dir_val + files[i]) for i in range(n_val)])\n",
    "\n",
    "# Select the directory for groundtruth images and load them\n",
    "gt_dir_val = root_dir + \"validating/groundtruth/\"\n",
    "print(\"Loading \" + str(n_val) + \" groundtruth images\")\n",
    "gt_imgs_val = np.asarray([load_image(gt_dir_val + files[i]) for i in range(n_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 400, 400, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 400, 400)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 400\n",
    "\n",
    "# We separate the images from the groundtruth images\n",
    "img_patches_val = [img_crop(imgs_val[i], image_size, image_size) for i in range(n_val)]\n",
    "gt_patches_val = [\n",
    "    img_crop(gt_imgs_val[i], image_size, image_size) for i in range(n_val)\n",
    "]\n",
    "\n",
    "# Linearize the list and labeling them X and Y\n",
    "X_val = np.asarray(\n",
    "    [\n",
    "        img_patches_val[i][j]\n",
    "        for i in range(len(img_patches_val))\n",
    "        for j in range(len(img_patches_val[i]))\n",
    "    ]\n",
    ")\n",
    "Y_val = np.asarray(\n",
    "    [\n",
    "        gt_patches_val[i][j]\n",
    "        for i in range(len(gt_patches_val))\n",
    "        for j in range(len(gt_patches_val[i]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 400, 400, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 400, 400)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating mini-batch and running data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatch(X, Y, n):\n",
    "\n",
    "    # Fix the seed\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # We define the window size of 72, batch size of 100 (empirically chosen)\n",
    "    # and patch size should correspond to 16\n",
    "    w_size = 64\n",
    "    batch_size = 100\n",
    "    patch_size = 16\n",
    "    num_images = n\n",
    "\n",
    "    while True:\n",
    "        # Generate one minibatch\n",
    "        batch_image = np.empty((batch_size, w_size, w_size, 3))\n",
    "        batch_label = np.empty((batch_size, 2))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            # Select a random index represnting an image\n",
    "            random_index = np.random.choice(num_images)\n",
    "\n",
    "            # Width of original image\n",
    "            width = 400\n",
    "\n",
    "            # Sample a random window from the image\n",
    "            random_sample = np.random.randint(w_size // 2, width - w_size // 2, 2)\n",
    "\n",
    "            # Create a sub image of size 72x72\n",
    "            sampled_image = X[random_index][\n",
    "                random_sample[0] - w_size // 2 : random_sample[0] + w_size // 2,\n",
    "                random_sample[1] - w_size // 2 : random_sample[1] + w_size // 2,\n",
    "            ]\n",
    "\n",
    "            # Take its corresponding ground-truth image\n",
    "            correspond_ground_truth = Y[random_index][\n",
    "                random_sample[0] - patch_size // 2 : random_sample[0] + patch_size // 2,\n",
    "                random_sample[1] - patch_size // 2 : random_sample[1] + patch_size // 2,\n",
    "            ]\n",
    "\n",
    "            # We set in the label depending on the threshold of 0.25\n",
    "            # The label becomes either 0 or 1 by applying to_categorical with parameter 2\n",
    "            label = to_categorical(\n",
    "                (np.array([np.mean(correspond_ground_truth)]) > 0.25) * 1, 2\n",
    "            )\n",
    "\n",
    "            # The image augmentation is based on both flipping and rotating (randomly in steps of 45°)\n",
    "            # Random vertical and horizontal flip\n",
    "            if np.random.choice(2) == 1:\n",
    "                sampled_image = np.flipud(sampled_image)\n",
    "\n",
    "            if np.random.choice(2) == 1:\n",
    "                sampled_image = np.fliplr(sampled_image)\n",
    "\n",
    "            # Random rotation in steps of 45°\n",
    "            rotations = [0, 45, 90, 135, 180, 225, 270, 315]\n",
    "\n",
    "            # We select a rotation degree randomly\n",
    "            rotation_choice = np.random.choice(len(rotations))\n",
    "\n",
    "            # Rotate it using the random value (uses the scipy library)\n",
    "            sampled_image = scipy.ndimage.rotate(\n",
    "                sampled_image,\n",
    "                rotations[rotation_choice],\n",
    "                order=1,\n",
    "                reshape=False,\n",
    "                mode=\"reflect\",\n",
    "            )\n",
    "\n",
    "            # We put in the sub image and its corresponding label before yielding it\n",
    "            batch_image[i] = sampled_image\n",
    "            batch_label[i] = label\n",
    "\n",
    "        # Yield the mini_batch to the model\n",
    "        yield (batch_image, batch_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the class (Same as in resnet_unet_model.py, but provided here for better readability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet_unet_model:\n",
    "\n",
    "    # Initialize the class\n",
    "    def __init__(self, shape, batch_normalization, activation):\n",
    "        self.shape = shape\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.activation = activation\n",
    "        self.model = self.initialize_resnet_unet_model(\n",
    "            shape, batch_normalization, activation\n",
    "        )\n",
    "\n",
    "    def conv_act(self, inputs, out_filters, activation=\"relu\"):\n",
    "        return Conv2D(\n",
    "            filters=out_filters,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "        )(inputs)\n",
    "\n",
    "    def decoder(\n",
    "        self,\n",
    "        inputs,\n",
    "        mid_filters=512,\n",
    "        out_filters=256,\n",
    "        activation=\"relu\",\n",
    "        block_name=\"decoder\",\n",
    "    ):\n",
    "        with K.name_scope(block_name):\n",
    "            conv = self.conv_act(inputs, mid_filters, activation)\n",
    "            conv_tr = Conv2DTranspose(\n",
    "                filters=out_filters,\n",
    "                activation=activation,\n",
    "                kernel_size=4,\n",
    "                strides=2,\n",
    "                padding=\"same\",\n",
    "            )(conv)\n",
    "        return conv_tr\n",
    "\n",
    "    def initialize_resnet_unet_model(self, shape, batch_normalization, activation):\n",
    "        #         print(activation)\n",
    "\n",
    "        # INPUT\n",
    "        # shape     - Size of the input images\n",
    "        # OUTPUT\n",
    "        # model    - Compiled CNN\n",
    "\n",
    "        # Define parameters\n",
    "        max_pooling_size = 2\n",
    "        max_pooling_strd = 2\n",
    "\n",
    "        # Load a pretrained ResNet\n",
    "        num_classes = 2\n",
    "        resnet50 = ResNet50(\n",
    "            include_top=False,\n",
    "            weights=\"imagenet\",\n",
    "            classes=num_classes,\n",
    "            input_shape=shape,\n",
    "        )\n",
    "        resnet50.compile(optimizer=Adam(lr=1e-3), loss=\"binary_crossentropy\")\n",
    "\n",
    "        # ResNet convolution outputs\n",
    "        conv5_out = resnet50.get_layer(\"conv5_block3_out\").output\n",
    "        conv4_out = resnet50.get_layer(\"conv4_block6_out\").output\n",
    "        conv3_out = resnet50.get_layer(\"conv3_block4_out\").output\n",
    "        conv2_out = resnet50.get_layer(\"conv2_block3_out\").output\n",
    "\n",
    "        pool = MaxPooling2D(max_pooling_size, strides=max_pooling_strd, padding=\"same\")(\n",
    "            resnet50.get_output_at(0)\n",
    "        )\n",
    "        dec_center = self.decoder(\n",
    "            pool,\n",
    "            mid_filters=shape[0] * 2,\n",
    "            out_filters=shape[0],\n",
    "            block_name=\"decoder_center\",\n",
    "        )\n",
    "        cat1 = Concatenate()([dec_center, conv5_out])\n",
    "        dec5 = self.decoder(\n",
    "            cat1,\n",
    "            mid_filters=int(shape[0] * 2),\n",
    "            out_filters=int(shape[0]),\n",
    "            block_name=\"decoder5\",\n",
    "        )\n",
    "        cat2 = Concatenate()([dec5, conv4_out])\n",
    "        dec4 = self.decoder(\n",
    "            cat2,\n",
    "            mid_filters=int(shape[0] * 2),\n",
    "            out_filters=int(shape[0]),\n",
    "            block_name=\"decoder4\",\n",
    "        )\n",
    "        cat3 = Concatenate()([dec4, conv3_out])\n",
    "        dec3 = self.decoder(\n",
    "            cat3,\n",
    "            mid_filters=int(shape[0]),\n",
    "            out_filters=int(shape[0] // 4),\n",
    "            block_name=\"decoder3\",\n",
    "        )\n",
    "        cat2 = Concatenate()([dec3, conv2_out])\n",
    "        dec2 = self.decoder(\n",
    "            cat2,\n",
    "            mid_filters=int(shape[0] // 2),\n",
    "            out_filters=int(shape[0] // 2),\n",
    "            block_name=\"decoder2\",\n",
    "        )\n",
    "        dec1 = self.decoder(\n",
    "            dec2,\n",
    "            mid_filters=int(shape[0] // 2),\n",
    "            out_filters=int(shape[0] // 8),\n",
    "            block_name=\"decoder1\",\n",
    "        )\n",
    "        dec0 = self.conv_act(dec1, out_filters=int(shape[0] // 8))\n",
    "        conv_f = Conv2D(1, 1, activation=\"sigmoid\", padding=\"same\")(dec0)\n",
    "        flatten_0 = Flatten()(conv_f)\n",
    "        dense_0 = Dense(\n",
    "            shape[0] / 2,\n",
    "            kernel_regularizer=l2(0.000001),\n",
    "            activity_regularizer=l2(0.000001),\n",
    "        )(flatten_0)\n",
    "        lk_relu_0 = LeakyReLU(alpha=0.1)(dense_0)\n",
    "        dropout_0 = Dropout(0.5)(lk_relu_0)\n",
    "        dense_1 = Dense(\n",
    "            2, kernel_regularizer=l2(0.000001), activity_regularizer=l2(0.000001)\n",
    "        )(dropout_0)\n",
    "        output = Activation(\"sigmoid\")(dense_1)\n",
    "\n",
    "        model = Model(inputs=resnet50.get_input_at(0), outputs=output)\n",
    "\n",
    "        # Compile the model using the binary crossentropy loss and the Adam optimizer for it\n",
    "        # We used the accuracy as a metric, but F1 score is also a plausible choice\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(lr=0.001),\n",
    "            metrics=[\"accuracy\", recall, f1],\n",
    "        )\n",
    "\n",
    "        # Print a summary of the model to see what has been generated\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # Early stopping callback after 10 steps\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\", min_delta=0, patience=10, verbose=1, mode=\"auto\"\n",
    "        )\n",
    "\n",
    "        # Reduce learning rate on plateau after 4 steps\n",
    "        lr_callback = ReduceLROnPlateau(\n",
    "            monitor=\"loss\", factor=0.5, patience=4, verbose=1, mode=\"auto\"\n",
    "        )\n",
    "\n",
    "        # Place the callbacks in a list to be used when training\n",
    "        #         callbacks = [cb, early_stopping, lr_callback]\n",
    "        callbacks = [early_stopping, lr_callback]\n",
    "\n",
    "        # Train the model using the previously defined functions and callbacks\n",
    "        history = self.model.fit_generator(\n",
    "            create_minibatch(X_train, Y_train, n_train),\n",
    "            steps_per_epoch=100,\n",
    "            epochs=EPOCHS,\n",
    "            use_multiprocessing=False,\n",
    "            workers=1,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "            validation_data=create_minibatch(X_val, Y_val, n_val),\n",
    "            validation_steps=100,\n",
    "        )\n",
    "\n",
    "    def classify(self, X):\n",
    "        # Subdivide the images into blocks with a stride and patch_size of 16\n",
    "        img_patches = create_patches(X, 16, 16, padding=28)\n",
    "\n",
    "        # Predict\n",
    "        predictions = self.model.predict(img_patches)\n",
    "        predictions = (predictions[:, 0] < predictions[:, 1]) * 1\n",
    "\n",
    "        # Regroup patches into images\n",
    "        return group_patches(predictions, X.shape[0])\n",
    "\n",
    "    def load(self, filename):\n",
    "        # Load the model (used for submission)\n",
    "        dependencies = {\"recall\": recall, \"f1\": f1}\n",
    "        self.model = load_model(filename, custom_objects=dependencies)\n",
    "\n",
    "    def save(self, filename):\n",
    "        # Save the model (used to then load to submit)\n",
    "        self.model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 70, 70, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 32, 32, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 32, 32, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 32, 32, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 34, 34, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 16, 16, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 16, 16, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 16, 16, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 16, 16, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 16, 16, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 16, 16, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 16, 16, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 16, 16, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 16, 16, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 16, 16, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 16, 16, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 16, 16, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 16, 16, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 16, 16, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 16, 16, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 8, 8, 128)    32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 8, 8, 128)    0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 8, 8, 128)    0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 8, 8, 512)    131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 8, 8, 512)    0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 8, 8, 512)    0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 8, 8, 128)    0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 8, 8, 128)    0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 8, 8, 512)    0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 8, 8, 512)    0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 8, 8, 128)    0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 8, 8, 128)    0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 8, 8, 512)    0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 8, 8, 512)    0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 8, 8, 128)    0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 8, 8, 128)    0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 8, 8, 512)    0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 8, 8, 512)    0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 4, 4, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 4, 4, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 4, 4, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 4, 4, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 4, 4, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 4, 4, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 4, 4, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 4, 4, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 4, 4, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 4, 4, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 4, 4, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 4, 4, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 4, 4, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 4, 4, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 4, 4, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 4, 4, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 4, 4, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 4, 4, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 4, 4, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 4, 4, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 4, 4, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 4, 4, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 4, 4, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 4, 4, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 2, 2, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 2, 2, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 2, 2, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 2, 2, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 2, 2, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 2, 2, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 2, 2, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 2, 2, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 2, 2, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 2, 2, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 2, 2, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 2, 2, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 2048)   0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 1, 1, 128)    2359424     max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 2, 2, 64)     131136      conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2, 2, 2112)   0           conv2d_transpose[0][0]           \n",
      "                                                                 conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 2, 2, 128)    2433152     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 4, 4, 64)     131136      conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 4, 1088)   0           conv2d_transpose_1[0][0]         \n",
      "                                                                 conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 4, 4, 128)    1253504     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 8, 8, 64)     131136      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8, 8, 576)    0           conv2d_transpose_2[0][0]         \n",
      "                                                                 conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 64)     331840      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 16, 16, 16)   16400       conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 16, 16, 272)  0           conv2d_transpose_3[0][0]         \n",
      "                                                                 conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 32)   78368       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 32, 32, 32)   16416       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 64, 64, 8)    4104        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 8)    584         conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 1)    9           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4096)         0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           131104      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 32)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32)           0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            66          dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 2)            0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 30,615,339\n",
      "Trainable params: 30,562,219\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "Executing op Shape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op StridedSlice in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Pad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2D in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op BiasAdd in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op FusedBatchNormV3 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignSubVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Relu in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MaxPool in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2D in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2D in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MaxPool in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Shape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2DBackpropInput in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ConcatV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sigmoid in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Square in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RealDiv in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LeakyRelu in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op GreaterEqual in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Minimum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Maximum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Log in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Neg in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mean in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DivNoNan in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op BroadcastGradientArgs in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Tile in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Maximum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op FloorDiv in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reciprocal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ZerosLike in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Select in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LessEqual in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op SigmoidGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op BiasAddGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LeakyReluGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ShapeN in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2DBackpropInput in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2DBackpropFilter in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReluGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2DBackpropFilter in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2D in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op SplitV in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MaxPoolGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op FusedBatchNormGradV3 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2DBackpropInput in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2DBackpropFilter in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2DBackpropInput in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2DBackpropFilter in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MaxPoolGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Slice in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Slice in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Pow in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sqrt in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RealDiv in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ResourceApplyAdam in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignAddVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Greater in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Equal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignAddVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Round in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.5518 - accuracy: 0.7311 - recall: 0.7115 - f1: 0.7255Executing op FusedBatchNormV3 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "100/100 [==============================] - 73s 732ms/step - loss: 0.5501 - accuracy: 0.7316 - recall: 0.7118 - f1: 0.7260 - val_loss: 0.8039 - val_accuracy: 0.7517 - val_recall: 0.7517 - val_f1: 0.7517\n",
      "Epoch 2/200\n",
      "100/100 [==============================] - 68s 683ms/step - loss: 0.4226 - accuracy: 0.7932 - recall: 0.7438 - f1: 0.7816 - val_loss: 1.4293 - val_accuracy: 0.7561 - val_recall: 0.7561 - val_f1: 0.7561\n",
      "Epoch 3/200\n",
      "100/100 [==============================] - 70s 700ms/step - loss: 0.3756 - accuracy: 0.8328 - recall: 0.8198 - f1: 0.8305 - val_loss: 0.9675 - val_accuracy: 0.7472 - val_recall: 0.7472 - val_f1: 0.7472\n",
      "Epoch 4/200\n",
      "100/100 [==============================] - 68s 679ms/step - loss: 0.3193 - accuracy: 0.8685 - recall: 0.8547 - f1: 0.8665 - val_loss: 0.5970 - val_accuracy: 0.7572 - val_recall: 0.7572 - val_f1: 0.7572\n",
      "Epoch 5/200\n",
      "100/100 [==============================] - 68s 683ms/step - loss: 0.2976 - accuracy: 0.8783 - recall: 0.8660 - f1: 0.8767 - val_loss: 0.7386 - val_accuracy: 0.2543 - val_recall: 0.2575 - val_f1: 0.2567\n",
      "Epoch 6/200\n",
      "100/100 [==============================] - 72s 721ms/step - loss: 0.2940 - accuracy: 0.8827 - recall: 0.8775 - f1: 0.8821 - val_loss: 0.6133 - val_accuracy: 0.7358 - val_recall: 0.7236 - val_f1: 0.7326\n",
      "Epoch 7/200\n",
      "100/100 [==============================] - 69s 688ms/step - loss: 0.2577 - accuracy: 0.8936 - recall: 0.8883 - f1: 0.8931 - val_loss: 0.7482 - val_accuracy: 0.4767 - val_recall: 0.4394 - val_f1: 0.4563\n",
      "Epoch 8/200\n",
      "100/100 [==============================] - 68s 675ms/step - loss: 0.2542 - accuracy: 0.8970 - recall: 0.8928 - f1: 0.8965 - val_loss: 0.8088 - val_accuracy: 0.5266 - val_recall: 0.5180 - val_f1: 0.5225\n",
      "Epoch 9/200\n",
      "100/100 [==============================] - 69s 693ms/step - loss: 0.2339 - accuracy: 0.9061 - recall: 0.9033 - f1: 0.9058 - val_loss: 1.0953 - val_accuracy: 0.5288 - val_recall: 0.5198 - val_f1: 0.5245\n",
      "Epoch 10/200\n",
      "100/100 [==============================] - 69s 694ms/step - loss: 0.2424 - accuracy: 0.8990 - recall: 0.8948 - f1: 0.8985 - val_loss: 0.6495 - val_accuracy: 0.6833 - val_recall: 0.6815 - val_f1: 0.6827\n",
      "Epoch 11/200\n",
      "100/100 [==============================] - 67s 675ms/step - loss: 0.2325 - accuracy: 0.9071 - recall: 0.9033 - f1: 0.9067 - val_loss: 0.7811 - val_accuracy: 0.6765 - val_recall: 0.6731 - val_f1: 0.6754\n",
      "Epoch 12/200\n",
      "100/100 [==============================] - 67s 673ms/step - loss: 0.2324 - accuracy: 0.9086 - recall: 0.9076 - f1: 0.9085 - val_loss: 0.3102 - val_accuracy: 0.8686 - val_recall: 0.8682 - val_f1: 0.8685\n",
      "Epoch 13/200\n",
      "100/100 [==============================] - 68s 679ms/step - loss: 0.2230 - accuracy: 0.9125 - recall: 0.9122 - f1: 0.9125 - val_loss: 0.6992 - val_accuracy: 0.7665 - val_recall: 0.7665 - val_f1: 0.7665\n",
      "Epoch 14/200\n",
      "100/100 [==============================] - 67s 673ms/step - loss: 0.2153 - accuracy: 0.9093 - recall: 0.9081 - f1: 0.9092 - val_loss: 0.4400 - val_accuracy: 0.8192 - val_recall: 0.8170 - val_f1: 0.8188\n",
      "Epoch 15/200\n",
      "100/100 [==============================] - 68s 684ms/step - loss: 0.2163 - accuracy: 0.9117 - recall: 0.9109 - f1: 0.9116 - val_loss: 0.3089 - val_accuracy: 0.8577 - val_recall: 0.8522 - val_f1: 0.8569\n",
      "Epoch 16/200\n",
      "100/100 [==============================] - 67s 675ms/step - loss: 0.2077 - accuracy: 0.9160 - recall: 0.9162 - f1: 0.9160 - val_loss: 0.4083 - val_accuracy: 0.8475 - val_recall: 0.8477 - val_f1: 0.8475\n",
      "Epoch 17/200\n",
      "100/100 [==============================] - 67s 674ms/step - loss: 0.2017 - accuracy: 0.9191 - recall: 0.9200 - f1: 0.9192 - val_loss: 0.2797 - val_accuracy: 0.8814 - val_recall: 0.8801 - val_f1: 0.8812\n",
      "Epoch 18/200\n",
      "100/100 [==============================] - 68s 676ms/step - loss: 0.2109 - accuracy: 0.9151 - recall: 0.9150 - f1: 0.9151 - val_loss: 0.3291 - val_accuracy: 0.8609 - val_recall: 0.8592 - val_f1: 0.8607\n",
      "Epoch 19/200\n",
      "100/100 [==============================] - 68s 682ms/step - loss: 0.2228 - accuracy: 0.9075 - recall: 0.9085 - f1: 0.9076 - val_loss: 2.1452 - val_accuracy: 0.7583 - val_recall: 0.7583 - val_f1: 0.7583\n",
      "Epoch 20/200\n",
      "100/100 [==============================] - 68s 676ms/step - loss: 0.2976 - accuracy: 0.8777 - recall: 0.8780 - f1: 0.8777 - val_loss: 0.3304 - val_accuracy: 0.8778 - val_recall: 0.8765 - val_f1: 0.8776\n",
      "Epoch 21/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2448 - accuracy: 0.9009 - recall: 0.9014 - f1: 0.9009\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "100/100 [==============================] - 68s 680ms/step - loss: 0.2439 - accuracy: 0.9014 - recall: 0.9019 - f1: 0.9014 - val_loss: 0.4784 - val_accuracy: 0.7706 - val_recall: 0.7682 - val_f1: 0.7700\n",
      "Epoch 22/200\n",
      "100/100 [==============================] - 68s 682ms/step - loss: 0.1973 - accuracy: 0.9204 - recall: 0.9210 - f1: 0.9205 - val_loss: 0.2878 - val_accuracy: 0.8713 - val_recall: 0.8722 - val_f1: 0.8715\n",
      "Epoch 23/200\n",
      "100/100 [==============================] - 68s 676ms/step - loss: 0.1977 - accuracy: 0.9233 - recall: 0.9247 - f1: 0.9234 - val_loss: 0.2733 - val_accuracy: 0.8770 - val_recall: 0.8781 - val_f1: 0.8771\n",
      "Epoch 24/200\n",
      "100/100 [==============================] - 67s 671ms/step - loss: 0.1905 - accuracy: 0.9257 - recall: 0.9271 - f1: 0.9258 - val_loss: 0.2675 - val_accuracy: 0.8806 - val_recall: 0.8823 - val_f1: 0.8808\n",
      "Epoch 25/200\n",
      "100/100 [==============================] - 67s 670ms/step - loss: 0.1835 - accuracy: 0.9278 - recall: 0.9293 - f1: 0.9279 - val_loss: 0.4504 - val_accuracy: 0.8337 - val_recall: 0.8341 - val_f1: 0.8338\n",
      "Epoch 26/200\n",
      "100/100 [==============================] - 67s 666ms/step - loss: 0.1769 - accuracy: 0.9314 - recall: 0.9318 - f1: 0.9314 - val_loss: 0.3743 - val_accuracy: 0.8420 - val_recall: 0.8426 - val_f1: 0.8421\n",
      "Epoch 27/200\n",
      " 25/100 [======>.......................] - ETA: 39s - loss: 0.1646 - accuracy: 0.9364 - recall: 0.9368 - f1: 0.9364"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "# We define the number of epochs and steps per epochs\n",
    "EPOCHS = 200\n",
    "# STEPS_PER_EPOCH = 150\n",
    "batch_normalization = False\n",
    "activation = \"LeakyReLU\"\n",
    "model = resnet_unet_model(\n",
    "    shape=(64, 64, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "# Train the model\n",
    "history = model.train()\n",
    "model.save(\"resnet_unet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history.history[\"loss\"]\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "N = EPOCHS\n",
    "print(N)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "plt.savefig(\"model1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "batch_normalization = False\n",
    "activation = \"relu\"\n",
    "model = resnet_unet_model(\n",
    "    shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "# Train the model with batch\n",
    "model.train()\n",
    "model.save(\"no_batch_relu_validation.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "batch_normalization = True\n",
    "activation = \"LeakyReLU\"\n",
    "model = resnet_unet_model(\n",
    "    shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "# Train the model with batch\n",
    "model.train()\n",
    "model.save(\"batch_LeakyReLU_validation.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "batch_normalization = True\n",
    "EPOCHS = 200\n",
    "\n",
    "activation = \"relu\"\n",
    "model = resnet_unet_model(\n",
    "    shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "# Train the model with batch\n",
    "history = model.train()\n",
    "model.save(\"batch_relu_validation_200.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "N = EPOCHS\n",
    "print(N)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "plt.savefig(\"batch_relu_validation_200.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "\n",
    "# from resnet_unet_model import resnet_unet_model\n",
    "\n",
    "# Instantiate the model\n",
    "batch_normalization = True\n",
    "activation = \"relu\"\n",
    "model = resnet_unet_model(\n",
    "    shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model.load(\"batch_relu_validation_200.h5\")\n",
    "\n",
    "# Print a summary to make sure the correct model is used\n",
    "model.model.summary()\n",
    "\n",
    "# We add all test images to an array, used later for generating a submission\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "    image_filenames.append(image_filename)\n",
    "\n",
    "# Set-up submission filename\n",
    "submission_filename = \"batch_relu_val_200.csv\"\n",
    "\n",
    "# Generates the submission\n",
    "generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
