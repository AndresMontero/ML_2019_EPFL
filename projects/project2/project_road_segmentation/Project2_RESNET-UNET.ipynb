{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext lab_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.applications import *\n",
    "from helpers import *\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.compat.v2.keras.layers import BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8233605458871974884\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3156787200\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5667836576534971957\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 980M, pci bus id: 0000:01:00.0, compute capability: 5.2\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(8)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training images, images loaded: 80 \n",
      "Loading groundtruth images, images loaded: 80 \n"
     ]
    }
   ],
   "source": [
    "image_dir_train = \"data/training/images/\"\n",
    "files = os.listdir(image_dir_train)\n",
    "n_train = len(files)\n",
    "print(f\"Loading training images, images loaded: {n_train} \")\n",
    "imgs_train = np.asarray(\n",
    "    [load_image(image_dir_train + files[i]) for i in range(n_train)]\n",
    ")\n",
    "gt_dir_train = \"data/training/groundtruth/\"\n",
    "print(f\"Loading groundtruth images, images loaded: {n_train} \")\n",
    "gt_imgs_train = np.asarray(\n",
    "    [load_image(gt_dir_train + files[i]) for i in range(n_train)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 400, 400, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 400, 400)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_imgs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 400\n",
    "# Patches for training\n",
    "img_patches_train = [\n",
    "    crop_image(imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "gt_patches_train = [\n",
    "    crop_image(gt_imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = np.asarray(\n",
    "    [\n",
    "        img_patches_train[i][j]\n",
    "        for i in range(len(img_patches_train))\n",
    "        for j in range(len(img_patches_train[i]))\n",
    "    ]\n",
    ")\n",
    "Y_train = np.asarray(\n",
    "    [\n",
    "        gt_patches_train[i][j]\n",
    "        for i in range(len(gt_patches_train))\n",
    "        for j in range(len(gt_patches_train[i]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 400, 400, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 400, 400)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validating images, images loaded: 20 \n",
      "Loading validating groundtruth, images loaded: 20 \n"
     ]
    }
   ],
   "source": [
    "image_dir_val = \"data/validating/images/\"\n",
    "files = os.listdir(image_dir_val)\n",
    "n_val = len(files)\n",
    "print(f\"Loading validating images, images loaded: {n_val} \")\n",
    "imgs_val = np.asarray([load_image(image_dir_val + files[i]) for i in range(n_val)])\n",
    "gt_dir_val = \"data/validating/groundtruth/\"\n",
    "print(f\"Loading validating groundtruth, images loaded: {n_val} \")\n",
    "gt_imgs_val = np.asarray([load_image(gt_dir_val + files[i]) for i in range(n_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 400, 400, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 400, 400)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 400\n",
    "# Patches for validating\n",
    "img_patches_val = [\n",
    "    crop_image(imgs_val[i], image_size, image_size) for i in range(n_val)\n",
    "]\n",
    "gt_patches_val = [\n",
    "    crop_image(gt_imgs_val[i], image_size, image_size) for i in range(n_val)\n",
    "]\n",
    "\n",
    "# Separate features and labels\n",
    "X_val = np.asarray(\n",
    "    [\n",
    "        img_patches_val[i][j]\n",
    "        for i in range(len(img_patches_val))\n",
    "        for j in range(len(img_patches_val[i]))\n",
    "    ]\n",
    ")\n",
    "Y_val = np.asarray(\n",
    "    [\n",
    "        gt_patches_val[i][j]\n",
    "        for i in range(len(gt_patches_val))\n",
    "        for j in range(len(gt_patches_val[i]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 400, 400, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 400, 400)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = imag_rotation_aug(imgs_train, gt_imgs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "Y_train = np.asarray(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 456, 456, 3)\n",
      "(720, 456, 456)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "n_train = Y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = imag_rotation_aug(imgs_val, gt_imgs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.asarray(X_val)\n",
    "Y_val = np.asarray(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 456, 456, 3)\n",
      "(180, 456, 456)\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "n_val = Y_val.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create functions to calcualte precision, recall and F-1 in the training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Compute the Precision for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "        Precision (numpy.float64): the Precision of the batch \n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Compute the Recall for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "       Recall (numpy.float64): the Recal of the batch \n",
    "    \"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    \"\"\"Compute the F-1 for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "       F-1 (numpy.float64): the F-1 of the batch \n",
    "    \"\"\"\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-UNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet_unet_model:\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    def conv_act(self, inputs, out_filters, activation=\"relu\"):\n",
    "        return Conv2D(\n",
    "            filters=out_filters,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "        )(inputs)\n",
    "\n",
    "    def decoder(\n",
    "        self,\n",
    "        inputs,\n",
    "        mid_filters=512,\n",
    "        out_filters=256,\n",
    "        activation=\"relu\",\n",
    "        block_name=\"decoder\",\n",
    "    ):\n",
    "        with K.name_scope(block_name):\n",
    "            if activation == \"leaky_relu\":\n",
    "                activation = None\n",
    "                conv = LeakyReLU(alpha=0.3)(\n",
    "                    self.conv_act(inputs, mid_filters, activation)\n",
    "                )\n",
    "            else:\n",
    "                conv = self.conv_act(inputs, mid_filters, activation)\n",
    "            conv_tr = Conv2DTranspose(\n",
    "                filters=out_filters,\n",
    "                activation=activation,\n",
    "                kernel_size=4,\n",
    "                strides=2,\n",
    "                padding=\"same\",\n",
    "            )(conv)\n",
    "        return conv_tr\n",
    "\n",
    "    def __init__(\n",
    "        self, shape, batch_normalization, activation, dropout, amsgrad=False, lr=1e-4\n",
    "    ):\n",
    "        self.shape = shape\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.amsgrad = amsgrad\n",
    "        self.lr = lr\n",
    "        self.model = self.create_resnet_unet_model()\n",
    "\n",
    "    def create_resnet_unet_model(self):\n",
    "        # Set max pooling parameters\n",
    "        max_pooling_size = 2\n",
    "        max_pooling_strd = 2\n",
    "\n",
    "        # load a pretrained ResNet\n",
    "        num_classes = 2\n",
    "        resnet50 = ResNet50(\n",
    "            include_top=False,\n",
    "            weights=\"imagenet\",\n",
    "            classes=num_classes,\n",
    "            input_shape=self.shape,\n",
    "        )\n",
    "\n",
    "        resnet50.compile(\n",
    "            optimizer=Adam(lr=self.lr, amsgrad=self.amsgrad), loss=\"binary_crossentropy\"\n",
    "        )\n",
    "\n",
    "        # ResNet convolution outputs\n",
    "        conv5_out = resnet50.get_layer(\"conv5_block3_out\").output\n",
    "        conv4_out = resnet50.get_layer(\"conv4_block6_out\").output\n",
    "        conv3_out = resnet50.get_layer(\"conv3_block4_out\").output\n",
    "        conv2_out = resnet50.get_layer(\"conv2_block3_out\").output\n",
    "\n",
    "        # Max pool center layer\n",
    "        pool = MaxPooling2D(max_pooling_size, strides=max_pooling_strd, padding=\"same\")(\n",
    "            resnet50.get_output_at(0)\n",
    "        )\n",
    "\n",
    "        # Decoder center layer\n",
    "        dec_center = self.decoder(\n",
    "            pool,\n",
    "            mid_filters=self.shape[0] * 2,\n",
    "            out_filters=self.shape[0],\n",
    "            activation=self.activation,\n",
    "            block_name=\"decoder_center\",\n",
    "        )\n",
    "        if self.batch_normalization:\n",
    "            dec_center = BatchNormalization()(dec_center)\n",
    "        if self.dropout > 0:\n",
    "            dec_center = Dropout(dropout)(dec_center)\n",
    "\n",
    "        # Decoder 5th layer\n",
    "        cat1 = Concatenate()([dec_center, conv5_out])\n",
    "        dec5 = self.decoder(\n",
    "            cat1,\n",
    "            mid_filters=int(self.shape[0] * 2),\n",
    "            out_filters=int(self.shape[0]),\n",
    "            activation=self.activation,\n",
    "            block_name=\"decoder5\",\n",
    "        )\n",
    "        if self.batch_normalization:\n",
    "            dec5 = BatchNormalization()(dec5)\n",
    "        if self.dropout > 0:\n",
    "            dec5 = Dropout(self.dropout)(dec5)\n",
    "\n",
    "        # Decoder 4th layer\n",
    "        cat2 = Concatenate()([dec5, conv4_out])\n",
    "        dec4 = self.decoder(\n",
    "            cat2,\n",
    "            mid_filters=int(self.shape[0] * 2),\n",
    "            out_filters=int(self.shape[0]),\n",
    "            activation=self.activation,\n",
    "            block_name=\"decoder4\",\n",
    "        )\n",
    "        if self.batch_normalization:\n",
    "            dec4 = BatchNormalization()(dec4)\n",
    "        if self.dropout > 0:\n",
    "            dec4 = Dropout(self.dropout)(dec4)\n",
    "\n",
    "        # Decoder 3rd layer\n",
    "        cat3 = Concatenate()([dec4, conv3_out])\n",
    "        dec3 = self.decoder(\n",
    "            cat3,\n",
    "            mid_filters=int(self.shape[0]),\n",
    "            out_filters=int(self.shape[0] // 4),\n",
    "            activation=self.activation,\n",
    "            block_name=\"decoder3\",\n",
    "        )\n",
    "        if self.batch_normalization:\n",
    "            dec3 = BatchNormalization()(dec3)\n",
    "        if self.dropout > 0:\n",
    "            dec3 = Dropout(self.dropout)(dec3)\n",
    "\n",
    "        # Decoder 2nd layer\n",
    "        cat2 = Concatenate()([dec3, conv2_out])\n",
    "        dec2 = self.decoder(\n",
    "            cat2,\n",
    "            mid_filters=int(self.shape[0] // 2),\n",
    "            out_filters=int(self.shape[0] // 2),\n",
    "            activation=self.activation,\n",
    "            block_name=\"decoder2\",\n",
    "        )\n",
    "        if self.batch_normalization:\n",
    "            dec2 = BatchNormalization()(dec2)\n",
    "        if dropout > 0:\n",
    "            dec2 = Dropout(self.dropout)(dec2)\n",
    "\n",
    "        # Decoder 1st layer\n",
    "        dec1 = self.decoder(\n",
    "            dec2,\n",
    "            mid_filters=int(self.shape[0] // 2),\n",
    "            out_filters=int(self.shape[0] // 8),\n",
    "            activation=self.activation,\n",
    "            block_name=\"decoder1\",\n",
    "        )\n",
    "        if self.batch_normalization:\n",
    "            dec1 = BatchNormalization()(dec1)\n",
    "        if self.dropout > 0:\n",
    "            dec1 = Dropout(self.dropout)(dec1)\n",
    "\n",
    "        # Decoder 0th layer\n",
    "        dec0 = self.conv_act(dec1, out_filters=int(self.shape[0] // 8))\n",
    "        conv_f = Conv2D(1, 1, activation=\"sigmoid\", padding=\"same\")(dec0)\n",
    "        flatten_0 = Flatten()(conv_f)\n",
    "        dense_0 = Dense(\n",
    "            self.shape[0] / 2,\n",
    "            kernel_regularizer=l2(1e-6),\n",
    "            activity_regularizer=l2(1e-6),\n",
    "        )(flatten_0)\n",
    "        dropout_0 = Dropout(0.5)(dense_0)\n",
    "        lk_relu_0 = LeakyReLU(alpha=0.1)(dropout_0)\n",
    "        dense_1 = Dense(2, kernel_regularizer=l2(1e-6), activity_regularizer=l2(1e-6))(\n",
    "            lk_relu_0\n",
    "        )\n",
    "        dropout_1 = Dropout(0.2)(dense_1)\n",
    "        output = Activation(\"sigmoid\")(dropout_1)\n",
    "        model = Model(inputs=resnet50.get_input_at(0), outputs=output)\n",
    "\n",
    "        # Compile the model using the Adam optimizer with accuracy, recall and f1 metrics\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(lr=lr, amsgrad=self.amsgrad),\n",
    "            metrics=[\"accuracy\", recall, f1],\n",
    "        )\n",
    "\n",
    "        # Print summary\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        epochs,\n",
    "        steps_per_epoch,\n",
    "        n_train=85,\n",
    "        n_val=15,\n",
    "        batch_size=100,\n",
    "        data_aug_factor=1,\n",
    "    ):\n",
    "\n",
    "        # Stop if the model does not get better after 20 steps\n",
    "        e_stop = EarlyStopping(\n",
    "            monitor=\"val_loss\", min_delta=0, patience=20, verbose=1, mode=\"auto\"\n",
    "        )\n",
    "\n",
    "        # Reduce the learning rate of the model after 30 steps\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor=\"loss\", factor=0.5, patience=30, verbose=1, mode=\"auto\"\n",
    "        )\n",
    "\n",
    "        # Save the best model\n",
    "        weights_filename = \"model_\"\n",
    "        if self.batch_normalization:\n",
    "            weights_filename = weights_filename + \"batch_\"\n",
    "        weights_filename = (\n",
    "            weights_filename\n",
    "            + self.activation\n",
    "            + \"_\"\n",
    "            + str(epochs)\n",
    "            + \"_\"\n",
    "            + \"dropout_\"\n",
    "            + str(self.dropout)\n",
    "            + \"_\"\n",
    "            + \"{epoch:03d}_\"\n",
    "            + \"{f1:03f}_\"\n",
    "            + \"{val_accuracy:03f}.h5\"\n",
    "        )\n",
    "        save_best_model = ModelCheckpoint(\n",
    "            weights_filename,\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"auto\",\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        cbs = [save_best_model, reduce_lr]\n",
    "\n",
    "        # Train the model\n",
    "        history = self.model.fit_generator(\n",
    "            create_minibatch(\n",
    "                X_train, Y_train, data_aug_factor * n_train, batch_size=batch_size\n",
    "            ),\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            epochs=epochs,\n",
    "            use_multiprocessing=False,\n",
    "            workers=1,\n",
    "            callbacks=cbs,\n",
    "            verbose=1,\n",
    "            validation_data=create_minibatch(\n",
    "                X_val, Y_val, data_aug_factor * n_val, batch_size=batch_size\n",
    "            ),\n",
    "            validation_steps=steps_per_epoch,\n",
    "        )\n",
    "\n",
    "        return history\n",
    "\n",
    "    def classify(self, X):\n",
    "        \"\"\"Classify Image as either road or not.\n",
    "        Args:\n",
    "            X (image): part of the image to classify\n",
    "        Returns:\n",
    "            Predictions : Predictions for each patch\n",
    "        \"\"\"\n",
    "        # Subdivide the images into blocks with a stride and patch_size of 16\n",
    "        img_patches = create_patches(X, 16, 16, padding=24)\n",
    "\n",
    "        # Predict\n",
    "        predictions = self.model.predict(img_patches)\n",
    "        predictions = (predictions[:, 0] < predictions[:, 1]) * 1\n",
    "\n",
    "        # Regroup patches into images\n",
    "        return predictions.reshape(X.shape[0], -1)\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"Loads Saved Model.\n",
    "        Args:\n",
    "           filename (string): name of the model\n",
    "           \n",
    "        \"\"\"\n",
    "        # Load the model (used for submission)\n",
    "        dependencies = {\"recall\": recall, \"f1\": f1}\n",
    "        self.model = load_model(filename, custom_objects=dependencies)\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Saves trained model.\n",
    "        Args:\n",
    "           filename (string): name of the model\n",
    "           \n",
    "        \"\"\"\n",
    "        self.model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug_factor = 1\n",
    "batch_normalization = True\n",
    "activation = \"relu\"\n",
    "dropout = 0\n",
    "amsgrad = False\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 70, 70, 3)    0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 32, 32, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 32, 32, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 32, 32, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 34, 34, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 16, 16, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 16, 16, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 16, 16, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 16, 16, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 16, 16, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 16, 16, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 16, 16, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 16, 16, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 16, 16, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 16, 16, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 16, 16, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 16, 16, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 16, 16, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 16, 16, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 16, 16, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 8, 8, 128)    32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 8, 8, 128)    0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 8, 8, 128)    0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 8, 8, 512)    131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 8, 8, 512)    0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 8, 8, 512)    0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 8, 8, 128)    0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 8, 8, 128)    0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 8, 8, 512)    0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 8, 8, 512)    0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 8, 8, 128)    0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 8, 8, 128)    0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 8, 8, 512)    0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 8, 8, 512)    0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 8, 8, 128)    0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 8, 8, 128)    0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 8, 8, 512)    0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 8, 8, 512)    0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 4, 4, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 4, 4, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 4, 4, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 4, 4, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 4, 4, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 4, 4, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 4, 4, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 4, 4, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 4, 4, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 4, 4, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 4, 4, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 4, 4, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 4, 4, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 4, 4, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 4, 4, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 4, 4, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 4, 4, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 4, 4, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 4, 4, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 4, 4, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 4, 4, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 4, 4, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 4, 4, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 4, 4, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 2, 2, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 2, 2, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 2, 2, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 2, 2, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 2, 2, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 2, 2, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 2, 2, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 2, 2, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 2, 2, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 2, 2, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 2, 2, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 2, 2, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 2048)   0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 1, 1, 128)    2359424     max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_12 (Conv2DTran (None, 2, 2, 64)     131136      conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 2, 2, 64)     256         conv2d_transpose_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 2, 2, 2112)   0           batch_normalization_12[0][0]     \n",
      "                                                                 conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 2, 2, 128)    2433152     concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_13 (Conv2DTran (None, 4, 4, 64)     131136      conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 4, 4, 64)     256         conv2d_transpose_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 4, 4, 1088)   0           batch_normalization_13[0][0]     \n",
      "                                                                 conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 4, 4, 128)    1253504     concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_14 (Conv2DTran (None, 8, 8, 64)     131136      conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_transpose_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 8, 8, 576)    0           batch_normalization_14[0][0]     \n",
      "                                                                 conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     331840      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_15 (Conv2DTran (None, 16, 16, 16)   16400       conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 16)   64          conv2d_transpose_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 16, 16, 272)  0           batch_normalization_15[0][0]     \n",
      "                                                                 conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 32)   78368       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_16 (Conv2DTran (None, 32, 32, 32)   16416       conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_transpose_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 32, 32)   9248        batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_17 (Conv2DTran (None, 64, 64, 8)    4104        conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 64, 64, 8)    32          conv2d_transpose_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 64, 64, 8)    584         batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 64, 64, 1)    9           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4096)         0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           131104      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 32)           0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2)            66          leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 2)            0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 2)            0           dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 30,616,331\n",
      "Trainable params: 30,562,715\n",
      "Non-trainable params: 53,616\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "EPOCHS = 80\n",
    "STEPS_PER_EPOCH = 100\n",
    "batch_size = 100\n",
    "\n",
    "model = resnet_unet_model(\n",
    "    shape=(64, 64, 3),\n",
    "    batch_normalization=batch_normalization,\n",
    "    activation=activation,\n",
    "    dropout=dropout,\n",
    "    amsgrad=amsgrad,\n",
    "    lr=lr,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.5650 - accuracy: 0.7157 - recall: 0.6178 - f1: 0.6844\n",
      "Epoch 00001: val_loss improved from inf to 0.57014, saving model to model_batch_relu_80_dropout_0_001_0.684290_0.746400.h5\n",
      "100/100 [==============================] - 75s 752ms/step - loss: 0.5646 - accuracy: 0.7156 - recall: 0.6177 - f1: 0.6843 - val_loss: 0.5701 - val_accuracy: 0.7464 - val_recall: 0.7464 - val_f1: 0.7464\n",
      "Epoch 2/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3854 - accuracy: 0.7895 - recall: 0.6910 - f1: 0.7660\n",
      "Epoch 00002: val_loss did not improve from 0.57014\n",
      "100/100 [==============================] - 71s 712ms/step - loss: 0.3840 - accuracy: 0.7904 - recall: 0.6917 - f1: 0.7669 - val_loss: 1.0381 - val_accuracy: 0.2591 - val_recall: 0.2591 - val_f1: 0.2591\n",
      "Epoch 3/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3085 - accuracy: 0.8383 - recall: 0.7390 - f1: 0.8199\n",
      "Epoch 00003: val_loss did not improve from 0.57014\n",
      "100/100 [==============================] - 71s 710ms/step - loss: 0.3081 - accuracy: 0.8385 - recall: 0.7391 - f1: 0.8202 - val_loss: 0.6131 - val_accuracy: 0.7401 - val_recall: 0.7408 - val_f1: 0.7402\n",
      "Epoch 4/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.8428 - recall: 0.7430 - f1: 0.8250\n",
      "Epoch 00004: val_loss did not improve from 0.57014\n",
      "100/100 [==============================] - 71s 710ms/step - loss: 0.2861 - accuracy: 0.8428 - recall: 0.7428 - f1: 0.8249 - val_loss: 0.5962 - val_accuracy: 0.7154 - val_recall: 0.7221 - val_f1: 0.7173\n",
      "Epoch 5/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2794 - accuracy: 0.8501 - recall: 0.7515 - f1: 0.8334\n",
      "Epoch 00005: val_loss did not improve from 0.57014\n",
      "100/100 [==============================] - 71s 708ms/step - loss: 0.2796 - accuracy: 0.8504 - recall: 0.7519 - f1: 0.8337 - val_loss: 0.6390 - val_accuracy: 0.7366 - val_recall: 0.7381 - val_f1: 0.7370\n",
      "Epoch 6/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2580 - accuracy: 0.8535 - recall: 0.7517 - f1: 0.8363\n",
      "Epoch 00006: val_loss did not improve from 0.57014\n",
      "100/100 [==============================] - 71s 711ms/step - loss: 0.2586 - accuracy: 0.8536 - recall: 0.7520 - f1: 0.8365 - val_loss: 0.6799 - val_accuracy: 0.6869 - val_recall: 0.6813 - val_f1: 0.6851\n",
      "Epoch 7/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2599 - accuracy: 0.8522 - recall: 0.7478 - f1: 0.8345\n",
      "Epoch 00007: val_loss did not improve from 0.57014\n",
      "100/100 [==============================] - 71s 710ms/step - loss: 0.2594 - accuracy: 0.8524 - recall: 0.7480 - f1: 0.8347 - val_loss: 0.6527 - val_accuracy: 0.6535 - val_recall: 0.6561 - val_f1: 0.6544\n",
      "Epoch 8/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2480 - accuracy: 0.8591 - recall: 0.7578 - f1: 0.8428\n",
      "Epoch 00008: val_loss did not improve from 0.57014\n",
      "100/100 [==============================] - 71s 712ms/step - loss: 0.2473 - accuracy: 0.8595 - recall: 0.7584 - f1: 0.8433 - val_loss: 0.7329 - val_accuracy: 0.6810 - val_recall: 0.6819 - val_f1: 0.6813\n",
      "Epoch 9/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2359 - accuracy: 0.8612 - recall: 0.7584 - f1: 0.8448\n",
      "Epoch 00009: val_loss improved from 0.57014 to 0.56954, saving model to model_batch_relu_80_dropout_0_009_0.844541_0.751900.h5\n",
      "100/100 [==============================] - 72s 724ms/step - loss: 0.2362 - accuracy: 0.8610 - recall: 0.7580 - f1: 0.8445 - val_loss: 0.5695 - val_accuracy: 0.7519 - val_recall: 0.7386 - val_f1: 0.7485\n",
      "Epoch 10/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2305 - accuracy: 0.8683 - recall: 0.7684 - f1: 0.8531\n",
      "Epoch 00010: val_loss improved from 0.56954 to 0.44434, saving model to model_batch_relu_80_dropout_0_010_0.853304_0.796400.h5\n",
      "100/100 [==============================] - 72s 725ms/step - loss: 0.2302 - accuracy: 0.8685 - recall: 0.7687 - f1: 0.8533 - val_loss: 0.4443 - val_accuracy: 0.7964 - val_recall: 0.8045 - val_f1: 0.7981\n",
      "Epoch 11/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2285 - accuracy: 0.8673 - recall: 0.7671 - f1: 0.8518\n",
      "Epoch 00011: val_loss improved from 0.44434 to 0.40898, saving model to model_batch_relu_80_dropout_0_011_0.851333_0.846100.h5\n",
      "100/100 [==============================] - 73s 728ms/step - loss: 0.2286 - accuracy: 0.8669 - recall: 0.7663 - f1: 0.8513 - val_loss: 0.4090 - val_accuracy: 0.8461 - val_recall: 0.8415 - val_f1: 0.8454\n",
      "Epoch 12/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2289 - accuracy: 0.8638 - recall: 0.7608 - f1: 0.8475\n",
      "Epoch 00012: val_loss improved from 0.40898 to 0.34501, saving model to model_batch_relu_80_dropout_0_012_0.847685_0.863950.h5\n",
      "100/100 [==============================] - 72s 724ms/step - loss: 0.2285 - accuracy: 0.8639 - recall: 0.7611 - f1: 0.8477 - val_loss: 0.3450 - val_accuracy: 0.8640 - val_recall: 0.8657 - val_f1: 0.8642\n",
      "Epoch 13/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2273 - accuracy: 0.8669 - recall: 0.7676 - f1: 0.8516\n",
      "Epoch 00013: val_loss did not improve from 0.34501\n",
      "100/100 [==============================] - 71s 709ms/step - loss: 0.2276 - accuracy: 0.8668 - recall: 0.7674 - f1: 0.8515 - val_loss: 0.4247 - val_accuracy: 0.8249 - val_recall: 0.8182 - val_f1: 0.8237\n",
      "Epoch 14/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2104 - accuracy: 0.8702 - recall: 0.7712 - f1: 0.8553\n",
      "Epoch 00014: val_loss improved from 0.34501 to 0.32971, saving model to model_batch_relu_80_dropout_0_014_0.854851_0.887500.h5\n",
      "100/100 [==============================] - 72s 718ms/step - loss: 0.2109 - accuracy: 0.8698 - recall: 0.7705 - f1: 0.8549 - val_loss: 0.3297 - val_accuracy: 0.8875 - val_recall: 0.8847 - val_f1: 0.8872\n",
      "Epoch 15/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2243 - accuracy: 0.8680 - recall: 0.7669 - f1: 0.8526\n",
      "Epoch 00015: val_loss improved from 0.32971 to 0.31309, saving model to model_batch_relu_80_dropout_0_015_0.852418_0.886750.h5\n",
      "100/100 [==============================] - 72s 721ms/step - loss: 0.2243 - accuracy: 0.8679 - recall: 0.7667 - f1: 0.8524 - val_loss: 0.3131 - val_accuracy: 0.8867 - val_recall: 0.8861 - val_f1: 0.8867\n",
      "Epoch 16/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2149 - accuracy: 0.8723 - recall: 0.7744 - f1: 0.8579\n",
      "Epoch 00016: val_loss did not improve from 0.31309\n",
      "100/100 [==============================] - 71s 711ms/step - loss: 0.2148 - accuracy: 0.8722 - recall: 0.7741 - f1: 0.8577 - val_loss: 0.3237 - val_accuracy: 0.8978 - val_recall: 0.8973 - val_f1: 0.8977\n",
      "Epoch 17/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2156 - accuracy: 0.8739 - recall: 0.7746 - f1: 0.8595\n",
      "Epoch 00017: val_loss did not improve from 0.31309\n",
      "100/100 [==============================] - 70s 704ms/step - loss: 0.2157 - accuracy: 0.8738 - recall: 0.7743 - f1: 0.8593 - val_loss: 0.3387 - val_accuracy: 0.8872 - val_recall: 0.8845 - val_f1: 0.8869\n",
      "Epoch 18/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2124 - accuracy: 0.8705 - recall: 0.7668 - f1: 0.8548\n",
      "Epoch 00018: val_loss improved from 0.31309 to 0.28775, saving model to model_batch_relu_80_dropout_0_018_0.855036_0.899850.h5\n",
      "100/100 [==============================] - 72s 722ms/step - loss: 0.2120 - accuracy: 0.8706 - recall: 0.7671 - f1: 0.8550 - val_loss: 0.2878 - val_accuracy: 0.8999 - val_recall: 0.8998 - val_f1: 0.8999\n",
      "Epoch 19/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2178 - accuracy: 0.8692 - recall: 0.7661 - f1: 0.8537\n",
      "Epoch 00019: val_loss did not improve from 0.28775\n",
      "100/100 [==============================] - 71s 709ms/step - loss: 0.2176 - accuracy: 0.8694 - recall: 0.7661 - f1: 0.8538 - val_loss: 0.3021 - val_accuracy: 0.8953 - val_recall: 0.8939 - val_f1: 0.8951\n",
      "Epoch 20/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2123 - accuracy: 0.8746 - recall: 0.7758 - f1: 0.8603\n",
      "Epoch 00020: val_loss did not improve from 0.28775\n",
      "100/100 [==============================] - 71s 714ms/step - loss: 0.2119 - accuracy: 0.8747 - recall: 0.7757 - f1: 0.8604 - val_loss: 0.2893 - val_accuracy: 0.8931 - val_recall: 0.8912 - val_f1: 0.8929\n",
      "Epoch 21/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2077 - accuracy: 0.8716 - recall: 0.7688 - f1: 0.8564\n",
      "Epoch 00021: val_loss did not improve from 0.28775\n",
      "100/100 [==============================] - 72s 718ms/step - loss: 0.2077 - accuracy: 0.8713 - recall: 0.7685 - f1: 0.8560 - val_loss: 0.3023 - val_accuracy: 0.9028 - val_recall: 0.9020 - val_f1: 0.9027\n",
      "Epoch 22/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2014 - accuracy: 0.8753 - recall: 0.7759 - f1: 0.8609\n",
      "Epoch 00022: val_loss improved from 0.28775 to 0.27913, saving model to model_batch_relu_80_dropout_0_022_0.860984_0.900700.h5\n",
      "100/100 [==============================] - 72s 721ms/step - loss: 0.2018 - accuracy: 0.8753 - recall: 0.7761 - f1: 0.8610 - val_loss: 0.2791 - val_accuracy: 0.9007 - val_recall: 0.8989 - val_f1: 0.9005\n",
      "Epoch 23/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2042 - accuracy: 0.8772 - recall: 0.7777 - f1: 0.8631\n",
      "Epoch 00023: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 715ms/step - loss: 0.2039 - accuracy: 0.8773 - recall: 0.7777 - f1: 0.8631 - val_loss: 0.3342 - val_accuracy: 0.8959 - val_recall: 0.8935 - val_f1: 0.8956\n",
      "Epoch 24/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2114 - accuracy: 0.8714 - recall: 0.7695 - f1: 0.8563\n",
      "Epoch 00024: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 70s 705ms/step - loss: 0.2108 - accuracy: 0.8716 - recall: 0.7697 - f1: 0.8565 - val_loss: 0.2933 - val_accuracy: 0.8966 - val_recall: 0.8902 - val_f1: 0.8959\n",
      "Epoch 25/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2027 - accuracy: 0.8772 - recall: 0.7772 - f1: 0.8630\n",
      "Epoch 00025: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 710ms/step - loss: 0.2028 - accuracy: 0.8774 - recall: 0.7776 - f1: 0.8632 - val_loss: 0.3485 - val_accuracy: 0.8896 - val_recall: 0.8857 - val_f1: 0.8891\n",
      "Epoch 26/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2051 - accuracy: 0.8726 - recall: 0.7696 - f1: 0.8573\n",
      "Epoch 00026: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 709ms/step - loss: 0.2055 - accuracy: 0.8727 - recall: 0.7698 - f1: 0.8575 - val_loss: 0.5058 - val_accuracy: 0.8705 - val_recall: 0.8675 - val_f1: 0.8701\n",
      "Epoch 27/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2009 - accuracy: 0.8748 - recall: 0.7710 - f1: 0.8597\n",
      "Epoch 00027: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 708ms/step - loss: 0.2016 - accuracy: 0.8747 - recall: 0.7708 - f1: 0.8596 - val_loss: 0.3582 - val_accuracy: 0.8978 - val_recall: 0.8947 - val_f1: 0.8974\n",
      "Epoch 28/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2039 - accuracy: 0.8748 - recall: 0.7733 - f1: 0.8602\n",
      "Epoch 00028: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 706ms/step - loss: 0.2038 - accuracy: 0.8748 - recall: 0.7732 - f1: 0.8601 - val_loss: 0.3348 - val_accuracy: 0.8871 - val_recall: 0.8842 - val_f1: 0.8867\n",
      "Epoch 29/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2039 - accuracy: 0.8765 - recall: 0.7765 - f1: 0.8623\n",
      "Epoch 00029: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 707ms/step - loss: 0.2039 - accuracy: 0.8766 - recall: 0.7768 - f1: 0.8624 - val_loss: 0.3036 - val_accuracy: 0.8948 - val_recall: 0.8882 - val_f1: 0.8941\n",
      "Epoch 30/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1974 - accuracy: 0.8779 - recall: 0.7747 - f1: 0.8633\n",
      "Epoch 00030: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 713ms/step - loss: 0.1971 - accuracy: 0.8780 - recall: 0.7750 - f1: 0.8634 - val_loss: 0.3387 - val_accuracy: 0.8975 - val_recall: 0.8945 - val_f1: 0.8972\n",
      "Epoch 31/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1980 - accuracy: 0.8808 - recall: 0.7825 - f1: 0.8673\n",
      "Epoch 00031: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 712ms/step - loss: 0.1977 - accuracy: 0.8808 - recall: 0.7825 - f1: 0.8673 - val_loss: 0.3193 - val_accuracy: 0.8989 - val_recall: 0.8979 - val_f1: 0.8988\n",
      "Epoch 32/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2008 - accuracy: 0.8737 - recall: 0.7718 - f1: 0.8588\n",
      "Epoch 00032: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 72s 715ms/step - loss: 0.2018 - accuracy: 0.8734 - recall: 0.7713 - f1: 0.8585 - val_loss: 0.3136 - val_accuracy: 0.8918 - val_recall: 0.8889 - val_f1: 0.8915\n",
      "Epoch 33/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1987 - accuracy: 0.8780 - recall: 0.7767 - f1: 0.8636\n",
      "Epoch 00033: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 72s 718ms/step - loss: 0.1985 - accuracy: 0.8780 - recall: 0.7766 - f1: 0.8636 - val_loss: 0.3492 - val_accuracy: 0.8956 - val_recall: 0.8931 - val_f1: 0.8953\n",
      "Epoch 34/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2016 - accuracy: 0.8791 - recall: 0.7792 - f1: 0.8652\n",
      "Epoch 00034: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 714ms/step - loss: 0.2022 - accuracy: 0.8788 - recall: 0.7787 - f1: 0.8649 - val_loss: 0.2837 - val_accuracy: 0.8938 - val_recall: 0.8900 - val_f1: 0.8933\n",
      "Epoch 35/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1977 - accuracy: 0.8773 - recall: 0.7757 - f1: 0.8629\n",
      "Epoch 00035: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 709ms/step - loss: 0.1981 - accuracy: 0.8771 - recall: 0.7754 - f1: 0.8627 - val_loss: 0.3543 - val_accuracy: 0.8930 - val_recall: 0.8926 - val_f1: 0.8930\n",
      "Epoch 36/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1956 - accuracy: 0.8781 - recall: 0.7785 - f1: 0.8640\n",
      "Epoch 00036: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 710ms/step - loss: 0.1961 - accuracy: 0.8781 - recall: 0.7785 - f1: 0.8640 - val_loss: 0.3007 - val_accuracy: 0.9008 - val_recall: 0.8982 - val_f1: 0.9006\n",
      "Epoch 37/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1886 - accuracy: 0.8780 - recall: 0.7748 - f1: 0.8634\n",
      "Epoch 00037: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 712ms/step - loss: 0.1887 - accuracy: 0.8778 - recall: 0.7744 - f1: 0.8632 - val_loss: 0.3589 - val_accuracy: 0.8957 - val_recall: 0.8953 - val_f1: 0.8956\n",
      "Epoch 38/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1905 - accuracy: 0.8801 - recall: 0.7814 - f1: 0.8665\n",
      "Epoch 00038: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 711ms/step - loss: 0.1903 - accuracy: 0.8803 - recall: 0.7819 - f1: 0.8668 - val_loss: 0.3456 - val_accuracy: 0.9034 - val_recall: 0.9017 - val_f1: 0.9032\n",
      "Epoch 39/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1872 - accuracy: 0.8822 - recall: 0.7806 - f1: 0.8684\n",
      "Epoch 00039: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 709ms/step - loss: 0.1869 - accuracy: 0.8822 - recall: 0.7805 - f1: 0.8684 - val_loss: 0.3930 - val_accuracy: 0.8962 - val_recall: 0.8971 - val_f1: 0.8963\n",
      "Epoch 40/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1949 - accuracy: 0.8783 - recall: 0.7758 - f1: 0.8638\n",
      "Epoch 00040: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 715ms/step - loss: 0.1946 - accuracy: 0.8785 - recall: 0.7761 - f1: 0.8641 - val_loss: 0.3068 - val_accuracy: 0.8971 - val_recall: 0.8965 - val_f1: 0.8970\n",
      "Epoch 41/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1907 - accuracy: 0.8767 - recall: 0.7736 - f1: 0.8621\n",
      "Epoch 00041: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 707ms/step - loss: 0.1907 - accuracy: 0.8766 - recall: 0.7736 - f1: 0.8621 - val_loss: 0.3607 - val_accuracy: 0.9047 - val_recall: 0.9038 - val_f1: 0.9046\n",
      "Epoch 42/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1933 - accuracy: 0.8807 - recall: 0.7818 - f1: 0.8671\n",
      "Epoch 00042: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 712ms/step - loss: 0.1937 - accuracy: 0.8802 - recall: 0.7806 - f1: 0.8664 - val_loss: 0.3526 - val_accuracy: 0.9022 - val_recall: 0.9020 - val_f1: 0.9022\n",
      "Epoch 43/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1902 - accuracy: 0.8812 - recall: 0.7819 - f1: 0.8675\n",
      "Epoch 00043: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 711ms/step - loss: 0.1902 - accuracy: 0.8809 - recall: 0.7814 - f1: 0.8672 - val_loss: 0.3612 - val_accuracy: 0.9044 - val_recall: 0.9042 - val_f1: 0.9044\n",
      "Epoch 44/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1960 - accuracy: 0.8772 - recall: 0.7760 - f1: 0.8628\n",
      "Epoch 00044: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 714ms/step - loss: 0.1958 - accuracy: 0.8773 - recall: 0.7762 - f1: 0.8629 - val_loss: 0.3325 - val_accuracy: 0.9074 - val_recall: 0.9058 - val_f1: 0.9072\n",
      "Epoch 45/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1927 - accuracy: 0.8768 - recall: 0.7703 - f1: 0.8615\n",
      "Epoch 00045: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 714ms/step - loss: 0.1929 - accuracy: 0.8765 - recall: 0.7696 - f1: 0.8611 - val_loss: 0.3214 - val_accuracy: 0.8866 - val_recall: 0.8840 - val_f1: 0.8863\n",
      "Epoch 46/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1958 - accuracy: 0.8764 - recall: 0.7732 - f1: 0.8617\n",
      "Epoch 00046: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 709ms/step - loss: 0.1954 - accuracy: 0.8766 - recall: 0.7736 - f1: 0.8620 - val_loss: 0.3948 - val_accuracy: 0.8979 - val_recall: 0.8965 - val_f1: 0.8978\n",
      "Epoch 47/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1865 - accuracy: 0.8841 - recall: 0.7841 - f1: 0.8708\n",
      "Epoch 00047: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 713ms/step - loss: 0.1863 - accuracy: 0.8841 - recall: 0.7839 - f1: 0.8707 - val_loss: 0.5402 - val_accuracy: 0.8794 - val_recall: 0.8807 - val_f1: 0.8795\n",
      "Epoch 48/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1902 - accuracy: 0.8784 - recall: 0.7772 - f1: 0.8642\n",
      "Epoch 00048: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 710ms/step - loss: 0.1900 - accuracy: 0.8788 - recall: 0.7778 - f1: 0.8646 - val_loss: 0.3006 - val_accuracy: 0.8989 - val_recall: 0.8977 - val_f1: 0.8988\n",
      "Epoch 49/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1899 - accuracy: 0.8825 - recall: 0.7831 - f1: 0.8691\n",
      "Epoch 00049: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 710ms/step - loss: 0.1900 - accuracy: 0.8826 - recall: 0.7832 - f1: 0.8692 - val_loss: 0.3129 - val_accuracy: 0.8964 - val_recall: 0.8943 - val_f1: 0.8962\n",
      "Epoch 50/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1927 - accuracy: 0.8784 - recall: 0.7791 - f1: 0.8643\n",
      "Epoch 00050: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 72s 716ms/step - loss: 0.1939 - accuracy: 0.8778 - recall: 0.7782 - f1: 0.8636 - val_loss: 0.3613 - val_accuracy: 0.8963 - val_recall: 0.8978 - val_f1: 0.8964\n",
      "Epoch 51/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1884 - accuracy: 0.8776 - recall: 0.7768 - f1: 0.8633\n",
      "Epoch 00051: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 706ms/step - loss: 0.1885 - accuracy: 0.8776 - recall: 0.7765 - f1: 0.8632 - val_loss: 0.3106 - val_accuracy: 0.9005 - val_recall: 0.8965 - val_f1: 0.9001\n",
      "Epoch 52/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1832 - accuracy: 0.8856 - recall: 0.7872 - f1: 0.8726\n",
      "Epoch 00052: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 707ms/step - loss: 0.1833 - accuracy: 0.8855 - recall: 0.7870 - f1: 0.8725 - val_loss: 0.3697 - val_accuracy: 0.8950 - val_recall: 0.8943 - val_f1: 0.8950\n",
      "Epoch 53/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1879 - accuracy: 0.8811 - recall: 0.7798 - f1: 0.8672\n",
      "Epoch 00053: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 708ms/step - loss: 0.1881 - accuracy: 0.8813 - recall: 0.7801 - f1: 0.8674 - val_loss: 0.3787 - val_accuracy: 0.9057 - val_recall: 0.9041 - val_f1: 0.9055\n",
      "Epoch 54/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1910 - accuracy: 0.8791 - recall: 0.7791 - f1: 0.8651\n",
      "Epoch 00054: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 72s 719ms/step - loss: 0.1914 - accuracy: 0.8786 - recall: 0.7782 - f1: 0.8645 - val_loss: 0.3990 - val_accuracy: 0.8946 - val_recall: 0.8991 - val_f1: 0.8951\n",
      "Epoch 55/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1877 - accuracy: 0.8805 - recall: 0.7798 - f1: 0.8664\n",
      "Epoch 00055: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 712ms/step - loss: 0.1873 - accuracy: 0.8809 - recall: 0.7807 - f1: 0.8670 - val_loss: 0.3622 - val_accuracy: 0.9007 - val_recall: 0.8990 - val_f1: 0.9006\n",
      "Epoch 56/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1781 - accuracy: 0.8881 - recall: 0.7929 - f1: 0.8760\n",
      "Epoch 00056: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 712ms/step - loss: 0.1779 - accuracy: 0.8883 - recall: 0.7931 - f1: 0.8762 - val_loss: 0.3470 - val_accuracy: 0.8989 - val_recall: 0.8989 - val_f1: 0.8989\n",
      "Epoch 57/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1866 - accuracy: 0.8794 - recall: 0.7765 - f1: 0.8650\n",
      "Epoch 00057: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 713ms/step - loss: 0.1872 - accuracy: 0.8790 - recall: 0.7757 - f1: 0.8645 - val_loss: 0.4507 - val_accuracy: 0.8880 - val_recall: 0.8876 - val_f1: 0.8879\n",
      "Epoch 58/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1837 - accuracy: 0.8848 - recall: 0.7864 - f1: 0.8717\n",
      "Epoch 00058: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 712ms/step - loss: 0.1837 - accuracy: 0.8850 - recall: 0.7869 - f1: 0.8720 - val_loss: 0.3549 - val_accuracy: 0.9036 - val_recall: 0.9047 - val_f1: 0.9037\n",
      "Epoch 59/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1866 - accuracy: 0.8827 - recall: 0.7821 - f1: 0.8691\n",
      "Epoch 00059: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 715ms/step - loss: 0.1864 - accuracy: 0.8826 - recall: 0.7821 - f1: 0.8690 - val_loss: 0.3859 - val_accuracy: 0.8891 - val_recall: 0.8885 - val_f1: 0.8890\n",
      "Epoch 60/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1805 - accuracy: 0.8815 - recall: 0.7794 - f1: 0.8677\n",
      "Epoch 00060: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 712ms/step - loss: 0.1801 - accuracy: 0.8820 - recall: 0.7803 - f1: 0.8683 - val_loss: 0.3359 - val_accuracy: 0.9074 - val_recall: 0.9056 - val_f1: 0.9072\n",
      "Epoch 61/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1836 - accuracy: 0.8833 - recall: 0.7830 - f1: 0.8699\n",
      "Epoch 00061: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 710ms/step - loss: 0.1831 - accuracy: 0.8838 - recall: 0.7839 - f1: 0.8704 - val_loss: 0.5326 - val_accuracy: 0.8891 - val_recall: 0.8886 - val_f1: 0.8891\n",
      "Epoch 62/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1870 - accuracy: 0.8804 - recall: 0.7786 - f1: 0.8663\n",
      "Epoch 00062: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 708ms/step - loss: 0.1876 - accuracy: 0.8802 - recall: 0.7782 - f1: 0.8660 - val_loss: 0.3475 - val_accuracy: 0.8992 - val_recall: 0.8978 - val_f1: 0.8991\n",
      "Epoch 63/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1816 - accuracy: 0.8823 - recall: 0.7817 - f1: 0.8687\n",
      "Epoch 00063: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 711ms/step - loss: 0.1813 - accuracy: 0.8826 - recall: 0.7820 - f1: 0.8690 - val_loss: 0.3885 - val_accuracy: 0.9032 - val_recall: 0.9037 - val_f1: 0.9033\n",
      "Epoch 64/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1841 - accuracy: 0.8798 - recall: 0.7764 - f1: 0.8654\n",
      "Epoch 00064: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 710ms/step - loss: 0.1839 - accuracy: 0.8798 - recall: 0.7761 - f1: 0.8653 - val_loss: 0.3663 - val_accuracy: 0.9022 - val_recall: 0.9003 - val_f1: 0.9021\n",
      "Epoch 65/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1774 - accuracy: 0.8852 - recall: 0.7853 - f1: 0.8718\n",
      "Epoch 00065: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 713ms/step - loss: 0.1776 - accuracy: 0.8850 - recall: 0.7849 - f1: 0.8716 - val_loss: 0.2905 - val_accuracy: 0.9159 - val_recall: 0.9165 - val_f1: 0.9160\n",
      "Epoch 66/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1878 - accuracy: 0.8809 - recall: 0.7790 - f1: 0.8668\n",
      "Epoch 00066: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 711ms/step - loss: 0.1878 - accuracy: 0.8809 - recall: 0.7790 - f1: 0.8668 - val_loss: 0.4283 - val_accuracy: 0.8928 - val_recall: 0.8929 - val_f1: 0.8929\n",
      "Epoch 67/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1857 - accuracy: 0.8859 - recall: 0.7895 - f1: 0.8732\n",
      "Epoch 00067: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 712ms/step - loss: 0.1857 - accuracy: 0.8859 - recall: 0.7894 - f1: 0.8732 - val_loss: 0.3230 - val_accuracy: 0.9046 - val_recall: 0.9039 - val_f1: 0.9045\n",
      "Epoch 68/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1728 - accuracy: 0.8898 - recall: 0.7924 - f1: 0.8774\n",
      "Epoch 00068: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 713ms/step - loss: 0.1734 - accuracy: 0.8898 - recall: 0.7924 - f1: 0.8774 - val_loss: 0.3670 - val_accuracy: 0.9044 - val_recall: 0.9044 - val_f1: 0.9044\n",
      "Epoch 69/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1818 - accuracy: 0.8852 - recall: 0.7864 - f1: 0.8722\n",
      "Epoch 00069: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 72s 717ms/step - loss: 0.1817 - accuracy: 0.8852 - recall: 0.7863 - f1: 0.8722 - val_loss: 0.3318 - val_accuracy: 0.9086 - val_recall: 0.9075 - val_f1: 0.9085\n",
      "Epoch 70/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1824 - accuracy: 0.8872 - recall: 0.7904 - f1: 0.8745\n",
      "Epoch 00070: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 713ms/step - loss: 0.1829 - accuracy: 0.8867 - recall: 0.7896 - f1: 0.8739 - val_loss: 0.3232 - val_accuracy: 0.9029 - val_recall: 0.9029 - val_f1: 0.9029\n",
      "Epoch 71/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1856 - accuracy: 0.8830 - recall: 0.7809 - f1: 0.8691\n",
      "Epoch 00071: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 709ms/step - loss: 0.1854 - accuracy: 0.8834 - recall: 0.7817 - f1: 0.8696 - val_loss: 0.3371 - val_accuracy: 0.9146 - val_recall: 0.9128 - val_f1: 0.9144\n",
      "Epoch 72/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1759 - accuracy: 0.8859 - recall: 0.7869 - f1: 0.8728\n",
      "Epoch 00072: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 711ms/step - loss: 0.1759 - accuracy: 0.8856 - recall: 0.7863 - f1: 0.8724 - val_loss: 0.4146 - val_accuracy: 0.8978 - val_recall: 0.8990 - val_f1: 0.8979\n",
      "Epoch 73/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1799 - accuracy: 0.8861 - recall: 0.7867 - f1: 0.8729\n",
      "Epoch 00073: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 715ms/step - loss: 0.1797 - accuracy: 0.8861 - recall: 0.7868 - f1: 0.8730 - val_loss: 0.3562 - val_accuracy: 0.8999 - val_recall: 0.8992 - val_f1: 0.8998\n",
      "Epoch 74/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1860 - accuracy: 0.8802 - recall: 0.7761 - f1: 0.8658\n",
      "Epoch 00074: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 712ms/step - loss: 0.1863 - accuracy: 0.8801 - recall: 0.7763 - f1: 0.8658 - val_loss: 0.3564 - val_accuracy: 0.8985 - val_recall: 0.8962 - val_f1: 0.8983\n",
      "Epoch 75/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1792 - accuracy: 0.8878 - recall: 0.7905 - f1: 0.8753\n",
      "Epoch 00075: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 709ms/step - loss: 0.1796 - accuracy: 0.8877 - recall: 0.7903 - f1: 0.8751 - val_loss: 0.3897 - val_accuracy: 0.9044 - val_recall: 0.9035 - val_f1: 0.9043\n",
      "Epoch 76/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1872 - accuracy: 0.8843 - recall: 0.7859 - f1: 0.8711\n",
      "Epoch 00076: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 709ms/step - loss: 0.1871 - accuracy: 0.8844 - recall: 0.7860 - f1: 0.8713 - val_loss: 0.3572 - val_accuracy: 0.9140 - val_recall: 0.9144 - val_f1: 0.9140\n",
      "Epoch 77/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1782 - accuracy: 0.8828 - recall: 0.7811 - f1: 0.8691\n",
      "Epoch 00077: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 712ms/step - loss: 0.1782 - accuracy: 0.8829 - recall: 0.7813 - f1: 0.8693 - val_loss: 0.3016 - val_accuracy: 0.9094 - val_recall: 0.9085 - val_f1: 0.9094\n",
      "Epoch 78/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1793 - accuracy: 0.8839 - recall: 0.7837 - f1: 0.8704\n",
      "Epoch 00078: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 71s 709ms/step - loss: 0.1794 - accuracy: 0.8839 - recall: 0.7836 - f1: 0.8703 - val_loss: 0.3608 - val_accuracy: 0.9068 - val_recall: 0.9057 - val_f1: 0.9067\n",
      "Epoch 79/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1782 - accuracy: 0.8837 - recall: 0.7813 - f1: 0.8700\n",
      "Epoch 00079: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 72s 716ms/step - loss: 0.1783 - accuracy: 0.8834 - recall: 0.7812 - f1: 0.8697 - val_loss: 0.3178 - val_accuracy: 0.8973 - val_recall: 0.8950 - val_f1: 0.8971\n",
      "Epoch 80/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1838 - accuracy: 0.8835 - recall: 0.7823 - f1: 0.8698\n",
      "Epoch 00080: val_loss did not improve from 0.27913\n",
      "100/100 [==============================] - 72s 717ms/step - loss: 0.1835 - accuracy: 0.8836 - recall: 0.7823 - f1: 0.8699 - val_loss: 0.3710 - val_accuracy: 0.8762 - val_recall: 0.8752 - val_f1: 0.8761\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.train(\n",
    "    EPOCHS, STEPS_PER_EPOCH, n_train, n_val, batch_size, data_aug_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(history.history[\"loss\"][1:], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"][1:], label=\"val_loss\")\n",
    "plt.plot(history.history[\"accuracy\"][1:], label=\"train_acc\")\n",
    "plt.plot(history.history[\"val_accuracy\"][1:], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"Unet_batchnorm.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 70, 70, 3)    0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 32, 32, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 32, 32, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 32, 32, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 34, 34, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 16, 16, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 16, 16, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 16, 16, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 16, 16, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 16, 16, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 16, 16, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 16, 16, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 16, 16, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 16, 16, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 16, 16, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 16, 16, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 16, 16, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 16, 16, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 16, 16, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 16, 16, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 8, 8, 128)    32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 8, 8, 128)    0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 8, 8, 128)    0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 8, 8, 512)    131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 8, 8, 512)    0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 8, 8, 512)    0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 8, 8, 128)    0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 8, 8, 128)    0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 8, 8, 512)    0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 8, 8, 512)    0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 8, 8, 128)    0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 8, 8, 128)    0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 8, 8, 512)    0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 8, 8, 512)    0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 8, 8, 128)    0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 8, 8, 128)    0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 8, 8, 512)    0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 8, 8, 512)    0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 4, 4, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 4, 4, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 4, 4, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 4, 4, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 4, 4, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 4, 4, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 4, 4, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 4, 4, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 4, 4, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 4, 4, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 4, 4, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 4, 4, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 4, 4, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 4, 4, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 4, 4, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 4, 4, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 4, 4, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 4, 4, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 4, 4, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 4, 4, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 4, 4, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 4, 4, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 4, 4, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 4, 4, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 2, 2, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 2, 2, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 2, 2, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 2, 2, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 2, 2, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 2, 2, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 2, 2, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 2, 2, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 2, 2, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 2, 2, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 2, 2, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 2, 2, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 2048)   0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 1, 1, 128)    2359424     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_24 (Conv2DTran (None, 2, 2, 64)     131136      conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 2, 2, 64)     256         conv2d_transpose_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 2, 2, 2112)   0           batch_normalization_24[0][0]     \n",
      "                                                                 conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 2, 2, 128)    2433152     concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_25 (Conv2DTran (None, 4, 4, 64)     131136      conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 4, 4, 64)     256         conv2d_transpose_25[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 4, 4, 1088)   0           batch_normalization_25[0][0]     \n",
      "                                                                 conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 4, 4, 128)    1253504     concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_26 (Conv2DTran (None, 8, 8, 64)     131136      conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 8, 8, 64)     256         conv2d_transpose_26[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 8, 8, 576)    0           batch_normalization_26[0][0]     \n",
      "                                                                 conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 8, 8, 64)     331840      concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_27 (Conv2DTran (None, 16, 16, 16)   16400       conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 16, 16, 16)   64          conv2d_transpose_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 272)  0           batch_normalization_27[0][0]     \n",
      "                                                                 conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 16, 16, 32)   78368       concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_28 (Conv2DTran (None, 32, 32, 32)   16416       conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 32, 32, 32)   128         conv2d_transpose_28[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 32, 32)   9248        batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_29 (Conv2DTran (None, 64, 64, 8)    4104        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 64, 64, 8)    32          conv2d_transpose_29[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 64, 64, 8)    584         batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 64, 64, 1)    9           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 4096)         0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 32)           131104      flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 32)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 32)           0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 2)            66          leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 2)            0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 2)            0           dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 30,616,331\n",
      "Trainable params: 30,562,715\n",
      "Non-trainable params: 53,616\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 70, 70, 3)    0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 32, 32, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 32, 32, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 32, 32, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 34, 34, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 16, 16, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 16, 16, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 16, 16, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 16, 16, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 16, 16, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 16, 16, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 16, 16, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 16, 16, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 16, 16, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 16, 16, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 16, 16, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 16, 16, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 16, 16, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 16, 16, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 16, 16, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 8, 8, 128)    32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 8, 8, 128)    0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 8, 8, 128)    0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 8, 8, 512)    131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 8, 8, 512)    0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 8, 8, 512)    0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 8, 8, 128)    0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 8, 8, 128)    0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 8, 8, 512)    0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 8, 8, 512)    0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 8, 8, 128)    0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 8, 8, 128)    0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 8, 8, 512)    0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 8, 8, 512)    0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 8, 8, 128)    0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 8, 8, 128)    0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 8, 8, 512)    0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 8, 8, 512)    0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 4, 4, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 4, 4, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 4, 4, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 4, 4, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 4, 4, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 4, 4, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 4, 4, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 4, 4, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 4, 4, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 4, 4, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 4, 4, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 4, 4, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 4, 4, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 4, 4, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 4, 4, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 4, 4, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 4, 4, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 4, 4, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 4, 4, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 4, 4, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 4, 4, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 4, 4, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 4, 4, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 4, 4, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 2, 2, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 2, 2, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 2, 2, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 2, 2, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 2, 2, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 2, 2, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 2, 2, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 2, 2, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 2, 2, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 2, 2, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 2, 2, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 2, 2, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 2048)   0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 1, 1, 128)    2359424     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_18 (Conv2DTran (None, 2, 2, 64)     131136      conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 2, 2, 64)     256         conv2d_transpose_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 2, 2, 2112)   0           batch_normalization_18[0][0]     \n",
      "                                                                 conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 2, 2, 128)    2433152     concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_19 (Conv2DTran (None, 4, 4, 64)     131136      conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 4, 4, 64)     256         conv2d_transpose_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 4, 4, 1088)   0           batch_normalization_19[0][0]     \n",
      "                                                                 conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 4, 4, 128)    1253504     concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_20 (Conv2DTran (None, 8, 8, 64)     131136      conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 64)     256         conv2d_transpose_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 8, 8, 576)    0           batch_normalization_20[0][0]     \n",
      "                                                                 conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 64)     331840      concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_21 (Conv2DTran (None, 16, 16, 16)   16400       conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 16)   64          conv2d_transpose_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 16, 16, 272)  0           batch_normalization_21[0][0]     \n",
      "                                                                 conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 32)   78368       concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_22 (Conv2DTran (None, 32, 32, 32)   16416       conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 32, 32, 32)   128         conv2d_transpose_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 32, 32, 32)   9248        batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_23 (Conv2DTran (None, 64, 64, 8)    4104        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 64, 64, 8)    32          conv2d_transpose_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 64, 64, 8)    584         batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 64, 64, 1)    9           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 4096)         0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           131104      flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 32)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            66          leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 2)            0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 2)            0           dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 30,616,331\n",
      "Trainable params: 30,562,715\n",
      "Non-trainable params: 53,616\n",
      "__________________________________________________________________________________________________\n",
      "Processing image => data/test_set_images/test_1/test_1.png\n",
      "Processing image => data/test_set_images/test_2/test_2.png\n",
      "Processing image => data/test_set_images/test_3/test_3.png\n",
      "Processing image => data/test_set_images/test_4/test_4.png\n",
      "Processing image => data/test_set_images/test_5/test_5.png\n",
      "Processing image => data/test_set_images/test_6/test_6.png\n",
      "Processing image => data/test_set_images/test_7/test_7.png\n",
      "Processing image => data/test_set_images/test_8/test_8.png\n",
      "Processing image => data/test_set_images/test_9/test_9.png\n",
      "Processing image => data/test_set_images/test_10/test_10.png\n",
      "Processing image => data/test_set_images/test_11/test_11.png\n",
      "Processing image => data/test_set_images/test_12/test_12.png\n",
      "Processing image => data/test_set_images/test_13/test_13.png\n",
      "Processing image => data/test_set_images/test_14/test_14.png\n",
      "Processing image => data/test_set_images/test_15/test_15.png\n",
      "Processing image => data/test_set_images/test_16/test_16.png\n",
      "Processing image => data/test_set_images/test_17/test_17.png\n",
      "Processing image => data/test_set_images/test_18/test_18.png\n",
      "Processing image => data/test_set_images/test_19/test_19.png\n",
      "Processing image => data/test_set_images/test_20/test_20.png\n",
      "Processing image => data/test_set_images/test_21/test_21.png\n",
      "Processing image => data/test_set_images/test_22/test_22.png\n",
      "Processing image => data/test_set_images/test_23/test_23.png\n",
      "Processing image => data/test_set_images/test_24/test_24.png\n",
      "Processing image => data/test_set_images/test_25/test_25.png\n",
      "Processing image => data/test_set_images/test_26/test_26.png\n",
      "Processing image => data/test_set_images/test_27/test_27.png\n",
      "Processing image => data/test_set_images/test_28/test_28.png\n",
      "Processing image => data/test_set_images/test_29/test_29.png\n",
      "Processing image => data/test_set_images/test_30/test_30.png\n",
      "Processing image => data/test_set_images/test_31/test_31.png\n",
      "Processing image => data/test_set_images/test_32/test_32.png\n",
      "Processing image => data/test_set_images/test_33/test_33.png\n",
      "Processing image => data/test_set_images/test_34/test_34.png\n",
      "Processing image => data/test_set_images/test_35/test_35.png\n",
      "Processing image => data/test_set_images/test_36/test_36.png\n",
      "Processing image => data/test_set_images/test_37/test_37.png\n",
      "Processing image => data/test_set_images/test_38/test_38.png\n",
      "Processing image => data/test_set_images/test_39/test_39.png\n",
      "Processing image => data/test_set_images/test_40/test_40.png\n",
      "Processing image => data/test_set_images/test_41/test_41.png\n",
      "Processing image => data/test_set_images/test_42/test_42.png\n",
      "Processing image => data/test_set_images/test_43/test_43.png\n",
      "Processing image => data/test_set_images/test_44/test_44.png\n",
      "Processing image => data/test_set_images/test_45/test_45.png\n",
      "Processing image => data/test_set_images/test_46/test_46.png\n",
      "Processing image => data/test_set_images/test_47/test_47.png\n",
      "Processing image => data/test_set_images/test_48/test_48.png\n",
      "Processing image => data/test_set_images/test_49/test_49.png\n",
      "Processing image => data/test_set_images/test_50/test_50.png\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "lr = 1e-4\n",
    "model = resnet_unet_model(\n",
    "    shape=(64, 64, 3),\n",
    "    batch_normalization=batch_normalization,\n",
    "    activation=activation,\n",
    "    dropout=dropout,\n",
    "    amsgrad=amsgrad,\n",
    "    lr=lr,\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model.load(\"best_model.h5\")\n",
    "\n",
    "model.model.summary()\n",
    "\n",
    "# We add all test images to an array, used later for generating a submission\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "    image_filenames.append(image_filename)\n",
    "\n",
    "# Set-up submission filename\n",
    "submission_filename = \"resnet_unet.csv\"\n",
    "\n",
    "# Generates the submission\n",
    "generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
