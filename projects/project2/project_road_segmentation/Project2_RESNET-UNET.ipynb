{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext lab_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.compat.v2.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.applications import *\n",
    "from tensorflow.python.client import device_lib\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6427347888322919609\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3156787200\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14205672412297688291\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 980M, pci bus id: 0000:01:00.0, compute capability: 5.2\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training images, images loaded: 85 \n",
      "Loading groundtruth images, images loaded: 85 \n"
     ]
    }
   ],
   "source": [
    "image_dir_train = \"data/training/images/\"\n",
    "files = os.listdir(image_dir_train)\n",
    "n_train = len(files)\n",
    "print(f\"Loading training images, images loaded: {n_train} \")\n",
    "imgs_train = np.asarray(\n",
    "    [load_image(image_dir_train + files[i]) for i in range(n_train)]\n",
    ")\n",
    "gt_dir_train = \"data/training/groundtruth/\"\n",
    "print(f\"Loading groundtruth images, images loaded: {n_train} \")\n",
    "gt_imgs_train = np.asarray(\n",
    "    [load_image(gt_dir_train + files[i]) for i in range(n_train)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 400, 400, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 400, 400)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_imgs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 400\n",
    "# Patches for training\n",
    "img_patches_train = [\n",
    "    crop_image(imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "gt_patches_train = [\n",
    "    crop_image(gt_imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = np.asarray(\n",
    "    [\n",
    "        img_patches_train[i][j]\n",
    "        for i in range(len(img_patches_train))\n",
    "        for j in range(len(img_patches_train[i]))\n",
    "    ]\n",
    ")\n",
    "Y_train = np.asarray(\n",
    "    [\n",
    "        gt_patches_train[i][j]\n",
    "        for i in range(len(gt_patches_train))\n",
    "        for j in range(len(gt_patches_train[i]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 400, 400, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 400, 400)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validating images, images loaded: 15 \n",
      "Loading validating groundtruth, images loaded: 15 \n"
     ]
    }
   ],
   "source": [
    "image_dir_val = \"data/validating/images/\"\n",
    "files = os.listdir(image_dir_val)\n",
    "n_val = len(files)\n",
    "print(f\"Loading validating images, images loaded: {n_val} \")\n",
    "imgs_val = np.asarray([load_image(image_dir_val + files[i]) for i in range(n_val)])\n",
    "gt_dir_val = \"data/validating/groundtruth/\"\n",
    "print(f\"Loading validating groundtruth, images loaded: {n_val} \")\n",
    "gt_imgs_val = np.asarray([load_image(gt_dir_val + files[i]) for i in range(n_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 400, 400, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 400, 400)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 400\n",
    "# Patches for validating\n",
    "img_patches_val = [\n",
    "    crop_image(imgs_val[i], image_size, image_size) for i in range(n_val)\n",
    "]\n",
    "gt_patches_val = [\n",
    "    crop_image(gt_imgs_val[i], image_size, image_size) for i in range(n_val)\n",
    "]\n",
    "\n",
    "# Separate features and labels\n",
    "X_val = np.asarray(\n",
    "    [\n",
    "        img_patches_val[i][j]\n",
    "        for i in range(len(img_patches_val))\n",
    "        for j in range(len(img_patches_val[i]))\n",
    "    ]\n",
    ")\n",
    "Y_val = np.asarray(\n",
    "    [\n",
    "        gt_patches_val[i][j]\n",
    "        for i in range(len(gt_patches_val))\n",
    "        for j in range(len(gt_patches_val[i]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 400, 400, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 400, 400)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, Y_train = imag_rotation_aug(imgs_train, gt_imgs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.asarray(X_train)\n",
    "# Y_train = np.asarray(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85, 400, 400, 3)\n",
      "(85, 400, 400)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "n_train = Y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val, Y_val = imag_rotation_aug(imgs_val, gt_imgs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val = np.asarray(X_val)\n",
    "# Y_val = np.asarray(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 400, 400, 3)\n",
      "(15, 400, 400)\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "n_val = Y_val.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create functions to calcualte precision, recall and F-1 in the training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Compute the Precision for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "        Precision (numpy.float64): the Precision of the batch \n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Compute the Recall for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "       Recall (numpy.float64): the Recal of the batch \n",
    "    \"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    \"\"\"Compute the F-1 for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "       F-1 (numpy.float64): the F-1 of the batch \n",
    "    \"\"\"\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-UNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet_unet_model:\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    def conv_act(self, inputs, out_filters, activation=\"relu\"):\n",
    "        return Conv2D(\n",
    "            filters=out_filters,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "        )(inputs)\n",
    "\n",
    "    def decoder(\n",
    "        self,\n",
    "        inputs,\n",
    "        mid_filters=512,\n",
    "        out_filters=256,\n",
    "        activation=\"relu\",\n",
    "        block_name=\"decoder\",\n",
    "    ):\n",
    "        with K.name_scope(block_name):\n",
    "            if activation == \"leaky_relu\":\n",
    "                activation = None\n",
    "                conv = LeakyReLU(alpha=0.3)(\n",
    "                    self.conv_act(inputs, mid_filters, activation)\n",
    "                )\n",
    "            else:\n",
    "                conv = self.conv_act(inputs, mid_filters, activation)\n",
    "            conv_tr = Conv2DTranspose(\n",
    "                filters=out_filters,\n",
    "                activation=activation,\n",
    "                kernel_size=4,\n",
    "                strides=2,\n",
    "                padding=\"same\",\n",
    "            )(conv)\n",
    "        return conv_tr\n",
    "\n",
    "    def __init__(\n",
    "        self, shape, batch_normalization, activation, dropout, amsgrad=False, lr=1e-4\n",
    "    ):\n",
    "        self.shape = shape\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.amsgrad = amsgrad\n",
    "        self.lr = lr\n",
    "        self.model = self.create_resnet_unet_model()\n",
    "\n",
    "    def create_resnet_unet_model(self):\n",
    "        # Set max pooling parameters\n",
    "        max_pooling_size = 2\n",
    "        max_pooling_strd = 2\n",
    "\n",
    "        # load a pretrained ResNet\n",
    "        num_classes = 2\n",
    "        resnet50 = ResNet50(\n",
    "            include_top=False,\n",
    "            weights=\"imagenet\",\n",
    "            classes=num_classes,\n",
    "            input_shape=self.shape,\n",
    "        )\n",
    "\n",
    "        resnet50.compile(\n",
    "            optimizer=Adam(lr=self.lr, amsgrad=self.amsgrad), loss=\"binary_crossentropy\"\n",
    "        )\n",
    "\n",
    "        # ResNet convolution outputs\n",
    "        conv5_out = resnet50.get_layer(\"conv5_block3_out\").output\n",
    "        conv4_out = resnet50.get_layer(\"conv4_block6_out\").output\n",
    "        conv3_out = resnet50.get_layer(\"conv3_block4_out\").output\n",
    "        conv2_out = resnet50.get_layer(\"conv2_block3_out\").output\n",
    "\n",
    "        # Max pool center layer\n",
    "        pool = MaxPooling2D(max_pooling_size, strides=max_pooling_strd, padding=\"same\")(\n",
    "            resnet50.get_output_at(0)\n",
    "        )\n",
    "\n",
    "        # Decoder center layer\n",
    "        dec_center = self.decoder(\n",
    "            pool,\n",
    "            mid_filters=self.shape[0] * 2,\n",
    "            out_filters=self.shape[0],\n",
    "            activation=self.activation,\n",
    "            block_name=\"decoder_center\",\n",
    "        )\n",
    "        if self.batch_normalization:\n",
    "            dec_center = BatchNormalization()(dec_center)\n",
    "        if self.dropout > 0:\n",
    "            dec_center = Dropout(dropout)(dec_center)\n",
    "\n",
    "        # Decoder 5th layer\n",
    "        cat1 = Concatenate()([dec_center, conv5_out])\n",
    "        dec5 = self.decoder(\n",
    "            cat1,\n",
    "            mid_filters=int(self.shape[0] * 2),\n",
    "            out_filters=int(self.shape[0]),\n",
    "            activation=self.activation,\n",
    "            block_name=\"decoder5\",\n",
    "        )\n",
    "        if self.batch_normalization:\n",
    "            dec5 = BatchNormalization()(dec5)\n",
    "        if self.dropout > 0:\n",
    "            dec5 = Dropout(self.dropout)(dec5)\n",
    "\n",
    "        # Decoder 4th layer\n",
    "        cat2 = Concatenate()([dec5, conv4_out])\n",
    "        dec4 = self.decoder(\n",
    "            cat2,\n",
    "            mid_filters=int(self.shape[0] * 2),\n",
    "            out_filters=int(self.shape[0]),\n",
    "            activation=self.activation,\n",
    "            block_name=\"decoder4\",\n",
    "        )\n",
    "        if self.batch_normalization:\n",
    "            dec4 = BatchNormalization()(dec4)\n",
    "        if self.dropout > 0:\n",
    "            dec4 = Dropout(self.dropout)(dec4)\n",
    "\n",
    "        # Decoder 3rd layer\n",
    "        cat3 = Concatenate()([dec4, conv3_out])\n",
    "        dec3 = self.decoder(\n",
    "            cat3,\n",
    "            mid_filters=int(self.shape[0]),\n",
    "            out_filters=int(self.shape[0] // 4),\n",
    "            activation=self.activation,\n",
    "            block_name=\"decoder3\",\n",
    "        )\n",
    "        if self.batch_normalization:\n",
    "            dec3 = BatchNormalization()(dec3)\n",
    "        if self.dropout > 0:\n",
    "            dec3 = Dropout(self.dropout)(dec3)\n",
    "\n",
    "        # Decoder 2nd layer\n",
    "        cat2 = Concatenate()([dec3, conv2_out])\n",
    "        dec2 = self.decoder(\n",
    "            cat2,\n",
    "            mid_filters=int(self.shape[0] // 2),\n",
    "            out_filters=int(self.shape[0] // 2),\n",
    "            activation=self.activation,\n",
    "            block_name=\"decoder2\",\n",
    "        )\n",
    "        if self.batch_normalization:\n",
    "            dec2 = BatchNormalization()(dec2)\n",
    "        if dropout > 0:\n",
    "            dec2 = Dropout(self.dropout)(dec2)\n",
    "\n",
    "        # Decoder 1st layer\n",
    "        dec1 = self.decoder(\n",
    "            dec2,\n",
    "            mid_filters=int(self.shape[0] // 2),\n",
    "            out_filters=int(self.shape[0] // 8),\n",
    "            activation=self.activation,\n",
    "            block_name=\"decoder1\",\n",
    "        )\n",
    "        if self.batch_normalization:\n",
    "            dec1 = BatchNormalization()(dec1)\n",
    "        if self.dropout > 0:\n",
    "            dec1 = Dropout(self.dropout)(dec1)\n",
    "\n",
    "        # Decoder 0th layer\n",
    "        dec0 = self.conv_act(dec1, out_filters=int(self.shape[0] // 8))\n",
    "        conv_f = Conv2D(1, 1, activation=\"sigmoid\", padding=\"same\")(dec0)\n",
    "        flatten_0 = Flatten()(conv_f)\n",
    "        dense_0 = Dense(\n",
    "            self.shape[0] / 2,\n",
    "            kernel_regularizer=l2(3e-6),\n",
    "            activity_regularizer=l2(3e-6),\n",
    "        )(flatten_0)\n",
    "        dropout_0 = Dropout(0.5)(dense_0)\n",
    "        lk_relu_0 = LeakyReLU(alpha=0.1)(dropout_0)\n",
    "        dense_1 = Dense(\n",
    "            2, \n",
    "            kernel_regularizer=l2(3e-6), \n",
    "            activity_regularizer=l2(3e-6))(\n",
    "            lk_relu_0\n",
    "        )\n",
    "        dropout_1 = Dropout(0.3)(dense_1)\n",
    "        output = Activation(\"sigmoid\")(dropout_1)\n",
    "        model = Model(inputs=resnet50.get_input_at(0), outputs=output)\n",
    "\n",
    "        # Compile the model using the Adam optimizer with accuracy, recall and f1 metrics\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(lr=lr, amsgrad=self.amsgrad),\n",
    "            metrics=[\"accuracy\", recall, f1],\n",
    "        )\n",
    "\n",
    "        # Print summary\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        epochs,\n",
    "        steps_per_epoch,\n",
    "        n_train=85,\n",
    "        n_val=15,\n",
    "        batch_size=100,\n",
    "        data_aug_factor=1,\n",
    "    ):\n",
    "\n",
    "        # Stop if the model does not get better after 4 steps\n",
    "        e_stop = EarlyStopping(\n",
    "            monitor=\"val_loss\", min_delta=0, patience=4, verbose=1, mode=\"auto\"\n",
    "        )\n",
    "\n",
    "        # Reduce the learning rate of the model after 30 steps\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor=\"loss\", factor=0.5, patience=10, verbose=1, mode=\"auto\"\n",
    "        )\n",
    "\n",
    "        # Save the best model\n",
    "        weights_filename = \"model_\"\n",
    "        if self.batch_normalization:\n",
    "            weights_filename = weights_filename + \"batch_\"\n",
    "        weights_filename = (\n",
    "            weights_filename\n",
    "            + self.activation\n",
    "            + \"_\"\n",
    "            + str(epochs)\n",
    "            + \"_\"\n",
    "            + \"dropout_\"\n",
    "            + str(self.dropout)\n",
    "            + \"_\"\n",
    "            + \"{epoch:03d}_\"\n",
    "            + \"{f1:03f}_\"\n",
    "            + \"{val_accuracy:03f}.h5\"\n",
    "        )\n",
    "        save_best_model = ModelCheckpoint(\n",
    "            weights_filename,\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"auto\",\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        cbs = [save_best_model, reduce_lr]\n",
    "\n",
    "        # Train the model\n",
    "        history = self.model.fit_generator(\n",
    "            create_minibatch(\n",
    "                X_train,\n",
    "                Y_train,\n",
    "                data_aug_factor * n_train,\n",
    "                batch_size=batch_size,\n",
    "                patch_size=16,\n",
    "                width=400,\n",
    "            ),\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            epochs=epochs,\n",
    "            use_multiprocessing=False,\n",
    "            workers=1,\n",
    "            callbacks=cbs,\n",
    "            verbose=1,\n",
    "            validation_data=create_minibatch(\n",
    "                X_val,\n",
    "                Y_val,\n",
    "                data_aug_factor * n_val,\n",
    "                batch_size=batch_size,\n",
    "                patch_size=16,\n",
    "                width=400,\n",
    "            ),\n",
    "            validation_steps=steps_per_epoch,\n",
    "        )\n",
    "\n",
    "        return history\n",
    "\n",
    "    def classify(self, X):\n",
    "        \"\"\"Classify Image as either road or not.\n",
    "        Args:\n",
    "            X (image): part of the image to classify\n",
    "        Returns:\n",
    "            Predictions : Predictions for each patch\n",
    "        \"\"\"\n",
    "        # Subdivide the images into blocks with a stride and patch_size of 16\n",
    "        img_patches = create_patches(X, 16, 16, padding=24)\n",
    "\n",
    "        # Predict\n",
    "        predictions = self.model.predict(img_patches)\n",
    "        predictions = (predictions[:, 0] < predictions[:, 1]) * 1\n",
    "\n",
    "        # Regroup patches into images\n",
    "        return predictions.reshape(X.shape[0], -1)\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"Loads Saved Model.\n",
    "        Args:\n",
    "           filename (string): name of the model\n",
    "           \n",
    "        \"\"\"\n",
    "        # Load the model (used for submission)\n",
    "        dependencies = {\"recall\": recall, \"f1\": f1}\n",
    "        self.model = load_model(filename, custom_objects=dependencies)\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Saves trained model.\n",
    "        Args:\n",
    "           filename (string): name of the model\n",
    "           \n",
    "        \"\"\"\n",
    "        self.model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug_factor = 1\n",
    "batch_normalization = True\n",
    "activation = \"relu\"\n",
    "dropout = 0\n",
    "amsgrad = False\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 70, 70, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 32, 32, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 32, 32, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 32, 32, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 34, 34, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 16, 16, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 16, 16, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 16, 16, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 16, 16, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 16, 16, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 16, 16, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 16, 16, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 16, 16, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 16, 16, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 16, 16, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 16, 16, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 16, 16, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 16, 16, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 16, 16, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 16, 16, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 8, 8, 128)    32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 8, 8, 128)    0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 8, 8, 128)    0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 8, 8, 512)    131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 8, 8, 512)    0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 8, 8, 512)    0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 8, 8, 128)    0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 8, 8, 128)    0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 8, 8, 512)    0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 8, 8, 512)    0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 8, 8, 128)    0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 8, 8, 128)    0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 8, 8, 512)    0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 8, 8, 512)    0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 8, 8, 128)    0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 8, 8, 128)    0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 8, 8, 512)    0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 8, 8, 512)    0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 4, 4, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 4, 4, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 4, 4, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 4, 4, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 4, 4, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 4, 4, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 4, 4, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 4, 4, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 4, 4, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 4, 4, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 4, 4, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 4, 4, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 4, 4, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 4, 4, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 4, 4, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 4, 4, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 4, 4, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 4, 4, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 4, 4, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 4, 4, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 4, 4, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 4, 4, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 4, 4, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 4, 4, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 2, 2, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 2, 2, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 2, 2, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 2, 2, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 2, 2, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 2, 2, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 2, 2, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 2, 2, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 2, 2, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 2, 2, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 2, 2, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 2, 2, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 2048)   0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 1, 1, 128)    2359424     max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 2, 2, 64)     131136      conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 2, 2, 64)     256         conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2, 2, 2112)   0           batch_normalization[0][0]        \n",
      "                                                                 conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 2, 2, 128)    2433152     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 4, 4, 64)     131136      conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 4, 4, 64)     256         conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 4, 1088)   0           batch_normalization_1[0][0]      \n",
      "                                                                 conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 4, 4, 128)    1253504     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 8, 8, 64)     131136      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 8, 8, 64)     256         conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8, 8, 576)    0           batch_normalization_2[0][0]      \n",
      "                                                                 conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 64)     331840      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 16, 16, 16)   16400       conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 16)   64          conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 16, 16, 272)  0           batch_normalization_3[0][0]      \n",
      "                                                                 conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 32)   78368       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 32, 32, 32)   16416       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 64, 64, 8)    4104        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 64, 8)    32          conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 8)    584         batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 1)    9           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4096)         0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           131104      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 32)           0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            66          leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2)            0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 2)            0           dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 30,616,331\n",
      "Trainable params: 30,562,715\n",
      "Non-trainable params: 53,616\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "EPOCHS = 80\n",
    "STEPS_PER_EPOCH = 100\n",
    "batch_size = 100\n",
    "\n",
    "model = resnet_unet_model(\n",
    "    shape=(64, 64, 3),\n",
    "    batch_normalization=batch_normalization,\n",
    "    activation=activation,\n",
    "    dropout=dropout,\n",
    "    amsgrad=amsgrad,\n",
    "    lr=lr,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6193 - accuracy: 0.6595 - recall: 0.5118 - f1: 0.5988\n",
      "Epoch 00001: val_loss improved from inf to 0.56765, saving model to model_batch_relu_80_dropout_0_001_0.598860_0.750900.h5\n",
      "100/100 [==============================] - 78s 777ms/step - loss: 0.6189 - accuracy: 0.6597 - recall: 0.5117 - f1: 0.5989 - val_loss: 0.5676 - val_accuracy: 0.7509 - val_recall: 0.7509 - val_f1: 0.7509\n",
      "Epoch 2/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.4581 - accuracy: 0.7317 - recall: 0.5787 - f1: 0.6824\n",
      "Epoch 00002: val_loss did not improve from 0.56765\n",
      "100/100 [==============================] - 70s 700ms/step - loss: 0.4578 - accuracy: 0.7319 - recall: 0.5789 - f1: 0.6827 - val_loss: 0.7657 - val_accuracy: 0.7519 - val_recall: 0.7519 - val_f1: 0.7519\n",
      "Epoch 3/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3620 - accuracy: 0.7963 - recall: 0.6494 - f1: 0.7603\n",
      "Epoch 00003: val_loss did not improve from 0.56765\n",
      "100/100 [==============================] - 70s 703ms/step - loss: 0.3610 - accuracy: 0.7970 - recall: 0.6505 - f1: 0.7613 - val_loss: 1.2148 - val_accuracy: 0.7488 - val_recall: 0.7488 - val_f1: 0.7488\n",
      "Epoch 4/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3366 - accuracy: 0.8031 - recall: 0.6554 - f1: 0.7682\n",
      "Epoch 00004: val_loss did not improve from 0.56765\n",
      "100/100 [==============================] - 69s 695ms/step - loss: 0.3360 - accuracy: 0.8036 - recall: 0.6560 - f1: 0.7687 - val_loss: 0.5850 - val_accuracy: 0.7444 - val_recall: 0.7451 - val_f1: 0.7446\n",
      "Epoch 5/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3198 - accuracy: 0.8069 - recall: 0.6535 - f1: 0.7709\n",
      "Epoch 00005: val_loss did not improve from 0.56765\n",
      "100/100 [==============================] - 69s 695ms/step - loss: 0.3202 - accuracy: 0.8067 - recall: 0.6532 - f1: 0.7706 - val_loss: 1.0249 - val_accuracy: 0.7510 - val_recall: 0.7510 - val_f1: 0.7510\n",
      "Epoch 6/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3123 - accuracy: 0.8072 - recall: 0.6571 - f1: 0.7722\n",
      "Epoch 00006: val_loss did not improve from 0.56765\n",
      "100/100 [==============================] - 70s 697ms/step - loss: 0.3124 - accuracy: 0.8068 - recall: 0.6566 - f1: 0.7718 - val_loss: 1.5450 - val_accuracy: 0.7419 - val_recall: 0.7419 - val_f1: 0.7419\n",
      "Epoch 7/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3042 - accuracy: 0.8121 - recall: 0.6629 - f1: 0.7785\n",
      "Epoch 00007: val_loss did not improve from 0.56765\n",
      "100/100 [==============================] - 70s 695ms/step - loss: 0.3041 - accuracy: 0.8122 - recall: 0.6629 - f1: 0.7785 - val_loss: 0.7989 - val_accuracy: 0.7595 - val_recall: 0.7595 - val_f1: 0.7595\n",
      "Epoch 8/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.8224 - recall: 0.6767 - f1: 0.7914\n",
      "Epoch 00008: val_loss did not improve from 0.56765\n",
      "100/100 [==============================] - 69s 693ms/step - loss: 0.2890 - accuracy: 0.8220 - recall: 0.6760 - f1: 0.7908 - val_loss: 0.8942 - val_accuracy: 0.7612 - val_recall: 0.7630 - val_f1: 0.7616\n",
      "Epoch 9/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.8241 - recall: 0.6799 - f1: 0.7939\n",
      "Epoch 00009: val_loss improved from 0.56765 to 0.39663, saving model to model_batch_relu_80_dropout_0_009_0.793845_0.824100.h5\n",
      "100/100 [==============================] - 71s 708ms/step - loss: 0.2924 - accuracy: 0.8240 - recall: 0.6800 - f1: 0.7938 - val_loss: 0.3966 - val_accuracy: 0.8241 - val_recall: 0.8295 - val_f1: 0.8250\n",
      "Epoch 10/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.8232 - recall: 0.6784 - f1: 0.7923\n",
      "Epoch 00010: val_loss did not improve from 0.39663\n",
      "100/100 [==============================] - 69s 695ms/step - loss: 0.2851 - accuracy: 0.8235 - recall: 0.6790 - f1: 0.7928 - val_loss: 0.4958 - val_accuracy: 0.8373 - val_recall: 0.8420 - val_f1: 0.8381\n",
      "Epoch 11/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2830 - accuracy: 0.8206 - recall: 0.6707 - f1: 0.7881\n",
      "Epoch 00011: val_loss improved from 0.39663 to 0.28185, saving model to model_batch_relu_80_dropout_0_011_0.788404_0.896700.h5\n",
      "100/100 [==============================] - 71s 709ms/step - loss: 0.2830 - accuracy: 0.8207 - recall: 0.6711 - f1: 0.7884 - val_loss: 0.2818 - val_accuracy: 0.8967 - val_recall: 0.8988 - val_f1: 0.8969\n",
      "Epoch 12/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.8216 - recall: 0.6720 - f1: 0.7894\n",
      "Epoch 00012: val_loss improved from 0.28185 to 0.25115, saving model to model_batch_relu_80_dropout_0_012_0.788803_0.893400.h5\n",
      "100/100 [==============================] - 71s 712ms/step - loss: 0.2872 - accuracy: 0.8212 - recall: 0.6712 - f1: 0.7888 - val_loss: 0.2511 - val_accuracy: 0.8934 - val_recall: 0.8961 - val_f1: 0.8937\n",
      "Epoch 13/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2745 - accuracy: 0.8261 - recall: 0.6777 - f1: 0.7950\n",
      "Epoch 00013: val_loss did not improve from 0.25115\n",
      "100/100 [==============================] - 70s 701ms/step - loss: 0.2744 - accuracy: 0.8264 - recall: 0.6784 - f1: 0.7955 - val_loss: 0.2525 - val_accuracy: 0.9032 - val_recall: 0.9054 - val_f1: 0.9034\n",
      "Epoch 14/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2730 - accuracy: 0.8240 - recall: 0.6726 - f1: 0.7917\n",
      "Epoch 00014: val_loss did not improve from 0.25115\n",
      "100/100 [==============================] - 70s 697ms/step - loss: 0.2727 - accuracy: 0.8242 - recall: 0.6731 - f1: 0.7920 - val_loss: 0.2723 - val_accuracy: 0.9044 - val_recall: 0.9072 - val_f1: 0.9047\n",
      "Epoch 15/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2737 - accuracy: 0.8285 - recall: 0.6818 - f1: 0.7981\n",
      "Epoch 00015: val_loss did not improve from 0.25115\n",
      "100/100 [==============================] - 70s 698ms/step - loss: 0.2748 - accuracy: 0.8285 - recall: 0.6821 - f1: 0.7982 - val_loss: 0.2748 - val_accuracy: 0.9074 - val_recall: 0.9110 - val_f1: 0.9078\n",
      "Epoch 16/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2759 - accuracy: 0.8227 - recall: 0.6699 - f1: 0.7895\n",
      "Epoch 00016: val_loss improved from 0.25115 to 0.23487, saving model to model_batch_relu_80_dropout_0_016_0.789667_0.903100.h5\n",
      "100/100 [==============================] - 71s 710ms/step - loss: 0.2755 - accuracy: 0.8228 - recall: 0.6700 - f1: 0.7897 - val_loss: 0.2349 - val_accuracy: 0.9031 - val_recall: 0.9101 - val_f1: 0.9038\n",
      "Epoch 17/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2717 - accuracy: 0.8260 - recall: 0.6747 - f1: 0.7942\n",
      "Epoch 00017: val_loss did not improve from 0.23487\n",
      "100/100 [==============================] - 70s 697ms/step - loss: 0.2721 - accuracy: 0.8257 - recall: 0.6744 - f1: 0.7938 - val_loss: 0.2517 - val_accuracy: 0.9100 - val_recall: 0.9147 - val_f1: 0.9104\n",
      "Epoch 18/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2633 - accuracy: 0.8298 - recall: 0.6822 - f1: 0.7996\n",
      "Epoch 00018: val_loss did not improve from 0.23487\n",
      "100/100 [==============================] - 70s 698ms/step - loss: 0.2632 - accuracy: 0.8296 - recall: 0.6816 - f1: 0.7992 - val_loss: 0.2404 - val_accuracy: 0.9022 - val_recall: 0.9076 - val_f1: 0.9027\n",
      "Epoch 19/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2706 - accuracy: 0.8296 - recall: 0.6830 - f1: 0.7996\n",
      "Epoch 00019: val_loss did not improve from 0.23487\n",
      "100/100 [==============================] - 70s 699ms/step - loss: 0.2703 - accuracy: 0.8295 - recall: 0.6828 - f1: 0.7994 - val_loss: 0.2382 - val_accuracy: 0.9027 - val_recall: 0.9017 - val_f1: 0.9026\n",
      "Epoch 20/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2652 - accuracy: 0.8298 - recall: 0.6821 - f1: 0.7996\n",
      "Epoch 00020: val_loss did not improve from 0.23487\n",
      "100/100 [==============================] - 71s 707ms/step - loss: 0.2656 - accuracy: 0.8299 - recall: 0.6824 - f1: 0.7998 - val_loss: 0.2384 - val_accuracy: 0.9062 - val_recall: 0.9087 - val_f1: 0.9065\n",
      "Epoch 21/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2699 - accuracy: 0.8255 - recall: 0.6745 - f1: 0.7937\n",
      "Epoch 00021: val_loss did not improve from 0.23487\n",
      "100/100 [==============================] - 70s 696ms/step - loss: 0.2698 - accuracy: 0.8256 - recall: 0.6750 - f1: 0.7939 - val_loss: 0.2368 - val_accuracy: 0.9100 - val_recall: 0.9110 - val_f1: 0.9101\n",
      "Epoch 22/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2696 - accuracy: 0.8300 - recall: 0.6814 - f1: 0.7998\n",
      "Epoch 00022: val_loss did not improve from 0.23487\n",
      "100/100 [==============================] - 71s 708ms/step - loss: 0.2696 - accuracy: 0.8299 - recall: 0.6812 - f1: 0.7997 - val_loss: 0.2574 - val_accuracy: 0.8988 - val_recall: 0.9006 - val_f1: 0.8990\n",
      "Epoch 23/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2674 - accuracy: 0.8277 - recall: 0.6765 - f1: 0.7961\n",
      "Epoch 00023: val_loss improved from 0.23487 to 0.23366, saving model to model_batch_relu_80_dropout_0_023_0.796018_0.910800.h5\n",
      "100/100 [==============================] - 71s 713ms/step - loss: 0.2676 - accuracy: 0.8275 - recall: 0.6764 - f1: 0.7960 - val_loss: 0.2337 - val_accuracy: 0.9108 - val_recall: 0.9123 - val_f1: 0.9109\n",
      "Epoch 24/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2527 - accuracy: 0.8335 - recall: 0.6864 - f1: 0.8039\n",
      "Epoch 00024: val_loss improved from 0.23366 to 0.20169, saving model to model_batch_relu_80_dropout_0_024_0.804204_0.920300.h5\n",
      "100/100 [==============================] - 71s 710ms/step - loss: 0.2528 - accuracy: 0.8337 - recall: 0.6868 - f1: 0.8042 - val_loss: 0.2017 - val_accuracy: 0.9203 - val_recall: 0.9245 - val_f1: 0.9206\n",
      "Epoch 25/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2638 - accuracy: 0.8296 - recall: 0.6821 - f1: 0.7992\n",
      "Epoch 00025: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 69s 695ms/step - loss: 0.2642 - accuracy: 0.8295 - recall: 0.6820 - f1: 0.7990 - val_loss: 0.2159 - val_accuracy: 0.9130 - val_recall: 0.9138 - val_f1: 0.9131\n",
      "Epoch 26/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2598 - accuracy: 0.8289 - recall: 0.6781 - f1: 0.7976\n",
      "Epoch 00026: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 695ms/step - loss: 0.2596 - accuracy: 0.8291 - recall: 0.6783 - f1: 0.7978 - val_loss: 0.2657 - val_accuracy: 0.9030 - val_recall: 0.9065 - val_f1: 0.9033\n",
      "Epoch 27/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2597 - accuracy: 0.8283 - recall: 0.6778 - f1: 0.7972\n",
      "Epoch 00027: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 696ms/step - loss: 0.2594 - accuracy: 0.8284 - recall: 0.6779 - f1: 0.7973 - val_loss: 0.2963 - val_accuracy: 0.9065 - val_recall: 0.9070 - val_f1: 0.9066\n",
      "Epoch 28/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2614 - accuracy: 0.8295 - recall: 0.6795 - f1: 0.7988\n",
      "Epoch 00028: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 69s 694ms/step - loss: 0.2615 - accuracy: 0.8296 - recall: 0.6796 - f1: 0.7990 - val_loss: 0.2553 - val_accuracy: 0.9052 - val_recall: 0.9073 - val_f1: 0.9054\n",
      "Epoch 29/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2518 - accuracy: 0.8349 - recall: 0.6874 - f1: 0.8054\n",
      "Epoch 00029: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 696ms/step - loss: 0.2521 - accuracy: 0.8347 - recall: 0.6870 - f1: 0.8051 - val_loss: 0.2520 - val_accuracy: 0.9003 - val_recall: 0.9020 - val_f1: 0.9005\n",
      "Epoch 30/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2543 - accuracy: 0.8328 - recall: 0.6826 - f1: 0.8024\n",
      "Epoch 00030: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 698ms/step - loss: 0.2542 - accuracy: 0.8324 - recall: 0.6819 - f1: 0.8018 - val_loss: 0.2721 - val_accuracy: 0.8922 - val_recall: 0.8953 - val_f1: 0.8925\n",
      "Epoch 31/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2534 - accuracy: 0.8340 - recall: 0.6848 - f1: 0.8041\n",
      "Epoch 00031: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 699ms/step - loss: 0.2537 - accuracy: 0.8339 - recall: 0.6846 - f1: 0.8039 - val_loss: 0.2252 - val_accuracy: 0.9211 - val_recall: 0.9234 - val_f1: 0.9213\n",
      "Epoch 32/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2580 - accuracy: 0.8317 - recall: 0.6815 - f1: 0.8012\n",
      "Epoch 00032: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 696ms/step - loss: 0.2577 - accuracy: 0.8317 - recall: 0.6816 - f1: 0.8013 - val_loss: 0.2709 - val_accuracy: 0.9146 - val_recall: 0.9167 - val_f1: 0.9147\n",
      "Epoch 33/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2633 - accuracy: 0.8260 - recall: 0.6745 - f1: 0.7943\n",
      "Epoch 00033: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 698ms/step - loss: 0.2633 - accuracy: 0.8260 - recall: 0.6744 - f1: 0.7942 - val_loss: 0.2378 - val_accuracy: 0.9166 - val_recall: 0.9148 - val_f1: 0.9165\n",
      "Epoch 34/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2562 - accuracy: 0.8280 - recall: 0.6741 - f1: 0.7958\n",
      "Epoch 00034: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 69s 695ms/step - loss: 0.2560 - accuracy: 0.8283 - recall: 0.6747 - f1: 0.7962 - val_loss: 0.2510 - val_accuracy: 0.9134 - val_recall: 0.9163 - val_f1: 0.9137\n",
      "Epoch 35/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2592 - accuracy: 0.8320 - recall: 0.6815 - f1: 0.8013\n",
      "Epoch 00035: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 698ms/step - loss: 0.2595 - accuracy: 0.8317 - recall: 0.6811 - f1: 0.8010 - val_loss: 0.2520 - val_accuracy: 0.9081 - val_recall: 0.9100 - val_f1: 0.9083\n",
      "Epoch 36/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2545 - accuracy: 0.8315 - recall: 0.6793 - f1: 0.8002\n",
      "Epoch 00036: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 697ms/step - loss: 0.2543 - accuracy: 0.8321 - recall: 0.6806 - f1: 0.8010 - val_loss: 0.2144 - val_accuracy: 0.9190 - val_recall: 0.9210 - val_f1: 0.9192\n",
      "Epoch 37/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2562 - accuracy: 0.8327 - recall: 0.6811 - f1: 0.8018\n",
      "Epoch 00037: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 698ms/step - loss: 0.2560 - accuracy: 0.8331 - recall: 0.6819 - f1: 0.8024 - val_loss: 0.2196 - val_accuracy: 0.9208 - val_recall: 0.9235 - val_f1: 0.9210\n",
      "Epoch 38/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2570 - accuracy: 0.8310 - recall: 0.6801 - f1: 0.8000\n",
      "Epoch 00038: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 695ms/step - loss: 0.2570 - accuracy: 0.8311 - recall: 0.6803 - f1: 0.8002 - val_loss: 0.2085 - val_accuracy: 0.9201 - val_recall: 0.9226 - val_f1: 0.9203\n",
      "Epoch 39/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2502 - accuracy: 0.8338 - recall: 0.6845 - f1: 0.8037\n",
      "Epoch 00039: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 699ms/step - loss: 0.2501 - accuracy: 0.8340 - recall: 0.6847 - f1: 0.8039 - val_loss: 0.2471 - val_accuracy: 0.9115 - val_recall: 0.9055 - val_f1: 0.9110\n",
      "Epoch 40/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2539 - accuracy: 0.8334 - recall: 0.6830 - f1: 0.8032\n",
      "Epoch 00040: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 69s 694ms/step - loss: 0.2537 - accuracy: 0.8334 - recall: 0.6830 - f1: 0.8032 - val_loss: 0.2318 - val_accuracy: 0.9119 - val_recall: 0.9118 - val_f1: 0.9119\n",
      "Epoch 41/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2513 - accuracy: 0.8329 - recall: 0.6812 - f1: 0.8021\n",
      "Epoch 00041: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 69s 692ms/step - loss: 0.2517 - accuracy: 0.8325 - recall: 0.6806 - f1: 0.8016 - val_loss: 0.2292 - val_accuracy: 0.9104 - val_recall: 0.9146 - val_f1: 0.9109\n",
      "Epoch 42/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2467 - accuracy: 0.8353 - recall: 0.6849 - f1: 0.8054\n",
      "Epoch 00042: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 695ms/step - loss: 0.2464 - accuracy: 0.8353 - recall: 0.6850 - f1: 0.8054 - val_loss: 0.2631 - val_accuracy: 0.9027 - val_recall: 0.9045 - val_f1: 0.9029\n",
      "Epoch 43/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2534 - accuracy: 0.8324 - recall: 0.6829 - f1: 0.8018\n",
      "Epoch 00043: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 71s 709ms/step - loss: 0.2538 - accuracy: 0.8325 - recall: 0.6832 - f1: 0.8021 - val_loss: 0.2452 - val_accuracy: 0.9115 - val_recall: 0.9165 - val_f1: 0.9119\n",
      "Epoch 44/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2515 - accuracy: 0.8347 - recall: 0.6862 - f1: 0.8053\n",
      "Epoch 00044: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 72s 715ms/step - loss: 0.2517 - accuracy: 0.8345 - recall: 0.6861 - f1: 0.8051 - val_loss: 0.2549 - val_accuracy: 0.9111 - val_recall: 0.9112 - val_f1: 0.9112\n",
      "Epoch 45/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2532 - accuracy: 0.8327 - recall: 0.6846 - f1: 0.8028\n",
      "Epoch 00045: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 701ms/step - loss: 0.2530 - accuracy: 0.8328 - recall: 0.6847 - f1: 0.8029 - val_loss: 0.2482 - val_accuracy: 0.9093 - val_recall: 0.9107 - val_f1: 0.9094\n",
      "Epoch 46/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2561 - accuracy: 0.8298 - recall: 0.6789 - f1: 0.7989\n",
      "Epoch 00046: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 699ms/step - loss: 0.2559 - accuracy: 0.8302 - recall: 0.6793 - f1: 0.7993 - val_loss: 0.2606 - val_accuracy: 0.9075 - val_recall: 0.9100 - val_f1: 0.9077\n",
      "Epoch 47/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2526 - accuracy: 0.8330 - recall: 0.6817 - f1: 0.8024\n",
      "Epoch 00047: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 704ms/step - loss: 0.2528 - accuracy: 0.8327 - recall: 0.6811 - f1: 0.8019 - val_loss: 0.2930 - val_accuracy: 0.9089 - val_recall: 0.9085 - val_f1: 0.9088\n",
      "Epoch 48/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2459 - accuracy: 0.8408 - recall: 0.6968 - f1: 0.8134\n",
      "Epoch 00048: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 695ms/step - loss: 0.2456 - accuracy: 0.8408 - recall: 0.6969 - f1: 0.8134 - val_loss: 0.2349 - val_accuracy: 0.9086 - val_recall: 0.9080 - val_f1: 0.9085\n",
      "Epoch 49/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2433 - accuracy: 0.8401 - recall: 0.6942 - f1: 0.8120\n",
      "Epoch 00049: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 69s 694ms/step - loss: 0.2435 - accuracy: 0.8400 - recall: 0.6939 - f1: 0.8118 - val_loss: 0.2301 - val_accuracy: 0.9133 - val_recall: 0.9138 - val_f1: 0.9133\n",
      "Epoch 50/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2487 - accuracy: 0.8359 - recall: 0.6865 - f1: 0.8064\n",
      "Epoch 00050: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 697ms/step - loss: 0.2489 - accuracy: 0.8358 - recall: 0.6862 - f1: 0.8062 - val_loss: 0.2520 - val_accuracy: 0.9176 - val_recall: 0.9188 - val_f1: 0.9177\n",
      "Epoch 51/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2557 - accuracy: 0.8323 - recall: 0.6810 - f1: 0.8018\n",
      "Epoch 00051: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 699ms/step - loss: 0.2557 - accuracy: 0.8322 - recall: 0.6807 - f1: 0.8017 - val_loss: 0.2867 - val_accuracy: 0.9107 - val_recall: 0.9120 - val_f1: 0.9109\n",
      "Epoch 52/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2485 - accuracy: 0.8336 - recall: 0.6835 - f1: 0.8035\n",
      "Epoch 00052: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 69s 695ms/step - loss: 0.2487 - accuracy: 0.8329 - recall: 0.6821 - f1: 0.8025 - val_loss: 0.2885 - val_accuracy: 0.9153 - val_recall: 0.9149 - val_f1: 0.9153\n",
      "Epoch 53/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2471 - accuracy: 0.8348 - recall: 0.6845 - f1: 0.8049\n",
      "Epoch 00053: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 696ms/step - loss: 0.2467 - accuracy: 0.8349 - recall: 0.6847 - f1: 0.8050 - val_loss: 0.3087 - val_accuracy: 0.9060 - val_recall: 0.9073 - val_f1: 0.9061\n",
      "Epoch 54/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2436 - accuracy: 0.8357 - recall: 0.6859 - f1: 0.8059\n",
      "Epoch 00054: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 69s 692ms/step - loss: 0.2435 - accuracy: 0.8357 - recall: 0.6858 - f1: 0.8060 - val_loss: 0.2345 - val_accuracy: 0.9140 - val_recall: 0.9163 - val_f1: 0.9142\n",
      "Epoch 55/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2541 - accuracy: 0.8316 - recall: 0.6807 - f1: 0.8008\n",
      "Epoch 00055: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 69s 693ms/step - loss: 0.2551 - accuracy: 0.8314 - recall: 0.6808 - f1: 0.8008 - val_loss: 0.2873 - val_accuracy: 0.9101 - val_recall: 0.9112 - val_f1: 0.9102\n",
      "Epoch 56/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2487 - accuracy: 0.8352 - recall: 0.6879 - f1: 0.8057\n",
      "Epoch 00056: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 69s 694ms/step - loss: 0.2487 - accuracy: 0.8354 - recall: 0.6883 - f1: 0.8060 - val_loss: 0.2972 - val_accuracy: 0.9083 - val_recall: 0.9098 - val_f1: 0.9084\n",
      "Epoch 57/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2517 - accuracy: 0.8333 - recall: 0.6834 - f1: 0.8030\n",
      "Epoch 00057: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 695ms/step - loss: 0.2522 - accuracy: 0.8327 - recall: 0.6822 - f1: 0.8020 - val_loss: 0.2720 - val_accuracy: 0.9190 - val_recall: 0.9216 - val_f1: 0.9192\n",
      "Epoch 58/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2454 - accuracy: 0.8357 - recall: 0.6858 - f1: 0.8059\n",
      "Epoch 00058: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 695ms/step - loss: 0.2453 - accuracy: 0.8356 - recall: 0.6857 - f1: 0.8058 - val_loss: 0.2937 - val_accuracy: 0.9053 - val_recall: 0.9051 - val_f1: 0.9052\n",
      "Epoch 59/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2437 - accuracy: 0.8378 - recall: 0.6901 - f1: 0.8089\n",
      "Epoch 00059: val_loss did not improve from 0.20169\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "100/100 [==============================] - 70s 705ms/step - loss: 0.2437 - accuracy: 0.8379 - recall: 0.6902 - f1: 0.8090 - val_loss: 0.2737 - val_accuracy: 0.9162 - val_recall: 0.9185 - val_f1: 0.9164\n",
      "Epoch 60/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2416 - accuracy: 0.8381 - recall: 0.6891 - f1: 0.8086\n",
      "Epoch 00060: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 69s 693ms/step - loss: 0.2415 - accuracy: 0.8385 - recall: 0.6898 - f1: 0.8091 - val_loss: 0.2273 - val_accuracy: 0.9219 - val_recall: 0.9226 - val_f1: 0.9220\n",
      "Epoch 61/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2398 - accuracy: 0.8444 - recall: 0.7016 - f1: 0.8175\n",
      "Epoch 00061: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 699ms/step - loss: 0.2395 - accuracy: 0.8444 - recall: 0.7017 - f1: 0.8175 - val_loss: 0.2405 - val_accuracy: 0.9232 - val_recall: 0.9247 - val_f1: 0.9233\n",
      "Epoch 62/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2377 - accuracy: 0.8447 - recall: 0.7005 - f1: 0.8176\n",
      "Epoch 00062: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 69s 695ms/step - loss: 0.2381 - accuracy: 0.8443 - recall: 0.6998 - f1: 0.8171 - val_loss: 0.2377 - val_accuracy: 0.9197 - val_recall: 0.9203 - val_f1: 0.9197\n",
      "Epoch 63/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2399 - accuracy: 0.8364 - recall: 0.6862 - f1: 0.8067\n",
      "Epoch 00063: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 69s 694ms/step - loss: 0.2398 - accuracy: 0.8361 - recall: 0.6856 - f1: 0.8064 - val_loss: 0.2484 - val_accuracy: 0.9218 - val_recall: 0.9216 - val_f1: 0.9218\n",
      "Epoch 64/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2370 - accuracy: 0.8413 - recall: 0.6929 - f1: 0.8130\n",
      "Epoch 00064: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 695ms/step - loss: 0.2369 - accuracy: 0.8412 - recall: 0.6926 - f1: 0.8128 - val_loss: 0.2607 - val_accuracy: 0.9156 - val_recall: 0.9174 - val_f1: 0.9158\n",
      "Epoch 65/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2371 - accuracy: 0.8396 - recall: 0.6898 - f1: 0.8107\n",
      "Epoch 00065: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 703ms/step - loss: 0.2368 - accuracy: 0.8398 - recall: 0.6901 - f1: 0.8109 - val_loss: 0.2543 - val_accuracy: 0.9157 - val_recall: 0.9170 - val_f1: 0.9158\n",
      "Epoch 66/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2360 - accuracy: 0.8392 - recall: 0.6890 - f1: 0.8099\n",
      "Epoch 00066: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 699ms/step - loss: 0.2364 - accuracy: 0.8392 - recall: 0.6890 - f1: 0.8099 - val_loss: 0.2529 - val_accuracy: 0.9126 - val_recall: 0.9127 - val_f1: 0.9126\n",
      "Epoch 67/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2450 - accuracy: 0.8344 - recall: 0.6809 - f1: 0.8036\n",
      "Epoch 00067: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 698ms/step - loss: 0.2446 - accuracy: 0.8342 - recall: 0.6805 - f1: 0.8033 - val_loss: 0.2374 - val_accuracy: 0.9162 - val_recall: 0.9167 - val_f1: 0.9162\n",
      "Epoch 68/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2339 - accuracy: 0.8446 - recall: 0.7001 - f1: 0.8175\n",
      "Epoch 00068: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 696ms/step - loss: 0.2339 - accuracy: 0.8447 - recall: 0.7002 - f1: 0.8176 - val_loss: 0.2495 - val_accuracy: 0.9160 - val_recall: 0.9178 - val_f1: 0.9162\n",
      "Epoch 69/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2342 - accuracy: 0.8405 - recall: 0.6916 - f1: 0.8117\n",
      "Epoch 00069: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 701ms/step - loss: 0.2342 - accuracy: 0.8406 - recall: 0.6919 - f1: 0.8119 - val_loss: 0.3154 - val_accuracy: 0.8986 - val_recall: 0.9010 - val_f1: 0.8989\n",
      "Epoch 70/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2360 - accuracy: 0.8370 - recall: 0.6852 - f1: 0.8071\n",
      "Epoch 00070: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 697ms/step - loss: 0.2358 - accuracy: 0.8369 - recall: 0.6850 - f1: 0.8070 - val_loss: 0.2718 - val_accuracy: 0.9133 - val_recall: 0.9152 - val_f1: 0.9135\n",
      "Epoch 71/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2351 - accuracy: 0.8424 - recall: 0.6933 - f1: 0.8140\n",
      "Epoch 00071: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 696ms/step - loss: 0.2349 - accuracy: 0.8425 - recall: 0.6934 - f1: 0.8141 - val_loss: 0.3167 - val_accuracy: 0.9053 - val_recall: 0.9060 - val_f1: 0.9054\n",
      "Epoch 72/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2352 - accuracy: 0.8403 - recall: 0.6903 - f1: 0.8112\n",
      "Epoch 00072: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 697ms/step - loss: 0.2354 - accuracy: 0.8402 - recall: 0.6902 - f1: 0.8111 - val_loss: 0.3216 - val_accuracy: 0.9122 - val_recall: 0.9127 - val_f1: 0.9122\n",
      "Epoch 73/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2353 - accuracy: 0.8404 - recall: 0.6917 - f1: 0.8118\n",
      "Epoch 00073: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 703ms/step - loss: 0.2349 - accuracy: 0.8405 - recall: 0.6918 - f1: 0.8120 - val_loss: 0.2798 - val_accuracy: 0.9172 - val_recall: 0.9155 - val_f1: 0.9171\n",
      "Epoch 74/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2320 - accuracy: 0.8428 - recall: 0.6951 - f1: 0.8148\n",
      "Epoch 00074: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 697ms/step - loss: 0.2319 - accuracy: 0.8426 - recall: 0.6946 - f1: 0.8145 - val_loss: 0.2655 - val_accuracy: 0.9165 - val_recall: 0.9163 - val_f1: 0.9164\n",
      "Epoch 75/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2363 - accuracy: 0.8421 - recall: 0.6945 - f1: 0.8140\n",
      "Epoch 00075: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 700ms/step - loss: 0.2361 - accuracy: 0.8424 - recall: 0.6950 - f1: 0.8143 - val_loss: 0.2265 - val_accuracy: 0.9211 - val_recall: 0.9219 - val_f1: 0.9211\n",
      "Epoch 76/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2328 - accuracy: 0.8407 - recall: 0.6917 - f1: 0.8121\n",
      "Epoch 00076: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 699ms/step - loss: 0.2329 - accuracy: 0.8407 - recall: 0.6920 - f1: 0.8123 - val_loss: 0.2543 - val_accuracy: 0.9130 - val_recall: 0.9134 - val_f1: 0.9130\n",
      "Epoch 77/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2266 - accuracy: 0.8446 - recall: 0.6972 - f1: 0.8168\n",
      "Epoch 00077: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 69s 694ms/step - loss: 0.2267 - accuracy: 0.8445 - recall: 0.6971 - f1: 0.8167 - val_loss: 0.3145 - val_accuracy: 0.9113 - val_recall: 0.9113 - val_f1: 0.9113\n",
      "Epoch 78/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2359 - accuracy: 0.8394 - recall: 0.6894 - f1: 0.8101\n",
      "Epoch 00078: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 697ms/step - loss: 0.2358 - accuracy: 0.8392 - recall: 0.6891 - f1: 0.8098 - val_loss: 0.2596 - val_accuracy: 0.9103 - val_recall: 0.9113 - val_f1: 0.9103\n",
      "Epoch 79/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2340 - accuracy: 0.8369 - recall: 0.6849 - f1: 0.8069\n",
      "Epoch 00079: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 695ms/step - loss: 0.2340 - accuracy: 0.8371 - recall: 0.6853 - f1: 0.8071 - val_loss: 0.2455 - val_accuracy: 0.9158 - val_recall: 0.9153 - val_f1: 0.9157\n",
      "Epoch 80/80\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2348 - accuracy: 0.8425 - recall: 0.6936 - f1: 0.8140\n",
      "Epoch 00080: val_loss did not improve from 0.20169\n",
      "100/100 [==============================] - 70s 701ms/step - loss: 0.2345 - accuracy: 0.8427 - recall: 0.6941 - f1: 0.8143 - val_loss: 0.2530 - val_accuracy: 0.9249 - val_recall: 0.9255 - val_f1: 0.9250\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.train(\n",
    "    EPOCHS, STEPS_PER_EPOCH, n_train, n_val, batch_size, data_aug_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(history.history[\"loss\"][1:], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"][1:], label=\"val_loss\")\n",
    "plt.plot(history.history[\"accuracy\"][1:], label=\"train_acc\")\n",
    "plt.plot(history.history[\"val_accuracy\"][1:], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"Unet_batchnorm.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 70, 70, 3)    0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 32, 32, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 32, 32, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 32, 32, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 34, 34, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 16, 16, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 16, 16, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 16, 16, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 16, 16, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 16, 16, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 16, 16, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 16, 16, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 16, 16, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 16, 16, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 16, 16, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 16, 16, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 16, 16, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 16, 16, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 16, 16, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 16, 16, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 8, 8, 128)    32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 8, 8, 128)    0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 8, 8, 128)    0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 8, 8, 512)    131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 8, 8, 512)    0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 8, 8, 512)    0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 8, 8, 128)    0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 8, 8, 128)    0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 8, 8, 512)    0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 8, 8, 512)    0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 8, 8, 128)    0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 8, 8, 128)    0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 8, 8, 512)    0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 8, 8, 512)    0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 8, 8, 128)    0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 8, 8, 128)    0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 8, 8, 512)    0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 8, 8, 512)    0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 4, 4, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 4, 4, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 4, 4, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 4, 4, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 4, 4, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 4, 4, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 4, 4, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 4, 4, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 4, 4, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 4, 4, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 4, 4, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 4, 4, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 4, 4, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 4, 4, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 4, 4, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 4, 4, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 4, 4, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 4, 4, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 4, 4, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 4, 4, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 4, 4, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 4, 4, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 4, 4, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 4, 4, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 2, 2, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 2, 2, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 2, 2, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 2, 2, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 2, 2, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 2, 2, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 2, 2, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 2, 2, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 2, 2, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 2, 2, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 2, 2, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 2, 2, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 2048)   0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 1, 1, 128)    2359424     max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_12 (Conv2DTran (None, 2, 2, 64)     131136      conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 2, 2, 64)     256         conv2d_transpose_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 2, 2, 2112)   0           batch_normalization_12[0][0]     \n",
      "                                                                 conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 2, 2, 128)    2433152     concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_13 (Conv2DTran (None, 4, 4, 64)     131136      conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 4, 4, 64)     256         conv2d_transpose_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 4, 4, 1088)   0           batch_normalization_13[0][0]     \n",
      "                                                                 conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 4, 4, 128)    1253504     concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_14 (Conv2DTran (None, 8, 8, 64)     131136      conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_transpose_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 8, 8, 576)    0           batch_normalization_14[0][0]     \n",
      "                                                                 conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     331840      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_15 (Conv2DTran (None, 16, 16, 16)   16400       conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 16)   64          conv2d_transpose_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 16, 16, 272)  0           batch_normalization_15[0][0]     \n",
      "                                                                 conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 32)   78368       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_16 (Conv2DTran (None, 32, 32, 32)   16416       conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_transpose_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 32, 32)   9248        batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_17 (Conv2DTran (None, 64, 64, 8)    4104        conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 64, 64, 8)    32          conv2d_transpose_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 64, 64, 8)    584         batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 64, 64, 1)    9           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4096)         0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           131104      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 32)           0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2)            66          leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 2)            0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 2)            0           dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 30,616,331\n",
      "Trainable params: 30,562,715\n",
      "Non-trainable params: 53,616\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 70, 70, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 32, 32, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 32, 32, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 32, 32, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 34, 34, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 16, 16, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 16, 16, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 16, 16, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 16, 16, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 16, 16, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 16, 16, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 16, 16, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 16, 16, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 16, 16, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 16, 16, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 16, 16, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 16, 16, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 16, 16, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 16, 16, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 16, 16, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 8, 8, 128)    32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 8, 8, 128)    0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 8, 8, 128)    0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 8, 8, 512)    131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 8, 8, 512)    0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 8, 8, 512)    0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 8, 8, 128)    0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 8, 8, 128)    0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 8, 8, 512)    0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 8, 8, 512)    0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 8, 8, 128)    0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 8, 8, 128)    0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 8, 8, 512)    0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 8, 8, 512)    0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 8, 8, 128)    0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 8, 8, 128)    0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 8, 8, 512)    0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 8, 8, 512)    0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 4, 4, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 4, 4, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 4, 4, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 4, 4, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 4, 4, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 4, 4, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 4, 4, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 4, 4, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 4, 4, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 4, 4, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 4, 4, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 4, 4, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 4, 4, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 4, 4, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 4, 4, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 4, 4, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 4, 4, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 4, 4, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 4, 4, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 4, 4, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 4, 4, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 4, 4, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 4, 4, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 4, 4, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 2, 2, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 2, 2, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 2, 2, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 2, 2, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 2, 2, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 2, 2, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 2, 2, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 2, 2, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 2, 2, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 2, 2, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 2, 2, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 2, 2, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 2048)   0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 1, 1, 128)    2359424     max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 2, 2, 64)     131136      conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 2, 2, 64)     256         conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2, 2, 2112)   0           batch_normalization[0][0]        \n",
      "                                                                 conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 2, 2, 128)    2433152     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 4, 4, 64)     131136      conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 4, 4, 64)     256         conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 4, 1088)   0           batch_normalization_1[0][0]      \n",
      "                                                                 conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 4, 4, 128)    1253504     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 8, 8, 64)     131136      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 8, 8, 64)     256         conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8, 8, 576)    0           batch_normalization_2[0][0]      \n",
      "                                                                 conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 64)     331840      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 16, 16, 16)   16400       conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 16)   64          conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 16, 16, 272)  0           batch_normalization_3[0][0]      \n",
      "                                                                 conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 32)   78368       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 32, 32, 32)   16416       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 64, 64, 8)    4104        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 64, 8)    32          conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 8)    584         batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 1)    9           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4096)         0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           131104      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 32)           0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            66          leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2)            0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 2)            0           dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 30,616,331\n",
      "Trainable params: 30,562,715\n",
      "Non-trainable params: 53,616\n",
      "__________________________________________________________________________________________________\n",
      "Processing image => data/test_set_images/test_1/test_1.png\n",
      "Processing image => data/test_set_images/test_2/test_2.png\n",
      "Processing image => data/test_set_images/test_3/test_3.png\n",
      "Processing image => data/test_set_images/test_4/test_4.png\n",
      "Processing image => data/test_set_images/test_5/test_5.png\n",
      "Processing image => data/test_set_images/test_6/test_6.png\n",
      "Processing image => data/test_set_images/test_7/test_7.png\n",
      "Processing image => data/test_set_images/test_8/test_8.png\n",
      "Processing image => data/test_set_images/test_9/test_9.png\n",
      "Processing image => data/test_set_images/test_10/test_10.png\n",
      "Processing image => data/test_set_images/test_11/test_11.png\n",
      "Processing image => data/test_set_images/test_12/test_12.png\n",
      "Processing image => data/test_set_images/test_13/test_13.png\n",
      "Processing image => data/test_set_images/test_14/test_14.png\n",
      "Processing image => data/test_set_images/test_15/test_15.png\n",
      "Processing image => data/test_set_images/test_16/test_16.png\n",
      "Processing image => data/test_set_images/test_17/test_17.png\n",
      "Processing image => data/test_set_images/test_18/test_18.png\n",
      "Processing image => data/test_set_images/test_19/test_19.png\n",
      "Processing image => data/test_set_images/test_20/test_20.png\n",
      "Processing image => data/test_set_images/test_21/test_21.png\n",
      "Processing image => data/test_set_images/test_22/test_22.png\n",
      "Processing image => data/test_set_images/test_23/test_23.png\n",
      "Processing image => data/test_set_images/test_24/test_24.png\n",
      "Processing image => data/test_set_images/test_25/test_25.png\n",
      "Processing image => data/test_set_images/test_26/test_26.png\n",
      "Processing image => data/test_set_images/test_27/test_27.png\n",
      "Processing image => data/test_set_images/test_28/test_28.png\n",
      "Processing image => data/test_set_images/test_29/test_29.png\n",
      "Processing image => data/test_set_images/test_30/test_30.png\n",
      "Processing image => data/test_set_images/test_31/test_31.png\n",
      "Processing image => data/test_set_images/test_32/test_32.png\n",
      "Processing image => data/test_set_images/test_33/test_33.png\n",
      "Processing image => data/test_set_images/test_34/test_34.png\n",
      "Processing image => data/test_set_images/test_35/test_35.png\n",
      "Processing image => data/test_set_images/test_36/test_36.png\n",
      "Processing image => data/test_set_images/test_37/test_37.png\n",
      "Processing image => data/test_set_images/test_38/test_38.png\n",
      "Processing image => data/test_set_images/test_39/test_39.png\n",
      "Processing image => data/test_set_images/test_40/test_40.png\n",
      "Processing image => data/test_set_images/test_41/test_41.png\n",
      "Processing image => data/test_set_images/test_42/test_42.png\n",
      "Processing image => data/test_set_images/test_43/test_43.png\n",
      "Processing image => data/test_set_images/test_44/test_44.png\n",
      "Processing image => data/test_set_images/test_45/test_45.png\n",
      "Processing image => data/test_set_images/test_46/test_46.png\n",
      "Processing image => data/test_set_images/test_47/test_47.png\n",
      "Processing image => data/test_set_images/test_48/test_48.png\n",
      "Processing image => data/test_set_images/test_49/test_49.png\n",
      "Processing image => data/test_set_images/test_50/test_50.png\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "lr = 1e-4\n",
    "model = resnet_unet_model(\n",
    "    shape=(64, 64, 3),\n",
    "    batch_normalization=batch_normalization,\n",
    "    activation=activation,\n",
    "    dropout=dropout,\n",
    "    amsgrad=amsgrad,\n",
    "    lr=lr,\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model.load(\"model_batch_relu_80_dropout_0_024_0.804204_0.920300.h5\")\n",
    "\n",
    "model.model.summary()\n",
    "\n",
    "# We add all test images to an array, used later for generating a submission\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "    image_filenames.append(image_filename)\n",
    "\n",
    "# Set-up submission filename\n",
    "submission_filename = \"resnet_unet.csv\"\n",
    "\n",
    "# Generates the submission\n",
    "generate_submission(model, submission_filename, *image_filenames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
