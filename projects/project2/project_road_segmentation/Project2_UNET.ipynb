{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext lab_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from utils import *\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.compat.v2.keras.layers import BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9337444532766287648\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4937233203\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8794020212488526578\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Define random seed and print available devices (CPU-GPU)\n",
    "np.random.seed(24)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training images, images loaded: 100 \n",
      "Loading groundtruth images, images loaded: 100 \n"
     ]
    }
   ],
   "source": [
    "image_dir_train = \"data/training/images/\"\n",
    "files = os.listdir(image_dir_train)\n",
    "n_train = len(files)\n",
    "print(f\"Loading training images, images loaded: {n_train} \")\n",
    "imgs_train = np.asarray(\n",
    "    [load_image(image_dir_train + files[i]) for i in range(n_train)]\n",
    ")\n",
    "gt_dir_train = \"data/training/groundtruth/\"\n",
    "print(f\"Loading groundtruth images, images loaded: {n_train} \")\n",
    "gt_imgs_train = np.asarray(\n",
    "    [load_image(gt_dir_train + files[i]) for i in range(n_train)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validating images, images loaded: 20 \n",
      "Loading validating groundtruth, images loaded: 20 \n"
     ]
    }
   ],
   "source": [
    "image_dir_val = \"data/validating/images/\"\n",
    "files = os.listdir(image_dir_val)\n",
    "n_val = len(files)\n",
    "print(f\"Loading validating images, images loaded: {n_val} \")\n",
    "imgs_val = np.asarray([load_image(image_dir_val + files[i]) for i in range(n_val)])\n",
    "gt_dir_val = \"data/validating/groundtruth/\"\n",
    "print(f\"Loading validating groundtruth, images loaded: {n_val} \")\n",
    "gt_imgs_val = np.asarray([load_image(gt_dir_val + files[i]) for i in range(n_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = imag_rotation_aug(imgs_train, gt_imgs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "Y_train = np.asarray(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 448, 448, 3)\n",
      "(900, 448, 448)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "n_train = Y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val, Y_val = imag_rotation_aug(imgs_val, gt_imgs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val = np.asarray(X_val)\n",
    "# Y_val = np.asarray(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_val.shape)\n",
    "# print(Y_val.shape)\n",
    "# n_val = Y_val.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create functions to calcualte precision, recall and F-1 in the training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Compute the Precision for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "        Precision (numpy.float64): the Precision of the batch \n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Compute the Recall for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "       Recall (numpy.float64): the Recal of the batch \n",
    "    \"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    \"\"\"Compute the F-1 for the batch.\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): the ground truth labels\n",
    "        y_pred (numpy.ndarray): the predicted labels \n",
    "    Returns:\n",
    "       F-1 (numpy.float64): the F-1 of the batch \n",
    "    \"\"\"\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down(input_layer, filters, pool=True):\n",
    "    \"\"\"Create convloutional and residual layers to reduce dimensions.\n",
    "    Args:\n",
    "        input_layer (layer): input layer before convolution\n",
    "        filters (numpy.int64): number of filters\n",
    "    Returns:\n",
    "        max_pool (layer): layer after max-pooling\n",
    "        residual (connection ): connection to connect with next layers\n",
    "    \"\"\"\n",
    "    batchnorm = BatchNormalization()(input_layer)\n",
    "    conv1 = Conv2D(filters, (5, 5), padding=\"same\", activation=\"relu\")(batchnorm)\n",
    "    residual = Conv2D(filters, (3, 3), padding=\"same\", activation=\"relu\")(conv1)\n",
    "    if pool:\n",
    "        max_pool = MaxPool2D()(residual)\n",
    "        return max_pool, residual\n",
    "    else:\n",
    "        return residual\n",
    "\n",
    "\n",
    "def up(input_layer, residual, filters):\n",
    "    \"\"\"Create convloutional and residual layers to increase dimensions.\n",
    "    Args:\n",
    "        input_layer (layer): input layer before convolution\\\n",
    "        residual (connection ): connection to connect with next layers\n",
    "        filters (numpy.int64): number of filters\n",
    "    Returns:\n",
    "        conv2 (layer): convolutional layer\n",
    "    \"\"\"\n",
    "    filters = int(filters)\n",
    "    batchnorm = BatchNormalization()(input_layer)\n",
    "    upsample = UpSampling2D()(batchnorm)\n",
    "    upconv = Conv2D(filters, kernel_size=(2, 2), padding=\"same\")(upsample)\n",
    "    concat = Concatenate(axis=3)([residual, upconv])\n",
    "    conv1 = Conv2D(filters, (5, 5), padding=\"same\", activation=\"relu\")(concat)\n",
    "    conv2 = Conv2D(filters, (3, 3), padding=\"same\", activation=\"relu\")(conv1)\n",
    "    return conv2\n",
    "\n",
    "\n",
    "class U_NET:\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        self.model = self.initialize_U_NET(shape)\n",
    "\n",
    "    def initialize_U_NET(self, shape):\n",
    "        \"\"\"Create Network Architecture.\n",
    "        Args:\n",
    "            shape (triplet): Size of the input layer height x width x colors (64 x 64 x 3)\n",
    "        Returns:\n",
    "            model (Neural Network): Architecture of the model\n",
    "        \"\"\"\n",
    "        # Make a custom U-nets implementation.\n",
    "        filters = 64\n",
    "        input_layer = Input(shape=shape)\n",
    "        layers = [input_layer]\n",
    "        residuals = []\n",
    "\n",
    "        # Down 1, 64\n",
    "        d1, res1 = down(input_layer, filters)\n",
    "        residuals.append(res1)\n",
    "        filters *= 2\n",
    "\n",
    "        # Down 2, 32\n",
    "        d2, res2 = down(d1, filters)\n",
    "        residuals.append(res2)\n",
    "        filters *= 2\n",
    "\n",
    "        # Down 3, 16\n",
    "        d3, res3 = down(d2, filters)\n",
    "        residuals.append(res3)\n",
    "        filters *= 2\n",
    "\n",
    "        # Down 4, 8\n",
    "        d4, res4 = down(d3, filters)\n",
    "        residuals.append(res4)\n",
    "        filters *= 2\n",
    "\n",
    "        # Up 1, 8\n",
    "        up1 = up(d4, residual=residuals[-1], filters=filters / 2)\n",
    "        filters /= 2\n",
    "\n",
    "        # Up 2,  16\n",
    "        up2 = up(up1, residual=residuals[-2], filters=filters / 2)\n",
    "        filters /= 2\n",
    "\n",
    "        # Up 3, 32\n",
    "        up3 = up(up2, residual=residuals[-3], filters=filters / 2)\n",
    "        filters /= 2\n",
    "\n",
    "        # Up 4, 64\n",
    "        up4 = up(up3, residual=residuals[-4], filters=filters / 2)\n",
    "\n",
    "        conv_1 = Conv2D(1, 1, activation=\"relu\")(up4)\n",
    "        flaten = Flatten()(conv_1)\n",
    "        batch_1 = BatchNormalization()(flaten)\n",
    "        out = Dense(\n",
    "            2,\n",
    "            activation=\"sigmoid\",\n",
    "            kernel_regularizer=l2(0.00001),\n",
    "            activity_regularizer=l2(0.00001),\n",
    "        )(batch_1)\n",
    "\n",
    "        model = Model(input_layer, out)\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            metrics=[\"accuracy\", recall, f1],\n",
    "        )\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the Model.\n",
    "\n",
    "        Returns:\n",
    "            History (History_Keras): History of the training\n",
    "        \"\"\"\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"loss\", patience=10, verbose=1, restore_best_weights=True,\n",
    "        )\n",
    "\n",
    "        lr_callback = ReduceLROnPlateau(\n",
    "            monitor=\"loss\", factor=0.2, patience=5, verbose=1, cooldown=1,\n",
    "        )\n",
    "\n",
    "        save_best = ModelCheckpoint(\n",
    "            \"Unet_batchnorm_fix-{epoch:03d}-{f1:03f}.h5\",\n",
    "            save_best_only=True,\n",
    "            monitor=\"loss\",\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        callbacks = [lr_callback, save_best, early_stopping]\n",
    "\n",
    "        history = self.model.fit_generator(\n",
    "            create_minibatch(\n",
    "                X_train, Y_train, n_train, WINDOW_SIZE, BATCH_SIZE, PATCH_SIZE, WIDTH\n",
    "            ),\n",
    "            steps_per_epoch=STEPS_PER_EPOCH,\n",
    "            epochs=EPOCHS,\n",
    "            use_multiprocessing=False,\n",
    "            workers=1,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "            #             validation_data=create_minibatch(\n",
    "            #                 X_val, Y_val, n_val, WINDOW_SIZE, BATCH_SIZE, PATCH_SIZE, WIDTH\n",
    "            #             ),\n",
    "            #             validation_steps=STEPS_PER_EPOCH / 3,\n",
    "        )\n",
    "        return history\n",
    "\n",
    "    def classify_patches(self, X):\n",
    "        \"\"\"Classify Image as either road or not.\n",
    "        Args:\n",
    "            X (image): part of the image to classify\n",
    "        Returns:\n",
    "            Predictions : Predictions for each patch\n",
    "        \"\"\"\n",
    "        img_patches = create_patches(X, 16, 16, padding=24)\n",
    "\n",
    "        predictions = self.model.predict(img_patches)\n",
    "        predictions = (predictions[:, 0] < predictions[:, 1]) * 1\n",
    "\n",
    "        return predictions.reshape(X.shape[0], -1)\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"Loads Saved Model.\n",
    "        Args:\n",
    "           filename (string): name of the model\n",
    "           \n",
    "        \"\"\"\n",
    "        dependencies = {\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "        self.model = load_model(filename, custom_objects=dependencies)\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Saves trained model.\n",
    "        Args:\n",
    "           filename (string): name of the model\n",
    "           \n",
    "        \"\"\"\n",
    "        self.model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 64, 64, 3)    12          input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 64, 64, 64)   4864        batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 64, 64, 64)   36928       conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 32, 32, 64)   0           conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 32, 32, 64)   256         max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 32, 32, 128)  204928      batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 32, 32, 128)  147584      conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 16, 16, 128)  0           conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 128)  512         max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 16, 16, 256)  819456      batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 16, 16, 256)  590080      conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 8, 8, 256)    0           conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 256)    1024        max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 8, 8, 512)    3277312     batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 8, 8, 512)    2359808     conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 4, 4, 512)    0           conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 4, 4, 512)    2048        max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_12 (UpSampling2D) (None, 8, 8, 512)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 8, 8, 512)    1049088     up_sampling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 8, 8, 1024)   0           conv2d_70[0][0]                  \n",
      "                                                                 conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 8, 8, 512)    13107712    concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 8, 8, 512)    2359808     conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 8, 8, 512)    2048        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_13 (UpSampling2D) (None, 16, 16, 512)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 16, 16, 256)  524544      up_sampling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 16, 16, 512)  0           conv2d_68[0][0]                  \n",
      "                                                                 conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 16, 16, 256)  3277056     concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 16, 16, 256)  590080      conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 16, 16, 256)  1024        conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_14 (UpSampling2D) (None, 32, 32, 256)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 32, 32, 128)  131200      up_sampling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 32, 32, 256)  0           conv2d_66[0][0]                  \n",
      "                                                                 conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 32, 32, 128)  819328      concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 32, 32, 128)  147584      conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 32, 32, 128)  512         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_15 (UpSampling2D) (None, 64, 64, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 64, 64, 64)   32832       up_sampling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 64, 64, 128)  0           conv2d_64[0][0]                  \n",
      "                                                                 conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 64, 64, 64)   204864      concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 64, 64, 64)   36928       conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 64, 64, 1)    65          conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 4096)         0           conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 4096)         16384       flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            8194        batch_normalization_35[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 29,754,063\n",
      "Trainable params: 29,742,153\n",
      "Non-trainable params: 11,910\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We define parameters of the model\n",
    "BATCH_SIZE = 100\n",
    "WINDOW_SIZE = 64\n",
    "PATCH_SIZE = 16\n",
    "EPOCHS = 200\n",
    "STEPS_PER_EPOCH = 100\n",
    "WIDTH = 448\n",
    "model = U_NET(shape=(WINDOW_SIZE, WINDOW_SIZE, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.5257 - accuracy: 0.7414 - recall: 0.7357 - f1: 0.7383\n",
      "Epoch 00001: loss improved from inf to 0.52557, saving model to Unet_batchnorm_fix-001-0.738185.h5\n",
      "100/100 [==============================] - 129s 1s/step - loss: 0.5256 - accuracy: 0.7412 - recall: 0.7356 - f1: 0.7382\n",
      "Epoch 2/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.4149 - accuracy: 0.8011 - recall: 0.8014 - f1: 0.8011\n",
      "Epoch 00002: loss improved from 0.52557 to 0.41461, saving model to Unet_batchnorm_fix-002-0.801520.h5\n",
      "100/100 [==============================] - 116s 1s/step - loss: 0.4146 - accuracy: 0.8015 - recall: 0.8018 - f1: 0.8015\n",
      "Epoch 3/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.3705 - accuracy: 0.8303 - recall: 0.8310 - f1: 0.8303\n",
      "Epoch 00003: loss improved from 0.41461 to 0.37022, saving model to Unet_batchnorm_fix-003-0.830402.h5\n",
      "100/100 [==============================] - 118s 1s/step - loss: 0.3702 - accuracy: 0.8304 - recall: 0.8309 - f1: 0.8304\n",
      "Epoch 4/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.3611 - accuracy: 0.8341 - recall: 0.8342 - f1: 0.8341\n",
      "Epoch 00004: loss improved from 0.37022 to 0.36012, saving model to Unet_batchnorm_fix-004-0.834381.h5\n",
      "100/100 [==============================] - 118s 1s/step - loss: 0.3601 - accuracy: 0.8343 - recall: 0.8345 - f1: 0.8344\n",
      "Epoch 5/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.3352 - accuracy: 0.8458 - recall: 0.8464 - f1: 0.8459\n",
      "Epoch 00005: loss improved from 0.36012 to 0.33550, saving model to Unet_batchnorm_fix-005-0.845200.h5\n",
      "100/100 [==============================] - 115s 1s/step - loss: 0.3355 - accuracy: 0.8451 - recall: 0.8456 - f1: 0.8452\n",
      "Epoch 6/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.3202 - accuracy: 0.8554 - recall: 0.8557 - f1: 0.8554\n",
      "Epoch 00006: loss improved from 0.33550 to 0.32011, saving model to Unet_batchnorm_fix-006-0.855744.h5\n",
      "100/100 [==============================] - 116s 1s/step - loss: 0.3201 - accuracy: 0.8557 - recall: 0.8560 - f1: 0.8557\n",
      "Epoch 7/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.3320 - accuracy: 0.8509 - recall: 0.8505 - f1: 0.8508\n",
      "Epoch 00007: loss did not improve from 0.32011\n",
      "100/100 [==============================] - 113s 1s/step - loss: 0.3311 - accuracy: 0.8513 - recall: 0.8509 - f1: 0.8512\n",
      "Epoch 8/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.3002 - accuracy: 0.8619 - recall: 0.8616 - f1: 0.8619\n",
      "Epoch 00008: loss improved from 0.32011 to 0.29956, saving model to Unet_batchnorm_fix-008-0.862216.h5\n",
      "100/100 [==============================] - 104s 1s/step - loss: 0.2996 - accuracy: 0.8622 - recall: 0.8619 - f1: 0.8622\n",
      "Epoch 9/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2976 - accuracy: 0.8629 - recall: 0.8627 - f1: 0.8629\n",
      "Epoch 00009: loss improved from 0.29956 to 0.29671, saving model to Unet_batchnorm_fix-009-0.863552.h5\n",
      "100/100 [==============================] - 102s 1s/step - loss: 0.2967 - accuracy: 0.8636 - recall: 0.8634 - f1: 0.8636\n",
      "Epoch 10/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2789 - accuracy: 0.8748 - recall: 0.8746 - f1: 0.8748\n",
      "Epoch 00010: loss improved from 0.29671 to 0.27918, saving model to Unet_batchnorm_fix-010-0.874618.h5\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.2792 - accuracy: 0.8747 - recall: 0.8745 - f1: 0.8746\n",
      "Epoch 11/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2720 - accuracy: 0.8828 - recall: 0.8822 - f1: 0.8827\n",
      "Epoch 00011: loss improved from 0.27918 to 0.27133, saving model to Unet_batchnorm_fix-011-0.882778.h5\n",
      "100/100 [==============================] - 103s 1s/step - loss: 0.2713 - accuracy: 0.8828 - recall: 0.8823 - f1: 0.8828\n",
      "Epoch 12/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2873 - accuracy: 0.8711 - recall: 0.8713 - f1: 0.8711\n",
      "Epoch 00012: loss did not improve from 0.27133\n",
      "100/100 [==============================] - 103s 1s/step - loss: 0.2869 - accuracy: 0.8711 - recall: 0.8713 - f1: 0.8711\n",
      "Epoch 13/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2698 - accuracy: 0.8846 - recall: 0.8848 - f1: 0.8847\n",
      "Epoch 00013: loss improved from 0.27133 to 0.27030, saving model to Unet_batchnorm_fix-013-0.884247.h5\n",
      "100/100 [==============================] - 104s 1s/step - loss: 0.2703 - accuracy: 0.8842 - recall: 0.8844 - f1: 0.8842\n",
      "Epoch 14/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.4068 - accuracy: 0.8245 - recall: 0.8240 - f1: 0.8245\n",
      "Epoch 00014: loss did not improve from 0.27030\n",
      "100/100 [==============================] - 102s 1s/step - loss: 0.4071 - accuracy: 0.8245 - recall: 0.8240 - f1: 0.8244\n",
      "Epoch 15/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.3350 - accuracy: 0.8560 - recall: 0.8563 - f1: 0.8560\n",
      "Epoch 00015: loss did not improve from 0.27030\n",
      "100/100 [==============================] - 101s 1s/step - loss: 0.3346 - accuracy: 0.8562 - recall: 0.8565 - f1: 0.8562\n",
      "Epoch 16/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.3036 - accuracy: 0.8671 - recall: 0.8666 - f1: 0.8670\n",
      "Epoch 00016: loss did not improve from 0.27030\n",
      "100/100 [==============================] - 100s 1s/step - loss: 0.3035 - accuracy: 0.8673 - recall: 0.8667 - f1: 0.8672\n",
      "Epoch 17/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.3115 - accuracy: 0.8649 - recall: 0.8643 - f1: 0.8649\n",
      "Epoch 00017: loss did not improve from 0.27030\n",
      "100/100 [==============================] - 100s 1s/step - loss: 0.3107 - accuracy: 0.8654 - recall: 0.8648 - f1: 0.8653\n",
      "Epoch 18/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2757 - accuracy: 0.8792 - recall: 0.8791 - f1: 0.8792\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.27030\n",
      "100/100 [==============================] - 101s 1s/step - loss: 0.2757 - accuracy: 0.8791 - recall: 0.8790 - f1: 0.8791\n",
      "Epoch 19/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2572 - accuracy: 0.8869 - recall: 0.8869 - f1: 0.8869\n",
      "Epoch 00019: loss improved from 0.27030 to 0.25676, saving model to Unet_batchnorm_fix-019-0.887143.h5\n",
      "100/100 [==============================] - 104s 1s/step - loss: 0.2568 - accuracy: 0.8871 - recall: 0.8871 - f1: 0.8871\n",
      "Epoch 20/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2303 - accuracy: 0.9049 - recall: 0.9047 - f1: 0.9049\n",
      "Epoch 00020: loss improved from 0.25676 to 0.23064, saving model to Unet_batchnorm_fix-020-0.904633.h5\n",
      "100/100 [==============================] - 104s 1s/step - loss: 0.2306 - accuracy: 0.9046 - recall: 0.9045 - f1: 0.9046\n",
      "Epoch 21/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2365 - accuracy: 0.8983 - recall: 0.8978 - f1: 0.8982\n",
      "Epoch 00021: loss did not improve from 0.23064\n",
      "100/100 [==============================] - 101s 1s/step - loss: 0.2362 - accuracy: 0.8982 - recall: 0.8977 - f1: 0.8982\n",
      "Epoch 22/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2310 - accuracy: 0.9026 - recall: 0.9025 - f1: 0.9026\n",
      "Epoch 00022: loss did not improve from 0.23064\n",
      "100/100 [==============================] - 101s 1s/step - loss: 0.2311 - accuracy: 0.9024 - recall: 0.9023 - f1: 0.9024\n",
      "Epoch 23/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2265 - accuracy: 0.9015 - recall: 0.9016 - f1: 0.9015\n",
      "Epoch 00023: loss improved from 0.23064 to 0.22827, saving model to Unet_batchnorm_fix-023-0.900968.h5\n",
      "100/100 [==============================] - 104s 1s/step - loss: 0.2283 - accuracy: 0.9010 - recall: 0.9011 - f1: 0.9010\n",
      "Epoch 24/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2296 - accuracy: 0.9020 - recall: 0.9021 - f1: 0.9020\n",
      "Epoch 00024: loss did not improve from 0.22827\n",
      "100/100 [==============================] - 101s 1s/step - loss: 0.2292 - accuracy: 0.9022 - recall: 0.9024 - f1: 0.9023\n",
      "Epoch 25/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2093 - accuracy: 0.9120 - recall: 0.9122 - f1: 0.9120\n",
      "Epoch 00025: loss improved from 0.22827 to 0.20899, saving model to Unet_batchnorm_fix-025-0.911918.h5\n",
      "100/100 [==============================] - 104s 1s/step - loss: 0.2090 - accuracy: 0.9119 - recall: 0.9121 - f1: 0.9119\n",
      "Epoch 26/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2102 - accuracy: 0.9116 - recall: 0.9118 - f1: 0.9116\n",
      "Epoch 00026: loss did not improve from 0.20899\n",
      "100/100 [==============================] - 101s 1s/step - loss: 0.2099 - accuracy: 0.9118 - recall: 0.9120 - f1: 0.9118\n",
      "Epoch 27/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2181 - accuracy: 0.9102 - recall: 0.9101 - f1: 0.9102\n",
      "Epoch 00027: loss did not improve from 0.20899\n",
      "100/100 [==============================] - 99s 989ms/step - loss: 0.2179 - accuracy: 0.9104 - recall: 0.9103 - f1: 0.9104\n",
      "Epoch 28/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2125 - accuracy: 0.9124 - recall: 0.9127 - f1: 0.9124\n",
      "Epoch 00028: loss did not improve from 0.20899\n",
      "100/100 [==============================] - 100s 1s/step - loss: 0.2119 - accuracy: 0.9126 - recall: 0.9130 - f1: 0.9127\n",
      "Epoch 29/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.2006 - accuracy: 0.9162 - recall: 0.9160 - f1: 0.9162\n",
      "Epoch 00029: loss improved from 0.20899 to 0.19994, saving model to Unet_batchnorm_fix-029-0.916431.h5\n",
      "100/100 [==============================] - 103s 1s/step - loss: 0.1999 - accuracy: 0.9165 - recall: 0.9162 - f1: 0.9164\n",
      "Epoch 30/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1996 - accuracy: 0.9180 - recall: 0.9181 - f1: 0.9180\n",
      "Epoch 00030: loss did not improve from 0.19994\n",
      "100/100 [==============================] - 104s 1s/step - loss: 0.2000 - accuracy: 0.9178 - recall: 0.9179 - f1: 0.9178\n",
      "Epoch 31/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2042 - accuracy: 0.9125 - recall: 0.9125 - f1: 0.9125\n",
      "Epoch 00031: loss did not improve from 0.19994\n",
      "100/100 [==============================] - 100s 998ms/step - loss: 0.2036 - accuracy: 0.9128 - recall: 0.9128 - f1: 0.9128\n",
      "Epoch 32/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1995 - accuracy: 0.9122 - recall: 0.9119 - f1: 0.9122\n",
      "Epoch 00032: loss improved from 0.19994 to 0.19859, saving model to Unet_batchnorm_fix-032-0.912833.h5\n",
      "100/100 [==============================] - 101s 1s/step - loss: 0.1986 - accuracy: 0.9129 - recall: 0.9126 - f1: 0.9128\n",
      "Epoch 33/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1958 - accuracy: 0.9195 - recall: 0.9196 - f1: 0.9195\n",
      "Epoch 00033: loss improved from 0.19859 to 0.19572, saving model to Unet_batchnorm_fix-033-0.919607.h5\n",
      "100/100 [==============================] - 104s 1s/step - loss: 0.1957 - accuracy: 0.9196 - recall: 0.9197 - f1: 0.9196\n",
      "Epoch 34/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1876 - accuracy: 0.9188 - recall: 0.9188 - f1: 0.9188\n",
      "Epoch 00034: loss improved from 0.19572 to 0.18775, saving model to Unet_batchnorm_fix-034-0.919143.h5\n",
      "100/100 [==============================] - 102s 1s/step - loss: 0.1877 - accuracy: 0.9191 - recall: 0.9191 - f1: 0.9191\n",
      "Epoch 35/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2004 - accuracy: 0.9145 - recall: 0.9142 - f1: 0.9145\n",
      "Epoch 00035: loss did not improve from 0.18775\n",
      "100/100 [==============================] - 99s 993ms/step - loss: 0.2001 - accuracy: 0.9147 - recall: 0.9144 - f1: 0.9146\n",
      "Epoch 36/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1948 - accuracy: 0.9181 - recall: 0.9181 - f1: 0.9181\n",
      "Epoch 00036: loss did not improve from 0.18775\n",
      "100/100 [==============================] - 101s 1s/step - loss: 0.1946 - accuracy: 0.9182 - recall: 0.9181 - f1: 0.9181\n",
      "Epoch 37/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1892 - accuracy: 0.9182 - recall: 0.9183 - f1: 0.9182\n",
      "Epoch 00037: loss did not improve from 0.18775\n",
      "100/100 [==============================] - 100s 996ms/step - loss: 0.1889 - accuracy: 0.9183 - recall: 0.9184 - f1: 0.9183\n",
      "Epoch 38/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1973 - accuracy: 0.9175 - recall: 0.9175 - f1: 0.9175\n",
      "Epoch 00038: loss did not improve from 0.18775\n",
      "100/100 [==============================] - 99s 985ms/step - loss: 0.1971 - accuracy: 0.9174 - recall: 0.9173 - f1: 0.9174\n",
      "Epoch 39/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1836 - accuracy: 0.9210 - recall: 0.9211 - f1: 0.9210\n",
      "Epoch 00039: loss improved from 0.18775 to 0.18408, saving model to Unet_batchnorm_fix-039-0.920607.h5\n",
      "100/100 [==============================] - 103s 1s/step - loss: 0.1841 - accuracy: 0.9206 - recall: 0.9207 - f1: 0.9206\n",
      "Epoch 40/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1905 - accuracy: 0.9189 - recall: 0.9186 - f1: 0.9189\n",
      "Epoch 00040: loss did not improve from 0.18408\n",
      "100/100 [==============================] - 99s 994ms/step - loss: 0.1906 - accuracy: 0.9190 - recall: 0.9187 - f1: 0.9190\n",
      "Epoch 41/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1848 - accuracy: 0.9199 - recall: 0.9201 - f1: 0.9200\n",
      "Epoch 00041: loss did not improve from 0.18408\n",
      "100/100 [==============================] - 99s 995ms/step - loss: 0.1843 - accuracy: 0.9204 - recall: 0.9205 - f1: 0.9204\n",
      "Epoch 42/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1890 - accuracy: 0.9197 - recall: 0.9196 - f1: 0.9197\n",
      "Epoch 00042: loss did not improve from 0.18408\n",
      "100/100 [==============================] - 101s 1s/step - loss: 0.1883 - accuracy: 0.9199 - recall: 0.9198 - f1: 0.9199\n",
      "Epoch 43/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1838 - accuracy: 0.9247 - recall: 0.9248 - f1: 0.9248\n",
      "Epoch 00043: loss improved from 0.18408 to 0.18401, saving model to Unet_batchnorm_fix-043-0.924608.h5\n",
      "100/100 [==============================] - 103s 1s/step - loss: 0.1840 - accuracy: 0.9246 - recall: 0.9247 - f1: 0.9246\n",
      "Epoch 44/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1724 - accuracy: 0.9254 - recall: 0.9254 - f1: 0.9254\n",
      "Epoch 00044: loss improved from 0.18401 to 0.17207, saving model to Unet_batchnorm_fix-044-0.925348.h5\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.1721 - accuracy: 0.9254 - recall: 0.9253 - f1: 0.9253\n",
      "Epoch 45/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1734 - accuracy: 0.9259 - recall: 0.9259 - f1: 0.9259\n",
      "Epoch 00045: loss did not improve from 0.17207\n",
      "100/100 [==============================] - 100s 999ms/step - loss: 0.1726 - accuracy: 0.9262 - recall: 0.9262 - f1: 0.9262\n",
      "Epoch 46/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1735 - accuracy: 0.9281 - recall: 0.9281 - f1: 0.9281\n",
      "Epoch 00046: loss did not improve from 0.17207\n",
      "100/100 [==============================] - 98s 982ms/step - loss: 0.1733 - accuracy: 0.9283 - recall: 0.9282 - f1: 0.9282\n",
      "Epoch 47/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1714 - accuracy: 0.9257 - recall: 0.9257 - f1: 0.9257\n",
      "Epoch 00047: loss did not improve from 0.17207\n",
      "100/100 [==============================] - 97s 973ms/step - loss: 0.1722 - accuracy: 0.9255 - recall: 0.9255 - f1: 0.9255\n",
      "Epoch 48/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1771 - accuracy: 0.9284 - recall: 0.9285 - f1: 0.9284\n",
      "Epoch 00048: loss did not improve from 0.17207\n",
      "100/100 [==============================] - 99s 986ms/step - loss: 0.1772 - accuracy: 0.9284 - recall: 0.9285 - f1: 0.9285\n",
      "Epoch 49/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1656 - accuracy: 0.9294 - recall: 0.9297 - f1: 0.9295\n",
      "Epoch 00049: loss improved from 0.17207 to 0.16580, saving model to Unet_batchnorm_fix-049-0.929166.h5\n",
      "100/100 [==============================] - 102s 1s/step - loss: 0.1658 - accuracy: 0.9291 - recall: 0.9294 - f1: 0.9292\n",
      "Epoch 50/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1634 - accuracy: 0.9295 - recall: 0.9296 - f1: 0.9295\n",
      "Epoch 00050: loss improved from 0.16580 to 0.16381, saving model to Unet_batchnorm_fix-050-0.929250.h5\n",
      "100/100 [==============================] - 101s 1s/step - loss: 0.1638 - accuracy: 0.9293 - recall: 0.9293 - f1: 0.9293\n",
      "Epoch 51/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1727 - accuracy: 0.9295 - recall: 0.9296 - f1: 0.9296\n",
      "Epoch 00051: loss did not improve from 0.16381\n",
      "100/100 [==============================] - 100s 999ms/step - loss: 0.1730 - accuracy: 0.9293 - recall: 0.9293 - f1: 0.9293\n",
      "Epoch 52/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1717 - accuracy: 0.9313 - recall: 0.9314 - f1: 0.9313\n",
      "Epoch 00052: loss did not improve from 0.16381\n",
      "100/100 [==============================] - 98s 978ms/step - loss: 0.1713 - accuracy: 0.9312 - recall: 0.9313 - f1: 0.9312\n",
      "Epoch 53/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1727 - accuracy: 0.9295 - recall: 0.9295 - f1: 0.9295\n",
      "Epoch 00053: loss did not improve from 0.16381\n",
      "100/100 [==============================] - 98s 983ms/step - loss: 0.1721 - accuracy: 0.9298 - recall: 0.9298 - f1: 0.9298\n",
      "Epoch 54/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1709 - accuracy: 0.9270 - recall: 0.9270 - f1: 0.9270\n",
      "Epoch 00054: loss did not improve from 0.16381\n",
      "100/100 [==============================] - 98s 977ms/step - loss: 0.1714 - accuracy: 0.9270 - recall: 0.9270 - f1: 0.9270\n",
      "Epoch 55/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1581 - accuracy: 0.9349 - recall: 0.9348 - f1: 0.9349\n",
      "Epoch 00055: loss improved from 0.16381 to 0.15793, saving model to Unet_batchnorm_fix-055-0.935194.h5\n",
      "100/100 [==============================] - 102s 1s/step - loss: 0.1579 - accuracy: 0.9352 - recall: 0.9351 - f1: 0.9352\n",
      "Epoch 56/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1678 - accuracy: 0.9301 - recall: 0.9302 - f1: 0.9301\n",
      "Epoch 00056: loss did not improve from 0.15793\n",
      "100/100 [==============================] - 97s 969ms/step - loss: 0.1676 - accuracy: 0.9302 - recall: 0.9303 - f1: 0.9302\n",
      "Epoch 57/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1540 - accuracy: 0.9357 - recall: 0.9357 - f1: 0.9357\n",
      "Epoch 00057: loss improved from 0.15793 to 0.15338, saving model to Unet_batchnorm_fix-057-0.935846.h5\n",
      "100/100 [==============================] - 100s 1s/step - loss: 0.1534 - accuracy: 0.9359 - recall: 0.9358 - f1: 0.9358\n",
      "Epoch 58/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1579 - accuracy: 0.9348 - recall: 0.9349 - f1: 0.9349\n",
      "Epoch 00058: loss did not improve from 0.15338\n",
      "100/100 [==============================] - 98s 976ms/step - loss: 0.1574 - accuracy: 0.9352 - recall: 0.9353 - f1: 0.9352\n",
      "Epoch 59/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1701 - accuracy: 0.9272 - recall: 0.9273 - f1: 0.9272\n",
      "Epoch 00059: loss did not improve from 0.15338\n",
      "100/100 [==============================] - 99s 989ms/step - loss: 0.1696 - accuracy: 0.9275 - recall: 0.9275 - f1: 0.9275\n",
      "Epoch 60/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1530 - accuracy: 0.9371 - recall: 0.9370 - f1: 0.9371\n",
      "Epoch 00060: loss improved from 0.15338 to 0.15326, saving model to Unet_batchnorm_fix-060-0.937041.h5\n",
      "100/100 [==============================] - 100s 1s/step - loss: 0.1533 - accuracy: 0.9370 - recall: 0.9369 - f1: 0.9370\n",
      "Epoch 61/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1572 - accuracy: 0.9334 - recall: 0.9335 - f1: 0.9334\n",
      "Epoch 00061: loss did not improve from 0.15326\n",
      "100/100 [==============================] - 97s 969ms/step - loss: 0.1569 - accuracy: 0.9335 - recall: 0.9336 - f1: 0.9335\n",
      "Epoch 62/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1498 - accuracy: 0.9389 - recall: 0.9388 - f1: 0.9389\n",
      "Epoch 00062: loss improved from 0.15326 to 0.14938, saving model to Unet_batchnorm_fix-062-0.939294.h5\n",
      "100/100 [==============================] - 101s 1s/step - loss: 0.1494 - accuracy: 0.9393 - recall: 0.9392 - f1: 0.9393\n",
      "Epoch 63/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1547 - accuracy: 0.9336 - recall: 0.9336 - f1: 0.9336\n",
      "Epoch 00063: loss did not improve from 0.14938\n",
      "100/100 [==============================] - 99s 986ms/step - loss: 0.1551 - accuracy: 0.9331 - recall: 0.9332 - f1: 0.9332\n",
      "Epoch 64/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1598 - accuracy: 0.9345 - recall: 0.9344 - f1: 0.9345\n",
      "Epoch 00064: loss did not improve from 0.14938\n",
      "100/100 [==============================] - 98s 977ms/step - loss: 0.1610 - accuracy: 0.9340 - recall: 0.9339 - f1: 0.9339\n",
      "Epoch 65/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1607 - accuracy: 0.9326 - recall: 0.9325 - f1: 0.9326\n",
      "Epoch 00065: loss did not improve from 0.14938\n",
      "100/100 [==============================] - 98s 983ms/step - loss: 0.1607 - accuracy: 0.9326 - recall: 0.9325 - f1: 0.9325\n",
      "Epoch 66/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1496 - accuracy: 0.9381 - recall: 0.9380 - f1: 0.9381\n",
      "Epoch 00066: loss improved from 0.14938 to 0.14916, saving model to Unet_batchnorm_fix-066-0.938191.h5\n",
      "100/100 [==============================] - 99s 990ms/step - loss: 0.1492 - accuracy: 0.9382 - recall: 0.9381 - f1: 0.9382\n",
      "Epoch 67/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1556 - accuracy: 0.9337 - recall: 0.9337 - f1: 0.9337\n",
      "Epoch 00067: loss did not improve from 0.14916\n",
      "100/100 [==============================] - 96s 962ms/step - loss: 0.1555 - accuracy: 0.9337 - recall: 0.9337 - f1: 0.9337\n",
      "Epoch 68/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1457 - accuracy: 0.9411 - recall: 0.9411 - f1: 0.9411\n",
      "Epoch 00068: loss improved from 0.14916 to 0.14556, saving model to Unet_batchnorm_fix-068-0.941004.h5\n",
      "100/100 [==============================] - 102s 1s/step - loss: 0.1456 - accuracy: 0.9410 - recall: 0.9410 - f1: 0.9410\n",
      "Epoch 69/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1536 - accuracy: 0.9352 - recall: 0.9351 - f1: 0.9351\n",
      "Epoch 00069: loss did not improve from 0.14556\n",
      "100/100 [==============================] - 98s 980ms/step - loss: 0.1544 - accuracy: 0.9347 - recall: 0.9346 - f1: 0.9347\n",
      "Epoch 70/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1479 - accuracy: 0.9396 - recall: 0.9396 - f1: 0.9396\n",
      "Epoch 00070: loss did not improve from 0.14556\n",
      "100/100 [==============================] - 96s 963ms/step - loss: 0.1477 - accuracy: 0.9398 - recall: 0.9397 - f1: 0.9397\n",
      "Epoch 71/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1544 - accuracy: 0.9381 - recall: 0.9382 - f1: 0.9381\n",
      "Epoch 00071: loss did not improve from 0.14556\n",
      "100/100 [==============================] - 96s 963ms/step - loss: 0.1546 - accuracy: 0.9378 - recall: 0.9379 - f1: 0.9378\n",
      "Epoch 72/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1470 - accuracy: 0.9405 - recall: 0.9404 - f1: 0.9405\n",
      "Epoch 00072: loss did not improve from 0.14556\n",
      "100/100 [==============================] - 97s 966ms/step - loss: 0.1465 - accuracy: 0.9408 - recall: 0.9407 - f1: 0.9408\n",
      "Epoch 73/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1453 - accuracy: 0.9392 - recall: 0.9392 - f1: 0.9392\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 00073: loss did not improve from 0.14556\n",
      "100/100 [==============================] - 96s 962ms/step - loss: 0.1456 - accuracy: 0.9390 - recall: 0.9390 - f1: 0.9390\n",
      "Epoch 74/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1360 - accuracy: 0.9420 - recall: 0.9420 - f1: 0.9420\n",
      "Epoch 00074: loss improved from 0.14556 to 0.13681, saving model to Unet_batchnorm_fix-074-0.941853.h5\n",
      "100/100 [==============================] - 98s 983ms/step - loss: 0.1368 - accuracy: 0.9419 - recall: 0.9419 - f1: 0.9419\n",
      "Epoch 75/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1431 - accuracy: 0.9411 - recall: 0.9410 - f1: 0.9411\n",
      "Epoch 00075: loss did not improve from 0.13681\n",
      "100/100 [==============================] - 96s 963ms/step - loss: 0.1428 - accuracy: 0.9410 - recall: 0.9410 - f1: 0.9410\n",
      "Epoch 76/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9518 - recall: 0.9518 - f1: 0.9518\n",
      "Epoch 00076: loss improved from 0.13681 to 0.12108, saving model to Unet_batchnorm_fix-076-0.951553.h5\n",
      "100/100 [==============================] - 99s 993ms/step - loss: 0.1211 - accuracy: 0.9516 - recall: 0.9516 - f1: 0.9516\n",
      "Epoch 77/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1295 - accuracy: 0.9469 - recall: 0.9469 - f1: 0.9469\n",
      "Epoch 00077: loss did not improve from 0.12108\n",
      "100/100 [==============================] - 96s 961ms/step - loss: 0.1296 - accuracy: 0.9466 - recall: 0.9466 - f1: 0.9466\n",
      "Epoch 78/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1300 - accuracy: 0.9465 - recall: 0.9465 - f1: 0.9465\n",
      "Epoch 00078: loss did not improve from 0.12108\n",
      "100/100 [==============================] - 96s 962ms/step - loss: 0.1306 - accuracy: 0.9460 - recall: 0.9460 - f1: 0.9460\n",
      "Epoch 79/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1311 - accuracy: 0.9464 - recall: 0.9464 - f1: 0.9464\n",
      "Epoch 00079: loss did not improve from 0.12108\n",
      "100/100 [==============================] - 97s 971ms/step - loss: 0.1310 - accuracy: 0.9463 - recall: 0.9463 - f1: 0.9463\n",
      "Epoch 80/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9497 - recall: 0.9497 - f1: 0.9497\n",
      "Epoch 00080: loss did not improve from 0.12108\n",
      "100/100 [==============================] - 98s 977ms/step - loss: 0.1253 - accuracy: 0.9495 - recall: 0.9494 - f1: 0.9494\n",
      "Epoch 81/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1271 - accuracy: 0.9470 - recall: 0.9470 - f1: 0.9470\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 00081: loss did not improve from 0.12108\n",
      "100/100 [==============================] - 98s 980ms/step - loss: 0.1273 - accuracy: 0.9470 - recall: 0.9469 - f1: 0.9469\n",
      "Epoch 82/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9469 - recall: 0.9469 - f1: 0.9469\n",
      "Epoch 00082: loss did not improve from 0.12108\n",
      "100/100 [==============================] - 99s 986ms/step - loss: 0.1265 - accuracy: 0.9467 - recall: 0.9467 - f1: 0.9467\n",
      "Epoch 83/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1234 - accuracy: 0.9502 - recall: 0.9501 - f1: 0.9502\n",
      "Epoch 00083: loss did not improve from 0.12108\n",
      "100/100 [==============================] - 100s 1s/step - loss: 0.1232 - accuracy: 0.9503 - recall: 0.9502 - f1: 0.9503\n",
      "Epoch 84/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9509 - recall: 0.9509 - f1: 0.9509\n",
      "Epoch 00084: loss did not improve from 0.12108\n",
      "100/100 [==============================] - 97s 974ms/step - loss: 0.1224 - accuracy: 0.9508 - recall: 0.9508 - f1: 0.9508\n",
      "Epoch 85/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9499 - recall: 0.9499 - f1: 0.9499\n",
      "Epoch 00085: loss did not improve from 0.12108\n",
      "100/100 [==============================] - 96s 965ms/step - loss: 0.1265 - accuracy: 0.9499 - recall: 0.9498 - f1: 0.9498\n",
      "Epoch 86/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9510 - recall: 0.9511 - f1: 0.9510\n",
      "Epoch 00086: loss improved from 0.12108 to 0.12013, saving model to Unet_batchnorm_fix-086-0.950905.h5\n",
      "100/100 [==============================] - 99s 990ms/step - loss: 0.1201 - accuracy: 0.9509 - recall: 0.9510 - f1: 0.9509\n",
      "Epoch 87/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1166 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9507\n",
      "Epoch 00087: loss improved from 0.12013 to 0.11704, saving model to Unet_batchnorm_fix-087-0.950742.h5\n",
      "100/100 [==============================] - 100s 1s/step - loss: 0.1170 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507\n",
      "Epoch 88/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1189 - accuracy: 0.9513 - recall: 0.9513 - f1: 0.9513\n",
      "Epoch 00088: loss did not improve from 0.11704\n",
      "100/100 [==============================] - 98s 977ms/step - loss: 0.1185 - accuracy: 0.9513 - recall: 0.9513 - f1: 0.9513\n",
      "Epoch 89/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488\n",
      "Epoch 00089: loss did not improve from 0.11704\n",
      "100/100 [==============================] - 99s 994ms/step - loss: 0.1229 - accuracy: 0.9487 - recall: 0.9487 - f1: 0.9487\n",
      "Epoch 90/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1154 - accuracy: 0.9534 - recall: 0.9534 - f1: 0.9534\n",
      "Epoch 00090: loss improved from 0.11704 to 0.11578, saving model to Unet_batchnorm_fix-090-0.953396.h5\n",
      "100/100 [==============================] - 103s 1s/step - loss: 0.1158 - accuracy: 0.9534 - recall: 0.9534 - f1: 0.9534\n",
      "Epoch 91/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9490 - recall: 0.9489 - f1: 0.9490\n",
      "Epoch 00091: loss did not improve from 0.11578\n",
      "100/100 [==============================] - 100s 996ms/step - loss: 0.1215 - accuracy: 0.9485 - recall: 0.9484 - f1: 0.9485\n",
      "Epoch 92/200\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.1191 - accuracy: 0.9521 - recall: 0.9520 - f1: 0.9521\n",
      "Epoch 00092: loss did not improve from 0.11578\n",
      "100/100 [==============================] - 101s 1s/step - loss: 0.1194 - accuracy: 0.9521 - recall: 0.9520 - f1: 0.9521\n",
      "Epoch 93/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1166 - accuracy: 0.9532 - recall: 0.9532 - f1: 0.9532\n",
      "Epoch 00093: loss did not improve from 0.11578\n",
      "100/100 [==============================] - 98s 978ms/step - loss: 0.1164 - accuracy: 0.9531 - recall: 0.9532 - f1: 0.9532\n",
      "Epoch 94/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9494 - recall: 0.9495 - f1: 0.9494\n",
      "Epoch 00094: loss did not improve from 0.11578\n",
      "100/100 [==============================] - 99s 989ms/step - loss: 0.1222 - accuracy: 0.9495 - recall: 0.9496 - f1: 0.9495\n",
      "Epoch 95/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1112 - accuracy: 0.9540 - recall: 0.9539 - f1: 0.9540\n",
      "Epoch 00095: loss improved from 0.11578 to 0.11134, saving model to Unet_batchnorm_fix-095-0.953749.h5\n",
      "100/100 [==============================] - 99s 995ms/step - loss: 0.1113 - accuracy: 0.9538 - recall: 0.9537 - f1: 0.9537\n",
      "Epoch 96/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 0.9485 - recall: 0.9487 - f1: 0.9485\n",
      "Epoch 00096: loss did not improve from 0.11134\n",
      "100/100 [==============================] - 97s 971ms/step - loss: 0.1261 - accuracy: 0.9485 - recall: 0.9487 - f1: 0.9486\n",
      "Epoch 97/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9469 - recall: 0.9468 - f1: 0.9469\n",
      "Epoch 00097: loss did not improve from 0.11134\n",
      "100/100 [==============================] - 98s 985ms/step - loss: 0.1275 - accuracy: 0.9468 - recall: 0.9467 - f1: 0.9468\n",
      "Epoch 98/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9479 - recall: 0.9479 - f1: 0.9479\n",
      "Epoch 00098: loss did not improve from 0.11134\n",
      "100/100 [==============================] - 97s 971ms/step - loss: 0.1238 - accuracy: 0.9484 - recall: 0.9483 - f1: 0.9483\n",
      "Epoch 99/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1287 - accuracy: 0.9471 - recall: 0.9471 - f1: 0.9471\n",
      "Epoch 00099: loss did not improve from 0.11134\n",
      "100/100 [==============================] - 99s 994ms/step - loss: 0.1286 - accuracy: 0.9471 - recall: 0.9471 - f1: 0.9471\n",
      "Epoch 100/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1233 - accuracy: 0.9510 - recall: 0.9510 - f1: 0.9510\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\n",
      "Epoch 00100: loss did not improve from 0.11134\n",
      "100/100 [==============================] - 99s 991ms/step - loss: 0.1231 - accuracy: 0.9511 - recall: 0.9511 - f1: 0.9511\n",
      "Epoch 101/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1175 - accuracy: 0.9512 - recall: 0.9512 - f1: 0.9512\n",
      "Epoch 00101: loss did not improve from 0.11134\n",
      "100/100 [==============================] - 98s 983ms/step - loss: 0.1179 - accuracy: 0.9512 - recall: 0.9512 - f1: 0.9512\n",
      "Epoch 102/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1166 - accuracy: 0.9517 - recall: 0.9517 - f1: 0.9517\n",
      "Epoch 00102: loss did not improve from 0.11134\n",
      "100/100 [==============================] - 98s 982ms/step - loss: 0.1172 - accuracy: 0.9513 - recall: 0.9514 - f1: 0.9514\n",
      "Epoch 103/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1140 - accuracy: 0.9548 - recall: 0.9547 - f1: 0.9548\n",
      "Epoch 00103: loss did not improve from 0.11134\n",
      "100/100 [==============================] - 100s 998ms/step - loss: 0.1141 - accuracy: 0.9546 - recall: 0.9546 - f1: 0.9546\n",
      "Epoch 104/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1151 - accuracy: 0.9528 - recall: 0.9527 - f1: 0.9528\n",
      "Epoch 00104: loss did not improve from 0.11134\n",
      "100/100 [==============================] - 96s 964ms/step - loss: 0.1152 - accuracy: 0.9528 - recall: 0.9527 - f1: 0.9527\n",
      "Epoch 105/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1156 - accuracy: 0.9533 - recall: 0.9534 - f1: 0.9533\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "\n",
      "Epoch 00105: loss did not improve from 0.11134\n",
      "Restoring model weights from the end of the best epoch.\n",
      "100/100 [==============================] - 98s 977ms/step - loss: 0.1159 - accuracy: 0.9531 - recall: 0.9532 - f1: 0.9531\n",
      "Epoch 00105: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(history.history[\"loss\"][1:], label=\"train_loss\")\n",
    "# plt.plot(history.history[\"val_loss\"][1:], label=\"val_loss\")\n",
    "plt.plot(history.history[\"accuracy\"][1:], label=\"train_acc\")\n",
    "# plt.plot(history.history[\"val_accuracy\"][1:], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"plots/Unet_batchnorm_fix.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 64, 64, 3)    12          input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 64, 64, 64)   4864        batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 64, 64, 64)   36928       conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling2D) (None, 32, 32, 64)   0           conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 32, 32, 64)   256         max_pooling2d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 32, 32, 128)  204928      batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 32, 32, 128)  147584      conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling2D) (None, 16, 16, 128)  0           conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 16, 16, 128)  512         max_pooling2d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 16, 16, 256)  819456      batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 16, 16, 256)  590080      conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling2D) (None, 8, 8, 256)    0           conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 8, 8, 256)    1024        max_pooling2d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 8, 8, 512)    3277312     batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 8, 8, 512)    2359808     conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling2D) (None, 4, 4, 512)    0           conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 4, 4, 512)    2048        max_pooling2d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_32 (UpSampling2D) (None, 8, 8, 512)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 8, 8, 512)    1049088     up_sampling2d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 8, 8, 1024)   0           conv2d_175[0][0]                 \n",
      "                                                                 conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 8, 8, 512)    13107712    concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 8, 8, 512)    2359808     conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 8, 8, 512)    2048        conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_33 (UpSampling2D) (None, 16, 16, 512)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 16, 16, 256)  524544      up_sampling2d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 16, 16, 512)  0           conv2d_173[0][0]                 \n",
      "                                                                 conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 16, 16, 256)  3277056     concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 16, 16, 256)  590080      conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 16, 16, 256)  1024        conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_34 (UpSampling2D) (None, 32, 32, 256)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 32, 32, 128)  131200      up_sampling2d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 32, 32, 256)  0           conv2d_171[0][0]                 \n",
      "                                                                 conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 32, 32, 128)  819328      concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 32, 32, 128)  147584      conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 32, 32, 128)  512         conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_35 (UpSampling2D) (None, 64, 64, 128)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 64, 64, 64)   32832       up_sampling2d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 64, 64, 128)  0           conv2d_169[0][0]                 \n",
      "                                                                 conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 64, 64, 64)   204864      concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 64, 64, 64)   36928       conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 64, 64, 1)    65          conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 4096)         0           conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 4096)         16384       flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 2)            8194        batch_normalization_80[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 29,754,063\n",
      "Trainable params: 29,742,153\n",
      "Non-trainable params: 11,910\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 64, 64, 3)    12          input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 64, 64, 64)   4864        batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 64, 64, 64)   36928       conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 32, 32, 64)   0           conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 32, 32, 64)   256         max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 32, 32, 128)  204928      batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 32, 32, 128)  147584      conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 16, 16, 128)  0           conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 128)  512         max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 16, 16, 256)  819456      batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 16, 16, 256)  590080      conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 8, 8, 256)    0           conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 256)    1024        max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 8, 8, 512)    3277312     batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 8, 8, 512)    2359808     conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 4, 4, 512)    0           conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 4, 4, 512)    2048        max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_12 (UpSampling2D) (None, 8, 8, 512)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 8, 8, 512)    1049088     up_sampling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 8, 8, 1024)   0           conv2d_70[0][0]                  \n",
      "                                                                 conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 8, 8, 512)    13107712    concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 8, 8, 512)    2359808     conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 8, 8, 512)    2048        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_13 (UpSampling2D) (None, 16, 16, 512)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 16, 16, 256)  524544      up_sampling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 16, 16, 512)  0           conv2d_68[0][0]                  \n",
      "                                                                 conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 16, 16, 256)  3277056     concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 16, 16, 256)  590080      conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 16, 16, 256)  1024        conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_14 (UpSampling2D) (None, 32, 32, 256)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 32, 32, 128)  131200      up_sampling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 32, 32, 256)  0           conv2d_66[0][0]                  \n",
      "                                                                 conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 32, 32, 128)  819328      concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 32, 32, 128)  147584      conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 32, 32, 128)  512         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_15 (UpSampling2D) (None, 64, 64, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 64, 64, 64)   32832       up_sampling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 64, 64, 128)  0           conv2d_64[0][0]                  \n",
      "                                                                 conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 64, 64, 64)   204864      concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 64, 64, 64)   36928       conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 64, 64, 1)    65          conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 4096)         0           conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 4096)         16384       flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            8194        batch_normalization_35[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 29,754,063\n",
      "Trainable params: 29,742,153\n",
      "Non-trainable params: 11,910\n",
      "__________________________________________________________________________________________________\n",
      "Processing image => data/test_set_images/test_1/test_1.png\n",
      "Processing image => data/test_set_images/test_2/test_2.png\n",
      "Processing image => data/test_set_images/test_3/test_3.png\n",
      "Processing image => data/test_set_images/test_4/test_4.png\n",
      "Processing image => data/test_set_images/test_5/test_5.png\n",
      "Processing image => data/test_set_images/test_6/test_6.png\n",
      "Processing image => data/test_set_images/test_7/test_7.png\n",
      "Processing image => data/test_set_images/test_8/test_8.png\n",
      "Processing image => data/test_set_images/test_9/test_9.png\n",
      "Processing image => data/test_set_images/test_10/test_10.png\n",
      "Processing image => data/test_set_images/test_11/test_11.png\n",
      "Processing image => data/test_set_images/test_12/test_12.png\n",
      "Processing image => data/test_set_images/test_13/test_13.png\n",
      "Processing image => data/test_set_images/test_14/test_14.png\n",
      "Processing image => data/test_set_images/test_15/test_15.png\n",
      "Processing image => data/test_set_images/test_16/test_16.png\n",
      "Processing image => data/test_set_images/test_17/test_17.png\n",
      "Processing image => data/test_set_images/test_18/test_18.png\n",
      "Processing image => data/test_set_images/test_19/test_19.png\n",
      "Processing image => data/test_set_images/test_20/test_20.png\n",
      "Processing image => data/test_set_images/test_21/test_21.png\n",
      "Processing image => data/test_set_images/test_22/test_22.png\n",
      "Processing image => data/test_set_images/test_23/test_23.png\n",
      "Processing image => data/test_set_images/test_24/test_24.png\n",
      "Processing image => data/test_set_images/test_25/test_25.png\n",
      "Processing image => data/test_set_images/test_26/test_26.png\n",
      "Processing image => data/test_set_images/test_27/test_27.png\n",
      "Processing image => data/test_set_images/test_28/test_28.png\n",
      "Processing image => data/test_set_images/test_29/test_29.png\n",
      "Processing image => data/test_set_images/test_30/test_30.png\n",
      "Processing image => data/test_set_images/test_31/test_31.png\n",
      "Processing image => data/test_set_images/test_32/test_32.png\n",
      "Processing image => data/test_set_images/test_33/test_33.png\n",
      "Processing image => data/test_set_images/test_34/test_34.png\n",
      "Processing image => data/test_set_images/test_35/test_35.png\n",
      "Processing image => data/test_set_images/test_36/test_36.png\n",
      "Processing image => data/test_set_images/test_37/test_37.png\n",
      "Processing image => data/test_set_images/test_38/test_38.png\n",
      "Processing image => data/test_set_images/test_39/test_39.png\n",
      "Processing image => data/test_set_images/test_40/test_40.png\n",
      "Processing image => data/test_set_images/test_41/test_41.png\n",
      "Processing image => data/test_set_images/test_42/test_42.png\n",
      "Processing image => data/test_set_images/test_43/test_43.png\n",
      "Processing image => data/test_set_images/test_44/test_44.png\n",
      "Processing image => data/test_set_images/test_45/test_45.png\n",
      "Processing image => data/test_set_images/test_46/test_46.png\n",
      "Processing image => data/test_set_images/test_47/test_47.png\n",
      "Processing image => data/test_set_images/test_48/test_48.png\n",
      "Processing image => data/test_set_images/test_49/test_49.png\n",
      "Processing image => data/test_set_images/test_50/test_50.png\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = U_NET(shape=(WINDOW_SIZE, WINDOW_SIZE, 3))\n",
    "\n",
    "# Load the model\n",
    "model.load(\"Unet_batchnorm_fix-095-0.953749.h5\")\n",
    "\n",
    "model.model.summary()\n",
    "\n",
    "# We add all test images to an array, used later for generating a submission\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "    image_filenames.append(image_filename)\n",
    "\n",
    "# Set-up submission filename\n",
    "submission_filename = \"Unet_batchnorm_fix-095-0.953749.csv\"\n",
    "\n",
    "# Generates the submission\n",
    "generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}