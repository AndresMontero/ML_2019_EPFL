{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext lab_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16819036702665989063\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4937233203\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10420552843420877318\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Create some tensors\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as scipy\n",
    "import os\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend as K\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary for our model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    LeakyReLU,\n",
    ")\n",
    "from tensorflow.compat.v2.keras.layers import BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "# keras, model definition...\n",
    "cb = TQDMNotebookCallback()\n",
    "setattr(cb, \"on_train_batch_begin\", lambda x, y: None)\n",
    "setattr(cb, \"on_train_batch_end\", lambda x, y: None)\n",
    "\n",
    "# model.fit(X_train, Y_train, verbose=0, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 85 images\n",
      "Loading 85 groundtruth images\n"
     ]
    }
   ],
   "source": [
    "# Load a set of images\n",
    "root_dir = \"data/\"\n",
    "\n",
    "# Select the directory for the images and load them\n",
    "image_dir_train = root_dir + \"training/images/\"\n",
    "files = os.listdir(image_dir_train)\n",
    "n_train = len(files)\n",
    "\n",
    "print(\"Loading \" + str(n_train) + \" images\")\n",
    "imgs_train = np.asarray(\n",
    "    [load_image(image_dir_train + files[i]) for i in range(n_train)]\n",
    ")\n",
    "\n",
    "# Select the directory for groundtruth images and load them\n",
    "gt_dir_train = root_dir + \"training/groundtruth/\"\n",
    "print(\"Loading \" + str(n_train) + \" groundtruth images\")\n",
    "gt_imgs_train = np.asarray(\n",
    "    [load_image(gt_dir_train + files[i]) for i in range(n_train)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 400, 400, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 400, 400)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_imgs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 400\n",
    "\n",
    "# We separate the images from the groundtruth images\n",
    "img_patches_train = [\n",
    "    img_crop(imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "gt_patches_train = [\n",
    "    img_crop(gt_imgs_train[i], image_size, image_size) for i in range(n_train)\n",
    "]\n",
    "\n",
    "# Linearize the list and labeling them X and Y\n",
    "X_train = np.asarray(\n",
    "    [\n",
    "        img_patches_train[i][j]\n",
    "        for i in range(len(img_patches_train))\n",
    "        for j in range(len(img_patches_train[i]))\n",
    "    ]\n",
    ")\n",
    "Y_train = np.asarray(\n",
    "    [\n",
    "        gt_patches_train[i][j]\n",
    "        for i in range(len(gt_patches_train))\n",
    "        for j in range(len(gt_patches_train[i]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 400, 400, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 400, 400)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 15 images\n",
      "Loading 15 groundtruth images\n"
     ]
    }
   ],
   "source": [
    "# Select the directory for the images and load them\n",
    "image_dir_val = root_dir + \"validating/images/\"\n",
    "files = os.listdir(image_dir_val)\n",
    "n_val = len(files)\n",
    "\n",
    "print(\"Loading \" + str(n_val) + \" images\")\n",
    "imgs_val = np.asarray([load_image(image_dir_val + files[i]) for i in range(n_val)])\n",
    "\n",
    "# Select the directory for groundtruth images and load them\n",
    "gt_dir_val = root_dir + \"validating/groundtruth/\"\n",
    "print(\"Loading \" + str(n_val) + \" groundtruth images\")\n",
    "gt_imgs_val = np.asarray([load_image(gt_dir_val + files[i]) for i in range(n_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 400, 400, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 400, 400)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 400\n",
    "\n",
    "# We separate the images from the groundtruth images\n",
    "img_patches_val = [img_crop(imgs_val[i], image_size, image_size) for i in range(n_val)]\n",
    "gt_patches_val = [\n",
    "    img_crop(gt_imgs_val[i], image_size, image_size) for i in range(n_val)\n",
    "]\n",
    "\n",
    "# Linearize the list and labeling them X and Y\n",
    "X_val = np.asarray(\n",
    "    [\n",
    "        img_patches_val[i][j]\n",
    "        for i in range(len(img_patches_val))\n",
    "        for j in range(len(img_patches_val[i]))\n",
    "    ]\n",
    ")\n",
    "Y_val = np.asarray(\n",
    "    [\n",
    "        gt_patches_val[i][j]\n",
    "        for i in range(len(gt_patches_val))\n",
    "        for j in range(len(gt_patches_val[i]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 400, 400, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 400, 400)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating mini-batch and running data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatch(X, Y, n):\n",
    "\n",
    "    # Fix the seed\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # We define the window size of 72, batch size of 100 (empirically chosen)\n",
    "    # and patch size should correspond to 16\n",
    "    w_size = 72\n",
    "    batch_size = 100\n",
    "    patch_size = 16\n",
    "    num_images = n\n",
    "\n",
    "    while True:\n",
    "        # Generate one minibatch\n",
    "        batch_image = np.empty((batch_size, w_size, w_size, 3))\n",
    "        batch_label = np.empty((batch_size, 2))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            # Select a random index represnting an image\n",
    "            random_index = np.random.choice(num_images)\n",
    "\n",
    "            # Width of original image\n",
    "            width = 400\n",
    "\n",
    "            # Sample a random window from the image\n",
    "            random_sample = np.random.randint(w_size // 2, width - w_size // 2, 2)\n",
    "\n",
    "            # Create a sub image of size 72x72\n",
    "            sampled_image = X[random_index][\n",
    "                random_sample[0] - w_size // 2 : random_sample[0] + w_size // 2,\n",
    "                random_sample[1] - w_size // 2 : random_sample[1] + w_size // 2,\n",
    "            ]\n",
    "\n",
    "            # Take its corresponding ground-truth image\n",
    "            correspond_ground_truth = Y[random_index][\n",
    "                random_sample[0] - patch_size // 2 : random_sample[0] + patch_size // 2,\n",
    "                random_sample[1] - patch_size // 2 : random_sample[1] + patch_size // 2,\n",
    "            ]\n",
    "\n",
    "            # We set in the label depending on the threshold of 0.25\n",
    "            # The label becomes either 0 or 1 by applying to_categorical with parameter 2\n",
    "            label = to_categorical(\n",
    "                (np.array([np.mean(correspond_ground_truth)]) > 0.25) * 1, 2\n",
    "            )\n",
    "\n",
    "            # The image augmentation is based on both flipping and rotating (randomly in steps of 45°)\n",
    "            # Random vertical and horizontal flip\n",
    "            if np.random.choice(2) == 1:\n",
    "                sampled_image = np.flipud(sampled_image)\n",
    "\n",
    "            if np.random.choice(2) == 1:\n",
    "                sampled_image = np.fliplr(sampled_image)\n",
    "\n",
    "            # Random rotation in steps of 45°\n",
    "            rotations = [0, 45, 90, 135, 180, 225, 270, 315]\n",
    "\n",
    "            # We select a rotation degree randomly\n",
    "            rotation_choice = np.random.choice(len(rotations))\n",
    "\n",
    "            # Rotate it using the random value (uses the scipy library)\n",
    "            sampled_image = scipy.ndimage.rotate(\n",
    "                sampled_image,\n",
    "                rotations[rotation_choice],\n",
    "                order=1,\n",
    "                reshape=False,\n",
    "                mode=\"reflect\",\n",
    "            )\n",
    "\n",
    "            # We put in the sub image and its corresponding label before yielding it\n",
    "            batch_image[i] = sampled_image\n",
    "            batch_label[i] = label\n",
    "\n",
    "        # Yield the mini_batch to the model\n",
    "        yield (batch_image, batch_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the class (Same as in cnn_model.py, but provided here for better readability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_model:\n",
    "\n",
    "    # Initialize the class\n",
    "    def __init__(self, shape, batch_normalization, activation):\n",
    "        self.shape = shape\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.activation = activation\n",
    "        self.model = self.initialize_cnn_model(shape, batch_normalization, activation)\n",
    "\n",
    "    def initialize_cnn_model(self, shape, batch_normalization, activation):\n",
    "        #         print(activation)\n",
    "\n",
    "        # INPUT\n",
    "        # shape     - Size of the input images\n",
    "        # OUTPUT\n",
    "        # model    - Compiled CNN\n",
    "\n",
    "        # Define hyperparamters\n",
    "        KERNEL3 = (3, 3)\n",
    "        KERNEL5 = (5, 5)\n",
    "\n",
    "        # Define a model\n",
    "        model = Sequential()\n",
    "\n",
    "        # Add the layers\n",
    "        # Selection of the model is described in the report\n",
    "        # We use padding = 'same' to avoid issues with the matrix sizes\n",
    "        model.add(Conv2D(64, KERNEL5, input_shape=shape, padding=\"same\"))\n",
    "        if batch_normalization:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "            Activation(activation)\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(128, KERNEL3, padding=\"same\"))\n",
    "        if batch_normalization:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "            Activation(activation)\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "        if batch_normalization:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "            Activation(activation)\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "        if batch_normalization:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "            Activation(activation)\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "        if batch_normalization:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "            Activation(activation)\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(256, KERNEL3, padding=\"same\"))\n",
    "        if batch_normalization:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "            Activation(activation)\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        # Flatten it and use regularizers to avoid overfitting\n",
    "        # The parameters have been chosen empirically\n",
    "        model.add(Flatten())\n",
    "        model.add(\n",
    "            Dense(\n",
    "                128, kernel_regularizer=l2(0.000001), activity_regularizer=l2(0.000001)\n",
    "            )\n",
    "        )\n",
    "        #         if batch_normalization:\n",
    "        #             model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.1)) if activation == \"LeakyReLU\" else model.add(\n",
    "            Activation(activation)\n",
    "        )\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # Add output layer\n",
    "        model.add(\n",
    "            Dense(2, kernel_regularizer=l2(0.000001), activity_regularizer=l2(0.000001))\n",
    "        )\n",
    "        model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "        # Compile the model using the binary crossentropy loss and the Adam optimizer for it\n",
    "        # We used the accuracy as a metric, but F1 score is also a plausible choice\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(lr=0.001),\n",
    "            metrics=[\"accuracy\", recall, f1],\n",
    "        )\n",
    "\n",
    "        # Print a summary of the model to see what has been generated\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # Early stopping callback after 10 steps\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0,\n",
    "            patience=15,\n",
    "            verbose=1,\n",
    "            mode=\"auto\",\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "\n",
    "        # Reduce learning rate on plateau after 4 steps\n",
    "        lr_callback = ReduceLROnPlateau(\n",
    "            monitor=\"loss\", factor=0.5, patience=4, verbose=1, mode=\"auto\"\n",
    "        )\n",
    "        save_best = ModelCheckpoint(\n",
    "            \"batch_LeakyReLU_validation_160_dropout-0.25-{epoch:03d}-{accuracy:03f}-{val_accuracy:03f}.h5\",\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"auto\",\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        # Place the callbacks in a list to be used when training\n",
    "        #         callbacks = [cb, early_stopping, lr_callback]\n",
    "        callbacks = [save_best, lr_callback]\n",
    "\n",
    "        # Train the model using the previously defined functions and callbacks\n",
    "        history = self.model.fit_generator(\n",
    "            create_minibatch(X_train, Y_train, n_train),\n",
    "            steps_per_epoch=STEPS_PER_EPOCH,\n",
    "            epochs=EPOCHS,\n",
    "            use_multiprocessing=False,\n",
    "            workers=1,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "            validation_data=create_minibatch(X_val, Y_val, n_val),\n",
    "            validation_steps=STEPS_PER_EPOCH,\n",
    "        )\n",
    "        #         to_plot = self.model.fit_generator(\n",
    "        #             create_minibatch(X_train, Y_train, n_train),\n",
    "        #             steps_per_epoch=100,\n",
    "        #             epochs=EPOCHS,\n",
    "        #             use_multiprocessing=False,\n",
    "        #             workers=1,\n",
    "        #             callbacks=callbacks,\n",
    "        #             verbose=1,\n",
    "        #             validation_data=create_minibatch(X_val, Y_val, n_val),\n",
    "        #             validation_steps=100,\n",
    "        #         )\n",
    "        return history\n",
    "\n",
    "    def classify(self, X):\n",
    "        # Subdivide the images into blocks with a stride and patch_size of 16\n",
    "        img_patches = create_patches(X, 16, 16, padding=28)\n",
    "\n",
    "        # Predict\n",
    "        predictions = self.model.predict(img_patches)\n",
    "        predictions = (predictions[:, 0] < predictions[:, 1]) * 1\n",
    "\n",
    "        # Regroup patches into images\n",
    "        return group_patches(predictions, X.shape[0])\n",
    "\n",
    "    def load(self, filename):\n",
    "        # Load the model (used for submission)\n",
    "        dependencies = {\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "        self.model = load_model(filename, custom_objects=dependencies)\n",
    "\n",
    "\n",
    "#     def save(self, filename):\n",
    "#         # Save the model (used to then load to submit)\n",
    "#         self.model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,450\n",
      "Trainable params: 2,278,018\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n",
      "Epoch 1/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.5724 - accuracy: 0.7374 - recall: 0.7419 - f1: 0.7388\n",
      "Epoch 00001: val_loss improved from inf to 0.62171, saving model to batch_LeakyReLU_validation_160_dropout-0.25-001-0.737467-0.750800.h5\n",
      "150/150 [==============================] - 78s 520ms/step - loss: 0.5720 - accuracy: 0.7375 - recall: 0.7419 - f1: 0.7388 - val_loss: 0.6217 - val_accuracy: 0.7508 - val_recall: 0.7508 - val_f1: 0.7508\n",
      "Epoch 2/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.4866 - accuracy: 0.7578 - recall: 0.7530 - f1: 0.7565\n",
      "Epoch 00002: val_loss did not improve from 0.62171\n",
      "150/150 [==============================] - 69s 462ms/step - loss: 0.4865 - accuracy: 0.7577 - recall: 0.7529 - f1: 0.7565 - val_loss: 0.7564 - val_accuracy: 0.3563 - val_recall: 0.3494 - val_f1: 0.3518\n",
      "Epoch 3/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3946 - accuracy: 0.8190 - recall: 0.8168 - f1: 0.8186\n",
      "Epoch 00003: val_loss improved from 0.62171 to 0.53719, saving model to batch_LeakyReLU_validation_160_dropout-0.25-003-0.819133-0.702733.h5\n",
      "150/150 [==============================] - 67s 448ms/step - loss: 0.3944 - accuracy: 0.8191 - recall: 0.8168 - f1: 0.8187 - val_loss: 0.5372 - val_accuracy: 0.7027 - val_recall: 0.7037 - val_f1: 0.7030\n",
      "Epoch 4/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3643 - accuracy: 0.8347 - recall: 0.8347 - f1: 0.8347\n",
      "Epoch 00004: val_loss improved from 0.53719 to 0.40577, saving model to batch_LeakyReLU_validation_160_dropout-0.25-004-0.834700-0.815500.h5\n",
      "150/150 [==============================] - 68s 452ms/step - loss: 0.3648 - accuracy: 0.8347 - recall: 0.8347 - f1: 0.8347 - val_loss: 0.4058 - val_accuracy: 0.8155 - val_recall: 0.8119 - val_f1: 0.8148\n",
      "Epoch 5/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3295 - accuracy: 0.8538 - recall: 0.8528 - f1: 0.8537\n",
      "Epoch 00005: val_loss did not improve from 0.40577\n",
      "150/150 [==============================] - 69s 458ms/step - loss: 0.3288 - accuracy: 0.8542 - recall: 0.8533 - f1: 0.8540 - val_loss: 0.4356 - val_accuracy: 0.7808 - val_recall: 0.7815 - val_f1: 0.7810\n",
      "Epoch 6/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3186 - accuracy: 0.8601 - recall: 0.8605 - f1: 0.8601\n",
      "Epoch 00006: val_loss improved from 0.40577 to 0.37243, saving model to batch_LeakyReLU_validation_160_dropout-0.25-006-0.860167-0.813767.h5\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.3184 - accuracy: 0.8602 - recall: 0.8606 - f1: 0.8602 - val_loss: 0.3724 - val_accuracy: 0.8138 - val_recall: 0.8119 - val_f1: 0.8134\n",
      "Epoch 7/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.8728 - recall: 0.8729 - f1: 0.8728\n",
      "Epoch 00007: val_loss did not improve from 0.37243\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.2868 - accuracy: 0.8732 - recall: 0.8733 - f1: 0.8732 - val_loss: 0.3788 - val_accuracy: 0.8103 - val_recall: 0.8125 - val_f1: 0.8107\n",
      "Epoch 8/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2699 - accuracy: 0.8876 - recall: 0.8878 - f1: 0.8876\n",
      "Epoch 00008: val_loss improved from 0.37243 to 0.36826, saving model to batch_LeakyReLU_validation_160_dropout-0.25-008-0.887767-0.825067.h5\n",
      "150/150 [==============================] - 68s 457ms/step - loss: 0.2697 - accuracy: 0.8878 - recall: 0.8879 - f1: 0.8878 - val_loss: 0.3683 - val_accuracy: 0.8251 - val_recall: 0.8227 - val_f1: 0.8246\n",
      "Epoch 9/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2615 - accuracy: 0.8901 - recall: 0.8900 - f1: 0.8901\n",
      "Epoch 00009: val_loss improved from 0.36826 to 0.31509, saving model to batch_LeakyReLU_validation_160_dropout-0.25-009-0.890167-0.863167.h5\n",
      "150/150 [==============================] - 69s 458ms/step - loss: 0.2614 - accuracy: 0.8902 - recall: 0.8901 - f1: 0.8902 - val_loss: 0.3151 - val_accuracy: 0.8632 - val_recall: 0.8651 - val_f1: 0.8634\n",
      "Epoch 10/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2536 - accuracy: 0.8929 - recall: 0.8921 - f1: 0.8928\n",
      "Epoch 00010: val_loss did not improve from 0.31509\n",
      "150/150 [==============================] - 68s 451ms/step - loss: 0.2532 - accuracy: 0.8929 - recall: 0.8921 - f1: 0.8928 - val_loss: 0.5587 - val_accuracy: 0.8009 - val_recall: 0.8029 - val_f1: 0.8013\n",
      "Epoch 11/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2430 - accuracy: 0.8976 - recall: 0.8984 - f1: 0.8976\n",
      "Epoch 00011: val_loss did not improve from 0.31509\n",
      "150/150 [==============================] - 70s 463ms/step - loss: 0.2432 - accuracy: 0.8975 - recall: 0.8983 - f1: 0.8976 - val_loss: 0.4471 - val_accuracy: 0.7999 - val_recall: 0.8003 - val_f1: 0.8000\n",
      "Epoch 12/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2341 - accuracy: 0.9021 - recall: 0.9025 - f1: 0.9022\n",
      "Epoch 00012: val_loss did not improve from 0.31509\n",
      "150/150 [==============================] - 82s 550ms/step - loss: 0.2341 - accuracy: 0.9021 - recall: 0.9024 - f1: 0.9021 - val_loss: 0.3904 - val_accuracy: 0.8233 - val_recall: 0.8233 - val_f1: 0.8233\n",
      "Epoch 13/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2348 - accuracy: 0.9038 - recall: 0.9037 - f1: 0.9038\n",
      "Epoch 00013: val_loss did not improve from 0.31509\n",
      "150/150 [==============================] - 75s 502ms/step - loss: 0.2345 - accuracy: 0.9038 - recall: 0.9037 - f1: 0.9038 - val_loss: 0.3618 - val_accuracy: 0.8291 - val_recall: 0.8289 - val_f1: 0.8291\n",
      "Epoch 14/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2312 - accuracy: 0.9008 - recall: 0.9003 - f1: 0.9008\n",
      "Epoch 00014: val_loss did not improve from 0.31509\n",
      "150/150 [==============================] - 75s 503ms/step - loss: 0.2313 - accuracy: 0.9008 - recall: 0.9003 - f1: 0.9008 - val_loss: 0.3567 - val_accuracy: 0.8316 - val_recall: 0.8319 - val_f1: 0.8316\n",
      "Epoch 15/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2180 - accuracy: 0.9080 - recall: 0.9084 - f1: 0.9080\n",
      "Epoch 00015: val_loss did not improve from 0.31509\n",
      "150/150 [==============================] - 94s 628ms/step - loss: 0.2178 - accuracy: 0.9082 - recall: 0.9086 - f1: 0.9082 - val_loss: 0.3749 - val_accuracy: 0.8544 - val_recall: 0.8541 - val_f1: 0.8544\n",
      "Epoch 16/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2132 - accuracy: 0.9118 - recall: 0.9109 - f1: 0.9117\n",
      "Epoch 00016: val_loss improved from 0.31509 to 0.29161, saving model to batch_LeakyReLU_validation_160_dropout-0.25-016-0.912100-0.882367.h5\n",
      "150/150 [==============================] - 78s 521ms/step - loss: 0.2127 - accuracy: 0.9121 - recall: 0.9111 - f1: 0.9120 - val_loss: 0.2916 - val_accuracy: 0.8824 - val_recall: 0.8810 - val_f1: 0.8822\n",
      "Epoch 17/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2125 - accuracy: 0.9130 - recall: 0.9127 - f1: 0.9130\n",
      "Epoch 00017: val_loss improved from 0.29161 to 0.25331, saving model to batch_LeakyReLU_validation_160_dropout-0.25-017-0.913000-0.890467.h5\n",
      "150/150 [==============================] - 78s 522ms/step - loss: 0.2125 - accuracy: 0.9130 - recall: 0.9127 - f1: 0.9130 - val_loss: 0.2533 - val_accuracy: 0.8905 - val_recall: 0.8899 - val_f1: 0.8904\n",
      "Epoch 18/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2079 - accuracy: 0.9140 - recall: 0.9145 - f1: 0.9140\n",
      "Epoch 00018: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 77s 516ms/step - loss: 0.2080 - accuracy: 0.9139 - recall: 0.9145 - f1: 0.9140 - val_loss: 1.0854 - val_accuracy: 0.7716 - val_recall: 0.7717 - val_f1: 0.7716\n",
      "Epoch 19/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2009 - accuracy: 0.9202 - recall: 0.9198 - f1: 0.9202\n",
      "Epoch 00019: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 68s 451ms/step - loss: 0.2007 - accuracy: 0.9203 - recall: 0.9199 - f1: 0.9202 - val_loss: 0.3170 - val_accuracy: 0.8553 - val_recall: 0.8551 - val_f1: 0.8553\n",
      "Epoch 20/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1977 - accuracy: 0.9163 - recall: 0.9159 - f1: 0.9162\n",
      "Epoch 00020: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 79s 524ms/step - loss: 0.1974 - accuracy: 0.9165 - recall: 0.9161 - f1: 0.9164 - val_loss: 0.2989 - val_accuracy: 0.8611 - val_recall: 0.8603 - val_f1: 0.8610\n",
      "Epoch 21/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1934 - accuracy: 0.9225 - recall: 0.9226 - f1: 0.9225\n",
      "Epoch 00021: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 73s 486ms/step - loss: 0.1933 - accuracy: 0.9225 - recall: 0.9226 - f1: 0.9225 - val_loss: 0.5145 - val_accuracy: 0.8027 - val_recall: 0.8027 - val_f1: 0.8027\n",
      "Epoch 22/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1943 - accuracy: 0.9185 - recall: 0.9182 - f1: 0.9184\n",
      "Epoch 00022: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 74s 496ms/step - loss: 0.1945 - accuracy: 0.9183 - recall: 0.9180 - f1: 0.9183 - val_loss: 0.3091 - val_accuracy: 0.8474 - val_recall: 0.8465 - val_f1: 0.8473\n",
      "Epoch 23/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1944 - accuracy: 0.9179 - recall: 0.9185 - f1: 0.9179\n",
      "Epoch 00023: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 78s 520ms/step - loss: 0.1948 - accuracy: 0.9177 - recall: 0.9183 - f1: 0.9178 - val_loss: 0.3409 - val_accuracy: 0.8375 - val_recall: 0.8379 - val_f1: 0.8375\n",
      "Epoch 24/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1979 - accuracy: 0.9191 - recall: 0.9195 - f1: 0.9192\n",
      "Epoch 00024: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 79s 526ms/step - loss: 0.1982 - accuracy: 0.9190 - recall: 0.9193 - f1: 0.9190 - val_loss: 0.3191 - val_accuracy: 0.8504 - val_recall: 0.8512 - val_f1: 0.8505\n",
      "Epoch 25/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1874 - accuracy: 0.9210 - recall: 0.9217 - f1: 0.9211\n",
      "Epoch 00025: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 78s 521ms/step - loss: 0.1873 - accuracy: 0.9210 - recall: 0.9217 - f1: 0.9211 - val_loss: 0.3768 - val_accuracy: 0.8315 - val_recall: 0.8319 - val_f1: 0.8315\n",
      "Epoch 26/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1867 - accuracy: 0.9267 - recall: 0.9271 - f1: 0.9268\n",
      "Epoch 00026: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 77s 512ms/step - loss: 0.1869 - accuracy: 0.9267 - recall: 0.9271 - f1: 0.9267 - val_loss: 0.2946 - val_accuracy: 0.8677 - val_recall: 0.8679 - val_f1: 0.8677\n",
      "Epoch 27/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1792 - accuracy: 0.9259 - recall: 0.9258 - f1: 0.9259\n",
      "Epoch 00027: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 77s 511ms/step - loss: 0.1791 - accuracy: 0.9259 - recall: 0.9259 - f1: 0.9259 - val_loss: 0.2674 - val_accuracy: 0.8904 - val_recall: 0.8899 - val_f1: 0.8904\n",
      "Epoch 28/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1755 - accuracy: 0.9284 - recall: 0.9281 - f1: 0.9284\n",
      "Epoch 00028: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 77s 514ms/step - loss: 0.1757 - accuracy: 0.9283 - recall: 0.9279 - f1: 0.9282 - val_loss: 0.4680 - val_accuracy: 0.8014 - val_recall: 0.8029 - val_f1: 0.8017\n",
      "Epoch 29/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1776 - accuracy: 0.9280 - recall: 0.9283 - f1: 0.9280\n",
      "Epoch 00029: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 80s 534ms/step - loss: 0.1779 - accuracy: 0.9278 - recall: 0.9281 - f1: 0.9278 - val_loss: 0.3898 - val_accuracy: 0.8406 - val_recall: 0.8401 - val_f1: 0.8405\n",
      "Epoch 30/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1802 - accuracy: 0.9276 - recall: 0.9274 - f1: 0.9276\n",
      "Epoch 00030: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 88s 587ms/step - loss: 0.1801 - accuracy: 0.9275 - recall: 0.9273 - f1: 0.9275 - val_loss: 0.2979 - val_accuracy: 0.8707 - val_recall: 0.8714 - val_f1: 0.8708\n",
      "Epoch 31/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1777 - accuracy: 0.9264 - recall: 0.9264 - f1: 0.9264\n",
      "Epoch 00031: val_loss did not improve from 0.25331\n",
      "150/150 [==============================] - 80s 532ms/step - loss: 0.1775 - accuracy: 0.9263 - recall: 0.9263 - f1: 0.9263 - val_loss: 0.3168 - val_accuracy: 0.8541 - val_recall: 0.8549 - val_f1: 0.8543\n",
      "Epoch 32/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1733 - accuracy: 0.9294 - recall: 0.9293 - f1: 0.9294\n",
      "Epoch 00032: val_loss improved from 0.25331 to 0.24671, saving model to batch_LeakyReLU_validation_160_dropout-0.25-032-0.929200-0.892433.h5\n",
      "150/150 [==============================] - 82s 550ms/step - loss: 0.1736 - accuracy: 0.9292 - recall: 0.9291 - f1: 0.9292 - val_loss: 0.2467 - val_accuracy: 0.8924 - val_recall: 0.8928 - val_f1: 0.8925\n",
      "Epoch 33/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.9330 - recall: 0.9330 - f1: 0.9330\n",
      "Epoch 00033: val_loss did not improve from 0.24671\n",
      "150/150 [==============================] - 83s 556ms/step - loss: 0.1655 - accuracy: 0.9328 - recall: 0.9328 - f1: 0.9328 - val_loss: 0.8405 - val_accuracy: 0.7871 - val_recall: 0.7870 - val_f1: 0.7871\n",
      "Epoch 34/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1722 - accuracy: 0.9329 - recall: 0.9327 - f1: 0.9328\n",
      "Epoch 00034: val_loss did not improve from 0.24671\n",
      "150/150 [==============================] - 77s 514ms/step - loss: 0.1720 - accuracy: 0.9331 - recall: 0.9329 - f1: 0.9331 - val_loss: 0.2583 - val_accuracy: 0.8829 - val_recall: 0.8825 - val_f1: 0.8828\n",
      "Epoch 35/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1661 - accuracy: 0.9316 - recall: 0.9317 - f1: 0.9316\n",
      "Epoch 00035: val_loss did not improve from 0.24671\n",
      "150/150 [==============================] - 77s 515ms/step - loss: 0.1662 - accuracy: 0.9315 - recall: 0.9317 - f1: 0.9315 - val_loss: 0.2525 - val_accuracy: 0.8886 - val_recall: 0.8885 - val_f1: 0.8886\n",
      "Epoch 36/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1689 - accuracy: 0.9295 - recall: 0.9297 - f1: 0.9295\n",
      "Epoch 00036: val_loss did not improve from 0.24671\n",
      "150/150 [==============================] - 82s 544ms/step - loss: 0.1688 - accuracy: 0.9295 - recall: 0.9297 - f1: 0.9295 - val_loss: 0.3462 - val_accuracy: 0.8695 - val_recall: 0.8695 - val_f1: 0.8695\n",
      "Epoch 37/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1700 - accuracy: 0.9303 - recall: 0.9301 - f1: 0.9303\n",
      "Epoch 00037: val_loss did not improve from 0.24671\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "150/150 [==============================] - 88s 590ms/step - loss: 0.1703 - accuracy: 0.9302 - recall: 0.9299 - f1: 0.9302 - val_loss: 0.3406 - val_accuracy: 0.8578 - val_recall: 0.8578 - val_f1: 0.8578\n",
      "Epoch 38/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1555 - accuracy: 0.9364 - recall: 0.9365 - f1: 0.9364\n",
      "Epoch 00038: val_loss did not improve from 0.24671\n",
      "150/150 [==============================] - 77s 512ms/step - loss: 0.1554 - accuracy: 0.9365 - recall: 0.9366 - f1: 0.9365 - val_loss: 0.2486 - val_accuracy: 0.8966 - val_recall: 0.8963 - val_f1: 0.8966\n",
      "Epoch 39/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1457 - accuracy: 0.9425 - recall: 0.9428 - f1: 0.9425\n",
      "Epoch 00039: val_loss did not improve from 0.24671\n",
      "150/150 [==============================] - 77s 513ms/step - loss: 0.1457 - accuracy: 0.9426 - recall: 0.9429 - f1: 0.9427 - val_loss: 0.2826 - val_accuracy: 0.8822 - val_recall: 0.8824 - val_f1: 0.8822\n",
      "Epoch 40/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1530 - accuracy: 0.9391 - recall: 0.9390 - f1: 0.9391\n",
      "Epoch 00040: val_loss did not improve from 0.24671\n",
      "150/150 [==============================] - 79s 529ms/step - loss: 0.1535 - accuracy: 0.9390 - recall: 0.9389 - f1: 0.9390 - val_loss: 0.4809 - val_accuracy: 0.8468 - val_recall: 0.8470 - val_f1: 0.8469\n",
      "Epoch 41/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9387 - recall: 0.9388 - f1: 0.9387\n",
      "Epoch 00041: val_loss did not improve from 0.24671\n",
      "150/150 [==============================] - 73s 490ms/step - loss: 0.1491 - accuracy: 0.9387 - recall: 0.9388 - f1: 0.9387 - val_loss: 0.2488 - val_accuracy: 0.8922 - val_recall: 0.8925 - val_f1: 0.8923\n",
      "Epoch 42/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1532 - accuracy: 0.9348 - recall: 0.9347 - f1: 0.9348\n",
      "Epoch 00042: val_loss improved from 0.24671 to 0.21734, saving model to batch_LeakyReLU_validation_160_dropout-0.25-042-0.934667-0.906233.h5\n",
      "150/150 [==============================] - 71s 476ms/step - loss: 0.1532 - accuracy: 0.9347 - recall: 0.9346 - f1: 0.9347 - val_loss: 0.2173 - val_accuracy: 0.9062 - val_recall: 0.9060 - val_f1: 0.9062\n",
      "Epoch 43/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1465 - accuracy: 0.9384 - recall: 0.9383 - f1: 0.9384\n",
      "Epoch 00043: val_loss did not improve from 0.21734\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "150/150 [==============================] - 75s 498ms/step - loss: 0.1465 - accuracy: 0.9383 - recall: 0.9382 - f1: 0.9383 - val_loss: 0.2562 - val_accuracy: 0.8901 - val_recall: 0.8900 - val_f1: 0.8901\n",
      "Epoch 44/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1483 - accuracy: 0.9434 - recall: 0.9435 - f1: 0.9434\n",
      "Epoch 00044: val_loss improved from 0.21734 to 0.20064, saving model to batch_LeakyReLU_validation_160_dropout-0.25-044-0.943500-0.913967.h5\n",
      "150/150 [==============================] - 84s 559ms/step - loss: 0.1480 - accuracy: 0.9435 - recall: 0.9436 - f1: 0.9435 - val_loss: 0.2006 - val_accuracy: 0.9140 - val_recall: 0.9141 - val_f1: 0.9140\n",
      "Epoch 45/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1364 - accuracy: 0.9461 - recall: 0.9463 - f1: 0.9462\n",
      "Epoch 00045: val_loss improved from 0.20064 to 0.18885, saving model to batch_LeakyReLU_validation_160_dropout-0.25-045-0.946233-0.916167.h5\n",
      "150/150 [==============================] - 78s 522ms/step - loss: 0.1361 - accuracy: 0.9462 - recall: 0.9464 - f1: 0.9462 - val_loss: 0.1889 - val_accuracy: 0.9162 - val_recall: 0.9162 - val_f1: 0.9162\n",
      "Epoch 46/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1327 - accuracy: 0.9478 - recall: 0.9478 - f1: 0.9478\n",
      "Epoch 00046: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 79s 528ms/step - loss: 0.1326 - accuracy: 0.9480 - recall: 0.9479 - f1: 0.9480 - val_loss: 0.1969 - val_accuracy: 0.9193 - val_recall: 0.9192 - val_f1: 0.9193\n",
      "Epoch 47/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.9452 - recall: 0.9450 - f1: 0.9452\n",
      "Epoch 00047: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 80s 536ms/step - loss: 0.1364 - accuracy: 0.9453 - recall: 0.9451 - f1: 0.9453 - val_loss: 0.3870 - val_accuracy: 0.8570 - val_recall: 0.8570 - val_f1: 0.8570\n",
      "Epoch 48/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1360 - accuracy: 0.9454 - recall: 0.9455 - f1: 0.9454\n",
      "Epoch 00048: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 69s 460ms/step - loss: 0.1357 - accuracy: 0.9455 - recall: 0.9456 - f1: 0.9455 - val_loss: 0.2036 - val_accuracy: 0.9154 - val_recall: 0.9153 - val_f1: 0.9154\n",
      "Epoch 49/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1325 - accuracy: 0.9460 - recall: 0.9460 - f1: 0.9460\n",
      "Epoch 00049: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 71s 472ms/step - loss: 0.1328 - accuracy: 0.9459 - recall: 0.9459 - f1: 0.9459 - val_loss: 0.2504 - val_accuracy: 0.8917 - val_recall: 0.8916 - val_f1: 0.8917\n",
      "Epoch 50/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1392 - accuracy: 0.9416 - recall: 0.9417 - f1: 0.9416\n",
      "Epoch 00050: val_loss did not improve from 0.18885\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "150/150 [==============================] - 82s 545ms/step - loss: 0.1392 - accuracy: 0.9418 - recall: 0.9418 - f1: 0.9418 - val_loss: 0.2183 - val_accuracy: 0.9081 - val_recall: 0.9082 - val_f1: 0.9081\n",
      "Epoch 51/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1368 - accuracy: 0.9460 - recall: 0.9462 - f1: 0.9460\n",
      "Epoch 00051: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 80s 530ms/step - loss: 0.1373 - accuracy: 0.9459 - recall: 0.9461 - f1: 0.9459 - val_loss: 0.2017 - val_accuracy: 0.9138 - val_recall: 0.9135 - val_f1: 0.9138\n",
      "Epoch 52/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1296 - accuracy: 0.9469 - recall: 0.9468 - f1: 0.9469\n",
      "Epoch 00052: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 77s 510ms/step - loss: 0.1293 - accuracy: 0.9470 - recall: 0.9469 - f1: 0.9470 - val_loss: 0.1925 - val_accuracy: 0.9194 - val_recall: 0.9193 - val_f1: 0.9194\n",
      "Epoch 53/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1294 - accuracy: 0.9482 - recall: 0.9481 - f1: 0.9482\n",
      "Epoch 00053: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 85s 566ms/step - loss: 0.1290 - accuracy: 0.9484 - recall: 0.9483 - f1: 0.9484 - val_loss: 0.2066 - val_accuracy: 0.9141 - val_recall: 0.9140 - val_f1: 0.9141\n",
      "Epoch 54/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1325 - accuracy: 0.9480 - recall: 0.9479 - f1: 0.9480\n",
      "Epoch 00054: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 74s 493ms/step - loss: 0.1323 - accuracy: 0.9480 - recall: 0.9479 - f1: 0.9480 - val_loss: 0.1966 - val_accuracy: 0.9188 - val_recall: 0.9187 - val_f1: 0.9188\n",
      "Epoch 55/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9466 - recall: 0.9465 - f1: 0.9466\n",
      "Epoch 00055: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 73s 485ms/step - loss: 0.1261 - accuracy: 0.9468 - recall: 0.9467 - f1: 0.9468 - val_loss: 0.2087 - val_accuracy: 0.9116 - val_recall: 0.9113 - val_f1: 0.9116\n",
      "Epoch 56/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9464 - recall: 0.9460 - f1: 0.9464\n",
      "Epoch 00056: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 80s 534ms/step - loss: 0.1304 - accuracy: 0.9463 - recall: 0.9459 - f1: 0.9463 - val_loss: 0.2095 - val_accuracy: 0.9089 - val_recall: 0.9089 - val_f1: 0.9089\n",
      "Epoch 57/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1350 - accuracy: 0.9459 - recall: 0.9456 - f1: 0.9459\n",
      "Epoch 00057: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 76s 505ms/step - loss: 0.1350 - accuracy: 0.9458 - recall: 0.9455 - f1: 0.9458 - val_loss: 0.2200 - val_accuracy: 0.9053 - val_recall: 0.9050 - val_f1: 0.9052\n",
      "Epoch 58/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9474 - recall: 0.9474 - f1: 0.9474\n",
      "Epoch 00058: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 72s 480ms/step - loss: 0.1304 - accuracy: 0.9474 - recall: 0.9474 - f1: 0.9474 - val_loss: 0.2203 - val_accuracy: 0.9085 - val_recall: 0.9085 - val_f1: 0.9085\n",
      "Epoch 59/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9465 - recall: 0.9467 - f1: 0.9465\n",
      "Epoch 00059: val_loss did not improve from 0.18885\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "150/150 [==============================] - 74s 492ms/step - loss: 0.1268 - accuracy: 0.9463 - recall: 0.9465 - f1: 0.9463 - val_loss: 0.1960 - val_accuracy: 0.9171 - val_recall: 0.9173 - val_f1: 0.9171\n",
      "Epoch 60/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9509 - recall: 0.9509 - f1: 0.9509\n",
      "Epoch 00060: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 78s 519ms/step - loss: 0.1232 - accuracy: 0.9510 - recall: 0.9510 - f1: 0.9510 - val_loss: 0.1898 - val_accuracy: 0.9224 - val_recall: 0.9223 - val_f1: 0.9224\n",
      "Epoch 61/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9501 - recall: 0.9500 - f1: 0.9501\n",
      "Epoch 00061: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 86s 573ms/step - loss: 0.1242 - accuracy: 0.9500 - recall: 0.9499 - f1: 0.9500 - val_loss: 0.1955 - val_accuracy: 0.9207 - val_recall: 0.9207 - val_f1: 0.9207\n",
      "Epoch 62/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9483 - recall: 0.9484 - f1: 0.9483\n",
      "Epoch 00062: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 74s 496ms/step - loss: 0.1273 - accuracy: 0.9485 - recall: 0.9486 - f1: 0.9485 - val_loss: 0.1927 - val_accuracy: 0.9195 - val_recall: 0.9196 - val_f1: 0.9195\n",
      "Epoch 63/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9478 - recall: 0.9477 - f1: 0.9478\n",
      "Epoch 00063: val_loss did not improve from 0.18885\n",
      "150/150 [==============================] - 78s 521ms/step - loss: 0.1267 - accuracy: 0.9479 - recall: 0.9478 - f1: 0.9479 - val_loss: 0.2019 - val_accuracy: 0.9148 - val_recall: 0.9151 - val_f1: 0.9149\n",
      "Epoch 64/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9472 - recall: 0.9473 - f1: 0.9473\n",
      "Epoch 00064: val_loss did not improve from 0.18885\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "150/150 [==============================] - 76s 505ms/step - loss: 0.1280 - accuracy: 0.9474 - recall: 0.9475 - f1: 0.9474 - val_loss: 0.1978 - val_accuracy: 0.9174 - val_recall: 0.9173 - val_f1: 0.9174\n",
      "Epoch 65/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1175 - accuracy: 0.9527 - recall: 0.9526 - f1: 0.9527"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "# We define the number of epochs and steps per epochs\n",
    "EPOCHS = 160\n",
    "STEPS_PER_EPOCH = 150\n",
    "batch_normalization = True\n",
    "activation = \"LeakyReLU\"\n",
    "model = cnn_model(\n",
    "    shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "# Train the model\n",
    "history = model.train()\n",
    "# model.save(\"no_batch_LeakyRelu_validation_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "plt.savefig(\"batch_LeakyReLU_validation_160_dropout-0.25.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "\n",
    "# from cnn_model import cnn_model\n",
    "\n",
    "# Instantiate the model\n",
    "batch_normalization = True\n",
    "activation = \"LeakyReLU\"\n",
    "model = cnn_model(\n",
    "    shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model.load(\"batch_relu_validation_200-074-0.946167-0.927667.h5\")\n",
    "\n",
    "# Print a summary to make sure the correct model is used\n",
    "model.model.summary()\n",
    "\n",
    "# We add all test images to an array, used later for generating a submission\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "    image_filenames.append(image_filename)\n",
    "\n",
    "# Set-up submission filename\n",
    "submission_filename = \"batch_relu_validation_200-074-0.946167-0.927667.csv\"\n",
    "\n",
    "# Generates the submission\n",
    "generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # history.history[\"loss\"]\n",
    "# print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history.history[\"accuracy\"])\n",
    "# plt.plot(history.history[\"val_accuracy\"])\n",
    "# plt.title(\"model accuracy\")\n",
    "# plt.ylabel(\"accuracy\")\n",
    "# plt.xlabel(\"epoch\")\n",
    "# plt.legend([\"train\", \"val\"], loc=\"upper left\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the training loss and accuracy\n",
    "# N = EPOCHS\n",
    "# print(N)\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure()\n",
    "# plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "# plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "# plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Loss/Accuracy\")\n",
    "# plt.legend(loc=\"lower left\")\n",
    "# plt.show()\n",
    "# plt.savefig(\"model1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "# batch_normalization = False\n",
    "# EPOCHS = 200\n",
    "\n",
    "# activation = \"relu\"\n",
    "# model = cnn_model(\n",
    "#     shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    "# )\n",
    "# # Train the model with batch\n",
    "# model.train()\n",
    "# model.save(\"no_batch_relu_validation.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "# batch_normalization = True\n",
    "# EPOCHS = 200\n",
    "\n",
    "# activation = \"LeakyReLU\"\n",
    "# model = cnn_model(\n",
    "#     shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    "# )\n",
    "# # Train the model with batch\n",
    "# model.train()\n",
    "# model.save(\"batch_LeakyReLU_validation.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,962\n",
      "Trainable params: 2,278,274\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "Executing op BiasAdd in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalAnd in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op FusedBatchNormV3 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignSubVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Relu in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MaxPool in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Shape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op GreaterEqual in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Square in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RealDiv in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mean in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op StopGradient in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op SquaredDifference in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Squeeze in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Rsqrt in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sigmoid in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Minimum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Maximum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Log in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Neg in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mean in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DivNoNan in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op BroadcastGradientArgs in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Tile in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Maximum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op FloorDiv in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reciprocal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ZerosLike in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Select in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LessEqual in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op SigmoidGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op BiasAddGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReluGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RsqrtGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MaxPoolGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op FusedBatchNormGradV3 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ShapeN in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2DBackpropInput in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Conv2DBackpropFilter in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Pow in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sqrt in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RealDiv in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ResourceApplyAdam in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignAddVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Greater in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Equal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignAddVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Round in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.5348 - accuracy: 0.7299 - recall: 0.7180 - f1: 0.7254Executing op FusedBatchNormV3 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.76977, saving model to batch_relu_validation_200-001-0.729967-0.299667.h5\n",
      "150/150 [==============================] - 69s 463ms/step - loss: 0.5342 - accuracy: 0.7300 - recall: 0.7183 - f1: 0.7256 - val_loss: 0.7698 - val_accuracy: 0.2997 - val_recall: 0.2601 - val_f1: 0.2708\n",
      "Epoch 2/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.4046 - accuracy: 0.8150 - recall: 0.8141 - f1: 0.8148\n",
      "Epoch 00002: val_loss did not improve from 0.76977\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.4042 - accuracy: 0.8152 - recall: 0.8142 - f1: 0.8149 - val_loss: 1.4418 - val_accuracy: 0.3293 - val_recall: 0.3305 - val_f1: 0.3301\n",
      "Epoch 3/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3579 - accuracy: 0.8358 - recall: 0.8350 - f1: 0.8357\n",
      "Epoch 00003: val_loss improved from 0.76977 to 0.60472, saving model to batch_relu_validation_200-003-0.836100-0.722567.h5\n",
      "150/150 [==============================] - 68s 450ms/step - loss: 0.3575 - accuracy: 0.8361 - recall: 0.8353 - f1: 0.8359 - val_loss: 0.6047 - val_accuracy: 0.7226 - val_recall: 0.7277 - val_f1: 0.7240\n",
      "Epoch 4/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3371 - accuracy: 0.8480 - recall: 0.8479 - f1: 0.8479\n",
      "Epoch 00004: val_loss did not improve from 0.60472\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.3374 - accuracy: 0.8476 - recall: 0.8475 - f1: 0.8476 - val_loss: 2.5459 - val_accuracy: 0.3527 - val_recall: 0.3521 - val_f1: 0.3523\n",
      "Epoch 5/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3004 - accuracy: 0.8666 - recall: 0.8666 - f1: 0.8666\n",
      "Epoch 00005: val_loss did not improve from 0.60472\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.3001 - accuracy: 0.8668 - recall: 0.8669 - f1: 0.8668 - val_loss: 0.6048 - val_accuracy: 0.7483 - val_recall: 0.7490 - val_f1: 0.7485\n",
      "Epoch 6/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.8753 - recall: 0.8746 - f1: 0.8752\n",
      "Epoch 00006: val_loss improved from 0.60472 to 0.52884, saving model to batch_relu_validation_200-006-0.875400-0.801233.h5\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.2911 - accuracy: 0.8754 - recall: 0.8748 - f1: 0.8753 - val_loss: 0.5288 - val_accuracy: 0.8012 - val_recall: 0.8015 - val_f1: 0.8013\n",
      "Epoch 7/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2592 - accuracy: 0.8859 - recall: 0.8860 - f1: 0.8859\n",
      "Epoch 00007: val_loss improved from 0.52884 to 0.36932, saving model to batch_relu_validation_200-007-0.886033-0.818900.h5\n",
      "150/150 [==============================] - 67s 445ms/step - loss: 0.2589 - accuracy: 0.8860 - recall: 0.8862 - f1: 0.8861 - val_loss: 0.3693 - val_accuracy: 0.8189 - val_recall: 0.8157 - val_f1: 0.8183\n",
      "Epoch 8/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2446 - accuracy: 0.8984 - recall: 0.8973 - f1: 0.8983\n",
      "Epoch 00008: val_loss did not improve from 0.36932\n",
      "150/150 [==============================] - 68s 451ms/step - loss: 0.2442 - accuracy: 0.8986 - recall: 0.8975 - f1: 0.8985 - val_loss: 0.4621 - val_accuracy: 0.7692 - val_recall: 0.7690 - val_f1: 0.7691\n",
      "Epoch 9/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2412 - accuracy: 0.9003 - recall: 0.8985 - f1: 0.9001\n",
      "Epoch 00009: val_loss improved from 0.36932 to 0.36775, saving model to batch_relu_validation_200-009-0.900133-0.816667.h5\n",
      "150/150 [==============================] - 67s 450ms/step - loss: 0.2413 - accuracy: 0.9001 - recall: 0.8983 - f1: 0.8999 - val_loss: 0.3678 - val_accuracy: 0.8167 - val_recall: 0.8115 - val_f1: 0.8157\n",
      "Epoch 10/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2381 - accuracy: 0.9005 - recall: 0.9007 - f1: 0.9005\n",
      "Epoch 00010: val_loss did not improve from 0.36775\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.2380 - accuracy: 0.9005 - recall: 0.9008 - f1: 0.9006 - val_loss: 0.3807 - val_accuracy: 0.8101 - val_recall: 0.8083 - val_f1: 0.8097\n",
      "Epoch 11/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2279 - accuracy: 0.9068 - recall: 0.9077 - f1: 0.9069\n",
      "Epoch 00011: val_loss did not improve from 0.36775\n",
      "150/150 [==============================] - 68s 454ms/step - loss: 0.2276 - accuracy: 0.9071 - recall: 0.9079 - f1: 0.9072 - val_loss: 0.4172 - val_accuracy: 0.7904 - val_recall: 0.7915 - val_f1: 0.7906\n",
      "Epoch 12/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2258 - accuracy: 0.9060 - recall: 0.9060 - f1: 0.9060\n",
      "Epoch 00012: val_loss did not improve from 0.36775\n",
      "150/150 [==============================] - 70s 468ms/step - loss: 0.2254 - accuracy: 0.9061 - recall: 0.9061 - f1: 0.9061 - val_loss: 0.5616 - val_accuracy: 0.7799 - val_recall: 0.7797 - val_f1: 0.7798\n",
      "Epoch 13/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2214 - accuracy: 0.9072 - recall: 0.9067 - f1: 0.9072\n",
      "Epoch 00013: val_loss did not improve from 0.36775\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.2212 - accuracy: 0.9073 - recall: 0.9068 - f1: 0.9073 - val_loss: 0.5648 - val_accuracy: 0.7388 - val_recall: 0.7327 - val_f1: 0.7372\n",
      "Epoch 14/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2235 - accuracy: 0.9080 - recall: 0.9068 - f1: 0.9078\n",
      "Epoch 00014: val_loss did not improve from 0.36775\n",
      "150/150 [==============================] - 69s 462ms/step - loss: 0.2237 - accuracy: 0.9078 - recall: 0.9068 - f1: 0.9077 - val_loss: 0.5860 - val_accuracy: 0.8134 - val_recall: 0.8133 - val_f1: 0.8133\n",
      "Epoch 15/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2093 - accuracy: 0.9114 - recall: 0.9115 - f1: 0.9114\n",
      "Epoch 00015: val_loss did not improve from 0.36775\n",
      "150/150 [==============================] - 68s 452ms/step - loss: 0.2090 - accuracy: 0.9115 - recall: 0.9116 - f1: 0.9115 - val_loss: 0.4771 - val_accuracy: 0.8019 - val_recall: 0.8017 - val_f1: 0.8018\n",
      "Epoch 16/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2046 - accuracy: 0.9185 - recall: 0.9182 - f1: 0.9184\n",
      "Epoch 00016: val_loss did not improve from 0.36775\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.2041 - accuracy: 0.9186 - recall: 0.9183 - f1: 0.9186 - val_loss: 0.6455 - val_accuracy: 0.8043 - val_recall: 0.8048 - val_f1: 0.8044\n",
      "Epoch 17/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2025 - accuracy: 0.9173 - recall: 0.9169 - f1: 0.9173\n",
      "Epoch 00017: val_loss improved from 0.36775 to 0.24256, saving model to batch_relu_validation_200-017-0.917400-0.897000.h5\n",
      "150/150 [==============================] - 68s 451ms/step - loss: 0.2025 - accuracy: 0.9174 - recall: 0.9171 - f1: 0.9174 - val_loss: 0.2426 - val_accuracy: 0.8970 - val_recall: 0.8962 - val_f1: 0.8969\n",
      "Epoch 18/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2041 - accuracy: 0.9160 - recall: 0.9160 - f1: 0.9160\n",
      "Epoch 00018: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 70s 466ms/step - loss: 0.2042 - accuracy: 0.9158 - recall: 0.9157 - f1: 0.9158 - val_loss: 0.5773 - val_accuracy: 0.8211 - val_recall: 0.8211 - val_f1: 0.8211\n",
      "Epoch 19/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1970 - accuracy: 0.9216 - recall: 0.9212 - f1: 0.9215\n",
      "Epoch 00019: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 69s 458ms/step - loss: 0.1969 - accuracy: 0.9217 - recall: 0.9213 - f1: 0.9216 - val_loss: 0.3018 - val_accuracy: 0.8634 - val_recall: 0.8643 - val_f1: 0.8635\n",
      "Epoch 20/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1964 - accuracy: 0.9198 - recall: 0.9198 - f1: 0.9198\n",
      "Epoch 00020: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1963 - accuracy: 0.9198 - recall: 0.9199 - f1: 0.9198 - val_loss: 0.3267 - val_accuracy: 0.8405 - val_recall: 0.8393 - val_f1: 0.8403\n",
      "Epoch 21/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1914 - accuracy: 0.9219 - recall: 0.9218 - f1: 0.9219\n",
      "Epoch 00021: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 67s 445ms/step - loss: 0.1915 - accuracy: 0.9220 - recall: 0.9219 - f1: 0.9220 - val_loss: 1.0318 - val_accuracy: 0.7651 - val_recall: 0.7646 - val_f1: 0.7650\n",
      "Epoch 22/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1948 - accuracy: 0.9173 - recall: 0.9177 - f1: 0.9174\n",
      "Epoch 00022: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.1948 - accuracy: 0.9173 - recall: 0.9177 - f1: 0.9174 - val_loss: 0.5788 - val_accuracy: 0.7910 - val_recall: 0.7910 - val_f1: 0.7910\n",
      "Epoch 23/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1939 - accuracy: 0.9183 - recall: 0.9182 - f1: 0.9183\n",
      "Epoch 00023: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.1939 - accuracy: 0.9182 - recall: 0.9181 - f1: 0.9182 - val_loss: 0.3419 - val_accuracy: 0.8397 - val_recall: 0.8395 - val_f1: 0.8396\n",
      "Epoch 24/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1921 - accuracy: 0.9218 - recall: 0.9221 - f1: 0.9219\n",
      "Epoch 00024: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1925 - accuracy: 0.9217 - recall: 0.9219 - f1: 0.9217 - val_loss: 0.3078 - val_accuracy: 0.8572 - val_recall: 0.8568 - val_f1: 0.8571\n",
      "Epoch 25/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1872 - accuracy: 0.9224 - recall: 0.9223 - f1: 0.9224\n",
      "Epoch 00025: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1872 - accuracy: 0.9224 - recall: 0.9223 - f1: 0.9224 - val_loss: 0.3463 - val_accuracy: 0.8407 - val_recall: 0.8415 - val_f1: 0.8409\n",
      "Epoch 26/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1849 - accuracy: 0.9247 - recall: 0.9245 - f1: 0.9247\n",
      "Epoch 00026: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1852 - accuracy: 0.9246 - recall: 0.9243 - f1: 0.9245 - val_loss: 0.2951 - val_accuracy: 0.8612 - val_recall: 0.8609 - val_f1: 0.8611\n",
      "Epoch 27/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1814 - accuracy: 0.9243 - recall: 0.9246 - f1: 0.9244\n",
      "Epoch 00027: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1811 - accuracy: 0.9243 - recall: 0.9246 - f1: 0.9243 - val_loss: 0.3892 - val_accuracy: 0.8394 - val_recall: 0.8399 - val_f1: 0.8395\n",
      "Epoch 28/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1758 - accuracy: 0.9283 - recall: 0.9281 - f1: 0.9283\n",
      "Epoch 00028: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 67s 448ms/step - loss: 0.1755 - accuracy: 0.9283 - recall: 0.9280 - f1: 0.9282 - val_loss: 0.2735 - val_accuracy: 0.8776 - val_recall: 0.8780 - val_f1: 0.8776\n",
      "Epoch 29/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1765 - accuracy: 0.9298 - recall: 0.9298 - f1: 0.9298\n",
      "Epoch 00029: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 67s 450ms/step - loss: 0.1771 - accuracy: 0.9294 - recall: 0.9293 - f1: 0.9294 - val_loss: 0.2866 - val_accuracy: 0.8688 - val_recall: 0.8696 - val_f1: 0.8689\n",
      "Epoch 30/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1800 - accuracy: 0.9252 - recall: 0.9251 - f1: 0.9252\n",
      "Epoch 00030: val_loss did not improve from 0.24256\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1798 - accuracy: 0.9252 - recall: 0.9251 - f1: 0.9252 - val_loss: 0.6177 - val_accuracy: 0.8203 - val_recall: 0.8202 - val_f1: 0.8203\n",
      "Epoch 31/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1778 - accuracy: 0.9281 - recall: 0.9279 - f1: 0.9280\n",
      "Epoch 00031: val_loss improved from 0.24256 to 0.24005, saving model to batch_relu_validation_200-031-0.928000-0.894400.h5\n",
      "150/150 [==============================] - 67s 445ms/step - loss: 0.1779 - accuracy: 0.9280 - recall: 0.9279 - f1: 0.9280 - val_loss: 0.2401 - val_accuracy: 0.8944 - val_recall: 0.8942 - val_f1: 0.8944\n",
      "Epoch 32/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1699 - accuracy: 0.9332 - recall: 0.9330 - f1: 0.9331\n",
      "Epoch 00032: val_loss did not improve from 0.24005\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1700 - accuracy: 0.9333 - recall: 0.9331 - f1: 0.9332 - val_loss: 0.2408 - val_accuracy: 0.8957 - val_recall: 0.8957 - val_f1: 0.8957\n",
      "Epoch 33/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1630 - accuracy: 0.9326 - recall: 0.9326 - f1: 0.9326\n",
      "Epoch 00033: val_loss did not improve from 0.24005\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1634 - accuracy: 0.9327 - recall: 0.9327 - f1: 0.9327 - val_loss: 0.6738 - val_accuracy: 0.8099 - val_recall: 0.8099 - val_f1: 0.8099\n",
      "Epoch 34/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1732 - accuracy: 0.9306 - recall: 0.9305 - f1: 0.9306\n",
      "Epoch 00034: val_loss did not improve from 0.24005\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1730 - accuracy: 0.9307 - recall: 0.9306 - f1: 0.9307 - val_loss: 0.3861 - val_accuracy: 0.8195 - val_recall: 0.8203 - val_f1: 0.8196\n",
      "Epoch 35/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1702 - accuracy: 0.9308 - recall: 0.9311 - f1: 0.9308\n",
      "Epoch 00035: val_loss did not improve from 0.24005\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1700 - accuracy: 0.9309 - recall: 0.9312 - f1: 0.9309 - val_loss: 0.2610 - val_accuracy: 0.8821 - val_recall: 0.8821 - val_f1: 0.8821\n",
      "Epoch 36/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1692 - accuracy: 0.9282 - recall: 0.9281 - f1: 0.9282\n",
      "Epoch 00036: val_loss improved from 0.24005 to 0.23758, saving model to batch_relu_validation_200-036-0.928267-0.895667.h5\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1690 - accuracy: 0.9283 - recall: 0.9282 - f1: 0.9283 - val_loss: 0.2376 - val_accuracy: 0.8957 - val_recall: 0.8954 - val_f1: 0.8956\n",
      "Epoch 37/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1700 - accuracy: 0.9293 - recall: 0.9296 - f1: 0.9294\n",
      "Epoch 00037: val_loss did not improve from 0.23758\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1701 - accuracy: 0.9294 - recall: 0.9297 - f1: 0.9294 - val_loss: 0.4739 - val_accuracy: 0.8300 - val_recall: 0.8297 - val_f1: 0.8300\n",
      "Epoch 38/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1536 - accuracy: 0.9376 - recall: 0.9377 - f1: 0.9376\n",
      "Epoch 00038: val_loss did not improve from 0.23758\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1533 - accuracy: 0.9379 - recall: 0.9379 - f1: 0.9379 - val_loss: 0.2818 - val_accuracy: 0.8797 - val_recall: 0.8799 - val_f1: 0.8797\n",
      "Epoch 39/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1501 - accuracy: 0.9399 - recall: 0.9400 - f1: 0.9399\n",
      "Epoch 00039: val_loss improved from 0.23758 to 0.23655, saving model to batch_relu_validation_200-039-0.939800-0.898233.h5\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1504 - accuracy: 0.9398 - recall: 0.9399 - f1: 0.9398 - val_loss: 0.2365 - val_accuracy: 0.8982 - val_recall: 0.8982 - val_f1: 0.8982\n",
      "Epoch 40/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1555 - accuracy: 0.9372 - recall: 0.9374 - f1: 0.9372\n",
      "Epoch 00040: val_loss did not improve from 0.23655\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1558 - accuracy: 0.9372 - recall: 0.9373 - f1: 0.9372 - val_loss: 0.3707 - val_accuracy: 0.8761 - val_recall: 0.8761 - val_f1: 0.8761\n",
      "Epoch 41/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1469 - accuracy: 0.9384 - recall: 0.9386 - f1: 0.9384\n",
      "Epoch 00041: val_loss improved from 0.23655 to 0.23002, saving model to batch_relu_validation_200-041-0.938367-0.903433.h5\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1467 - accuracy: 0.9384 - recall: 0.9386 - f1: 0.9384 - val_loss: 0.2300 - val_accuracy: 0.9034 - val_recall: 0.9034 - val_f1: 0.9034\n",
      "Epoch 42/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1489 - accuracy: 0.9402 - recall: 0.9399 - f1: 0.9402\n",
      "Epoch 00042: val_loss did not improve from 0.23002\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1489 - accuracy: 0.9400 - recall: 0.9397 - f1: 0.9400 - val_loss: 0.2344 - val_accuracy: 0.9055 - val_recall: 0.9055 - val_f1: 0.9055\n",
      "Epoch 43/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1477 - accuracy: 0.9405 - recall: 0.9407 - f1: 0.9405\n",
      "Epoch 00043: val_loss did not improve from 0.23002\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1478 - accuracy: 0.9404 - recall: 0.9406 - f1: 0.9404 - val_loss: 0.2849 - val_accuracy: 0.8847 - val_recall: 0.8847 - val_f1: 0.8847\n",
      "Epoch 44/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1513 - accuracy: 0.9395 - recall: 0.9396 - f1: 0.9395\n",
      "Epoch 00044: val_loss did not improve from 0.23002\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1508 - accuracy: 0.9397 - recall: 0.9398 - f1: 0.9397 - val_loss: 0.2475 - val_accuracy: 0.8939 - val_recall: 0.8940 - val_f1: 0.8939\n",
      "Epoch 45/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1379 - accuracy: 0.9463 - recall: 0.9464 - f1: 0.9463\n",
      "Epoch 00045: val_loss did not improve from 0.23002\n",
      "150/150 [==============================] - 67s 448ms/step - loss: 0.1377 - accuracy: 0.9463 - recall: 0.9465 - f1: 0.9463 - val_loss: 0.2551 - val_accuracy: 0.9064 - val_recall: 0.9063 - val_f1: 0.9064\n",
      "Epoch 46/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1411 - accuracy: 0.9428 - recall: 0.9426 - f1: 0.9427\n",
      "Epoch 00046: val_loss did not improve from 0.23002\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1410 - accuracy: 0.9427 - recall: 0.9425 - f1: 0.9427 - val_loss: 0.2369 - val_accuracy: 0.9015 - val_recall: 0.9016 - val_f1: 0.9015\n",
      "Epoch 47/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1439 - accuracy: 0.9424 - recall: 0.9424 - f1: 0.9424\n",
      "Epoch 00047: val_loss did not improve from 0.23002\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1437 - accuracy: 0.9425 - recall: 0.9425 - f1: 0.9425 - val_loss: 0.2907 - val_accuracy: 0.8718 - val_recall: 0.8717 - val_f1: 0.8718\n",
      "Epoch 48/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1444 - accuracy: 0.9426 - recall: 0.9423 - f1: 0.9426\n",
      "Epoch 00048: val_loss did not improve from 0.23002\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1444 - accuracy: 0.9427 - recall: 0.9424 - f1: 0.9427 - val_loss: 0.5852 - val_accuracy: 0.8341 - val_recall: 0.8342 - val_f1: 0.8341\n",
      "Epoch 49/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1380 - accuracy: 0.9440 - recall: 0.9442 - f1: 0.9440\n",
      "Epoch 00049: val_loss did not improve from 0.23002\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1379 - accuracy: 0.9440 - recall: 0.9442 - f1: 0.9440 - val_loss: 0.2356 - val_accuracy: 0.8998 - val_recall: 0.8996 - val_f1: 0.8998\n",
      "Epoch 50/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1381 - accuracy: 0.9448 - recall: 0.9446 - f1: 0.9448\n",
      "Epoch 00050: val_loss did not improve from 0.23002\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1380 - accuracy: 0.9449 - recall: 0.9447 - f1: 0.9449 - val_loss: 0.2405 - val_accuracy: 0.9005 - val_recall: 0.9003 - val_f1: 0.9004\n",
      "Epoch 51/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1411 - accuracy: 0.9451 - recall: 0.9451 - f1: 0.9451\n",
      "Epoch 00051: val_loss improved from 0.23002 to 0.19262, saving model to batch_relu_validation_200-051-0.944800-0.921800.h5\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1416 - accuracy: 0.9448 - recall: 0.9448 - f1: 0.9448 - val_loss: 0.1926 - val_accuracy: 0.9218 - val_recall: 0.9216 - val_f1: 0.9218\n",
      "Epoch 52/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1358 - accuracy: 0.9446 - recall: 0.9448 - f1: 0.9446\n",
      "Epoch 00052: val_loss did not improve from 0.19262\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1356 - accuracy: 0.9447 - recall: 0.9449 - f1: 0.9447 - val_loss: 0.2057 - val_accuracy: 0.9138 - val_recall: 0.9137 - val_f1: 0.9138\n",
      "Epoch 53/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1294 - accuracy: 0.9468 - recall: 0.9469 - f1: 0.9468\n",
      "Epoch 00053: val_loss did not improve from 0.19262\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1289 - accuracy: 0.9471 - recall: 0.9473 - f1: 0.9471 - val_loss: 0.1975 - val_accuracy: 0.9183 - val_recall: 0.9181 - val_f1: 0.9183\n",
      "Epoch 54/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9443 - recall: 0.9447 - f1: 0.9444\n",
      "Epoch 00054: val_loss did not improve from 0.19262\n",
      "150/150 [==============================] - 65s 437ms/step - loss: 0.1340 - accuracy: 0.9445 - recall: 0.9449 - f1: 0.9445 - val_loss: 0.2130 - val_accuracy: 0.9120 - val_recall: 0.9121 - val_f1: 0.9120\n",
      "Epoch 55/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9459 - recall: 0.9463 - f1: 0.9459\n",
      "Epoch 00055: val_loss did not improve from 0.19262\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1305 - accuracy: 0.9459 - recall: 0.9463 - f1: 0.9460 - val_loss: 0.2132 - val_accuracy: 0.9110 - val_recall: 0.9111 - val_f1: 0.9110\n",
      "Epoch 56/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1343 - accuracy: 0.9448 - recall: 0.9448 - f1: 0.9448\n",
      "Epoch 00056: val_loss did not improve from 0.19262\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1345 - accuracy: 0.9447 - recall: 0.9447 - f1: 0.9447 - val_loss: 0.2233 - val_accuracy: 0.9005 - val_recall: 0.9006 - val_f1: 0.9005\n",
      "Epoch 57/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1378 - accuracy: 0.9456 - recall: 0.9457 - f1: 0.9456\n",
      "Epoch 00057: val_loss did not improve from 0.19262\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1378 - accuracy: 0.9457 - recall: 0.9458 - f1: 0.9457 - val_loss: 0.2106 - val_accuracy: 0.9125 - val_recall: 0.9124 - val_f1: 0.9125\n",
      "Epoch 58/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1306 - accuracy: 0.9455 - recall: 0.9457 - f1: 0.9455\n",
      "Epoch 00058: val_loss did not improve from 0.19262\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1310 - accuracy: 0.9454 - recall: 0.9455 - f1: 0.9454 - val_loss: 0.2069 - val_accuracy: 0.9145 - val_recall: 0.9143 - val_f1: 0.9145\n",
      "Epoch 59/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.9469 - recall: 0.9471 - f1: 0.9469\n",
      "Epoch 00059: val_loss improved from 0.19262 to 0.18740, saving model to batch_relu_validation_200-059-0.946800-0.924667.h5\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1313 - accuracy: 0.9468 - recall: 0.9470 - f1: 0.9468 - val_loss: 0.1874 - val_accuracy: 0.9247 - val_recall: 0.9246 - val_f1: 0.9247\n",
      "Epoch 60/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9503 - recall: 0.9504 - f1: 0.9503\n",
      "Epoch 00060: val_loss did not improve from 0.18740\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1250 - accuracy: 0.9503 - recall: 0.9505 - f1: 0.9503 - val_loss: 0.2038 - val_accuracy: 0.9121 - val_recall: 0.9120 - val_f1: 0.9121\n",
      "Epoch 61/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9517 - recall: 0.9516 - f1: 0.9517\n",
      "Epoch 00061: val_loss did not improve from 0.18740\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1199 - accuracy: 0.9517 - recall: 0.9516 - f1: 0.9517 - val_loss: 0.1888 - val_accuracy: 0.9252 - val_recall: 0.9253 - val_f1: 0.9252\n",
      "Epoch 62/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1284 - accuracy: 0.9491 - recall: 0.9491 - f1: 0.9491\n",
      "Epoch 00062: val_loss did not improve from 0.18740\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1281 - accuracy: 0.9493 - recall: 0.9493 - f1: 0.9493 - val_loss: 0.2029 - val_accuracy: 0.9137 - val_recall: 0.9138 - val_f1: 0.9137\n",
      "Epoch 63/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9489 - recall: 0.9491 - f1: 0.9489\n",
      "Epoch 00063: val_loss did not improve from 0.18740\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1269 - accuracy: 0.9490 - recall: 0.9491 - f1: 0.9490 - val_loss: 0.2000 - val_accuracy: 0.9154 - val_recall: 0.9154 - val_f1: 0.9154\n",
      "Epoch 64/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9479 - recall: 0.9479 - f1: 0.9479\n",
      "Epoch 00064: val_loss did not improve from 0.18740\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1262 - accuracy: 0.9482 - recall: 0.9482 - f1: 0.9482 - val_loss: 0.2004 - val_accuracy: 0.9159 - val_recall: 0.9160 - val_f1: 0.9159\n",
      "Epoch 65/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488\n",
      "Epoch 00065: val_loss did not improve from 0.18740\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1199 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488 - val_loss: 0.1875 - val_accuracy: 0.9218 - val_recall: 0.9219 - val_f1: 0.9218\n",
      "Epoch 66/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9483 - recall: 0.9483 - f1: 0.9483\n",
      "Epoch 00066: val_loss did not improve from 0.18740\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1261 - accuracy: 0.9483 - recall: 0.9483 - f1: 0.9483 - val_loss: 0.1899 - val_accuracy: 0.9221 - val_recall: 0.9220 - val_f1: 0.9221\n",
      "Epoch 67/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9498 - recall: 0.9497 - f1: 0.9498\n",
      "Epoch 00067: val_loss improved from 0.18740 to 0.18255, saving model to batch_relu_validation_200-067-0.949700-0.924167.h5\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1241 - accuracy: 0.9497 - recall: 0.9496 - f1: 0.9497 - val_loss: 0.1826 - val_accuracy: 0.9242 - val_recall: 0.9243 - val_f1: 0.9242\n",
      "Epoch 68/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9511 - recall: 0.9511 - f1: 0.9511\n",
      "Epoch 00068: val_loss did not improve from 0.18255\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1188 - accuracy: 0.9510 - recall: 0.9509 - f1: 0.9510 - val_loss: 0.1869 - val_accuracy: 0.9206 - val_recall: 0.9205 - val_f1: 0.9206\n",
      "Epoch 69/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9492 - recall: 0.9493 - f1: 0.9492\n",
      "Epoch 00069: val_loss did not improve from 0.18255\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1230 - accuracy: 0.9492 - recall: 0.9493 - f1: 0.9492 - val_loss: 0.1842 - val_accuracy: 0.9250 - val_recall: 0.9250 - val_f1: 0.9250\n",
      "Epoch 70/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9506 - recall: 0.9504 - f1: 0.9506\n",
      "Epoch 00070: val_loss did not improve from 0.18255\n",
      "150/150 [==============================] - 65s 435ms/step - loss: 0.1207 - accuracy: 0.9506 - recall: 0.9504 - f1: 0.9506 - val_loss: 0.1908 - val_accuracy: 0.9214 - val_recall: 0.9213 - val_f1: 0.9214\n",
      "Epoch 71/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9510 - recall: 0.9511 - f1: 0.9510\n",
      "Epoch 00071: val_loss did not improve from 0.18255\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1214 - accuracy: 0.9509 - recall: 0.9510 - f1: 0.9509 - val_loss: 0.1847 - val_accuracy: 0.9229 - val_recall: 0.9231 - val_f1: 0.9229\n",
      "Epoch 72/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9541 - recall: 0.9542 - f1: 0.9541\n",
      "Epoch 00072: val_loss did not improve from 0.18255\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1217 - accuracy: 0.9543 - recall: 0.9545 - f1: 0.9543 - val_loss: 0.1829 - val_accuracy: 0.9234 - val_recall: 0.9236 - val_f1: 0.9234\n",
      "Epoch 73/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9484 - recall: 0.9486 - f1: 0.9484\n",
      "Epoch 00073: val_loss did not improve from 0.18255\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1239 - accuracy: 0.9485 - recall: 0.9487 - f1: 0.9485 - val_loss: 0.1879 - val_accuracy: 0.9227 - val_recall: 0.9227 - val_f1: 0.9227\n",
      "Epoch 74/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1196 - accuracy: 0.9507 - recall: 0.9509 - f1: 0.9507\n",
      "Epoch 00074: val_loss improved from 0.18255 to 0.17611, saving model to batch_relu_validation_200-074-0.950733-0.929300.h5\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1195 - accuracy: 0.9507 - recall: 0.9509 - f1: 0.9507 - val_loss: 0.1761 - val_accuracy: 0.9293 - val_recall: 0.9293 - val_f1: 0.9293\n",
      "Epoch 75/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9497 - recall: 0.9500 - f1: 0.9497\n",
      "Epoch 00075: val_loss did not improve from 0.17611\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1229 - accuracy: 0.9496 - recall: 0.9499 - f1: 0.9496 - val_loss: 0.1956 - val_accuracy: 0.9195 - val_recall: 0.9195 - val_f1: 0.9195\n",
      "Epoch 76/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9484 - recall: 0.9483 - f1: 0.9484\n",
      "Epoch 00076: val_loss did not improve from 0.17611\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1212 - accuracy: 0.9485 - recall: 0.9484 - f1: 0.9485 - val_loss: 0.1831 - val_accuracy: 0.9224 - val_recall: 0.9223 - val_f1: 0.9224\n",
      "Epoch 77/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1179 - accuracy: 0.9524 - recall: 0.9522 - f1: 0.9524\n",
      "Epoch 00077: val_loss did not improve from 0.17611\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1176 - accuracy: 0.9524 - recall: 0.9523 - f1: 0.9524 - val_loss: 0.1763 - val_accuracy: 0.9283 - val_recall: 0.9285 - val_f1: 0.9283\n",
      "Epoch 78/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9504 - recall: 0.9504 - f1: 0.9504\n",
      "Epoch 00078: val_loss did not improve from 0.17611\n",
      "150/150 [==============================] - 65s 437ms/step - loss: 0.1216 - accuracy: 0.9504 - recall: 0.9504 - f1: 0.9504 - val_loss: 0.1799 - val_accuracy: 0.9275 - val_recall: 0.9276 - val_f1: 0.9275\n",
      "Epoch 79/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1149 - accuracy: 0.9529 - recall: 0.9528 - f1: 0.9528\n",
      "Epoch 00079: val_loss did not improve from 0.17611\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1147 - accuracy: 0.9529 - recall: 0.9528 - f1: 0.9529 - val_loss: 0.1859 - val_accuracy: 0.9236 - val_recall: 0.9235 - val_f1: 0.9236\n",
      "Epoch 80/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 0.9516 - recall: 0.9515 - f1: 0.9516\n",
      "Epoch 00080: val_loss did not improve from 0.17611\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1173 - accuracy: 0.9517 - recall: 0.9517 - f1: 0.9517 - val_loss: 0.1814 - val_accuracy: 0.9248 - val_recall: 0.9249 - val_f1: 0.9248\n",
      "Epoch 81/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1257 - accuracy: 0.9485 - recall: 0.9485 - f1: 0.9485\n",
      "Epoch 00081: val_loss did not improve from 0.17611\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1258 - accuracy: 0.9485 - recall: 0.9485 - f1: 0.9485 - val_loss: 0.1864 - val_accuracy: 0.9236 - val_recall: 0.9236 - val_f1: 0.9236\n",
      "Epoch 82/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9497 - recall: 0.9497 - f1: 0.9497\n",
      "Epoch 00082: val_loss did not improve from 0.17611\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1212 - accuracy: 0.9498 - recall: 0.9498 - f1: 0.9498 - val_loss: 0.1879 - val_accuracy: 0.9233 - val_recall: 0.9232 - val_f1: 0.9233\n",
      "Epoch 83/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9533 - recall: 0.9533 - f1: 0.9533\n",
      "Epoch 00083: val_loss did not improve from 0.17611\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1213 - accuracy: 0.9533 - recall: 0.9533 - f1: 0.9533 - val_loss: 0.1785 - val_accuracy: 0.9283 - val_recall: 0.9285 - val_f1: 0.9283\n",
      "Epoch 84/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9474 - recall: 0.9474 - f1: 0.9474\n",
      "Epoch 00084: val_loss improved from 0.17611 to 0.17608, saving model to batch_relu_validation_200-084-0.947367-0.928833.h5\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1276 - accuracy: 0.9474 - recall: 0.9474 - f1: 0.9474 - val_loss: 0.1761 - val_accuracy: 0.9288 - val_recall: 0.9288 - val_f1: 0.9288\n",
      "Epoch 85/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9523 - recall: 0.9526 - f1: 0.9524\n",
      "Epoch 00085: val_loss did not improve from 0.17608\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1187 - accuracy: 0.9523 - recall: 0.9525 - f1: 0.9523 - val_loss: 0.1852 - val_accuracy: 0.9262 - val_recall: 0.9262 - val_f1: 0.9262\n",
      "Epoch 86/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9529 - recall: 0.9530 - f1: 0.9529\n",
      "Epoch 00086: val_loss did not improve from 0.17608\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1201 - accuracy: 0.9529 - recall: 0.9530 - f1: 0.9529 - val_loss: 0.1835 - val_accuracy: 0.9270 - val_recall: 0.9269 - val_f1: 0.9270\n",
      "Epoch 87/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9522 - recall: 0.9525 - f1: 0.9522\n",
      "Epoch 00087: val_loss did not improve from 0.17608\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1220 - accuracy: 0.9521 - recall: 0.9524 - f1: 0.9521 - val_loss: 0.1818 - val_accuracy: 0.9260 - val_recall: 0.9261 - val_f1: 0.9260\n",
      "Epoch 88/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 0.9498 - recall: 0.9497 - f1: 0.9498\n",
      "Epoch 00088: val_loss did not improve from 0.17608\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1226 - accuracy: 0.9498 - recall: 0.9497 - f1: 0.9498 - val_loss: 0.1836 - val_accuracy: 0.9246 - val_recall: 0.9246 - val_f1: 0.9246\n",
      "Epoch 89/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1181 - accuracy: 0.9512 - recall: 0.9511 - f1: 0.9512\n",
      "Epoch 00089: val_loss did not improve from 0.17608\n",
      "150/150 [==============================] - 65s 434ms/step - loss: 0.1180 - accuracy: 0.9513 - recall: 0.9512 - f1: 0.9513 - val_loss: 0.1824 - val_accuracy: 0.9235 - val_recall: 0.9234 - val_f1: 0.9235\n",
      "Epoch 90/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9515 - recall: 0.9515 - f1: 0.9515\n",
      "Epoch 00090: val_loss did not improve from 0.17608\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1219 - accuracy: 0.9516 - recall: 0.9516 - f1: 0.9516 - val_loss: 0.1817 - val_accuracy: 0.9264 - val_recall: 0.9264 - val_f1: 0.9264\n",
      "Epoch 91/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1181 - accuracy: 0.9528 - recall: 0.9528 - f1: 0.9528\n",
      "Epoch 00091: val_loss did not improve from 0.17608\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1184 - accuracy: 0.9528 - recall: 0.9527 - f1: 0.9528 - val_loss: 0.1776 - val_accuracy: 0.9251 - val_recall: 0.9251 - val_f1: 0.9251\n",
      "Epoch 92/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1233 - accuracy: 0.9510 - recall: 0.9511 - f1: 0.9510\n",
      "Epoch 00092: val_loss did not improve from 0.17608\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1229 - accuracy: 0.9512 - recall: 0.9513 - f1: 0.9512 - val_loss: 0.1862 - val_accuracy: 0.9265 - val_recall: 0.9265 - val_f1: 0.9265\n",
      "Epoch 93/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9486 - recall: 0.9486 - f1: 0.9486\n",
      "Epoch 00093: val_loss improved from 0.17608 to 0.17592, saving model to batch_relu_validation_200-093-0.948367-0.928700.h5\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1242 - accuracy: 0.9484 - recall: 0.9483 - f1: 0.9484 - val_loss: 0.1759 - val_accuracy: 0.9287 - val_recall: 0.9288 - val_f1: 0.9287\n",
      "Epoch 94/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9496 - recall: 0.9494 - f1: 0.9496\n",
      "Epoch 00094: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1242 - accuracy: 0.9495 - recall: 0.9493 - f1: 0.9495 - val_loss: 0.1832 - val_accuracy: 0.9241 - val_recall: 0.9240 - val_f1: 0.9241\n",
      "Epoch 95/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1184 - accuracy: 0.9506 - recall: 0.9507 - f1: 0.9506\n",
      "Epoch 00095: val_loss did not improve from 0.17592\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1182 - accuracy: 0.9507 - recall: 0.9508 - f1: 0.9507 - val_loss: 0.1808 - val_accuracy: 0.9282 - val_recall: 0.9283 - val_f1: 0.9282\n",
      "Epoch 96/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1182 - accuracy: 0.9522 - recall: 0.9519 - f1: 0.9522\n",
      "Epoch 00096: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1180 - accuracy: 0.9523 - recall: 0.9521 - f1: 0.9523 - val_loss: 0.1807 - val_accuracy: 0.9277 - val_recall: 0.9278 - val_f1: 0.9277\n",
      "Epoch 97/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9505 - recall: 0.9503 - f1: 0.9505\n",
      "Epoch 00097: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1198 - accuracy: 0.9506 - recall: 0.9505 - f1: 0.9506 - val_loss: 0.1889 - val_accuracy: 0.9230 - val_recall: 0.9229 - val_f1: 0.9230\n",
      "Epoch 98/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9488 - recall: 0.9490 - f1: 0.9488\n",
      "Epoch 00098: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 65s 433ms/step - loss: 0.1233 - accuracy: 0.9487 - recall: 0.9488 - f1: 0.9487 - val_loss: 0.1817 - val_accuracy: 0.9273 - val_recall: 0.9274 - val_f1: 0.9273\n",
      "Epoch 99/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9508 - recall: 0.9508 - f1: 0.9508\n",
      "Epoch 00099: val_loss did not improve from 0.17592\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1217 - accuracy: 0.9509 - recall: 0.9509 - f1: 0.9509 - val_loss: 0.1842 - val_accuracy: 0.9242 - val_recall: 0.9243 - val_f1: 0.9242\n",
      "Epoch 100/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 0.9488 - recall: 0.9487 - f1: 0.9488\n",
      "Epoch 00100: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 67s 448ms/step - loss: 0.1258 - accuracy: 0.9491 - recall: 0.9490 - f1: 0.9491 - val_loss: 0.1764 - val_accuracy: 0.9276 - val_recall: 0.9277 - val_f1: 0.9276\n",
      "Epoch 101/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9510 - recall: 0.9509 - f1: 0.9510\n",
      "Epoch 00101: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1218 - accuracy: 0.9509 - recall: 0.9509 - f1: 0.9509 - val_loss: 0.1852 - val_accuracy: 0.9244 - val_recall: 0.9245 - val_f1: 0.9244\n",
      "Epoch 102/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9505 - recall: 0.9506 - f1: 0.9505\n",
      "Epoch 00102: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1217 - accuracy: 0.9505 - recall: 0.9506 - f1: 0.9505 - val_loss: 0.1885 - val_accuracy: 0.9240 - val_recall: 0.9240 - val_f1: 0.9240\n",
      "Epoch 103/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9509 - recall: 0.9507 - f1: 0.9509\n",
      "Epoch 00103: val_loss did not improve from 0.17592\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1234 - accuracy: 0.9511 - recall: 0.9509 - f1: 0.9511 - val_loss: 0.1838 - val_accuracy: 0.9244 - val_recall: 0.9243 - val_f1: 0.9244\n",
      "Epoch 104/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1188 - accuracy: 0.9498 - recall: 0.9497 - f1: 0.9498\n",
      "Epoch 00104: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1190 - accuracy: 0.9497 - recall: 0.9496 - f1: 0.9497 - val_loss: 0.1820 - val_accuracy: 0.9268 - val_recall: 0.9267 - val_f1: 0.9268\n",
      "Epoch 105/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9512 - recall: 0.9512 - f1: 0.9512\n",
      "Epoch 00105: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1210 - accuracy: 0.9510 - recall: 0.9511 - f1: 0.9510 - val_loss: 0.1862 - val_accuracy: 0.9206 - val_recall: 0.9205 - val_f1: 0.9206\n",
      "Epoch 106/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9479 - recall: 0.9481 - f1: 0.9479\n",
      "Epoch 00106: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 65s 434ms/step - loss: 0.1236 - accuracy: 0.9480 - recall: 0.9481 - f1: 0.9480 - val_loss: 0.1833 - val_accuracy: 0.9258 - val_recall: 0.9259 - val_f1: 0.9258\n",
      "Epoch 107/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9493 - recall: 0.9493 - f1: 0.9493\n",
      "Epoch 00107: val_loss did not improve from 0.17592\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1210 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495 - val_loss: 0.1890 - val_accuracy: 0.9237 - val_recall: 0.9237 - val_f1: 0.9237\n",
      "Epoch 108/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9497 - recall: 0.9499 - f1: 0.9497\n",
      "Epoch 00108: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 65s 437ms/step - loss: 0.1220 - accuracy: 0.9499 - recall: 0.9501 - f1: 0.9499 - val_loss: 0.1827 - val_accuracy: 0.9220 - val_recall: 0.9219 - val_f1: 0.9220\n",
      "Epoch 109/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9514 - recall: 0.9515 - f1: 0.9514\n",
      "Epoch 00109: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1224 - accuracy: 0.9514 - recall: 0.9515 - f1: 0.9514 - val_loss: 0.1825 - val_accuracy: 0.9286 - val_recall: 0.9287 - val_f1: 0.9286\n",
      "Epoch 110/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9498 - recall: 0.9499 - f1: 0.9498\n",
      "Epoch 00110: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 65s 437ms/step - loss: 0.1240 - accuracy: 0.9494 - recall: 0.9495 - f1: 0.9494 - val_loss: 0.1768 - val_accuracy: 0.9274 - val_recall: 0.9273 - val_f1: 0.9274\n",
      "Epoch 111/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9528 - recall: 0.9529 - f1: 0.9528\n",
      "Epoch 00111: val_loss did not improve from 0.17592\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1187 - accuracy: 0.9529 - recall: 0.9531 - f1: 0.9529 - val_loss: 0.1843 - val_accuracy: 0.9251 - val_recall: 0.9251 - val_f1: 0.9251\n",
      "Epoch 112/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9497 - recall: 0.9499 - f1: 0.9497\n",
      "Epoch 00112: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1213 - accuracy: 0.9497 - recall: 0.9499 - f1: 0.9497 - val_loss: 0.1762 - val_accuracy: 0.9280 - val_recall: 0.9280 - val_f1: 0.9280\n",
      "Epoch 113/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9489 - recall: 0.9487 - f1: 0.9489\n",
      "Epoch 00113: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1208 - accuracy: 0.9489 - recall: 0.9487 - f1: 0.9489 - val_loss: 0.1793 - val_accuracy: 0.9272 - val_recall: 0.9273 - val_f1: 0.9272\n",
      "Epoch 114/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1159 - accuracy: 0.9522 - recall: 0.9522 - f1: 0.9522\n",
      "Epoch 00114: val_loss did not improve from 0.17592\n",
      "150/150 [==============================] - 65s 433ms/step - loss: 0.1158 - accuracy: 0.9522 - recall: 0.9522 - f1: 0.9522 - val_loss: 0.1835 - val_accuracy: 0.9247 - val_recall: 0.9247 - val_f1: 0.9247\n",
      "Epoch 115/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9511 - recall: 0.9511 - f1: 0.9511\n",
      "Epoch 00115: val_loss improved from 0.17592 to 0.17498, saving model to batch_relu_validation_200-115-0.951267-0.929667.h5\n",
      "\n",
      "Epoch 00115: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1188 - accuracy: 0.9513 - recall: 0.9512 - f1: 0.9513 - val_loss: 0.1750 - val_accuracy: 0.9297 - val_recall: 0.9297 - val_f1: 0.9297\n",
      "Epoch 116/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9497 - recall: 0.9497 - f1: 0.9497\n",
      "Epoch 00116: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1191 - accuracy: 0.9498 - recall: 0.9497 - f1: 0.9498 - val_loss: 0.1791 - val_accuracy: 0.9268 - val_recall: 0.9268 - val_f1: 0.9268\n",
      "Epoch 117/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9527 - recall: 0.9526 - f1: 0.9527\n",
      "Epoch 00117: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1174 - accuracy: 0.9528 - recall: 0.9527 - f1: 0.9528 - val_loss: 0.1810 - val_accuracy: 0.9256 - val_recall: 0.9258 - val_f1: 0.9257\n",
      "Epoch 118/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1161 - accuracy: 0.9527 - recall: 0.9528 - f1: 0.9527\n",
      "Epoch 00118: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 65s 435ms/step - loss: 0.1163 - accuracy: 0.9526 - recall: 0.9527 - f1: 0.9526 - val_loss: 0.1775 - val_accuracy: 0.9278 - val_recall: 0.9281 - val_f1: 0.9279\n",
      "Epoch 119/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 0.9537 - recall: 0.9540 - f1: 0.9537\n",
      "Epoch 00119: val_loss did not improve from 0.17498\n",
      "\n",
      "Epoch 00119: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1170 - accuracy: 0.9538 - recall: 0.9541 - f1: 0.9538 - val_loss: 0.1807 - val_accuracy: 0.9257 - val_recall: 0.9257 - val_f1: 0.9257\n",
      "Epoch 120/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9508 - recall: 0.9511 - f1: 0.9508\n",
      "Epoch 00120: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1228 - accuracy: 0.9505 - recall: 0.9508 - f1: 0.9505 - val_loss: 0.1776 - val_accuracy: 0.9262 - val_recall: 0.9261 - val_f1: 0.9262\n",
      "Epoch 121/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1209 - accuracy: 0.9501 - recall: 0.9503 - f1: 0.9501\n",
      "Epoch 00121: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1205 - accuracy: 0.9504 - recall: 0.9505 - f1: 0.9504 - val_loss: 0.1941 - val_accuracy: 0.9202 - val_recall: 0.9202 - val_f1: 0.9202\n",
      "Epoch 122/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9501 - recall: 0.9500 - f1: 0.9501\n",
      "Epoch 00122: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 65s 434ms/step - loss: 0.1251 - accuracy: 0.9502 - recall: 0.9501 - f1: 0.9502 - val_loss: 0.1757 - val_accuracy: 0.9279 - val_recall: 0.9279 - val_f1: 0.9279\n",
      "Epoch 123/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9501 - recall: 0.9503 - f1: 0.9501\n",
      "Epoch 00123: val_loss did not improve from 0.17498\n",
      "\n",
      "Epoch 00123: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1194 - accuracy: 0.9498 - recall: 0.9500 - f1: 0.9498 - val_loss: 0.1799 - val_accuracy: 0.9246 - val_recall: 0.9247 - val_f1: 0.9246\n",
      "Epoch 124/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9515 - recall: 0.9515 - f1: 0.9515\n",
      "Epoch 00124: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 72s 477ms/step - loss: 0.1214 - accuracy: 0.9516 - recall: 0.9516 - f1: 0.9516 - val_loss: 0.1854 - val_accuracy: 0.9237 - val_recall: 0.9238 - val_f1: 0.9237\n",
      "Epoch 125/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9490 - recall: 0.9490 - f1: 0.9490\n",
      "Epoch 00125: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1227 - accuracy: 0.9489 - recall: 0.9489 - f1: 0.9489 - val_loss: 0.1808 - val_accuracy: 0.9260 - val_recall: 0.9261 - val_f1: 0.9260\n",
      "Epoch 126/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1194 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507\n",
      "Epoch 00126: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1193 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9508 - val_loss: 0.1789 - val_accuracy: 0.9264 - val_recall: 0.9264 - val_f1: 0.9264\n",
      "Epoch 127/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9506 - recall: 0.9506 - f1: 0.9506\n",
      "Epoch 00127: val_loss did not improve from 0.17498\n",
      "\n",
      "Epoch 00127: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.1210 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507 - val_loss: 0.1759 - val_accuracy: 0.9259 - val_recall: 0.9259 - val_f1: 0.9259\n",
      "Epoch 128/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9506 - recall: 0.9508 - f1: 0.9506\n",
      "Epoch 00128: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 67s 445ms/step - loss: 0.1211 - accuracy: 0.9504 - recall: 0.9506 - f1: 0.9504 - val_loss: 0.1851 - val_accuracy: 0.9238 - val_recall: 0.9238 - val_f1: 0.9238\n",
      "Epoch 129/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1271 - accuracy: 0.9470 - recall: 0.9468 - f1: 0.9470\n",
      "Epoch 00129: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1274 - accuracy: 0.9469 - recall: 0.9467 - f1: 0.9469 - val_loss: 0.1815 - val_accuracy: 0.9269 - val_recall: 0.9269 - val_f1: 0.9269\n",
      "Epoch 130/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9493 - recall: 0.9494 - f1: 0.9493\n",
      "Epoch 00130: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1211 - accuracy: 0.9491 - recall: 0.9492 - f1: 0.9491 - val_loss: 0.1771 - val_accuracy: 0.9276 - val_recall: 0.9277 - val_f1: 0.9276\n",
      "Epoch 131/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9489 - recall: 0.9489 - f1: 0.9489\n",
      "Epoch 00131: val_loss did not improve from 0.17498\n",
      "\n",
      "Epoch 00131: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1266 - accuracy: 0.9491 - recall: 0.9491 - f1: 0.9491 - val_loss: 0.1892 - val_accuracy: 0.9224 - val_recall: 0.9225 - val_f1: 0.9224\n",
      "Epoch 132/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9512 - recall: 0.9511 - f1: 0.9512\n",
      "Epoch 00132: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1206 - accuracy: 0.9513 - recall: 0.9511 - f1: 0.9513 - val_loss: 0.1923 - val_accuracy: 0.9202 - val_recall: 0.9203 - val_f1: 0.9202\n",
      "Epoch 133/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1172 - accuracy: 0.9542 - recall: 0.9542 - f1: 0.9542\n",
      "Epoch 00133: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1171 - accuracy: 0.9544 - recall: 0.9544 - f1: 0.9544 - val_loss: 0.1872 - val_accuracy: 0.9241 - val_recall: 0.9241 - val_f1: 0.9241\n",
      "Epoch 134/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1199 - accuracy: 0.9501 - recall: 0.9501 - f1: 0.9501\n",
      "Epoch 00134: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1196 - accuracy: 0.9502 - recall: 0.9501 - f1: 0.9502 - val_loss: 0.1907 - val_accuracy: 0.9223 - val_recall: 0.9222 - val_f1: 0.9223\n",
      "Epoch 135/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9490 - recall: 0.9489 - f1: 0.9490\n",
      "Epoch 00135: val_loss did not improve from 0.17498\n",
      "\n",
      "Epoch 00135: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1218 - accuracy: 0.9490 - recall: 0.9489 - f1: 0.9490 - val_loss: 0.1875 - val_accuracy: 0.9233 - val_recall: 0.9233 - val_f1: 0.9233\n",
      "Epoch 136/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1189 - accuracy: 0.9529 - recall: 0.9528 - f1: 0.9529\n",
      "Epoch 00136: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1187 - accuracy: 0.9530 - recall: 0.9529 - f1: 0.9530 - val_loss: 0.1877 - val_accuracy: 0.9214 - val_recall: 0.9215 - val_f1: 0.9214\n",
      "Epoch 137/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9509 - recall: 0.9509 - f1: 0.9509\n",
      "Epoch 00137: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1232 - accuracy: 0.9511 - recall: 0.9511 - f1: 0.9511 - val_loss: 0.1862 - val_accuracy: 0.9241 - val_recall: 0.9241 - val_f1: 0.9241\n",
      "Epoch 138/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1188 - accuracy: 0.9510 - recall: 0.9512 - f1: 0.9511\n",
      "Epoch 00138: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1191 - accuracy: 0.9509 - recall: 0.9511 - f1: 0.9509 - val_loss: 0.1846 - val_accuracy: 0.9231 - val_recall: 0.9231 - val_f1: 0.9231\n",
      "Epoch 139/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9503 - recall: 0.9506 - f1: 0.9503\n",
      "Epoch 00139: val_loss did not improve from 0.17498\n",
      "\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1209 - accuracy: 0.9505 - recall: 0.9507 - f1: 0.9505 - val_loss: 0.1780 - val_accuracy: 0.9253 - val_recall: 0.9253 - val_f1: 0.9253\n",
      "Epoch 140/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1172 - accuracy: 0.9519 - recall: 0.9518 - f1: 0.9519\n",
      "Epoch 00140: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1170 - accuracy: 0.9520 - recall: 0.9519 - f1: 0.9520 - val_loss: 0.1788 - val_accuracy: 0.9263 - val_recall: 0.9262 - val_f1: 0.9263\n",
      "Epoch 141/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9492 - recall: 0.9493 - f1: 0.9492\n",
      "Epoch 00141: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1236 - accuracy: 0.9493 - recall: 0.9493 - f1: 0.9493 - val_loss: 0.1870 - val_accuracy: 0.9228 - val_recall: 0.9229 - val_f1: 0.9228\n",
      "Epoch 142/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9490 - recall: 0.9493 - f1: 0.9490\n",
      "Epoch 00142: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1211 - accuracy: 0.9491 - recall: 0.9493 - f1: 0.9491 - val_loss: 0.1792 - val_accuracy: 0.9308 - val_recall: 0.9308 - val_f1: 0.9308\n",
      "Epoch 143/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9519 - recall: 0.9523 - f1: 0.9519\n",
      "Epoch 00143: val_loss did not improve from 0.17498\n",
      "\n",
      "Epoch 00143: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1199 - accuracy: 0.9520 - recall: 0.9523 - f1: 0.9520 - val_loss: 0.1852 - val_accuracy: 0.9235 - val_recall: 0.9236 - val_f1: 0.9235\n",
      "Epoch 144/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9521 - recall: 0.9520 - f1: 0.9521\n",
      "Epoch 00144: val_loss did not improve from 0.17498\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1184 - accuracy: 0.9522 - recall: 0.9521 - f1: 0.9522 - val_loss: 0.1888 - val_accuracy: 0.9242 - val_recall: 0.9242 - val_f1: 0.9242\n",
      "Epoch 145/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9513 - recall: 0.9512 - f1: 0.9513\n",
      "Epoch 00145: val_loss improved from 0.17498 to 0.17405, saving model to batch_relu_validation_200-145-0.951267-0.930033.h5\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1195 - accuracy: 0.9513 - recall: 0.9512 - f1: 0.9513 - val_loss: 0.1741 - val_accuracy: 0.9300 - val_recall: 0.9302 - val_f1: 0.9300\n",
      "Epoch 146/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1146 - accuracy: 0.9547 - recall: 0.9547 - f1: 0.9547\n",
      "Epoch 00146: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1147 - accuracy: 0.9545 - recall: 0.9545 - f1: 0.9545 - val_loss: 0.1858 - val_accuracy: 0.9243 - val_recall: 0.9242 - val_f1: 0.9243\n",
      "Epoch 147/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9510 - recall: 0.9509 - f1: 0.9510\n",
      "Epoch 00147: val_loss did not improve from 0.17405\n",
      "\n",
      "Epoch 00147: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      "150/150 [==============================] - 65s 437ms/step - loss: 0.1200 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9508 - val_loss: 0.1803 - val_accuracy: 0.9255 - val_recall: 0.9255 - val_f1: 0.9255\n",
      "Epoch 148/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1175 - accuracy: 0.9523 - recall: 0.9523 - f1: 0.9523\n",
      "Epoch 00148: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1176 - accuracy: 0.9523 - recall: 0.9523 - f1: 0.9523 - val_loss: 0.1854 - val_accuracy: 0.9247 - val_recall: 0.9247 - val_f1: 0.9247\n",
      "Epoch 149/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1156 - accuracy: 0.9528 - recall: 0.9530 - f1: 0.9528\n",
      "Epoch 00149: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1158 - accuracy: 0.9526 - recall: 0.9528 - f1: 0.9526 - val_loss: 0.1824 - val_accuracy: 0.9264 - val_recall: 0.9267 - val_f1: 0.9265\n",
      "Epoch 150/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1171 - accuracy: 0.9524 - recall: 0.9523 - f1: 0.9524\n",
      "Epoch 00150: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1170 - accuracy: 0.9524 - recall: 0.9523 - f1: 0.9524 - val_loss: 0.1879 - val_accuracy: 0.9228 - val_recall: 0.9229 - val_f1: 0.9228\n",
      "Epoch 151/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9517 - recall: 0.9515 - f1: 0.9517\n",
      "Epoch 00151: val_loss did not improve from 0.17405\n",
      "\n",
      "Epoch 00151: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1184 - accuracy: 0.9518 - recall: 0.9517 - f1: 0.9518 - val_loss: 0.1752 - val_accuracy: 0.9269 - val_recall: 0.9269 - val_f1: 0.9269\n",
      "Epoch 152/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9485 - recall: 0.9485 - f1: 0.9485\n",
      "Epoch 00152: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1231 - accuracy: 0.9485 - recall: 0.9485 - f1: 0.9485 - val_loss: 0.1871 - val_accuracy: 0.9257 - val_recall: 0.9258 - val_f1: 0.9257\n",
      "Epoch 153/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9508 - recall: 0.9508 - f1: 0.9508\n",
      "Epoch 00153: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1213 - accuracy: 0.9505 - recall: 0.9505 - f1: 0.9505 - val_loss: 0.1857 - val_accuracy: 0.9239 - val_recall: 0.9239 - val_f1: 0.9239\n",
      "Epoch 154/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1141 - accuracy: 0.9542 - recall: 0.9544 - f1: 0.9542\n",
      "Epoch 00154: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1140 - accuracy: 0.9542 - recall: 0.9543 - f1: 0.9542 - val_loss: 0.1764 - val_accuracy: 0.9268 - val_recall: 0.9267 - val_f1: 0.9268\n",
      "Epoch 155/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1175 - accuracy: 0.9511 - recall: 0.9512 - f1: 0.9511\n",
      "Epoch 00155: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.1180 - accuracy: 0.9507 - recall: 0.9508 - f1: 0.9507 - val_loss: 0.1872 - val_accuracy: 0.9241 - val_recall: 0.9242 - val_f1: 0.9241\n",
      "Epoch 156/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1172 - accuracy: 0.9522 - recall: 0.9522 - f1: 0.9522\n",
      "Epoch 00156: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1172 - accuracy: 0.9523 - recall: 0.9523 - f1: 0.9523 - val_loss: 0.1850 - val_accuracy: 0.9239 - val_recall: 0.9240 - val_f1: 0.9239\n",
      "Epoch 157/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1144 - accuracy: 0.9535 - recall: 0.9535 - f1: 0.9535\n",
      "Epoch 00157: val_loss did not improve from 0.17405\n",
      "150/150 [==============================] - 65s 436ms/step - loss: 0.1141 - accuracy: 0.9537 - recall: 0.9537 - f1: 0.9537 - val_loss: 0.1763 - val_accuracy: 0.9290 - val_recall: 0.9289 - val_f1: 0.9290\n",
      "Epoch 158/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1175 - accuracy: 0.9537 - recall: 0.9537 - f1: 0.9537\n",
      "Epoch 00158: val_loss improved from 0.17405 to 0.17313, saving model to batch_relu_validation_200-158-0.953833-0.928867.h5\n",
      "\n",
      "Epoch 00158: ReduceLROnPlateau reducing learning rate to 2.980232380322967e-11.\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1171 - accuracy: 0.9538 - recall: 0.9539 - f1: 0.9538 - val_loss: 0.1731 - val_accuracy: 0.9289 - val_recall: 0.9289 - val_f1: 0.9289\n",
      "Epoch 159/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9496 - recall: 0.9496 - f1: 0.9496\n",
      "Epoch 00159: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1220 - accuracy: 0.9494 - recall: 0.9494 - f1: 0.9494 - val_loss: 0.1790 - val_accuracy: 0.9281 - val_recall: 0.9283 - val_f1: 0.9281\n",
      "Epoch 160/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1094 - accuracy: 0.9557 - recall: 0.9556 - f1: 0.9557\n",
      "Epoch 00160: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1099 - accuracy: 0.9557 - recall: 0.9555 - f1: 0.9557 - val_loss: 0.1847 - val_accuracy: 0.9233 - val_recall: 0.9232 - val_f1: 0.9233\n",
      "Epoch 161/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9478 - recall: 0.9477 - f1: 0.9478\n",
      "Epoch 00161: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1276 - accuracy: 0.9476 - recall: 0.9476 - f1: 0.9476 - val_loss: 0.1789 - val_accuracy: 0.9270 - val_recall: 0.9269 - val_f1: 0.9270\n",
      "Epoch 162/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1180 - accuracy: 0.9519 - recall: 0.9517 - f1: 0.9519\n",
      "Epoch 00162: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 65s 434ms/step - loss: 0.1179 - accuracy: 0.9519 - recall: 0.9517 - f1: 0.9519 - val_loss: 0.1760 - val_accuracy: 0.9258 - val_recall: 0.9257 - val_f1: 0.9258\n",
      "Epoch 163/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9522 - recall: 0.9521 - f1: 0.9522\n",
      "Epoch 00163: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1225 - accuracy: 0.9523 - recall: 0.9522 - f1: 0.9523 - val_loss: 0.1775 - val_accuracy: 0.9289 - val_recall: 0.9290 - val_f1: 0.9289\n",
      "Epoch 164/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9499 - recall: 0.9503 - f1: 0.9500\n",
      "Epoch 00164: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00164: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1246 - accuracy: 0.9498 - recall: 0.9501 - f1: 0.9498 - val_loss: 0.1813 - val_accuracy: 0.9257 - val_recall: 0.9255 - val_f1: 0.9257\n",
      "Epoch 165/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9516 - recall: 0.9519 - f1: 0.9517\n",
      "Epoch 00165: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1201 - accuracy: 0.9517 - recall: 0.9519 - f1: 0.9517 - val_loss: 0.1837 - val_accuracy: 0.9222 - val_recall: 0.9222 - val_f1: 0.9222\n",
      "Epoch 166/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9514 - recall: 0.9513 - f1: 0.9514\n",
      "Epoch 00166: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1207 - accuracy: 0.9514 - recall: 0.9513 - f1: 0.9514 - val_loss: 0.1780 - val_accuracy: 0.9262 - val_recall: 0.9261 - val_f1: 0.9262\n",
      "Epoch 167/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9506 - recall: 0.9511 - f1: 0.9507\n",
      "Epoch 00167: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 69s 462ms/step - loss: 0.1242 - accuracy: 0.9506 - recall: 0.9511 - f1: 0.9507 - val_loss: 0.1843 - val_accuracy: 0.9231 - val_recall: 0.9233 - val_f1: 0.9231\n",
      "Epoch 168/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9489 - recall: 0.9490 - f1: 0.9489\n",
      "Epoch 00168: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00168: ReduceLROnPlateau reducing learning rate to 7.450580950807417e-12.\n",
      "150/150 [==============================] - 71s 475ms/step - loss: 0.1267 - accuracy: 0.9491 - recall: 0.9492 - f1: 0.9491 - val_loss: 0.1861 - val_accuracy: 0.9215 - val_recall: 0.9215 - val_f1: 0.9215\n",
      "Epoch 169/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9512 - recall: 0.9511 - f1: 0.9512\n",
      "Epoch 00169: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 73s 488ms/step - loss: 0.1208 - accuracy: 0.9510 - recall: 0.9510 - f1: 0.9510 - val_loss: 0.1843 - val_accuracy: 0.9257 - val_recall: 0.9258 - val_f1: 0.9257\n",
      "Epoch 170/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1269 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488\n",
      "Epoch 00170: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 68s 454ms/step - loss: 0.1265 - accuracy: 0.9490 - recall: 0.9491 - f1: 0.9490 - val_loss: 0.1811 - val_accuracy: 0.9262 - val_recall: 0.9263 - val_f1: 0.9262\n",
      "Epoch 171/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9500 - recall: 0.9501 - f1: 0.9500\n",
      "Epoch 00171: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1229 - accuracy: 0.9502 - recall: 0.9502 - f1: 0.9502 - val_loss: 0.1810 - val_accuracy: 0.9274 - val_recall: 0.9275 - val_f1: 0.9274\n",
      "Epoch 172/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9514 - recall: 0.9513 - f1: 0.9514\n",
      "Epoch 00172: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00172: ReduceLROnPlateau reducing learning rate to 3.725290475403709e-12.\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1212 - accuracy: 0.9517 - recall: 0.9516 - f1: 0.9517 - val_loss: 0.1880 - val_accuracy: 0.9231 - val_recall: 0.9232 - val_f1: 0.9231\n",
      "Epoch 173/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 0.9493 - recall: 0.9493 - f1: 0.9493\n",
      "Epoch 00173: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1249 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495 - val_loss: 0.1973 - val_accuracy: 0.9189 - val_recall: 0.9189 - val_f1: 0.9189\n",
      "Epoch 174/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1209 - accuracy: 0.9501 - recall: 0.9501 - f1: 0.9501\n",
      "Epoch 00174: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1206 - accuracy: 0.9501 - recall: 0.9501 - f1: 0.9501 - val_loss: 0.1835 - val_accuracy: 0.9232 - val_recall: 0.9232 - val_f1: 0.9232\n",
      "Epoch 175/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1163 - accuracy: 0.9520 - recall: 0.9521 - f1: 0.9520\n",
      "Epoch 00175: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1161 - accuracy: 0.9522 - recall: 0.9522 - f1: 0.9522 - val_loss: 0.1811 - val_accuracy: 0.9281 - val_recall: 0.9279 - val_f1: 0.9281\n",
      "Epoch 176/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9496 - recall: 0.9495 - f1: 0.9496\n",
      "Epoch 00176: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00176: ReduceLROnPlateau reducing learning rate to 1.8626452377018543e-12.\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1242 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495 - val_loss: 0.1861 - val_accuracy: 0.9219 - val_recall: 0.9219 - val_f1: 0.9219\n",
      "Epoch 177/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9498 - recall: 0.9501 - f1: 0.9498\n",
      "Epoch 00177: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1211 - accuracy: 0.9497 - recall: 0.9500 - f1: 0.9497 - val_loss: 0.1772 - val_accuracy: 0.9266 - val_recall: 0.9269 - val_f1: 0.9267\n",
      "Epoch 178/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1184 - accuracy: 0.9514 - recall: 0.9513 - f1: 0.9514\n",
      "Epoch 00178: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 437ms/step - loss: 0.1192 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9508 - val_loss: 0.1821 - val_accuracy: 0.9251 - val_recall: 0.9252 - val_f1: 0.9251\n",
      "Epoch 179/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507\n",
      "Epoch 00179: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1223 - accuracy: 0.9509 - recall: 0.9509 - f1: 0.9509 - val_loss: 0.1776 - val_accuracy: 0.9262 - val_recall: 0.9261 - val_f1: 0.9262\n",
      "Epoch 180/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9505 - recall: 0.9505 - f1: 0.9505\n",
      "Epoch 00180: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00180: ReduceLROnPlateau reducing learning rate to 9.313226188509272e-13.\n",
      "150/150 [==============================] - 67s 445ms/step - loss: 0.1215 - accuracy: 0.9505 - recall: 0.9505 - f1: 0.9505 - val_loss: 0.1794 - val_accuracy: 0.9263 - val_recall: 0.9263 - val_f1: 0.9263\n",
      "Epoch 181/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9504 - recall: 0.9505 - f1: 0.9504\n",
      "Epoch 00181: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1210 - accuracy: 0.9505 - recall: 0.9506 - f1: 0.9505 - val_loss: 0.1864 - val_accuracy: 0.9236 - val_recall: 0.9235 - val_f1: 0.9236\n",
      "Epoch 182/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1194 - accuracy: 0.9517 - recall: 0.9520 - f1: 0.9518\n",
      "Epoch 00182: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1192 - accuracy: 0.9519 - recall: 0.9521 - f1: 0.9519 - val_loss: 0.1779 - val_accuracy: 0.9263 - val_recall: 0.9263 - val_f1: 0.9263\n",
      "Epoch 183/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1188 - accuracy: 0.9532 - recall: 0.9533 - f1: 0.9532\n",
      "Epoch 00183: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1196 - accuracy: 0.9529 - recall: 0.9529 - f1: 0.9529 - val_loss: 0.1751 - val_accuracy: 0.9291 - val_recall: 0.9291 - val_f1: 0.9291\n",
      "Epoch 184/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9475 - recall: 0.9475 - f1: 0.9475\n",
      "Epoch 00184: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00184: ReduceLROnPlateau reducing learning rate to 4.656613094254636e-13.\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1226 - accuracy: 0.9476 - recall: 0.9476 - f1: 0.9476 - val_loss: 0.1831 - val_accuracy: 0.9242 - val_recall: 0.9243 - val_f1: 0.9242\n",
      "Epoch 185/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9528 - recall: 0.9528 - f1: 0.9528\n",
      "Epoch 00185: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1175 - accuracy: 0.9526 - recall: 0.9525 - f1: 0.9526 - val_loss: 0.1802 - val_accuracy: 0.9269 - val_recall: 0.9270 - val_f1: 0.9269\n",
      "Epoch 186/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9480 - recall: 0.9479 - f1: 0.9480\n",
      "Epoch 00186: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 65s 434ms/step - loss: 0.1240 - accuracy: 0.9481 - recall: 0.9479 - f1: 0.9481 - val_loss: 0.1793 - val_accuracy: 0.9275 - val_recall: 0.9276 - val_f1: 0.9275\n",
      "Epoch 187/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9486 - recall: 0.9486 - f1: 0.9486\n",
      "Epoch 00187: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1228 - accuracy: 0.9487 - recall: 0.9487 - f1: 0.9487 - val_loss: 0.1831 - val_accuracy: 0.9233 - val_recall: 0.9233 - val_f1: 0.9233\n",
      "Epoch 188/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1188 - accuracy: 0.9512 - recall: 0.9511 - f1: 0.9512\n",
      "Epoch 00188: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00188: ReduceLROnPlateau reducing learning rate to 2.328306547127318e-13.\n",
      "150/150 [==============================] - 67s 445ms/step - loss: 0.1191 - accuracy: 0.9510 - recall: 0.9509 - f1: 0.9510 - val_loss: 0.1836 - val_accuracy: 0.9251 - val_recall: 0.9251 - val_f1: 0.9251\n",
      "Epoch 189/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9501 - recall: 0.9501 - f1: 0.9501\n",
      "Epoch 00189: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1230 - accuracy: 0.9502 - recall: 0.9502 - f1: 0.9502 - val_loss: 0.1791 - val_accuracy: 0.9261 - val_recall: 0.9261 - val_f1: 0.9261\n",
      "Epoch 190/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495\n",
      "Epoch 00190: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1215 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495 - val_loss: 0.1796 - val_accuracy: 0.9272 - val_recall: 0.9273 - val_f1: 0.9272\n",
      "Epoch 191/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 0.9483 - recall: 0.9483 - f1: 0.9483\n",
      "Epoch 00191: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1268 - accuracy: 0.9481 - recall: 0.9481 - f1: 0.9481 - val_loss: 0.1756 - val_accuracy: 0.9261 - val_recall: 0.9263 - val_f1: 0.9261\n",
      "Epoch 192/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507\n",
      "Epoch 00192: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00192: ReduceLROnPlateau reducing learning rate to 1.164153273563659e-13.\n",
      "150/150 [==============================] - 68s 452ms/step - loss: 0.1196 - accuracy: 0.9508 - recall: 0.9508 - f1: 0.9508 - val_loss: 0.1804 - val_accuracy: 0.9246 - val_recall: 0.9247 - val_f1: 0.9246\n",
      "Epoch 193/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9487 - recall: 0.9488 - f1: 0.9487\n",
      "Epoch 00193: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 441ms/step - loss: 0.1228 - accuracy: 0.9485 - recall: 0.9486 - f1: 0.9485 - val_loss: 0.1831 - val_accuracy: 0.9242 - val_recall: 0.9241 - val_f1: 0.9242\n",
      "Epoch 194/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1167 - accuracy: 0.9513 - recall: 0.9513 - f1: 0.9513\n",
      "Epoch 00194: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 439ms/step - loss: 0.1167 - accuracy: 0.9513 - recall: 0.9513 - f1: 0.9513 - val_loss: 0.1847 - val_accuracy: 0.9234 - val_recall: 0.9233 - val_f1: 0.9234\n",
      "Epoch 195/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9520 - recall: 0.9521 - f1: 0.9521\n",
      "Epoch 00195: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 440ms/step - loss: 0.1219 - accuracy: 0.9520 - recall: 0.9521 - f1: 0.9520 - val_loss: 0.1862 - val_accuracy: 0.9250 - val_recall: 0.9251 - val_f1: 0.9250\n",
      "Epoch 196/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 0.9486 - recall: 0.9485 - f1: 0.9486\n",
      "Epoch 00196: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00196: ReduceLROnPlateau reducing learning rate to 5.820766367818295e-14.\n",
      "150/150 [==============================] - 67s 445ms/step - loss: 0.1265 - accuracy: 0.9486 - recall: 0.9485 - f1: 0.9486 - val_loss: 0.1796 - val_accuracy: 0.9247 - val_recall: 0.9247 - val_f1: 0.9247\n",
      "Epoch 197/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 0.9478 - recall: 0.9477 - f1: 0.9478\n",
      "Epoch 00197: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 438ms/step - loss: 0.1269 - accuracy: 0.9478 - recall: 0.9477 - f1: 0.9478 - val_loss: 0.1844 - val_accuracy: 0.9248 - val_recall: 0.9248 - val_f1: 0.9248\n",
      "Epoch 198/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1289 - accuracy: 0.9480 - recall: 0.9479 - f1: 0.9480\n",
      "Epoch 00198: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1286 - accuracy: 0.9482 - recall: 0.9481 - f1: 0.9482 - val_loss: 0.1828 - val_accuracy: 0.9272 - val_recall: 0.9271 - val_f1: 0.9272\n",
      "Epoch 199/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9489 - recall: 0.9488 - f1: 0.9489\n",
      "Epoch 00199: val_loss did not improve from 0.17313\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1225 - accuracy: 0.9492 - recall: 0.9491 - f1: 0.9492 - val_loss: 0.1799 - val_accuracy: 0.9246 - val_recall: 0.9247 - val_f1: 0.9246\n",
      "Epoch 200/200\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 0.9531 - recall: 0.9533 - f1: 0.9531\n",
      "Epoch 00200: val_loss did not improve from 0.17313\n",
      "\n",
      "Epoch 00200: ReduceLROnPlateau reducing learning rate to 2.9103831839091474e-14.\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1171 - accuracy: 0.9533 - recall: 0.9535 - f1: 0.9533 - val_loss: 0.1827 - val_accuracy: 0.9245 - val_recall: 0.9245 - val_f1: 0.9245\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "batch_normalization = True\n",
    "EPOCHS = 200\n",
    "STEPS_PER_EPOCH = 150\n",
    "activation = \"relu\"\n",
    "model = cnn_model(\n",
    "    shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "# Train the model with batch\n",
    "history = model.train()\n",
    "# model.save(\"batch_relu_validation_200.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd3wUdfoH8M+U7bvpJIEQpIQqIlUUOEGIiFhPUVTgECwgZ8MT8bw7vVNELHieXdET5RB7BVFAfgoiKBBQAYVEAoIkhPSybcrz+2Oyk2zqJpAQzfN+vaLs7JRnZ2fnmW+bEYiIwBhjjAEQT3YAjDHG2g5OCowxxkycFBhjjJk4KTDGGDNxUmCMMWbipMAYY8zESaGN+OmnnyAIArZt29ak5ZKTk/HYY4+1UFTt1/PPPw+3232yw2Cs1XFSiJAgCA3+de3a9bjW37NnT+Tk5GDgwIFNWu6HH37AnDlzjmvbkeIEVLevvvoKkiRhxIgRJzuU373k5GTzN2ez2dCpUydMmDABr7zyCjRNa9K6srKyIAgCtmzZ0kLR1m/dunUQBAG5ubmtvu3GcFKIUE5Ojvn34YcfAgC+/fZbc9rWrVvrXC4YDEa0fkmSkJycDFmWmxRXhw4d4HQ6m7QMO7FefPFF3HLLLdi1axd27dp1ssMBEPlx91t07733IicnBz///DM+/PBDjBo1CrfddhvGjx+PQCBwssP77SPWZBs3biQAlJ2dXeu9pKQk+uc//0k33HADxcbG0qhRo4iI6NFHH6XTTjuNnE4ndezYkaZMmUJHjx41l/vxxx8JAG3dujXs9bvvvksTJkwgh8NBPXr0oDfeeKPW9h599NGw1wsWLKA5c+ZQdHQ0JSUl0d13302appnzlJeX04wZM8jj8VBsbCzdcsstdMcdd9Cpp57a4Oeuua2adu3aReeddx45nU5yu910ySWXhO2jwsJCmjp1KiUmJpLNZqMuXbrQ3Xffbb6/fv16OvPMM8nlcpHH46GBAwfS+vXr693evn376JJLLqGkpCRyOBw0YMCAWvtn+PDhNGfOHPrHP/5BHTp0oLi4OLr++uvJ6/Wa86iqSvPnz6f4+Hhyu900ZcoUevjhh8nlcjW4P0KfyeFw0N69e2nGjBl0yy231JqnpKSE/vznP1OnTp3IarVSt27dwvbjkSNHaNq0adShQwey2WzUu3dvWrZsGRERrV69mgDQsWPHzPkVRSEAtGLFCiKqOlbeeOMNOvfcc8nhcNDf/vY3CgaDNHPmTOrWrRvZ7Xbq3r073XvvvRQMBsPiW716NY0YMYIcDgdFR0fTmDFj6ODBg/TJJ5+QxWKh3NzcsPmff/55iomJCduHNS1ZsoR69epFFouFOnfuTPfdd1/YMRjJ91KX+o7Bb775hkRRpIcfftictnTpUho6dCh5PB5KSEigiy66iLKysoiIyOfzEYCwv969exNRZMdVY8fqr7/+SlOmTKH4+HjyeDw0atQo2rRpU9j3Vf3vvPPOa/BztyZOCs3QWFLweDy0YMEC2rdvH+3Zs4eIiBYvXkyff/457d+/n7766isaNmwYjR8/3lyuvqSQlpZG7777LmVmZtLtt99OVquVDhw4ELa9mkkhNjaWHnvsMdq3bx+99tprJIoivf766+Y8N9xwA3Xq1IlWrVpFP/74I91xxx0UFRV1XEmhrKyMOnbsSBMmTKCMjAz69ttvaeTIkdS3b19SFMXc7pAhQ+jbb7+lAwcO0MaNG+nll18mIiK/309ut5vmz59PmZmZtHfvXnrnnXfo66+/rjee7du303PPPUfff/89ZWVl0eLFi0kURfPHR2ScfKKjo+muu+6in376iVauXElut5sWLlxozrNo0SLyeDz0v//9j/bu3UsLFiygqKioiJLCv//9bzrrrLOIiOjLL7+sdbLUNI3OOuss6tmzJ3388cf0888/0/r1683PXVZWRj169KBhw4bR559/Tj///DN98skn9NZbbxFR05JCly5daMWKFbR//37Kzs4mn89H9957L3377beUnZ1N7733HiUkJIR99lWrVpEoinTnnXfSd999R7t376YXXniBsrKySNM06tq1Ky1atCjsMw8dOpRuvvnmevfJO++8Q5Ikmcfg8uXLKSoqihYsWNCk76UuDR2D48aNoyFDhpivX3zxRVq1ahVlZWXRtm3baMKECdSvXz/zeNy8eTMBoFWrVlFOTo65jxs7rho7VsvKyigtLY2uuuoq2r59O+3bt4/uvfdestvtlJWVRaqq0ltvvUUA6Pvvv6ecnBwqLCxs8HO3Jk4KzdBYUpg4cWKj6/j6668JAOXn5xNR/UnhmWeeMZcJBAJktVpp6dKlYdurmRSuuOKKsG2NHj2arr32WiIyrmxlWab//e9/YfMMHDjwuJLC008/TR6Ph4qKisxphw4dIovFQm+++SYREY0fP55mzZpV5/JHjhwhALR58+YGY2jM+PHjw05Yw4cPp2HDhoXNM336dBozZoz5OiEhge6///6weS644IKIkkLfvn3p+eefN1/36NGDXn31VfP1ypUrzR9/XZ5++mlyuVy1rsZDmpIUHnnkkUbjXbhwIfXv3998PXToULr88svrnf/BBx+ktLQ00nWdiIh27tzZ4OcJrXPatGlh0xYtWkRut9ssLUTyvdSloWPwtttuo9jY2HqXDR1j27ZtIyKizMzMiI+56sdVY8fqc889R926dQsrGRERnXXWWTR//nwiIlq7di0BoJycnEa33dq4TaEFnHHGGbWmrVu3Dueeey5SU1Ph8XiQnp4OADh48GCD66re8Gy1WpGQkICjR49GvAwApKSkmMvs27cPqqrizDPPDJun5uum2r17NwYMGICYmBhzWufOndG9e3fs3r0bAHDzzTfjtddew+mnn4477rgDa9asAVXej7Fjx46YOnUqxowZgwsuuACPPPIIsrKyGtxmeXk55s2bh379+iE2NhZutxvr16+vtU8b2h95eXnIz8+v1Ug8atSoRj/zhg0bsH//fkyePNmc9qc//Qkvvvii+Xr79u3o2LEjTjvttDrXsX37dgwYMABJSUmNbq8xdR13zz77LIYNG4bExES43W7861//MvcPEWHHjh0YP358veucOXMmDh48iC+++AIAsGTJEgwfPrzezwMAe/bswdlnnx02bfTo0SgvLw/7bhr6XpqDiCAIgvl6+/btuOSSS9C1a1d4PB707NkTQOO/ucaOq8aO1a1bt+KXX35BVFQU3G63+bd161ZkZmY2+/O1Fk4KLcDlcoW9zsrKwoUXXojevXvjzTffxLZt2/D2228DaLxB0Gq1hr0WBAG6rh/3MtV/PCdKXeus/kO96KKL8Msvv+Cuu+5CaWkpJk+ejPPOO8+MbdmyZfj2229xzjnn4PPPP0e/fv2wdOnSerd322234e2338b999+PL774Ajt37sS4ceNq7dOG9kcoKTVnf7z44osIBAJISEiALMuQZRn/+te/sGnTJuzZs6fB/VIznvqIohgWJwAoilLnvDWPu2XLluGOO+7AtGnTsHr1auzYsQPz58+vtX8a2n5ycjIuueQSLFmyBD6fD8uXL8eNN97Y4Oepa5117efmHNsN2bVrF3r06AEAKCkpwbnnngu73Y5XX30VW7duxddffw2g8d9cJMdVQ8eqrusYOHAgdu7cGfb3448/4umnn27252stnBRawTfffANFUfDEE09gxIgR6N2790nritarVy/IsozNmzeHTT/ebnmnnnoqvvvuOxQXF5vTDh8+jOzsbJx66qnmtISEBEyZMgUvvfQS3n//faxduxY///yz+f6AAQNw55134rPPPsM111yDJUuW1LvNDRs2YPr06Zg0aRJOP/10dO3atclXYklJSYiPj8emTZvCptd8XVNBQQHeeecdLFmyJOyH/91332HkyJFmaWHIkCE4cuQIfvjhhzrXM2TIEHz33Xf1XiEnJiYCAI4cOWJOy8jIiOizbdiwAcOHD8ett96KIUOGoGfPnsjOzjbfFwQBgwYNwmeffdbgembNmoX33nsPL7zwAnRdDysZ1aVfv3748ssva8Xi8XjQpUuXiGJvqm+++QZffPGFGduuXbtQVFSERYsWYfTo0ejTpw/y8/PDlgklpZpdWSM9ruo7VocOHYrMzEzExcUhLS0t7K9jx44Nbrst4KTQCnr16gVd1/Hvf/8b2dnZePfdd/HQQw+dlFhiY2MxY8YMzJ8/H6tXr8bevXsxb948ZGdnR3S1fOTIkVpXQL/++iumT58Ot9uNq6++Gjt27MDWrVtx1VVXIS0tDX/84x8BAPPnz8cHH3yAffv2Ye/evVixYgWioqKQkpKCPXv24J577sGmTZtw8OBBbNq0CZs3b0a/fv3qjaV379547733sH37duzevRszZ86s9cOPxF/+8hc89thjWLFiBTIzM7Fo0SJs2LChwWVeffVVOBwO/OlPf0L//v3D/q655hq89tpr8Pv9mDBhAs444wxcfvnlWLlyJbKzs7Fx40a88sorAIzqpsTERFx00UVYv349srOzsXbtWrzzzjsAgL59+6JTp0649957sXfvXnz55Ze46667IvpcvXv3RkZGBlatWoWsrCw89thjWLlyZdg89957L9577z3MmzcPP/zwA3766Se8/PLLYYl63LhxSE1Nxfz583HNNdfUKpHU9Ne//hWvv/46Fi9ejMzMTLz++utYuHAh5s+fb5Z8jkdZWRlyc3Nx+PBhbN26FQsWLMC5556LcePG4eabbwYAdOvWDRaLBU8++ST279+PNWvWYN68eWHrSU5Oht1ux2effYajR4+aFzSNHVeNHavTp09HcnIyLrjgAqxbtw4HDhzAli1bsGDBAqxatQoAzHFNq1atQl5eHkpLS497v5wwJ7E94zersYbmuhrCHn/8cUpJSSG73U6jR4+mjz/+OKyxqr6G5tDrkJSUFHrooYfq3V5d258yZUpYl7fy8nK69tprye12U0xMDN1yyy1000030dChQxv83ElJSbW60gGg2267jYiMLqnjx483u6RefPHFYfvo73//O/Xr14+cTidFR0fTOeecY37+X375hS655BKz22anTp1o9uzZVFpaWm88+/fvp7Fjx5rdfB944IFan3X48OH05z//OWy5v/3tb2b3QyKjS+qdd95JcXFx5HK5aPLkybRo0aIGG5p79+5tNt7XdPToUZIkyexWWlRURLNnz6akpCSyWq3UvXt3Wrx4sTn/4cOH6eqrr6a4uDiy2WzUp0+fsI4AGzdupNNPP53sdjsNHDjQPP5qNjTXPFb8fj/NmDGDYmJiKCoqiqZNm0aLFy8mm80WNt/HH39Mw4YNI5vNRtHR0TR27Fg6ePBg2DyLFi0iAJSRkVHvPqmuri6pqqqa70fyvdSl+jFosVgoOTmZzjvvPHrllVdqNey+/vrr1L17d7LZbDRkyBD68ssvw/ZbKM5TTjmFJEkyt93YcRXJsZqXl0fXX389JScnk8VioZSUFLr88svDGugfeOAB6tixIwmC0Ka6pApE/OQ1BowYMQLdunXD8uXLT3YorA269dZbsXnz5noHabLfj6YNn2W/Czt27MDu3bsxfPhw+P1+/Pe//8XmzZvx4IMPnuzQWBtTUlKCHTt24JVXXmmwfYf9fnBSaKeefPJJ/PTTTwCMeutVq1bhnHPOOclRsbbmvPPOw/fff4+pU6c22sDMfh+4+ogxxpiJex8xxhgzcVJgjDFm+s23KVQf1NMUCQkJzerT3hraamwcV9O01biAthsbx9U0zY2rU6dO9b7HJQXGGGMmTgqMMcZMnBQYY4yZOCkwxhgzcVJgjDFm4qTAGGPMxEmBMcaYiZNCpawCPzILfCc7DMYYO6k4KVR6bWcelu44drLDYIyxk6pVRjTn5+fjmWeeQXFxMQRBQHp6OiZOnBg2z+7du/HII4+Yjx8cPnw4Jk2a1BrhAQAUjaDzrQEZY+1cqyQFSZIwbdo0dO/eHT6fD3fffTcGDBiAzp07h83Xt29f3H333a0RUi3G45w4KzDG2rdWqT6KjY1F9+7dAQAOhwMpKSkoLCxsjU1HTCcuKTDGWKvfEC8vLw/Z2dlIS0ur9d6+ffswb948xMbGYtq0aUhNTa01z7p167Bu3ToAwKJFi5CQkNCsOGRZDltWkg4DhGav70SqGVtbwXE1TVuNC2i7sXFcTdMScbXqQ3b8fj/uu+8+XHbZZRg+fHjYe16vF6Iowm63IyMjA0uXLsWTTz7Z6DpP1F1S71h9ADoRnpjYrVnrO5F+b3dkbGkcV9O11dg4rqb5Td8lVVVVLF68GH/4wx9qJQQAcDqdsNvtAIDBgwdD0zSUlpa2VnggIvAz6Bhj7V2rJAUiwvPPP4+UlBRceOGFdc5TXFyMUKElKysLuq7D4/G0RngAAJ2MdgXGGGvPWqVNYe/evdiwYQO6dOmCefPmAQCuvvpqs9gzfvx4bNmyBWvWrIEkSbBarbj99tshCEJrhAcAIAI3NDPG2r1WSQp9+vTBW2+91eA8EyZMwIQJE1ojnDpp3PuIMcZ4RHMIj1NgjDFOCiYep8AYY5wUTERAK/bOZYyxNomTQiWdCBrnBMZYO8dJoZJO4HEKjLF2j5NCJR6nwBhjnBRMPKKZMcY4KZi4pMAYY5wUTHrlH2OMtWecFCrpRNA5KzDG2jlOCpWIeEQzY4xxUqjEI5oZY4yTgknnu6QyxhgnhRDufcQYY5wUTDxOgTHGOCmYdKq8fTZnBsZYO8ZJAZWlhMp/c7sCY6w946SA8ETAOYEx1p5xUkB4IuDGZsZYe8ZJAeGJgHMCY6w946SA8ESgcVZgjLVjnBRQo02BcwJjrB3jpIDw6iPufcQYa884KSC8dMDjFBhj7RknBXBJgTHGQjgpIPzhOvxIBcZYe8ZJAeGlAx6nwBhrzzgpILwdgXMCY6w946QALikwxlgIJwVwQzNjjIVwUkDNksLJi4Mxxk42uTU2kp+fj2eeeQbFxcUQBAHp6emYOHFi2DxEhFdeeQU7duyAzWbDnDlz0L1799YIj8cpMMZYpVZJCpIkYdq0aejevTt8Ph/uvvtuDBgwAJ07dzbn2bFjB3Jzc/Hkk08iMzMTL730EhYuXNga4XH1EWOMVWqV6qPY2Fjzqt/hcCAlJQWFhYVh82zbtg1nn302BEFAr169UFFRgaKiotYIL3ycApcUGGPtWKu3KeTl5SE7OxtpaWlh0wsLC5GQkGC+jo+Pr5U4Woquc0mBMcaAVqo+CvH7/Vi8eDGuvfZaOJ3OsPfqqssXBKHWtHXr1mHdunUAgEWLFoUlkqaQZdlctojKzenRMTFISHA3a50nSvXY2hKOq2naalxA242N42qaloir1ZKCqqpYvHgx/vCHP2D48OG13o+Pj0d+fr75uqCgALGxsbXmS09PR3p6uvm6+jJNkZCQYC5bWOQ3pxcWFSFf9Ne3WKuoHltbwnE1TVuNC2i7sXFcTdPcuDp16lTve61SfUREeP7555GSkoILL7ywznmGDh2KDRs2gIiwb98+OJ3OOpNCS+CGZsYYM7RKSWHv3r3YsGEDunTpgnnz5gEArr76ajPDjR8/HoMGDUJGRgZuvfVWWK1WzJkzpzVCA8AjmhljLKRVkkKfPn3w1ltvNTiPIAi4/vrrWyOcWoifvMYYYwB4RDMArj5ijLEQTgoILx1w9RFjrD3jpABA45ICY4wB4KQAAKB6/s0YY+0NJwXU6H3ERQXGWDvGSQE1nrx2EuNgjLGTjZMCeJwCY4yFcFIAd0lljLEQTgoITwRcUGCMtWecFMDjFBhjLCTipFBWVtaScZxUXH3EGGOGiO99dNNNN2HAgAE4++yzMXToUMhyqz6KoUVxQzNjjBkiLik8++yz6N+/Pz788EPccMMNeOGFF/DTTz+1ZGytRucuqYwxBqAJJYWoqChMnDgREydOxJEjR7BhwwY89dRTEAQBf/jDHzB27Fh06NChJWNtMdUTAVcfMcbas2Y1NBcXF6O4uBg+nw9JSUkoLCzEXXfdhQ8++OBEx9cquPqIMcYMEZcUDh06hI0bN2Ljxo2w2+0YPXo0HnvsMcTFxQEALr/8csybNw+XXnppiwXbUrihmTHGDBEnhfvuuw8jR47EX/7yF6SlpdV6PzExERMnTjyhwbUWHqfAGGOGiJPCiy++2GiPo8mTJx93QCcDj1NgjDFDxG0Kr732Gvbu3Rs2be/evVi6dOmJjqnVcfURY4wZIk4KmzZtQo8ePcKmde/eHV999dUJD6q1cUMzY4wZIk4KgiBA1/Wwabquh912+reKwOMUGGMMaEJS6NOnD9544w0zMei6jrfffht9+vRpseBaC5cUGGPMEHFD84wZM7Bo0SLMmjULCQkJyM/PR2xsLObPn9+S8bWK8IbmkxcHY4ydbBEnhfj4eDz88MPIyspCQUEB4uPjkZaWBlH87d9oVeOGZsYYA9CEpAAAoiiiV69eLRXLSUNh4xQ4KzDG2q+Ik4LX68Xbb7+NPXv2oKysLOzk+dxzz7VIcK2Fu6Qyxpgh4rqfl156CdnZ2Zg0aRLKy8sxc+ZMJCQk4IILLmjJ+FoF8YhmxhgD0ISk8P333+Mvf/kLhg0bBlEUMWzYMMydOxcbN25syfhaRfXSgcZZgTHWjkWcFIgITqcTAGC321FRUYGYmBjk5ua2WHCtRa8cnSAKXFJgjLVvEbcpnHLKKdizZw9OO+009OnTBy+//DLsdjs6duzYkvG1Cl03EoIo8DgFxlj7FnFJYdasWeZDdGbOnAmr1YqKigrcfPPNLRZcayGEkoLAI5oZY+1aRCUFXdfxxRdf4LLLLgNgPIVt9uzZLRpYa9KJIECAAO59xBhr3yJKCqIo4rPPPsMVV1zRrI08++yzyMjIQHR0NBYvXlzr/d27d+ORRx5BYmIiAGD48OGYNGlSs7bVHDpVlRS4+ogx1p5F3KYwevRorF27Fuedd16TNzJmzBhMmDABzzzzTL3z9O3bF3fffXeT130i6EQQBKGyTeGkhMAYY21CxEkhKysLn376KT766CPEx8dDEATzvX/9618NLtuvXz/k5eU1P8oWRgRIQqj3EWcFxlj7FXFSGDduHMaNG9digezbtw/z5s1DbGwspk2bhtTU1DrnW7duHdatWwcAWLRoERISEpq1PVmWzWWt9hKIYhkkEbDa7M1e54lSPba2hONqmrYaF9B2Y+O4mqYl4oo4KYwZM+aEbri6bt264dlnn4XdbkdGRgYeffRRPPnkk3XOm56ejvT0dPN1fn5+s7YZutMrAHi9PggggACvz9/sdZ4o1WNrSziupmmrcQFtNzaOq2maG1enTp3qfS/ipLB+/fp63xs7dmzTIqohNCgOAAYPHoyXX34ZpaWliIqKOq71RkonQBCMBwkRd0pljLVjESeFmrezKC4uRm5uLvr06XPcSaG4uBjR0dEQBAFZWVnQdR0ej+e41tkUBILIDc2MMRZ5UrjvvvtqTVu/fj1+/fXXRpd94oknzLurzp49G1deeSVUVQUAjB8/Hlu2bMGaNWsgSRKsVituv/32sIbslrI0Iw/Rdsnokgrjj5MCY6w9a9LzFGoaM2YMrrvuOkybNq3B+W6//fYG358wYQImTJhwPKE0S8aRCsQ7ZcQ4ZGOcgsjjFBhj7VvESSH0bOaQYDCIDRs2wOVynfCgWoui61CJzHEKAviGeIyx9i3ipHD11VfXmhYXF4dZs2ad0IBak6IRVI1AxDfEY4wxoAlJ4emnnw57bbPZWq13UEtRdIKqGyWFqttcnOyoGGPs5Ik4KYQagd1utzmtvLwcwWAQcXFxLRJcS1O0UFIwEoLAvY8YY+1cxLfOfvTRR1FYWBg2rbCwEI899tgJD6q1KDpB06vGKYg8ToEx1s5FnBSOHDmCLl26hE3r0qVLRF1S2yIigqIRFJ14nAJjjFWKOClERUXVevRmbm5uqw4yO5E03SgTVFUf8Q3xGGMs4jaFc845B4sXL8ZVV12FpKQk5Obm4s033zzu0cwnS1AzTv6qTqDKhmYB3NDMGGvfIk4Kl156KWRZxrJly1BQUICEhAScc845uPDCC1syvhajaMa4C62ypCCAq48YYyzipCCKIi6++GJcfPHFLRlPqwlWJoXq1UcCP3mNMdbORdym8MEHHyArKytsWlZWFj788MMTHlRrCFUfKXrViGZJ4BHNjLH2LeKk8Mknn6Bz585h0zp37oxPPvnkhAfVGpRqJYXQk9cEHtHMGGvnIk4KqqpClsNrm2RZRjAYPOFBtYZQ9ZFOgEZkjlPgNgXGWHsWcVLo3r07Pvvss7Bpa9asQffu3U94UK1B0arO/kGNeEQzY4yhCQ3N06dPx4IFC7BhwwYkJSXh6NGjKC4uxj/+8Y+WjK/FBNWqu74GNYLDEhrRrDewFGOM/b5FnBRSU1Pxn//8B9u3b0dBQQGGDx+OIUOGwG63t2R8LSZUfQQY7QuiIPNDdhhj7V6THrJjt9sxcuRI8/WhQ4fw5ZdfYurUqSc8sJamaOElBRHgcQqMsXavyU9eKy0txVdffYUNGzYgOzsbgwYNaom4Wlz1NgVFIx6nwBhjiDApqKqK7du348svv8TOnTsRHx+PoqIiPPTQQ7/ZhuZgjZKCUHlDPM4JjLH2rNGk8PLLL+Prr7+GJEk488wz8c9//hO9evXCjTfeiPj4+NaIsUVUrz5SdJ2fvMYYY4ggKaxZswZutxtXXHEFRo4cCafT2RpxtbigWnXyV3Xwk9cYYwwRJIWnnnoKGzZswEcffYSlS5di0KBBGDVq1G/+FtPVq48Aoz2Bxykwxtq7RgevJSYmYtKkSXjqqafw97//HW63G88//zxKS0uxYsUKHD58uDXiPOGUGklB5CevMcZY5COaAaBv376YPXs2XnzxRdxyyy0oKCjAvHnzWiq2FlW99xEAiBB4nAJjrN1rtProjTfewKBBg9CrVy8IggAAsFqtGDVqFEaNGlXruc2/FTWrj0QBEEV+8hpjrH1rNCnYbDYsX74cOTk5OO200zBo0CAMHDjQfAxnXFxciwfZEmq3KRgP2tE4JzDG2rFGk8If//hH/PGPf0RFRQW++7XIg18AACAASURBVO47ZGRkYNmyZUhMTMSgQYMwaNCg3+RYhVrVR5UNzVxQYIy1ZxGPaHa5XBgxYgRGjBgBIkJWVhZ27NiBJUuWoLCwENOnT8eIESNaMtYTqq6SgiQIXH3EGGvXmnybC8DovtmzZ0/07NkTV155JUpKSuD1ek90bC2qZu8jibukMsZY5L2PVq5ciQMHDgAA9u3bh5tuugk333wz9u3bh+joaHTs2LGlYmwRikawy4L5WgiNaD6JMTHG2MkWcVJYtWoVEhMTAQArVqzAhRdeiMsuuwxLly5tqdhaVFDT4ZCrPj7fEI8xxpqQFLxeL5xOJ3w+Hw4cOIDzzz8fY8eOxZEjR1oyvhYTVHU4LJL5urVuiLft13J8e7isZTfCGGPNFHGbQnx8PPbu3YtDhw6hb9++EEURXq8Xoth4Xnn22WeRkZGB6OhoLF68uNb7RIRXXnkFO3bsgM1mw5w5c1q8R5Oi6XBYwksKxuC1ls0K7+0pgKoDZ3T2tOh2GGOsOSIuKUydOhWPP/443n//fUyaNAkAkJGRgbS0tEaXHTNmDO65555639+xYwdyc3Px5JNP4sYbb8RLL70UaVjNFtQIjmptCsZDdlr+hnhBjRBQueWCMdY2RVxSGDx4MF544YWwaWeeeSbOPPPMRpft168f8vLy6n1/27ZtOPvssyEIAnr16oWKigoUFRUhNjY20vCaTNF0RIWVFFqn95GqE/ycFBhjbVTESeHw4cNwu92IiYmB3+/HRx99BFEUcdFFF0GWm9Wz1VRYWIiEhATzdXx8PAoLC+tMCuvWrcO6desAAIsWLQpbrikU/Re4nQ7IoheqTnA5ncZtLlDU7HVGgoSDUIga3IYsyy0aQ3NxXE3TVuMC2m5sHFfTtERcEZ/N//Of/2Du3LmIiYnBa6+9hpycHFgsFvPmeMejrgFjofss1ZSeno709HTzdX5+frO2GVB16KoCSQBUAH6/z+iSqlOz1xkJf1CFL6g1uI2EhIQWjaG5OK6maatxAW03No6raZobV6dOnep9L+KkcOzYMXTq1AlEhK1bt2Lx4sWwWq24+eabmxxQTfHx8WEfrKCgoEWrjgCj+sgqCZAlAYHKZzSLAkAwklR9Sen4t2tUH7XkNtoSXSMEgwQiwG4XIIj1f2YigqoQBFGAJAGaCgT8OgJ+QiCgw2oTERsvVa4XkOTKbsQaoaxEgyQLEEXA59UhCIDNLsLuECAIAlTFmMfuFOFwivB5dQQDoWo8ARarALtDgK4DmmrEoSgEQQAcThEWq7EeIoKmAUqQoAQJssXYjiQJIJ2gqoBsMXqx+f0alKAOQTTiEgSY6w+tr+pzA6pKsFqNdq2KUg1WuwiH05gvGDD2g2wxYgWAijINSpCg68Z6rVYB7igRomRUhYb+NBUIBqly/wOqAgT8mvn5ZFmo9T34vARvuTFPdKwMh1MAEVBeqkPXCTa7aHwmERBgrF8UBXN/axrB59VBZNxoEgC85TpUlRAVLUGUBKgqQVOMC0JBFCBKQFSUbh43ZaU6vBUa3B4JsqVyHwQImmr8dqJiJOg6oSBPRVwHGW6PCF+FDlU14nI4Reia8T2Kld+BJAmQZOP7URUy96WmonI+wGozpilBQiBAsNtFc7+QblQxk16130kn6GQck6pKcHlEWK0CvBU6ZFmArdrySpAgScZnrf79a6pxPANAwE9QVWO+0P6sTlMJxUUabDYBLVF4iTgpWCwW+Hw+HD58GPHx8YiKioKmaVAU5biDGDp0KD799FOMHDkSmZmZcDqdrZIULKIAufIkFRqnABhfutRC52ul8gBSdaBaj9gTjsg4GQcDBLtDhCxXHfiqEjr5Gj/U3CMKLBYBbo9k/MJR9aNRFONHCAAORx68FX54vTpEEbA7RPM9STZOzrpunIgUheCt0FFepiP0iApBqDqRVz9pCQKgVsZmzisCVEfTiyQDmgaAjOVkiwBdK4FWz50MZRmAYJwIQ2x2AQF/0xqPJMn4jIpCdcYlW4z9GzoJ6joAlNS7PtkCWCxC5fdR/3YFEZBEY/8ct8o7AetaeGw2u3Hi8vuME76uheKvtqhQldQaIoqAKFXG26z2uTJYrAKUYNMXDu3PxlR9P8b+BWocawJqxS6IJXV+7/Wx2gQEA8ZK7A7jRxUIVB07oWQsWyqP/SBVJgrjOAqRLcZ8QuX5SdOM3zQR0L2XDV1boJNmxElh5MiRuP/+++Hz+TBhwgQAQHZ2tjmgrSFPPPEE9uzZg7KyMsyePRtXXnkl1MqjfPz48Rg0aBAyMjJw6623wmq1Ys6cOc38OJELagRZqkoKoXEKQDOP5QiFbsQXUHVYpPqzglZ59RvwEyAAMbESdB0oLlSRf1SFqhAkWYDdIUKSAAiA02VcsZaXasjOCsJbHtlRbLUZB5tWx4kndDIEAFHUQKQbV2A6UFyomAesUhmPcftx4wfqcovo2NkCu8P45fm8OjTNuLIiqhwTQkY3YFk2roAtVuNqXQkaV802u1h50hJQUa6jIE9FUCmCKImw22KgKgSn046KQA40VYPV4kBcfDQEQYDfp6O8VDM/oy6UoKiwHN5yHV27xMDllqHpOiyyBaRbEAwY+1KSAI2CcDgs0HVCSVE5Skq9IF1AbEwCdPigqhWw2mQ47NFQFAmlJeWw2y1wOKwoLimD1SIjMSkRFRUVIN34PoNBP2TZAqtNRkWZBlXVYbUaV8GSTCgrPwZdk2C1OREf70BxcTnKSr0AyYiOccPuEJGXlwMiCXZ7FGJj7YCgQlUDcDqdUFUR3nIdkmiFrhPKy4thszlhs9lgsQqoKFehKApsNgEujxt+nx8gCSXFAVRUlMIdQ3C7omCxWuByi3C5RUiygJJCDT6fcdUfFSNBkoBgwLgAIAJ0XYckadB1EX4fUFB4BIQgTunSFbJFht/nw7GCI+iQEIu4+HiUlaggEoyETkFYLFaABPj9fkC3orREhd0hw+kCJEsQqmIF6QIsVqDCV4RgoByACG+5ALvdiS5d43HkkA+lJV7EJdihqOUIBBRY5Rg4nI7KZKEAkKBrRqlGkgBB1OCtCEDTVEgyISYmGkQiAn7dOB9IGoLBUlgsUbBZ3CgtL4QkybDIMgANOnTIkgir1QIigj9QBp+/DJriBHQ3nB4Ffl8AFWU6XM4o2BwSbHYRiqLC5/MjGAhC1yVYLVa4o6xQAgIIgMttlExVhVBeqkFVAU1TEQh6YbPa4XLbEBMnm6XmEy3ipHDttdfiu+++gyRJ6N+/PwDjRDp9+vRGl7399tsbfF8QBFx//fWRhnJC1FVSEBEqKRDMS+YmKilSUXBMgywDVpsIIkJejmoWVUdRFIIi4eDPAbisEnxeHWUlGlTFuLL3+wiCUAolqNc7kE6qXLeqUL1XVLHxErql2WG1GVeAqkqwWIwfomwRIMuCefKOSzBKCKErG6DqKlwUjSqTLVu2wG63IyUlBUVFRbBarUhNTa3VyUDTjPaSLVu2wFfow1ldzsKuXbtw7NgxjBo1Cp2TkhAMBs1SZ25uLnRdh8/nQzAYRFxcHPx+v9n7zG63IxgMIhgMQtd1aJqG0tJSAEDXrl0hSRJyc3NRUVFhxmCz2SqvqjRIkgSLxQJFUYyTTqXd+2rvM1mWYbPZEAwG6y0Bi6IIvdrlsiiKsNls8Pl8td63Wq1VpU9dh6IosFgsSE5OxrFjx+D3++F0OuFyuVBWVhYW3/FITk6G1+s195PdbocoivD5fLXa7+x2e9h2JUlCUlISPB4PysrKUFZWBofDASKCrutwOp0IBoMoLy+HLMvmd6eqKgRBgN1uN/fFj/ussFgs8Hq95nYlSYKmaWFVJ7Isw+FwoKysLCyu0HcuiiLsdjtUVUUwGKz1ee1b7fXuO5fLBavViuLiYkiShOjoaDgcDpSWlpr7JyT03cTHxyMvLw9Hjx6FpmmwWq1wuVwoKiqqcxvVYw2peZxYLBbYbDb4/X7zgrgmWZZhsVhgsVgQCATCjsHq65JlGaIoYuDAgUjpfGGd6zoeAjXxtqD5+fkoLCxEXFxcm2iNb86IaiLCpa/vxeTT4vHVwTL8WhrEtNM7QBSAV3cew1uTe8Em1z2Eo7xUQ+6vCnTdKIaqKpl138VFGirKal+dyzIqrxAIuf4g7BDhFIwsLwiA2yPCYhNgsRhX/k6nA4rqR0ycZFTRaEaykSQBnmgJsXFGnSxgXIGSbjSQeyt06DoqrwRrX0UUFBSgsLAQUVFRSEpKqlxeQ2ZmJmw2G1JSUmC1Wmstt2nTJmzfvr2OzyXD6XQiKSkJp556KjIyMvDLL78AMH4okiShoqICoigiOjq61o9KEAQkJCTAYrHAbrfDYrGgoKAAVqsV8fHxKCoqgqIosFqNk0voRJKamgqfz4cffvgBNpsNqamp6NixI2w2G8rKysz2KVmWoaqqecJKSUlBdHQ0dF1HaWlp5RWuhGAwiEAgYP7Jsozo6Gjzx+tyucyTYV5eHjweD6Kjo6FpGo4ePYry8nIkJSVBVVX4fD7ExsZCURQoimLeKFIQBLjdbhQXFyMnJweJiYlwu92oqKhARUUFbDYbevToYQ4K9Xq95nZVVUV5eTlUVTXb9UpLSxEIBGC1WmGz2eD1eqFpGoLBIA4cOACr1YpevXrB5/OhoqICqqrC5XLBbrebn6moqMjcjsfjgSRJOHr0KI4ePYqysjK4XC5ER0fD7/cbV86VsVksFng8HqiqClEU4XA44HA4oCgKSktLccopp8DpdCIrKwtEBLfbjdTUVBQUFJgXFEQEIjKTgdfrRYcOHRATE4Njx47B5/PBarXC7XajvLwcgUAAgiAgOTkZsbGx5gVCSUkJcnNz4fF44HK5oCgKPB6PeSwVFBTA7/ejQ4cOUBQFxcXF8Pl8cLlcSExMNEpRFuNK/+jRo8jJyTF7Q6akpCAhIcF85HDo2TGKokCWZfP4Kikpgd1uR0xMDKKiolBSUoKioiJERUXBbrebx4mmabDZbLDb7eZ2Q9+Zoijm/43SnM3cT4CRTN1uN4LBoFH6JELnzp1xxhlnnPCG5oiTQlFREZ544glkZmbC7XajrKwMvXr1wm233XZSH7TTnKQQ1HRc8cY+TDu9AzYcLMXB4gCmD+wAQQCW7jiGFVf2hNMiQVUIP/3gw9HKK31Vpcr62HCSbDTyRcVI6JBsQXKKBT/8sBNORxQ6deqK6FgJkiRA0QiT3tgLAHjknFNwSqwNVqtgnuBD6utRUFFRgR9++AF+vx99+/Y1T+yAkej27NmDqKgopKamAjCSwKZNm3DmmWeivLwcK1euBGCcoC666CLY7Xb83//9H44dOwYAcDqduPrqq+FyuQAA77//PnJycqCqKvr374/x48fj+++/R0JCArxeL3755Rf4fD7s37/fPHkPGDAAMTEx6NGjBwDg+++/R7du3RAXF4fMzMywA75Dhw5wOp1N/v5q+r31DGkNbTU2jqtpTmrvoyVLluCUU07BX//6V7PIuWLFCixZsgTz589vclAnU6he3xLWpmAMYAOMq+6cw0Hs2emHt0JHUooMu12EbBHgcIpITrHAZjfqvqVqvQhCdF3Htu1bkJCQgP4DqkZ8K9WKgLpMZl17fYLBYNgV8s6dO7F9+3YIgoCKigpccMEFAIBAIIBPPvkEhw4dQlxcHKZOnYr8/Hy899578Pv9OHbsGHRdR0JCAtLT07F+/XqsXLkSuq7D4XDg/PPPhyzLWLVqFTZu3IgJEyaguLgYhw4dQrdu3dClSxecdtppiImJQb9+/cz4unbtCsBIVtnZ2ejWrZuZUEKGDRtm/rt3796NfzmMsZMq4qSwd+9e3HHHHWYdst1ux9SpUzF79uwWC66lKJXDlmVRQKiWSBQEiABOEWzY/qUXpUU63FEiRox1I75D3bupvts+FRUVQVVV5OXlmfXIAKBW6yHjb+S5nxUVFVi2bBlcLhfOOusspKWl4dChQ+jUqRM8Hg8OHz5sdmvds2cPDh06hNTUVBw6dAhlZWVYs2YNJEnCBRdcgDVr1kDTNFx22WWIj4/HxRdfjPXr16Njx4447bTTzCqjYcOG4ZtvvkGfPn1QXFwMADj77LMRHR3dYKwul8tsZ2KM/bY16clrhw8fNq8OAaPq5kQU/08kIoLf74eu6/WOA9AVDTeeHo3e0QJiejhRnGJDWowAeIFeA2MhSTr6DpDgdIsQxSC83tqNWw0JBAIYOXIkACNBlJSUQBAExHVIwo2nGyfYWFmF1+s167fdbrdZr1lcXIyioiKcccYZsFqtKCkpQWFhIXr16oX4+HhIkoSEhASUlpaa9fFjx45FSkoK9u/fj7y8PPTu3dusf73yyivNUkGojnvs2LEAYNa5A8Cpp54Kp9OJ8vJy2O12DBkyBFFRUc36Hhhjv00RJ4WLL74YDzzwAMaOHYsOHTrg2LFj+OKLLzB58uSWjK/J/H4/LBZLg7fekDUdvTvKSHRb4HBq8Ck6EiwWwAb4SUdCjAxZivhegbWoqorU1FQQESwWi3lilUWgd8cYAEC82wK7RURpaanZ2CkIAqKjo6EoClwuF+Lj4+Fyucw6w5SUFMTGxkIQBLM3Q6hRNtSL5ZRTToGmafB4PEhISIAoik1K3GlpaWavjKSkJPj9fjgcjmbvC8bYb0vEZ7709HTMnTsXZWVl2L59O8rKynDzzTejoKCgJeNrMl3XG70XU6hpXYAACQJiBdm414UMlEMzGhiOg6qqZtey0MnebrfD5/VCrBy9ohNQVlaGQCAAl8tVOVJWQ1RUlDly1ul0QpIkWK1WKIpiJgNZliEIgtljgYjM7o+hqiCbzRbRbc1rCvUaAoyG5+pd4Rhjv39NupNd//79w+qOFUXBwoUL21RpIZJbR4Rq8wUCbKoIAiDYAEgAlMrh7ASzPSDU510UxUbXT0RQVRUOh8M8cTscDjidTvj9fsikIihYjX7rAWPQkdvthsPhgK7rsFgscDgcZtdIAHA4HLUanWVZhqIoZlyhdotQn/nmXt0LggCPxwOv1wur1QpNq6O7FWPsd+v4bm/6GxXqhatVDtYqIRVxkhw2XK2srAw+nw8ejwdWqxWFhYUQBAEOhwMulyssOQQCAZSXl8PlckEURbPaSJIkBCpP/JIkQZJkSLoKwApNCZj9tAFUvm9codcs6dhsNnNgVUhoUBBgXNGH4rHZbEhISDDX1Rw2my1sW4yx9qN9JoXQ/zVAEwmaRmEJwVtRDr/PZ44C1XXdrKKpqKiAruvweDzmiTg0SrGkpOp+MqHBLfHx8eY0yWKBVDnyUg8GGm37CBEEIWw9gFHNEyqFhAYkmds5joTAGGvfGj0j7dq1q9736huu3dYRAaFyAYkAQjUklZkh4PebI2zLysrMkYgxMTEoLy9HRUUFNE2D2+2GLMsIBoOw2+1mT6FVq1bhuuuuq7VdSbZAgA82PQDSNTjcVX36p02bhqeffrrR7p8hFosFcXFxuP3225Geno4LLzzxw90ZY+1Po0nhueeea/D9tnCri6YiAJbqSQEABCMnyKSa1TqyLKO8vDysmidURRR6OlxMTAx0XYfVaoXD4UB+fj6WL19eKylomgax8t64Vj0AiFLYFf6yZcta+mMzxlijGk0KzzzzTGvE0SL0N5aADmXXmm7VCfGacRM8uwA4dYK18lYTSZoKAUZVDwGI0VToOkG2WKABEFK7wXnVDbBarSgoKDC7b4YaehcuXIiDBw/i3HPPhcViMe8NtHv3bqxc8znm3nEHjuYeRVBRMOvGGzB16lQAwPDhw7F69WpUVFRg2rRpGDZsGLZt24bk5GT897//jajheOPGjXjggQegaRpOP/10PPTQQ7DZbFi4cCHWrFkDWZZx9tln495778XHH3+Mf//73xBFEVFRUXjvvfdOyD5njP22tcs2BUk0HsZcV0cigQhCtTp5SZIhSbVHH4cafgOBAERRNOvx77nnHuzduxdr167F119/jT/96U9Yv349unTpgrKAhrvufxgx0TEg0nHd5EsxceLEWveO2r9/P55++mk8+uijmDVrFj755BNcfvnlDX4mv9+PuXPn4s0330SPHj1w66234rXXXsOkSZOwevVqbNiwAYIgmO0eTzzxBJYvX46OHTuGtYUwxtq333VSEK+6oc7pukbwlmhwOEWU6xqKfSqS3FboqgJ/eQncUdGwOex1Llud0+k071RZX1fVgQMHokuXLgAAAuHt1/+HjevXQhCAo0eOIDs7u1ZS6NKli9n1d8CAATh06FCjsfz888/o0qWLeSO6K664Aq+++ipmzJgBm82GO++8E+PGjTMfZTp06FDMnTsXF110Ec4///xG188Yax+aP2z3Nyx0DyJJFiAAsOhBGI+3qPaIsAiEbu3b0Ijh6u99s3kztn/zNV56/V0sf/8T9O/fH4FAoM71hoTuP9+Y+m52G7rR3cSJE/Hpp59iypQpAICHH34Yd911F44cOYLx48ejsLCw0W0wxn7/ftclhfrIkgC3xwJJ0gFNgV33Q9Ns5lDnpjw7ueZdQV0uF8rLy+uct6ysDG5PFJxOB/bv/xkZGRnN/xA1hG6YF7pb6bvvvoszzzwTFRUV8Pl8GDduHAYPHoxRo0YBAA4cOIDBgwdj8ODBWLt2LY4cOXJSb4HOGGsb2mVSECUBVpsEVaWqe15UPvTD0PzbXMTFxWHYsGEYO3Ys7HZ7WO+skX8YjWXLlmHKpecjtWt3DB48+Dg+RTi73Y7HH38cs2bNMhuap02bhuLiYsycOROBgDFY7r777gMALFiwANnZ2SAijBo1CqeeeuoJi4Ux9tvV5CevtTU1H7Lj9XojugFc6KlJBaUVUH3lsLs8INIR8FbAExsPp/XE58tCn4JCrwqPTYJX0dEttu52i1BsbUH1/fl7e9BIS2urcQFtNzaOq2la4iE77bJNIUxlTiSiak0Kx3dDvEY2BVEQ6n3+cqRUnVDsU+ttS2CMseZol9VH4cybXrT4CbayxQJGj9imbeuee+7B1q1bzdcaAZde/SfcdO0UWKSWSWKMsfaHk0L1kgIIdBztCaH1BDWCTa5dCCMyOjaFCiI6kfkI0MYsXLgw7HWRT0WBV4FGgOW4ImaMsSpcfYQaDc2CgOMpL5QFNRwqCUDVa6+FYDRhi6FbbBzHhkKlGr2O7TDGWHNxUghrU6DjSggAoGj1n6yJCGK1ksLxVFeFVs85gTF2InFSMJsUjJICQcDxZIZQCaGu55URjIQghqqPmrDeX4oDKPVX9UgKLatxQzNj7ATipICqkoJx5X581UdmUqjrZE0AIJi9myI9n+tECGo6AlrVAmb1EecExtgJ1O6TgmD2CSIIMKqPAmrTnkus6oTygGb+G6j7hG+WFCpf60To2bNnves9dOgQxo4dW62qqGqloWktUVLQdIJP4cdwMtYetfukQBReUhBFEb4mJAVNJ/xaGkRueRCqTggtqhOgaDqOlgfNkzmR0dBc1aYQ2TaqL18Vd+V7TctfESkLaPi1NAiNiyGMtTu/6y6pL207iuwif53vCYIAIoKiKGZXUaN2R4CGYthlsc7Oqd1i7bh+aBIUTUeJX4NX0aFoxpnZp+h47vGHkdSxE268bgZ8KuHJfz+OaIcF2779BvmFxVBVBXfddRf6DB8DLeKkYDwH+tH77kPWj7sgSRLmzLsH/QcPR+a+vZjxj/kIBoMgIrz44otITk7GrFmzkJOTA13Xcdttt+GSSy6JeL9p1EAVGGPsd+13nRQiQTX+L1Q2NOsENDQmrMSvodivwiaLSHRZkFehwKtoGDvhAjz9yALcMHMGCMD/rfkEr732P9w060aU6FaUFBfihqsvx6sfjY646kcnwgdvLAMR4fPPP0dWVhaunHwVln28Dm+/YTzl7bLLLkMwGISmaVi/fj2Sk5PNp7mFHgQUKc2srmrSYoyx34HfdVK4fmhSndNJUyGpKjSLFcfy80FEZsnB6XThaFBCnENGnLP+YWGKTrBIIlKjbQCAAp8Kr6KjV99TUVRYiNzcXBQVFcATFY24DolY9NAD2Pj1ZoiiiNzcXBQW5CPe2TGiz6ET8P2O7Zg8dToA446oyZ1ScPhgNvqfPghPPfUUcnJycP7556N79+7o06cPHnjgATz44INIT0/H8OHDm7TfdLOxvEmLMcZ+B1otKezcuROvvPIKdF3HuHHjcOmll4a9/8UXX2DZsmXm7ZsnTJiAcePGtUwwfh+0Y7lAx9SwNgUAEEUBVrnxdgVFJ1jEqqKEVRLgU4xlxpw7AZ99+gkK8o9h7IQL8dEH76OgoABL3/oIDrsVl6b/AWow2ITqI6pxF9eqks25F1yM9JFn4PPPP8eUKVPw6KOPYtSoUVi9ejXWr1+Phx56CKNHj8bcuXMj3DlVyYDvq8RY+9MqSUHXdbz88sv4+9//jvj4ePz1r3/F0KFD0blz57D5RowYUeuB9y1CNkoApCgAAFEUoVe22AqCAFu1E3xdiAiqRnDYqtrprZJoLpN+/oV47P6/obioCE+8vBzf/N+nSEhIgGSRsW3LZhw+fBiSKETckKsTMGDIMHy28iNcPmEsfv75ZxzNOYLUrt1w6OAvGDmgJ6677jocPHgQP/74I9LS0hATE4PLL78cLpcLb731VpN2j1atu2u774nAWDvTKkkhKysLycnJSEoyqnNGjBiBrVu31koKrcZSmRRUIylIkhSWFGRRgKqTWa1Uk07G1Xv1koKtsgFCFgX06NkL3ooKdEhMQnyHRJx/4aW4Y871mHHFJejbrx/S0tIgCpF3J9WJcOnkqXj8gX9g3LhxkCQJdz/wCKxWG9Z9uhL/uP1jyLKMxMREzJ07F9999x0WLFgAQRBgsVjw0EMPNWn3VO8Cy0mBsfalVZ6nsGXLFuzcuROzZ88GAGzYsAGZmZlhpYIvvvgCr7/+OqKiotCxY0dMnz497AE1IevWrcO6desAdaUOeAAAIABJREFUAIsWLUIwGAx7/+jRo7DZbI3GpGRnQne6kK8S7HY7/H6jl1JcXBwCkJFb6kePBBcsUu3Tok/RcLDQi5RoBzx2OWyawyJBI4JNEqEDqAioiHZY0DHKjsxj5fDYZCRH2XG42AdF09Et3lVr/TXllQVQ6DU+Z+9ENwBgb145REEwxjp0cEMST9ydUjOPlUPTCYkeG1wSmcm8LT3noTqOq+naamwcV9M0N67qj/yttc7jCShSdeWdmlfgQ4YMwciRI2GxWLBmzRo888wz5lPCqktPTzcfPg+g1gMmAoEAJElqNCbBYoWmKIAgh8Wi6zpEwSg1+IMKBEvtdQWCxsAuEbr5hYiVn1ESjCttTSezS6eqGfMZTQMEVVUhgIxxDXV8oTW/aFWrqsoKKlXTZVFAUCMEFcVMXkQEn6rDIYvNei4EESG0OVXVEFAVcx//3h400tLaalxA242N42qalnjITqskhfj4eBQUFJivCwoKEBsbGzaPx+Mx/52eno7ly5e3aEyCxQryewFZhihWlQYEQYBcWRWk6gRF06ERYK92K2ylsn5FrnZ1LokCPDYJTouE0oAKnajWqOPQXVJD82uVVVQ//fQTbr311rAYrFYrVq5cCaDmSGYKWwe08F5CPlXHkdIgOkfZYLc0IylU+69ePWDGWLvQKkmhR48eyMnJQV5eHuLi4vD111+HnQQBoKioyEwU27Zta/n2BqsV5K0AgLCShSiKECtP9opOKPeq8Cs6usbazCtvRTOeg1CzyibJbRTJyoMaFKoqKYRGHRttFMa/pcp/6AT07dsXa9euNddTs6RQ/aSvU9WI6FCeqt42EbrNhqIT6n7YZ8Oq392VkwJj7U+rJAVJkjBz5kw8+OCD0HUd55xzDlJTU/Hmm2+iR48eGDp0KFavXo1t27ZBkiS43W7MmTOnRWMSLBazW2fNkoIoCJAEAapm3AdJI6OqJ/SEM6Xav+tct2AkgOoNtqEqtKqr/Kr3pEbOvNVLCtVr4kIllbCkUZmAmnuLCi0sAXGXVMbam1YbpzB48GAMHjw4bNrkyZPNf19zzTW45pprWisco/qo8pK7ZlIAAFkSENB088o7oBJCzQuqrsNWRwN0iAjB7KEEGCdac8R05fpDJQVNB+potgijE8xGZR0EoXJlZlKolgDUUDtGM5NCXTfdY4y1H+23x6HFaj56s86kIAphd0sNVLa+6kRQNDLbHeoiClVX6qGR0qETrFlSCCWFCK7GdaKwUoFeMymElRRCiai5SQFm3FxSYKz9abdJQRBFUGUyCCWF6r11qjciWyQBftU4QZZV3iLbUcczmM11V8sXobEMVUnCmC7V0R5QH52qJwAyq5BC06qvI1T9c7wlBVkUuKTAWDvUbpMCgP9v78zD5KqqRf/b59Rc1V09VE/pdJM5ZGIICcEwXhKRKypeLoMgai4RvEaMwCOA6L14H6NCvqCAV+UhKuK7gBIRfIgShmggEBJCGBJCxk7Sc1d3dc1Vp85+f+zu6u6kOyM9kOzf9/XXXdVnWLXPrr32XmvttZAOh5q5d21S6/6BnsHcaRp4HAZpy8aWknDSwuMw8Dn7b7pIJMKTjz+Wf907kgl6VgpGL/MRwFe+8hUikQgAmZxNvCvsVXY5rLt1kJRg06NglFmp5/7dyid3mCm1u5WK0xD5+2g0mmOHozoh3nvrEnR29F8sRghBNuvGtiVOsxPLVrucnc4ooAbVTM7GNASGUBFHdUYGh18wc7ZvwD0AnZ2d/M/jj/Hpiy5X1+tSLt37C7pPU2+L/Cy/O6MpQGssQzRlMa7E6NdU1K2OVARUX6fyEa8U8uG2kBp5e3U0Gs0gc1QrhQMjVOW1XA5E35l/78FbzeolOVvidxj49uMZvuuuu9hVt5OFl3we0+GgMOCnsKSM7Zs38ujyv/Ctb1xNc2MD6XSaCy//Kpdf8WUA5syZw/PPP088HueyK77M9JNOYfO766msrOA/lv43pl/t47BlT5yoAJ77/RMsf+r/Qs5i7NixXP+DH+HyeGhtbeGOG2+jrq6OnJTcctsdfOasT/HUU0/x85//HFChsA888EAf+VXIq8AwRJ+oKY1Gc2xwVCuF6TN9A/7P4XDQ2tpKLmNRkojQHigGw6CkRA2+UkrakxZBjwNDQEvCwucwCLj3Hyp06623snHTJv7PU8/y9prVfPfaq3n0D/+P4yeMJZKyuPtH91FdXkoymeS88z/LZ87/LBWBnhTfti3ZvXMH/3HPMs5ceh/XX7uIV1/8C1/50qVdzt+eEqKGgPPO/2fOv+gyxhW7+dGPfsSzTz/JZVd+jZ/c8785Y85p/PKXv2RHOEFnNM6HH37IT37yE5555hlKSkpob2/fR/6clJiix7ylnc0azbHFUa0UDoSUEuFwgNOFM5MEf8+uaiFEn3oK5f6BayvsjxNOPImq0TV5E89vfvVLVvz1BQCaGuvZsWM7U2orsKWkvjON27aprB7NxOOnkslJpk2fQWP9nq4VS/eO5h7/x46tm1l6771kElHiiQQzTzsDl2mw7s3XefAnP1Y+CQx8gQCvvPgnLrjggnx68r13lUNP+Gu3n/1g03trNJqjg2Pb0dydBbWimoCdIxCLfOzmEr9frVYsKXl7zWpeX/UPnn32WV588UWmTJ1GLJEknslhS0hZNtGMjcvpUj6PnI1hGOQsK2/GUo7mHof192+6ketuvY3nXvgbi79zHZl0Gpeje/8CffdL2P1nfe2NLWVeAcHh+yY0Gs0nk2NWKWQyGbLZLA6HQ60WSkKQzUA8ekTX9fv9xOPx/GuBGszTWUk8FiUYLMLr9bJlyxbefWc9hhA0RDNdxwoVdSQEHodBJifpDiLqnr3bXUn1ugfteDxGaaicRCrDM39cDqg03qfMmctvH3+MbE6Sy+WIx6Kcetpcnn32WcLhMEC/5iNbqkJDe0dHaTSaY4Nj0nwkpaSjowPDMPD7u1JXe/3g8kBHGOkLIIzD05clJSXMmjWLBf/yz7g9HqoryzENFb30mXnn8uIfn2T+/PmMGzeOmTNnEnApH4UQ4HMZJBJqlu4yBZ1pu6ciXG/zkewJnV2yZAn/fuW/Ul1dzZTjp9Da0YnLNPj2zf/Bj+/4Pk8/+QRSGFz//f/i7E/NYfHixVx88cUYhsH06dO5//77+8ifsyUuU/RaKWitoNEcSwxJPYXBpL6+vs/rRCKBzzewgxkgmUzS2dlJMBjE4+lJGycTcWiuV5XZikMIf+CwZLJsyY72FKYhGFvsob4zg0QyqsC1j/nG7uXQTls2DdEMIb8LgaQlniXocRBJWYwt9tAUz2LZEodQoazd9aHrOtI4DIHfZdASzzKmyM2uzgw+p4HDELQnLQwhCLhMygP7941sC6cocJsUuE12R9K47Ay1ZUHg6EsfPNiMVLlg5Mqm5To0PrGps0caHo8Hh8OB09l3gBQ+P7J8FHS0QWsj0lWLcA5cjGIgjHw4q/qjqkDdpz97viEEpV0ObdNpUOx1EPQ6SWVUVbhoOteVpE9dVzmOe+4BalWRztnk7O60Hd3V40CiUmSYhiDbNeu3bEldR5oyv5OCXtFU2ZydT6mRXyl8sucMGo3mEDkmlYIQAq/X22+BG+HzI11uqN8J7W1QXnXo1+/63T2wHmyxG9GlIBymgd2VB0MAFQEn3/ve93j9jTe7QlLVP755zdVcdtllOE1BLCOx8pFDyifRmcrhMAVOU2V97c7fFEvnsKWkLZHF7zLyyiuWUf8PuHoK9BxutlWNRvPJ5JhUCgdCOBzIYAm0tyI7O6AgeEhVzLrTbx9JhUzTEIwudOMw1az/rrvuojWeJZLO4TAEblNQWaBWMa6uVBqJTC6fU6nQbRJJWWRzEq/DgWlALKM2o3VmchhC1aFuT6qKcAG3STSdw+MwcJpGT9U4rRQ0mmOKYzb66IAUBsHnh3ALNNUj29uQsSgymznwufTkJToSPF0+gW66zUcqtLTnOL/LxGUaWLbM39PtMPB25WdymiKfbiOetclYNiU+B26HQXvSoiNlsaczQyZn5zfndV8+ls6xqq5T72zWaI4RtFIYACEMKKuC4hDkLOhsh9ZG2FOHbG1CJmLITBrZlQpi70Ez4DIHTJp3uHQX9rGlxNWrnoMhBFUFTgwh+iiRQrdaCLrMnhKjzbEsIChwmVQEnJT5ndQE3fnzeqKh1GqnOW7xo7/X80FLcr+y6Z3PGs3RgTYf7QchBASLIViMlDZksxDrhGhE/QYwjHzxZVlajujaFV12mDug90eB25HPu7R3KVCnaVAT7BvdFHAZGAUufE6VWM9lGpiGUhamITAReeUyutClIpt6XbeywIlR7sXnjPLXLR2cPbU2/z8pJbsiGeLZHK9u72TFtgifnVTMFSeEcO8nrbhGoxnZaKVwkAhhgMsNJWXK32Bl1Wa3dEol00unoKURGe0EjwfMrqaVErx+hPPjURJ7K4PeOM29k/oJ/F0zf1NAbZF7v9fd+9o+p0l5wMVZYwp5aVuEDfWdvLyxhZmj/Px1S4SXtkXy155W7uOPG8P8bUsH40s9XDWznNqgm1V1UWZU+CjymKxvTCClJORTpVCrC119lJBGoxl+tFI4DIRpgmmC2wOBQkDNnIm0QyIGHeG9zmhBOpxKebjdanWRzYIQTJp7JpvfXK2u5fGAcYDanMPAeROK+MtHHXzzqQ0APPleGwAXTS1hWrmP2qCb8oCTDY1xVtVFeWNXlP9csYsJJR7WNcQp9jqYUOJhzZ5Yn+seV+Tmqpnl5GzJ6KCLioCLTM7O+0U2NMYxDcG0cl9X0R9JRypHscekOZ7l4beaOWtMIf9SWjqg7NmcJJJW+zRKvB9/d09kc4QTFtWF++5BsWzJns4MfqdBUa97q1QiKo1JXSRDTdDVxxx4JEgp+dOmdvwug/njiw75/GTW5u2GGLs7M5T5nBxX5KY+msHjMKgMOEGoAlOFbke/dcqzOdnHzNkUy+J2GPtt+2xOkrLsfHi0CngQ+UlONifJ2vZ+sxMfCip/2L5RgR0piz9tDPP6riiXTA9x7rjgx3K/w0FKieTI/ZKHw1G9eW3lypW0tLT0e153mcxDpaysjLPOOmu/x0jbBjuHctdKiEXVyiKXUysKKcHpBCmZdPY8Nq/4C/lyasJAuN0qLNbbtds6Z4FlKWXicILHe9g7rg+V7va8Z+UevB43l04p5O2GOGU+J7NH97+5r74zw60v1hFJWfzr1FL+vrOT5niWL59YxvEhL+GkRcqy+d07LbSneupdjC500RDNIFGmrlRXOVSf06Dc7ySctOhM5zhllJ/dnRmaYmovx+zaIs4c7SVp2bzblODthjjFHgdFHpMPW1NkbTUIzK0t4PTjCkhmbZ77sJ2OVI4ij0mRx4EtJbFMDqehHPQ+p8GYYjdNsSyv10U5pTrA1DIvb+yO4TIFRR4HmZzN6l0xkpZNbdCF32Vi2ZLTawuoi2T4x84oma4w4KoCJyGfM+/UL/Y6iGdypCyJx2EwocSN0zQ4vsyLQwhW744yocTDiVV+6jrSbGtPkbYk500I4jINtrenaE9aFLodhPwOGqJZnIagLpJmVZ1K1XLp9FK8DoN3mhJsC6cYXehiYqmHygIXk0aF2FDXzAsfdVDoNpkU8lIZcLJ8Y5i2xMEV0vA7DcaWeDhtdADTELy1J8ba+jhTy7yU+Z2srY8Ry9i4TMFXTyrj3aYEm1uTTC330RTL0hTLUB5wUd+ZIZ2z+eKUEpqSsGq7mlRNDnmZXe3n2Q/biaRyBN0m544Lcu64ICG/g/ebkryxO8r7zUnGFLs5qdLPqEInlg1NsQzbwmkaYhmsnOSESh/10SzvNSXoSFmYQqhn73Xkn//6hji2VGbThmiWSaUeSnwONremcDtNPj2ukI6URSxjM6nUgwQ2tSRZvSvK2GIP508sYnTQxVt7YnzQkqQy4Oy6NmxvTxFJ5RACJpR4CPmdINXEyLIlG1sSNMeztCYsWhMWnSkLr9NgdnWAtqRFa9wi5HNQ5nficQgaolk+VVvAFadN+Ng3r2mlcIgcSCnceeedVFdXs2DBAgCWLl2KEILVq1cTiUTIZrPcdNNNnH/++QBMnDiRzZs3K2WRSYOVRWQzyFSSeDzOVbd8n0g0StayuOmahXzmzDPAMPn9S6/ws18/hpA2U8aP5yd33U5Lewe33HEXdXv2gGFy9z33MHv27EP+jL3p3Z6HsnuyNZGlPWkxsdRLIpsjkspRVdB3I2BnOsf7TQkK3SbvNSfY1JJkbLEb0xBE0zlmjvJjS1jfEKc1YRFwGZT6nDz3YRiB4L/m1bCxJcGfPozQFldRYUUek5mjAnSmLNpTOaaVexld6KYpluHPmzvyiua4oJuJIQ+RlEVHKochlJM9a0tSWZvOdI7GWBaXKTi5ys/bDXEyObUr3WkIOtJq4Dyx0s+kUg+v74oigHRO8lFbCrcpOH9KBccFIJLKsblNDeJ+l8FxRW7CCfWlnxzy8kFLgt2RDEnLZkd7GgmML3GzsyOTDwkeVeDCsm2a4z0Dtt9pkMiq+nhmd14s4MsnhqjrSPP3nUo51AZdTCj1UNeRYWdHmmyvMOPpFT6klGxpS5HOSWqDLhaeUsHkkJf6aIY9nRlGF7pI52yaYlkEkLJkV7tZvNOYYHdnT9t/qqaAtxvixDI5Th0dYEqZj7/v6GRDUwKPQ3ByVYDNbUnK/U5qgi6aYlkqA+r6r2zvxOMw+PzkYkwDXtoWoTluMaXMy+zqAFvCKVbvivapNOh1GEwt97ItnOozwQDlU6sudJGzYUvXTv1TqvyU+Z1YtqSjq49EUhZpS3Lq6ACfnhCkKuDimY1h1uyJ0dGVTSCaFWxo6MRldu0B6irL63cZnFod4IOWZH6SIoAxxeoZd6aVIqgJugn5HGS6+kfK6ps+xmUKyv1OQn4nIZ9SVM3xLGv3xCgPOKkqcNGWyNISVxOqqgIn500o4spPTdRKYW8OJ80FqHoK/W1eO1Lee+89brvtNv7whz8AcM455/D4449TWFhIQUEB4XCYz3/+8/zjH/9ACMHEiRP56KOP9pEtm8lgxaMkkykKiosJRzr5/Be+wD9e/Cub33mHq2/4X/zxvx+gpKyC9mSSYr+Pb97yPU6ZMZ2vX3oxOTtH3OOnsLiky+8hwF/QZ4Uhc6pjC3PgZfnhKoXBJJy0yOZsKgJKyRSXlLJ6826KPCblfueAe0pSlp2flU4OeQ+4NI91faH9LjMfujumyH3APSv1nRkK3CZjqysOub06UhYZS1IecBJJWTTGstQEXficJjlb8k5jHLfDYFyxB69TrabCCYsyvxNbSpKWTZHHQc6WbG5NUlngongv01V70iJl+sgk1AwX1CbFpliWMr+zX7PQQEgpaUuqmXeh28z7pfIZiLuu/fednUwr9+03AGNTS5IJ1WU4MsrMmM1JGmIZanqZ5ppjWd5vTtAUzzI55GV6uQ+nqUyLLfEsDVGlyEt9jj59IZbO7RPifSiEQiHe215Pic+B0xA0x7O4TCP/mXO2ZGdHmsZYhrHFnvwESJXT7esLzNkSy5b5FYQhBBNKPYclm05z8Qlg+vTptLa20tjYSFtbG8FgkPLycn7wgx/wxhtvIISgsbGRlpYWysvLB7yOMAzw+PjhPT/Kn9fU1ERrLMFrGz/kgi9cSMnx08HtoaSr469a9zY//sXDYAjMpnoKU3Fo6MnYSnubKs8jbaUkbBuEgSwOgculHOfZbNf7Anx+ZCqFbG9F7tlBpnYssnwUYgC/h8x2zZQ+Jqf6QOxtnzYNweSQ94DneRwG40o8Bzyum94FlYq9jj6D6/4YVXjoqVG6KfL03CPocRDs9do0BDNH9TXZeRxGr/uJfOSXaQimlO87OepOqxIKBWltzfa59uHILYQg5Nv3efdWnKYhOGfsge3zx5d5CRV6aG1VSsFpCmqDfYMjygNOygP7XssQgoqAKz9R2JsDFcc6GCp7rXT3vo9pCMaVePbpX0II9taxvYM6pvbzjIYbrRQGgQsuuIA///nPNDc3c+GFF/L000/T1tbG888/j9PpZM6cOaTT6QNeZ6DzpJQIw0B4+h8IhdOFHFWrzFE5S2V/zVkqjFYI5ZuwJTgckEpAuLnXyUI5u6UN0Q7k1s3Y/6PKd7YDBIsRM+eC6UDWbUWUqapxcuc2aKgDhxNx4qlQEIRAAeKE2VAzTvlMtmxUMnm84PVBxSiEw6kitqQNgcKP3VciUwlwuFR6dLqSHrY2Qc3YQ9qlfkj3jMeUkh1mZDZ7UApaRiMqzLqqRj2fWBRRWjYEEn58yHgUop2Iyuqe92x7n/4kLUs9/8IildIm0g5CIAqL+hzT3V8GXe5IOxgGomBfRSelhOYGiHZAMoFMpRA1YxCVowdVJq0UBoELL7yQJUuWEA6H+cMf/sCzzz5LKBTC6XSyatUqdu/efVDXiUaj/Z53xhlnsHDhQq6++up8Wc3i4mLOOOMMfvOb33D11VdjS0kia1FQ0FVNzqkc1HsjC4sgmeg6xgUOh/K32DakkiAF4uv/CzGqloJEJ5GXnkf+429qkK8Zi9zwljq3dhxixikQ60S+86ZyrCcTyGd+pzYAOl0qA21vfH6oqIYdH6nruT0weYZSYI171B4RpwuScSVjKgmZDIw+DnwBaNwNLjftVaOxSyugtQlpZREzP6XOe3ct8s2VKlJs9Bh1z7qtymk/egxi3GQ1mMRjqo5GNoOoHQ9OJ7JhNyJUAVIid3wEiXiPkh07CTFpGphOiHciW5thz06lbEENOqaDtupacgileNNpOG48wuNFtjVDW7OKRisJqQCEVAKSSfXb40VMnqF2z8c6welCjB4LVaPVIJFKqnv5/OAvBK9XybZ1E3L7R2o3fqRDtXfteETFKJWupbMDTAeiajQdhiAX6VCmxR1blFIuCUFnRD27qhr1GUvKoKNNfcZEDFFWqZRI4x6oPg7h9SETccSoGujsQH6wXoVuB0sQwWI1OXA4IZOCUbUqWm/zeypkuyQERaVqZZqIQjRKe8NOcju3KlPn+CmIE09FtjSo9vf5EeWj1J6g3TuRm99DeH2Is8/HfmSZaveT5qgV7+4d6vOGKqD6OPWZWpugpVG1t+mAskrVhwAKi5Qs8ah6NlU1iAlToKwKuWYlzW3NyJIyCFWCaUBTveqzLrf6cbrA5VKTnI4wJOOI8ccjmxvUZMjtgVAFIlSBrK9Tz8s0VTsaBkycpvq4w4GYejI01yM3bVCfoff3FaCiGnHiqYhTz4RQ6KDGkkNB+xQGiXnz5lFcXMzvf/97wuEwX/va17Asi2nTprFmzRp++9vfUlNTM6BPwbKs/Z735JNP8rOf/axPXYSWlhZuuukm6urqMAyDu+++m1mzZh3R5+jPpyAzaTW7OkAGWdnZgXx3LXL9G+pLcsan1UCbSqrB+P23kU17ENNOVoNFwy7kxg3qi1F9nBrI7Bx4fAiPTw1+homs2wrpFKKqBplJY4ZbyO3ZqZSPlCo1CYDLjZg7DwxDfRGFQFSPgfIq5Mq/qC+cv6BrcC1QK6Ttm9XAP6pWDSJSIsZNVgOtU5ky5Mb1atCRUp1bHFLXFagBp3YcJOO42ppJJ5NqRWc6kDu3qAGrtBxRWg7SRoZbwens+nw+pbg7wsjN76vXhUVqANm9XSkz0wSPTw2kyXjfBg8UwISpKiza60eMqkV+9L76nIVFPddq3IPDH8ByupSSmDAFikPI99YhQuVdf69VyjqZUJ+xtFwp4uYGNThXjlZtms2oAa9pDzjdiBmz1OeKtKsQ7VinGpCdThWFBz1RdXvLLwTm6DHYo2pV//jwPXX9gSirhEhYTRR8AcTcc5GvvQRFJYixE6GwGNmwWylHpwtKy9Qsu2KUykywZwdi8gyltOrrkB1hhNcHZZXIXdth60aljEaPxTtjJsk9dV1KxYLK0aq/ZNOqTTOZLvNrRrWzyw1bN6mV9fRTIGchm+pVnxpVi/D6kZmU6lvJuJpcFZWoCcqOj9R5k0+A42eoVZvHB04XcstG5DtvwKZ3EedfRPnXr9OO5r0ZqUrhSBhJso1ER/PehEIhWpqbEIapVjh1W9UgUFaJcA28Ye9I6e1MHUiuj6u9ZDqlBsCS8h5TWC6nFEAqqQbdwqIB/T2HI5uUEtJJpbAOJJ+l0qfsz+wiW5vUTLxmrHpWqQS0h5Wi8xeA10dZeXleLpmIwbYPlVmrsEgNmE17lDKuqkEEi5HNDcgXn0Gc/c+I6uMO6rMfLNK2VXqbYAllZWWH/Cxll2/uUM2UMtm1WtzPeTKZgJxF2Zhx2tGs0fRH92AoDAPGTByaew7hxiLh9kB53y+yME1lnunHHv2x3FMINUM9mGMdB/ZdiFCFMud0v/b4oGrg6wtfAKaf0vNGUYn66X1MeRXiin8/KBkPFWEYyqR0JOcfznneA7f5wRxzuGilMALYuHEjixcvzr8WQuByuXjuueeGUSqNRnMsctQphU+iNWzKlCn87W9/y78eSeajT2J7ajSaw+eoS2dpGMaIGVA/6ViWhTFE6TQ0Gs3I4KhbKXg8HlKpFOl0er82X7fbfVB7BYaDkSCblBLDMPB4Dn6zl0aj+eQzZEph/fr1PProo9i2zbx58/jiF7/Y5//ZbJYHH3yQbdu2UVBQwHXXXbffHb8D0V1/+UCM1EgaGNmyaTSao5shsQ3Yts0jjzzCrbfeyrJly/rdwPXSSy/h9/t54IEHuOCCC3j88ceHQjSNRqPR9GJIlMKWLVuorKykoqICh8PB3LlzWbNmTZ9j3nrrLc455xwATjvtNN577z3t5NRoNJohZkjMR+FwmNJehVBKS0v32cXb+xjTNPH5fESjUQoLC/sc9+KLL/Liiy8CcM899xA6zG3eDofjsM8dbEaqbFquQ2OkygUjVzYt16ExGHINiVLob8a/txP4YI4BmD9/PvML5vH7AAALBklEQVTnz8+/dh1B4rEjOXewGamyabkOjZEqF4xc2bRch8bHLdeQmI9KS0tpa2vLv25ra6O4uHjAY3K5HIlEgkCg/8peHwe33HLLoF37SBmpsmm5Do2RKheMXNm0XIfGYMg1JEph/PjxNDQ00NzcjGVZvPbaa/skajvllFN45ZVXAFi9ejXTpk0b0jQCGo1Goxki85Fpmlx11VXceeed2LbNP/3TP1FTU8MTTzzB+PHjmTVrFueeey4PPvgg3/72twkEAlx33XVDIZpGo9FoejFk+xRmzpzJzJkz+7x32WWX5f92uVzccMMNQyVOH7/ESGOkyqblOjRGqlwwcmXTch0agyHXJz51tkaj0Wg+PnRiG41Go9Hk0UpBo9FoNHmOuoR4B8OB8jANFa2trTz00EN0dHQghGD+/Pl89rOf5cknn2TFihX5jXuXX375Pv6YweZb3/oWHo8HwzAwTZN77rmHWCzGsmXLaGlpoaysjOuvv35Qw4b7o76+nmXLluVfNzc3c+mllxKPx4e8zX7605+ybt06gsEgS5cuBRiwjaSUPProo7z99tu43W4WLVrEuHHjhkyuxx57jLVr1+JwOKioqGDRokX4/X6am5u5/vrr85W4Jk6cyDXXXDNkcu2vry9fvpyXXnoJwzD4t3/7N0466aRBkWsg2ZYtW5av7NhdgfDee+8d0jYbaIwY1H4mjzFyuZy89tprZWNjo8xms/LGG2+Uu3btGhZZwuGw3Lp1q5RSykQiIRcvXix37doln3jiCfnMM88Mi0zdLFq0SEYikT7vPfbYY3L58uVSSimXL18uH3vsseEQLU8ul5Nf//rXZXNz87C02fvvvy+3bt0qb7jhhvx7A7XR2rVr5Z133ilt25Yffvih/O53vzukcq1fv15alpWXsVuupqamPscNJv3JNdBz27Vrl7zxxhtlJpORTU1N8tprr5W5XG5IZevNr3/9a/nUU09JKYe2zQYaIwaznx1z5qODycM0VBQXF+e1uNfrpbq6mnA4PCyyHAxr1qzh7LPPBuDss88etnbr5t1336WyspKysrJhuf/UqVP3WSkN1EZvvfUWZ511FkIIJk2aRDwep729fcjkOvHEEzFNVbJ00qRJw9LP+pNrINasWcPcuXNxOp2Ul5dTWVnJli1bhkU2KSWvv/46p59++qDdfyAGGiMGs58dc+ajg8nDNBw0Nzezfft2JkyYwKZNm3jhhRdYuXIl48aN46tf/eqQm2kA7rzzTgA+/elPM3/+fCKRSH4nenFxMZ2dnUMuU29WrVrV54s6EtpsoDYKh8N9ctSUlpYSDof32dk/FLz00kvMnTs3/7q5uZmbbroJr9fLl770JaZMmTKk8vT33MLhMBMn9tTaLikpGbYJ08aNGwkGg1RVVeXfG4426z1GDGY/O+aUgjzIHEtDSSqVYunSpSxYsACfz8d5553HxRdfDMATTzzBb37zGxYtWjSkMt1+++2UlJQQiUS444478vbTkYJlWaxdu5YrrrgCYES02f4YKf3u6aefxjRNzjzzTEANKD/96U8pKChg27Zt3HvvvSxduhSfb/AKw/dmoOfWX3sNF3tPPoajzfYeIwbi4+hnx5z56GDyMA0llmWxdOlSzjzzTObMmQNAUVERhmFgGAbz5s1j69atQy5XSUkJAMFgkNmzZ7NlyxaCwWB+Kdre3r5PBtuh5O2332bs2LEUFRUBI6PNgAHbqLS0tE/hpOHod6+88gpr165l8eLF+YHC6XRSUFAAwLhx46ioqKChoWHIZBroue39PQ2Hw/k+OZTkcjnefPPNPiuroW6z/saIwexnx5xSOJg8TEOFlJKf/exnVFdX87nPfS7/fm8b4JtvvklNTc2QypVKpUgmk/m/N2zYQG1tLbNmzeLVV18F4NVXX2X27NlDKldv9p69DXebdTNQG82aNYuVK1cipWTz5s34fL4hVQrr16/nmWee4eabb8btduff7+zsxLZtAJqammhoaKCiomLI5Brouc2aNYvXXnuNbDZLc3MzDQ0NTJgwYcjk6ubdd99l1KhRfUzOQ9lmA40Rg9nPjskdzevWrePXv/51Pg/TRRddNCxybNq0if/8z/+ktrY2P3O7/PLLWbVqFTt27EAIQVlZGddcc82QDiBNTU3cd999gJopnXHGGVx00UVEo1GWLVtGa2sroVCIG264YVjs9ul0mm9+85s8+OCD+aX0Aw88MORtdv/99/PBBx8QjUYJBoNceumlzJ49u982klLyyCOP8M477+ByuVi0aBHjx48fMrmWL1+OZVn559UdRrl69WqefPJJTNPEMAwuueSSQZsk9SfX+++/P+Bze/rpp3n55ZcxDIMFCxZw8sknD4pcA8l27rnn8tBDDzFx4kTOO++8/LFD2WYDjRETJ04ctH52TCoFjUaj0fTPMWc+0mg0Gs3AaKWg0Wg0mjxaKWg0Go0mj1YKGo1Go8mjlYJGo9Fo8miloNEMEZdeeimNjY3DLYZGs1+OuTQXGg2o1OAdHR0YRs+86JxzzmHhwoXDKFX/vPDCC4TDYS6//HJuu+02rrrqKo477rjhFktzlKKVguaY5eabb+aEE04YbjEOyLZt25g5cya2bbN7925Gjx493CJpjmK0UtBo9uKVV15hxYoVjB07lldffZXi4mIWLlzIjBkzAJWH5+GHH2bTpk0EAgEuvPDCfAF127b54x//yMsvv0wkEqGqqoolS5bkM1du2LCBu+66i2g0yumnn87ChQsPmLBs27ZtXHzxxdTX11NeXp5Pga3RDAZaKWg0/fDRRx8xZ84cHnnkEd58803uu+8+HnroIQKBAD/+8Y+pqanh5z//OfX19dx+++1UVFQwY8YMnnvuOVatWsV3v/tdqqqq2LlzZ59cQ+vWrePuu+8mmUxy8803M2vWrH4rimWzWa6++mqklKRSKZYsWYJlWdi2zYIFC/jCF74wbOlZNEc3WilojlnuvffePrPuK6+8Mj/jDwaDXHDBBQghmDt3Ls8++yzr1q1j6tSpbNq0iVtuuQWXy8WYMWOYN28eK1euZMaMGaxYsYIrr7wyn2p8zJgxfe75xS9+Eb/fj9/vZ9q0aezYsaNfpeB0OvnVr37FihUr2LVrFwsWLOCOO+7gS1/60rAkhtMcO2iloDlmWbJkyYA+hZKSkj5mnbKyMsLhMO3t7QQCAbxeb/5/oVAon/K5ra1tvxkzu1N9A7jdblKpVL/H3X///axfv550Oo3T6eTll18mlUqxZcsWqqqquPvuuw/ps2o0B4tWChpNP4TDYaSUecXQ2trKrFmzKC4uJhaLkUwm84qhtbU1n+u/tLSUpqYmamtrj+j+1113HbZtc8011/CLX/yCtWvX8vrrr7N48eIj+2AazQHQ+xQ0mn6IRCI8//zzWJbF66+/zp49ezj55JMJhUJMnjyZ3/3ud2QyGXbu3MnLL7+cr2Q2b948nnjiCRoaGpBSsnPnTqLR6GHJsGfPHioqKjAMg+3btw9aqm2Npjd6paA5ZvnhD3/YZ5/CCSecwJIlSwBVb6ChoYGFCxdSVFTEDTfckK+29Z3vfIeHH36Yb3zjGwQCAS655JK8Gepzn/sc2WyWO+64g2g0SnV1NTfeeONhybdt2zbGjh2b//vCCy88ko+r0RwUup6CRrMX3SGpt99++3CLotEMOdp8pNFoNJo8WiloNBqNJo82H2k0Go0mj14paDQajSaPVgoajUajyaOVgkaj0WjyaKWg0Wg0mjxaKWg0Go0mz/8H7JU2Fv+hdXgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "plt.savefig(\"batch_relu_validation_200.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,962\n",
      "Trainable params: 2,278,274\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,962\n",
      "Trainable params: 2,278,274\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_distributed_function_35057589 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_1/test_1.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_2/test_2.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_3/test_3.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_4/test_4.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_5/test_5.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_6/test_6.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_7/test_7.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_8/test_8.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_9/test_9.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_10/test_10.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_11/test_11.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_12/test_12.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_13/test_13.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_14/test_14.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_15/test_15.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_16/test_16.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_17/test_17.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_18/test_18.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_19/test_19.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_20/test_20.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_21/test_21.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_22/test_22.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_23/test_23.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_24/test_24.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_25/test_25.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_26/test_26.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_27/test_27.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_28/test_28.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_29/test_29.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_30/test_30.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_31/test_31.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_32/test_32.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_33/test_33.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_34/test_34.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_35/test_35.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_36/test_36.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_37/test_37.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_38/test_38.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_39/test_39.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_40/test_40.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_41/test_41.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_42/test_42.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_43/test_43.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_44/test_44.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_45/test_45.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_46/test_46.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_47/test_47.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_48/test_48.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_49/test_49.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_50/test_50.png\n"
     ]
    }
   ],
   "source": [
    "from helpers import *\n",
    "\n",
    "# from cnn_model import cnn_model\n",
    "\n",
    "# Instantiate the model\n",
    "batch_normalization = True\n",
    "activation = \"relu\"\n",
    "model = cnn_model(\n",
    "    shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model.load(\"batch_relu_validation_200-145-0.951267-0.930033.h5\")\n",
    "\n",
    "# Print a summary to make sure the correct model is used\n",
    "model.model.summary()\n",
    "\n",
    "# We add all test images to an array, used later for generating a submission\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "    image_filenames.append(image_filename)\n",
    "\n",
    "# Set-up submission filename\n",
    "submission_filename = \"batch_relu_validation_200-145-0.951267-0.930033.csv\"\n",
    "\n",
    "# Generates the submission\n",
    "generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,962\n",
      "Trainable params: 2,278,274\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/160\n",
      "Executing op LeakyRelu in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LeakyReluGrad in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.5554 - accuracy: 0.7090 - recall: 0.6821 - f1: 0.6979\n",
      "Epoch 00001: val_loss improved from inf to 0.85737, saving model to batch_relu_validation_200-001-0.709167-0.259867.h5\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.5549 - accuracy: 0.7092 - recall: 0.6823 - f1: 0.6981 - val_loss: 0.8574 - val_accuracy: 0.2599 - val_recall: 0.2593 - val_f1: 0.2594\n",
      "Epoch 2/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.4262 - accuracy: 0.8009 - recall: 0.8000 - f1: 0.8007\n",
      "Epoch 00002: val_loss did not improve from 0.85737\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.4258 - accuracy: 0.8010 - recall: 0.8001 - f1: 0.8007 - val_loss: 0.9403 - val_accuracy: 0.4142 - val_recall: 0.4127 - val_f1: 0.4132\n",
      "Epoch 3/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3690 - accuracy: 0.8337 - recall: 0.8334 - f1: 0.8336\n",
      "Epoch 00003: val_loss improved from 0.85737 to 0.44966, saving model to batch_relu_validation_200-003-0.833967-0.789200.h5\n",
      "150/150 [==============================] - 68s 456ms/step - loss: 0.3685 - accuracy: 0.8340 - recall: 0.8335 - f1: 0.8339 - val_loss: 0.4497 - val_accuracy: 0.7892 - val_recall: 0.7810 - val_f1: 0.7875\n",
      "Epoch 4/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3478 - accuracy: 0.8426 - recall: 0.8421 - f1: 0.8425\n",
      "Epoch 00004: val_loss did not improve from 0.44966\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.3485 - accuracy: 0.8424 - recall: 0.8419 - f1: 0.8424 - val_loss: 0.6745 - val_accuracy: 0.6829 - val_recall: 0.6749 - val_f1: 0.6803\n",
      "Epoch 5/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3120 - accuracy: 0.8630 - recall: 0.8628 - f1: 0.8630\n",
      "Epoch 00005: val_loss improved from 0.44966 to 0.39337, saving model to batch_relu_validation_200-005-0.863133-0.811033.h5\n",
      "150/150 [==============================] - 67s 445ms/step - loss: 0.3116 - accuracy: 0.8631 - recall: 0.8629 - f1: 0.8631 - val_loss: 0.3934 - val_accuracy: 0.8110 - val_recall: 0.8060 - val_f1: 0.8101\n",
      "Epoch 6/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.3003 - accuracy: 0.8704 - recall: 0.8699 - f1: 0.8703\n",
      "Epoch 00006: val_loss improved from 0.39337 to 0.37381, saving model to batch_relu_validation_200-006-0.870500-0.835467.h5\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.3001 - accuracy: 0.8705 - recall: 0.8700 - f1: 0.8704 - val_loss: 0.3738 - val_accuracy: 0.8355 - val_recall: 0.8409 - val_f1: 0.8364\n",
      "Epoch 7/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2713 - accuracy: 0.8837 - recall: 0.8837 - f1: 0.8837\n",
      "Epoch 00007: val_loss did not improve from 0.37381\n",
      "150/150 [==============================] - 68s 450ms/step - loss: 0.2708 - accuracy: 0.8841 - recall: 0.8841 - f1: 0.8841 - val_loss: 0.4487 - val_accuracy: 0.7863 - val_recall: 0.7887 - val_f1: 0.7868\n",
      "Epoch 8/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2511 - accuracy: 0.8944 - recall: 0.8944 - f1: 0.8944\n",
      "Epoch 00008: val_loss did not improve from 0.37381\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.2507 - accuracy: 0.8947 - recall: 0.8947 - f1: 0.8947 - val_loss: 0.4410 - val_accuracy: 0.7769 - val_recall: 0.7761 - val_f1: 0.7767\n",
      "Epoch 9/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2493 - accuracy: 0.8953 - recall: 0.8954 - f1: 0.8953\n",
      "Epoch 00009: val_loss improved from 0.37381 to 0.36026, saving model to batch_relu_validation_200-009-0.895367-0.821100.h5\n",
      "150/150 [==============================] - 68s 454ms/step - loss: 0.2491 - accuracy: 0.8954 - recall: 0.8955 - f1: 0.8954 - val_loss: 0.3603 - val_accuracy: 0.8211 - val_recall: 0.8223 - val_f1: 0.8213\n",
      "Epoch 10/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2449 - accuracy: 0.8965 - recall: 0.8954 - f1: 0.8964\n",
      "Epoch 00010: val_loss did not improve from 0.36026\n",
      "150/150 [==============================] - 67s 450ms/step - loss: 0.2448 - accuracy: 0.8964 - recall: 0.8953 - f1: 0.8963 - val_loss: 0.4443 - val_accuracy: 0.7939 - val_recall: 0.7914 - val_f1: 0.7934\n",
      "Epoch 11/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2319 - accuracy: 0.9028 - recall: 0.9021 - f1: 0.9027\n",
      "Epoch 00011: val_loss did not improve from 0.36026\n",
      "150/150 [==============================] - 68s 456ms/step - loss: 0.2318 - accuracy: 0.9030 - recall: 0.9023 - f1: 0.9030 - val_loss: 0.4460 - val_accuracy: 0.7894 - val_recall: 0.7901 - val_f1: 0.7895\n",
      "Epoch 12/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2256 - accuracy: 0.9075 - recall: 0.9066 - f1: 0.9074\n",
      "Epoch 00012: val_loss did not improve from 0.36026\n",
      "150/150 [==============================] - 68s 455ms/step - loss: 0.2254 - accuracy: 0.9077 - recall: 0.9067 - f1: 0.9076 - val_loss: 0.5510 - val_accuracy: 0.7635 - val_recall: 0.7634 - val_f1: 0.7635\n",
      "Epoch 13/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2270 - accuracy: 0.9051 - recall: 0.9046 - f1: 0.9051\n",
      "Epoch 00013: val_loss did not improve from 0.36026\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.2269 - accuracy: 0.9050 - recall: 0.9044 - f1: 0.9049 - val_loss: 0.4410 - val_accuracy: 0.7951 - val_recall: 0.7941 - val_f1: 0.7949\n",
      "Epoch 14/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2204 - accuracy: 0.9082 - recall: 0.9076 - f1: 0.9081\n",
      "Epoch 00014: val_loss improved from 0.36026 to 0.35490, saving model to batch_relu_validation_200-014-0.908000-0.835900.h5\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.2207 - accuracy: 0.9080 - recall: 0.9074 - f1: 0.9079 - val_loss: 0.3549 - val_accuracy: 0.8359 - val_recall: 0.8343 - val_f1: 0.8356\n",
      "Epoch 15/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2091 - accuracy: 0.9141 - recall: 0.9138 - f1: 0.9141\n",
      "Epoch 00015: val_loss did not improve from 0.35490\n",
      "150/150 [==============================] - 67s 450ms/step - loss: 0.2087 - accuracy: 0.9143 - recall: 0.9141 - f1: 0.9143 - val_loss: 0.5355 - val_accuracy: 0.7889 - val_recall: 0.7886 - val_f1: 0.7888\n",
      "Epoch 16/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2126 - accuracy: 0.9137 - recall: 0.9144 - f1: 0.9138\n",
      "Epoch 00016: val_loss did not improve from 0.35490\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.2119 - accuracy: 0.9141 - recall: 0.9148 - f1: 0.9141 - val_loss: 0.4850 - val_accuracy: 0.8035 - val_recall: 0.8043 - val_f1: 0.8037\n",
      "Epoch 17/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2060 - accuracy: 0.9180 - recall: 0.9182 - f1: 0.9180\n",
      "Epoch 00017: val_loss improved from 0.35490 to 0.31614, saving model to batch_relu_validation_200-017-0.918167-0.867033.h5\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.2056 - accuracy: 0.9182 - recall: 0.9183 - f1: 0.9182 - val_loss: 0.3161 - val_accuracy: 0.8670 - val_recall: 0.8684 - val_f1: 0.8672\n",
      "Epoch 18/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.2014 - accuracy: 0.9162 - recall: 0.9163 - f1: 0.9162\n",
      "Epoch 00018: val_loss did not improve from 0.31614\n",
      "150/150 [==============================] - 67s 448ms/step - loss: 0.2014 - accuracy: 0.9162 - recall: 0.9163 - f1: 0.9162 - val_loss: 0.6109 - val_accuracy: 0.8019 - val_recall: 0.8021 - val_f1: 0.8019\n",
      "Epoch 19/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1981 - accuracy: 0.9209 - recall: 0.9207 - f1: 0.9209\n",
      "Epoch 00019: val_loss improved from 0.31614 to 0.26036, saving model to batch_relu_validation_200-019-0.920833-0.885067.h5\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1984 - accuracy: 0.9208 - recall: 0.9207 - f1: 0.9208 - val_loss: 0.2604 - val_accuracy: 0.8851 - val_recall: 0.8868 - val_f1: 0.8853\n",
      "Epoch 20/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1984 - accuracy: 0.9167 - recall: 0.9165 - f1: 0.9167\n",
      "Epoch 00020: val_loss did not improve from 0.26036\n",
      "150/150 [==============================] - 68s 456ms/step - loss: 0.1982 - accuracy: 0.9170 - recall: 0.9167 - f1: 0.9169 - val_loss: 0.3679 - val_accuracy: 0.8204 - val_recall: 0.8199 - val_f1: 0.8203\n",
      "Epoch 21/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1940 - accuracy: 0.9212 - recall: 0.9202 - f1: 0.9211\n",
      "Epoch 00021: val_loss did not improve from 0.26036\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1947 - accuracy: 0.9208 - recall: 0.9199 - f1: 0.9208 - val_loss: 0.7710 - val_accuracy: 0.7882 - val_recall: 0.7877 - val_f1: 0.7881\n",
      "Epoch 22/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1979 - accuracy: 0.9187 - recall: 0.9185 - f1: 0.9187\n",
      "Epoch 00022: val_loss did not improve from 0.26036\n",
      "150/150 [==============================] - 78s 522ms/step - loss: 0.1981 - accuracy: 0.9184 - recall: 0.9182 - f1: 0.9184 - val_loss: 0.3391 - val_accuracy: 0.8423 - val_recall: 0.8441 - val_f1: 0.8426\n",
      "Epoch 23/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1946 - accuracy: 0.9206 - recall: 0.9207 - f1: 0.9206\n",
      "Epoch 00023: val_loss did not improve from 0.26036\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.1946 - accuracy: 0.9204 - recall: 0.9206 - f1: 0.9204 - val_loss: 0.5076 - val_accuracy: 0.7750 - val_recall: 0.7753 - val_f1: 0.7751\n",
      "Epoch 24/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1922 - accuracy: 0.9227 - recall: 0.9227 - f1: 0.9226\n",
      "Epoch 00024: val_loss did not improve from 0.26036\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1927 - accuracy: 0.9226 - recall: 0.9226 - f1: 0.9226 - val_loss: 0.3248 - val_accuracy: 0.8459 - val_recall: 0.8459 - val_f1: 0.8459\n",
      "Epoch 25/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1889 - accuracy: 0.9193 - recall: 0.9190 - f1: 0.9193\n",
      "Epoch 00025: val_loss did not improve from 0.26036\n",
      "150/150 [==============================] - 67s 448ms/step - loss: 0.1891 - accuracy: 0.9192 - recall: 0.9189 - f1: 0.9192 - val_loss: 0.2984 - val_accuracy: 0.8669 - val_recall: 0.8678 - val_f1: 0.8670\n",
      "Epoch 26/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1856 - accuracy: 0.9258 - recall: 0.9258 - f1: 0.9258\n",
      "Epoch 00026: val_loss did not improve from 0.26036\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1859 - accuracy: 0.9256 - recall: 0.9257 - f1: 0.9256 - val_loss: 0.3464 - val_accuracy: 0.8355 - val_recall: 0.8365 - val_f1: 0.8357\n",
      "Epoch 27/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1873 - accuracy: 0.9263 - recall: 0.9262 - f1: 0.9263\n",
      "Epoch 00027: val_loss did not improve from 0.26036\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1868 - accuracy: 0.9265 - recall: 0.9264 - f1: 0.9265 - val_loss: 0.3248 - val_accuracy: 0.8483 - val_recall: 0.8489 - val_f1: 0.8484\n",
      "Epoch 28/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1805 - accuracy: 0.9274 - recall: 0.9275 - f1: 0.9274\n",
      "Epoch 00028: val_loss improved from 0.26036 to 0.25365, saving model to batch_relu_validation_200-028-0.927567-0.887833.h5\n",
      "150/150 [==============================] - 69s 458ms/step - loss: 0.1803 - accuracy: 0.9276 - recall: 0.9277 - f1: 0.9276 - val_loss: 0.2536 - val_accuracy: 0.8878 - val_recall: 0.8883 - val_f1: 0.8879\n",
      "Epoch 29/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1780 - accuracy: 0.9280 - recall: 0.9283 - f1: 0.9280\n",
      "Epoch 00029: val_loss did not improve from 0.25365\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1782 - accuracy: 0.9280 - recall: 0.9283 - f1: 0.9281 - val_loss: 0.4213 - val_accuracy: 0.8170 - val_recall: 0.8174 - val_f1: 0.8171\n",
      "Epoch 30/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1824 - accuracy: 0.9253 - recall: 0.9248 - f1: 0.9253\n",
      "Epoch 00030: val_loss did not improve from 0.25365\n",
      "150/150 [==============================] - 67s 448ms/step - loss: 0.1825 - accuracy: 0.9253 - recall: 0.9248 - f1: 0.9252 - val_loss: 0.3049 - val_accuracy: 0.8625 - val_recall: 0.8617 - val_f1: 0.8624\n",
      "Epoch 31/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1805 - accuracy: 0.9266 - recall: 0.9266 - f1: 0.9266\n",
      "Epoch 00031: val_loss did not improve from 0.25365\n",
      "150/150 [==============================] - 68s 455ms/step - loss: 0.1802 - accuracy: 0.9267 - recall: 0.9267 - f1: 0.9267 - val_loss: 0.2652 - val_accuracy: 0.8831 - val_recall: 0.8828 - val_f1: 0.8830\n",
      "Epoch 32/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1727 - accuracy: 0.9297 - recall: 0.9293 - f1: 0.9296\n",
      "Epoch 00032: val_loss improved from 0.25365 to 0.25281, saving model to batch_relu_validation_200-032-0.929600-0.897400.h5\n",
      "150/150 [==============================] - 69s 463ms/step - loss: 0.1727 - accuracy: 0.9296 - recall: 0.9292 - f1: 0.9296 - val_loss: 0.2528 - val_accuracy: 0.8974 - val_recall: 0.8979 - val_f1: 0.8975\n",
      "Epoch 33/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1671 - accuracy: 0.9328 - recall: 0.9328 - f1: 0.9328\n",
      "Epoch 00033: val_loss did not improve from 0.25281\n",
      "150/150 [==============================] - 70s 466ms/step - loss: 0.1677 - accuracy: 0.9325 - recall: 0.9324 - f1: 0.9325 - val_loss: 0.7300 - val_accuracy: 0.8027 - val_recall: 0.8026 - val_f1: 0.8026\n",
      "Epoch 34/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1701 - accuracy: 0.9303 - recall: 0.9302 - f1: 0.9303\n",
      "Epoch 00034: val_loss did not improve from 0.25281\n",
      "150/150 [==============================] - 67s 448ms/step - loss: 0.1700 - accuracy: 0.9303 - recall: 0.9302 - f1: 0.9303 - val_loss: 0.2873 - val_accuracy: 0.8718 - val_recall: 0.8722 - val_f1: 0.8719\n",
      "Epoch 35/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1691 - accuracy: 0.9310 - recall: 0.9310 - f1: 0.9310\n",
      "Epoch 00035: val_loss did not improve from 0.25281\n",
      "150/150 [==============================] - 68s 456ms/step - loss: 0.1689 - accuracy: 0.9310 - recall: 0.9311 - f1: 0.9310 - val_loss: 0.3470 - val_accuracy: 0.8608 - val_recall: 0.8606 - val_f1: 0.8608\n",
      "Epoch 36/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1685 - accuracy: 0.9309 - recall: 0.9308 - f1: 0.9309\n",
      "Epoch 00036: val_loss improved from 0.25281 to 0.25094, saving model to batch_relu_validation_200-036-0.931133-0.900033.h5\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1682 - accuracy: 0.9311 - recall: 0.9310 - f1: 0.9311 - val_loss: 0.2509 - val_accuracy: 0.9000 - val_recall: 0.9005 - val_f1: 0.9001\n",
      "Epoch 37/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1748 - accuracy: 0.9297 - recall: 0.9297 - f1: 0.9297\n",
      "Epoch 00037: val_loss improved from 0.25094 to 0.24714, saving model to batch_relu_validation_200-037-0.929433-0.891333.h5\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "150/150 [==============================] - 68s 451ms/step - loss: 0.1750 - accuracy: 0.9294 - recall: 0.9294 - f1: 0.9294 - val_loss: 0.2471 - val_accuracy: 0.8913 - val_recall: 0.8914 - val_f1: 0.8913\n",
      "Epoch 38/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9367 - recall: 0.9368 - f1: 0.9367\n",
      "Epoch 00038: val_loss improved from 0.24714 to 0.21298, saving model to batch_relu_validation_200-038-0.936967-0.908300.h5\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1563 - accuracy: 0.9370 - recall: 0.9371 - f1: 0.9370 - val_loss: 0.2130 - val_accuracy: 0.9083 - val_recall: 0.9084 - val_f1: 0.9083\n",
      "Epoch 39/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1509 - accuracy: 0.9397 - recall: 0.9396 - f1: 0.9397\n",
      "Epoch 00039: val_loss improved from 0.21298 to 0.20633, saving model to batch_relu_validation_200-039-0.939467-0.913300.h5\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1515 - accuracy: 0.9395 - recall: 0.9394 - f1: 0.9395 - val_loss: 0.2063 - val_accuracy: 0.9133 - val_recall: 0.9133 - val_f1: 0.9133\n",
      "Epoch 40/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1552 - accuracy: 0.9370 - recall: 0.9366 - f1: 0.9370\n",
      "Epoch 00040: val_loss did not improve from 0.20633\n",
      "150/150 [==============================] - 68s 455ms/step - loss: 0.1554 - accuracy: 0.9368 - recall: 0.9365 - f1: 0.9368 - val_loss: 0.2725 - val_accuracy: 0.8959 - val_recall: 0.8959 - val_f1: 0.8959\n",
      "Epoch 41/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1496 - accuracy: 0.9375 - recall: 0.9374 - f1: 0.9375\n",
      "Epoch 00041: val_loss did not improve from 0.20633\n",
      "150/150 [==============================] - 67s 448ms/step - loss: 0.1496 - accuracy: 0.9375 - recall: 0.9375 - f1: 0.9375 - val_loss: 0.2605 - val_accuracy: 0.8835 - val_recall: 0.8835 - val_f1: 0.8835\n",
      "Epoch 42/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1519 - accuracy: 0.9381 - recall: 0.9381 - f1: 0.9381\n",
      "Epoch 00042: val_loss did not improve from 0.20633\n",
      "150/150 [==============================] - 68s 451ms/step - loss: 0.1517 - accuracy: 0.9381 - recall: 0.9381 - f1: 0.9381 - val_loss: 0.2105 - val_accuracy: 0.9089 - val_recall: 0.9085 - val_f1: 0.9089\n",
      "Epoch 43/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1482 - accuracy: 0.9401 - recall: 0.9397 - f1: 0.9401\n",
      "Epoch 00043: val_loss did not improve from 0.20633\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1485 - accuracy: 0.9400 - recall: 0.9397 - f1: 0.9400 - val_loss: 0.2329 - val_accuracy: 0.9008 - val_recall: 0.9009 - val_f1: 0.9008\n",
      "Epoch 44/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1560 - accuracy: 0.9364 - recall: 0.9364 - f1: 0.9364\n",
      "Epoch 00044: val_loss did not improve from 0.20633\n",
      "150/150 [==============================] - 68s 457ms/step - loss: 0.1556 - accuracy: 0.9364 - recall: 0.9364 - f1: 0.9364 - val_loss: 0.2334 - val_accuracy: 0.9002 - val_recall: 0.8997 - val_f1: 0.9001\n",
      "Epoch 45/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1455 - accuracy: 0.9418 - recall: 0.9416 - f1: 0.9418\n",
      "Epoch 00045: val_loss improved from 0.20633 to 0.20597, saving model to batch_relu_validation_200-045-0.941767-0.913333.h5\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1454 - accuracy: 0.9418 - recall: 0.9416 - f1: 0.9418 - val_loss: 0.2060 - val_accuracy: 0.9133 - val_recall: 0.9133 - val_f1: 0.9133\n",
      "Epoch 46/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1474 - accuracy: 0.9402 - recall: 0.9401 - f1: 0.9402\n",
      "Epoch 00046: val_loss did not improve from 0.20597\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1471 - accuracy: 0.9403 - recall: 0.9402 - f1: 0.9403 - val_loss: 0.2141 - val_accuracy: 0.9074 - val_recall: 0.9074 - val_f1: 0.9074\n",
      "Epoch 47/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9404 - recall: 0.9403 - f1: 0.9404\n",
      "Epoch 00047: val_loss did not improve from 0.20597\n",
      "150/150 [==============================] - 68s 452ms/step - loss: 0.1488 - accuracy: 0.9405 - recall: 0.9405 - f1: 0.9405 - val_loss: 0.2138 - val_accuracy: 0.9088 - val_recall: 0.9089 - val_f1: 0.9088\n",
      "Epoch 48/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1468 - accuracy: 0.9411 - recall: 0.9411 - f1: 0.9411\n",
      "Epoch 00048: val_loss improved from 0.20597 to 0.20063, saving model to batch_relu_validation_200-048-0.940933-0.915300.h5\n",
      "150/150 [==============================] - 68s 454ms/step - loss: 0.1470 - accuracy: 0.9409 - recall: 0.9410 - f1: 0.9409 - val_loss: 0.2006 - val_accuracy: 0.9153 - val_recall: 0.9150 - val_f1: 0.9153\n",
      "Epoch 49/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1410 - accuracy: 0.9409 - recall: 0.9410 - f1: 0.9409\n",
      "Epoch 00049: val_loss did not improve from 0.20063\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.1407 - accuracy: 0.9409 - recall: 0.9410 - f1: 0.9409 - val_loss: 0.2050 - val_accuracy: 0.9142 - val_recall: 0.9142 - val_f1: 0.9142\n",
      "Epoch 50/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1451 - accuracy: 0.9384 - recall: 0.9385 - f1: 0.9384\n",
      "Epoch 00050: val_loss did not improve from 0.20063\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.1454 - accuracy: 0.9385 - recall: 0.9386 - f1: 0.9385 - val_loss: 0.2074 - val_accuracy: 0.9132 - val_recall: 0.9131 - val_f1: 0.9132\n",
      "Epoch 51/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1522 - accuracy: 0.9385 - recall: 0.9387 - f1: 0.9385\n",
      "Epoch 00051: val_loss did not improve from 0.20063\n",
      "150/150 [==============================] - 68s 455ms/step - loss: 0.1528 - accuracy: 0.9380 - recall: 0.9382 - f1: 0.9380 - val_loss: 0.2674 - val_accuracy: 0.8881 - val_recall: 0.8886 - val_f1: 0.8881\n",
      "Epoch 52/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1437 - accuracy: 0.9381 - recall: 0.9381 - f1: 0.9381\n",
      "Epoch 00052: val_loss did not improve from 0.20063\n",
      "150/150 [==============================] - 68s 452ms/step - loss: 0.1436 - accuracy: 0.9381 - recall: 0.9381 - f1: 0.9381 - val_loss: 0.2301 - val_accuracy: 0.9160 - val_recall: 0.9158 - val_f1: 0.9160\n",
      "Epoch 53/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1431 - accuracy: 0.9434 - recall: 0.9434 - f1: 0.9434\n",
      "Epoch 00053: val_loss did not improve from 0.20063\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "150/150 [==============================] - 67s 444ms/step - loss: 0.1426 - accuracy: 0.9435 - recall: 0.9435 - f1: 0.9435 - val_loss: 0.2233 - val_accuracy: 0.9081 - val_recall: 0.9082 - val_f1: 0.9081\n",
      "Epoch 54/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1382 - accuracy: 0.9416 - recall: 0.9415 - f1: 0.9416\n",
      "Epoch 00054: val_loss did not improve from 0.20063\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1378 - accuracy: 0.9417 - recall: 0.9417 - f1: 0.9417 - val_loss: 0.2025 - val_accuracy: 0.9148 - val_recall: 0.9148 - val_f1: 0.9148\n",
      "Epoch 55/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1340 - accuracy: 0.9456 - recall: 0.9458 - f1: 0.9456\n",
      "Epoch 00055: val_loss did not improve from 0.20063\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1341 - accuracy: 0.9454 - recall: 0.9457 - f1: 0.9455 - val_loss: 0.2094 - val_accuracy: 0.9140 - val_recall: 0.9139 - val_f1: 0.9140\n",
      "Epoch 56/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1383 - accuracy: 0.9434 - recall: 0.9433 - f1: 0.9434\n",
      "Epoch 00056: val_loss improved from 0.20063 to 0.18905, saving model to batch_relu_validation_200-056-0.943367-0.921667.h5\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.1383 - accuracy: 0.9434 - recall: 0.9433 - f1: 0.9434 - val_loss: 0.1891 - val_accuracy: 0.9217 - val_recall: 0.9215 - val_f1: 0.9217\n",
      "Epoch 57/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1400 - accuracy: 0.9439 - recall: 0.9440 - f1: 0.9439\n",
      "Epoch 00057: val_loss did not improve from 0.18905\n",
      "150/150 [==============================] - 67s 445ms/step - loss: 0.1398 - accuracy: 0.9440 - recall: 0.9441 - f1: 0.9440 - val_loss: 0.2419 - val_accuracy: 0.8981 - val_recall: 0.8981 - val_f1: 0.8981\n",
      "Epoch 58/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1375 - accuracy: 0.9417 - recall: 0.9417 - f1: 0.9417\n",
      "Epoch 00058: val_loss did not improve from 0.18905\n",
      "150/150 [==============================] - 66s 443ms/step - loss: 0.1381 - accuracy: 0.9414 - recall: 0.9414 - f1: 0.9414 - val_loss: 0.3047 - val_accuracy: 0.8763 - val_recall: 0.8764 - val_f1: 0.8763\n",
      "Epoch 59/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1351 - accuracy: 0.9441 - recall: 0.9438 - f1: 0.9441\n",
      "Epoch 00059: val_loss did not improve from 0.18905\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.1351 - accuracy: 0.9439 - recall: 0.9437 - f1: 0.9439 - val_loss: 0.2165 - val_accuracy: 0.9181 - val_recall: 0.9179 - val_f1: 0.9181\n",
      "Epoch 60/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1311 - accuracy: 0.9475 - recall: 0.9474 - f1: 0.9475\n",
      "Epoch 00060: val_loss did not improve from 0.18905\n",
      "150/150 [==============================] - 68s 452ms/step - loss: 0.1311 - accuracy: 0.9475 - recall: 0.9473 - f1: 0.9475 - val_loss: 0.2034 - val_accuracy: 0.9129 - val_recall: 0.9126 - val_f1: 0.9128\n",
      "Epoch 61/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1305 - accuracy: 0.9453 - recall: 0.9454 - f1: 0.9453\n",
      "Epoch 00061: val_loss did not improve from 0.18905\n",
      "150/150 [==============================] - 67s 443ms/step - loss: 0.1305 - accuracy: 0.9450 - recall: 0.9451 - f1: 0.9450 - val_loss: 0.1944 - val_accuracy: 0.9192 - val_recall: 0.9188 - val_f1: 0.9192\n",
      "Epoch 62/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9487 - recall: 0.9484 - f1: 0.9487\n",
      "Epoch 00062: val_loss did not improve from 0.18905\n",
      "150/150 [==============================] - 66s 442ms/step - loss: 0.1301 - accuracy: 0.9487 - recall: 0.9483 - f1: 0.9486 - val_loss: 0.2199 - val_accuracy: 0.9049 - val_recall: 0.9047 - val_f1: 0.9048\n",
      "Epoch 63/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1287 - accuracy: 0.9460 - recall: 0.9460 - f1: 0.9460\n",
      "Epoch 00063: val_loss did not improve from 0.18905\n",
      "150/150 [==============================] - 68s 452ms/step - loss: 0.1286 - accuracy: 0.9461 - recall: 0.9461 - f1: 0.9461 - val_loss: 0.1920 - val_accuracy: 0.9171 - val_recall: 0.9169 - val_f1: 0.9171\n",
      "Epoch 64/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9488 - recall: 0.9487 - f1: 0.9488\n",
      "Epoch 00064: val_loss did not improve from 0.18905\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.1254 - accuracy: 0.9488 - recall: 0.9487 - f1: 0.9488 - val_loss: 0.1953 - val_accuracy: 0.9160 - val_recall: 0.9157 - val_f1: 0.9160\n",
      "Epoch 65/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9502 - recall: 0.9503 - f1: 0.9502\n",
      "Epoch 00065: val_loss improved from 0.18905 to 0.17861, saving model to batch_relu_validation_200-065-0.950100-0.924933.h5\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1214 - accuracy: 0.9501 - recall: 0.9503 - f1: 0.9501 - val_loss: 0.1786 - val_accuracy: 0.9249 - val_recall: 0.9248 - val_f1: 0.9249\n",
      "Epoch 66/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1290 - accuracy: 0.9485 - recall: 0.9483 - f1: 0.9485\n",
      "Epoch 00066: val_loss did not improve from 0.17861\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.1287 - accuracy: 0.9487 - recall: 0.9485 - f1: 0.9487 - val_loss: 0.1907 - val_accuracy: 0.9209 - val_recall: 0.9208 - val_f1: 0.9209\n",
      "Epoch 67/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1296 - accuracy: 0.9468 - recall: 0.9467 - f1: 0.9468\n",
      "Epoch 00067: val_loss did not improve from 0.17861\n",
      "150/150 [==============================] - 68s 455ms/step - loss: 0.1296 - accuracy: 0.9466 - recall: 0.9465 - f1: 0.9466 - val_loss: 0.1827 - val_accuracy: 0.9263 - val_recall: 0.9261 - val_f1: 0.9263\n",
      "Epoch 68/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9505 - recall: 0.9505 - f1: 0.9505\n",
      "Epoch 00068: val_loss did not improve from 0.17861\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1249 - accuracy: 0.9502 - recall: 0.9502 - f1: 0.9502 - val_loss: 0.1965 - val_accuracy: 0.9164 - val_recall: 0.9162 - val_f1: 0.9164\n",
      "Epoch 69/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9469 - recall: 0.9469 - f1: 0.9469\n",
      "Epoch 00069: val_loss did not improve from 0.17861\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "150/150 [==============================] - 67s 447ms/step - loss: 0.1277 - accuracy: 0.9468 - recall: 0.9468 - f1: 0.9468 - val_loss: 0.1817 - val_accuracy: 0.9265 - val_recall: 0.9264 - val_f1: 0.9265\n",
      "Epoch 70/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9483 - recall: 0.9481 - f1: 0.9483\n",
      "Epoch 00070: val_loss improved from 0.17861 to 0.17550, saving model to batch_relu_validation_200-070-0.948267-0.928433.h5\n",
      "150/150 [==============================] - 67s 450ms/step - loss: 0.1237 - accuracy: 0.9483 - recall: 0.9480 - f1: 0.9483 - val_loss: 0.1755 - val_accuracy: 0.9284 - val_recall: 0.9283 - val_f1: 0.9284\n",
      "Epoch 71/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9465 - recall: 0.9463 - f1: 0.9465\n",
      "Epoch 00071: val_loss did not improve from 0.17550\n",
      "150/150 [==============================] - 68s 455ms/step - loss: 0.1269 - accuracy: 0.9466 - recall: 0.9463 - f1: 0.9466 - val_loss: 0.1813 - val_accuracy: 0.9240 - val_recall: 0.9238 - val_f1: 0.9240\n",
      "Epoch 72/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9497 - recall: 0.9497 - f1: 0.9497\n",
      "Epoch 00072: val_loss did not improve from 0.17550\n",
      "150/150 [==============================] - 68s 452ms/step - loss: 0.1263 - accuracy: 0.9499 - recall: 0.9499 - f1: 0.9499 - val_loss: 0.1866 - val_accuracy: 0.9224 - val_recall: 0.9221 - val_f1: 0.9224\n",
      "Epoch 73/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9486 - recall: 0.9486 - f1: 0.9486\n",
      "Epoch 00073: val_loss did not improve from 0.17550\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "150/150 [==============================] - 67s 449ms/step - loss: 0.1256 - accuracy: 0.9486 - recall: 0.9485 - f1: 0.9486 - val_loss: 0.1881 - val_accuracy: 0.9251 - val_recall: 0.9251 - val_f1: 0.9251\n",
      "Epoch 74/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1269 - accuracy: 0.9461 - recall: 0.9459 - f1: 0.9461\n",
      "Epoch 00074: val_loss improved from 0.17550 to 0.17388, saving model to batch_relu_validation_200-074-0.946167-0.927667.h5\n",
      "150/150 [==============================] - 68s 450ms/step - loss: 0.1269 - accuracy: 0.9462 - recall: 0.9459 - f1: 0.9461 - val_loss: 0.1739 - val_accuracy: 0.9277 - val_recall: 0.9277 - val_f1: 0.9277\n",
      "Epoch 75/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9485 - recall: 0.9486 - f1: 0.9485\n",
      "Epoch 00075: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 466ms/step - loss: 0.1228 - accuracy: 0.9487 - recall: 0.9488 - f1: 0.9487 - val_loss: 0.1921 - val_accuracy: 0.9201 - val_recall: 0.9201 - val_f1: 0.9201\n",
      "Epoch 76/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9482 - recall: 0.9481 - f1: 0.9482\n",
      "Epoch 00076: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 68s 454ms/step - loss: 0.1272 - accuracy: 0.9483 - recall: 0.9483 - f1: 0.9483 - val_loss: 0.1887 - val_accuracy: 0.9195 - val_recall: 0.9193 - val_f1: 0.9195\n",
      "Epoch 77/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9496 - recall: 0.9496 - f1: 0.9496\n",
      "Epoch 00077: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "150/150 [==============================] - 68s 450ms/step - loss: 0.1249 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495 - val_loss: 0.1750 - val_accuracy: 0.9266 - val_recall: 0.9265 - val_f1: 0.9266\n",
      "Epoch 78/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9493 - recall: 0.9495 - f1: 0.9493\n",
      "Epoch 00078: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 67s 446ms/step - loss: 0.1238 - accuracy: 0.9491 - recall: 0.9493 - f1: 0.9491 - val_loss: 0.1765 - val_accuracy: 0.9249 - val_recall: 0.9246 - val_f1: 0.9249\n",
      "Epoch 79/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9526 - recall: 0.9528 - f1: 0.9526\n",
      "Epoch 00079: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 467ms/step - loss: 0.1201 - accuracy: 0.9524 - recall: 0.9526 - f1: 0.9524 - val_loss: 0.1861 - val_accuracy: 0.9240 - val_recall: 0.9238 - val_f1: 0.9240\n",
      "Epoch 80/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9484 - recall: 0.9483 - f1: 0.9484\n",
      "Epoch 00080: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 461ms/step - loss: 0.1212 - accuracy: 0.9484 - recall: 0.9483 - f1: 0.9484 - val_loss: 0.1762 - val_accuracy: 0.9259 - val_recall: 0.9258 - val_f1: 0.9259\n",
      "Epoch 81/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9483 - recall: 0.9483 - f1: 0.9483\n",
      "Epoch 00081: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 463ms/step - loss: 0.1264 - accuracy: 0.9484 - recall: 0.9484 - f1: 0.9484 - val_loss: 0.1847 - val_accuracy: 0.9250 - val_recall: 0.9249 - val_f1: 0.9250\n",
      "Epoch 82/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507\n",
      "Epoch 00082: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 72s 481ms/step - loss: 0.1200 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507 - val_loss: 0.1851 - val_accuracy: 0.9212 - val_recall: 0.9211 - val_f1: 0.9212\n",
      "Epoch 83/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1212 - accuracy: 0.9511 - recall: 0.9509 - f1: 0.9511\n",
      "Epoch 00083: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "150/150 [==============================] - 72s 478ms/step - loss: 0.1209 - accuracy: 0.9512 - recall: 0.9510 - f1: 0.9512 - val_loss: 0.1753 - val_accuracy: 0.9279 - val_recall: 0.9279 - val_f1: 0.9279\n",
      "Epoch 84/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1315 - accuracy: 0.9471 - recall: 0.9472 - f1: 0.9472\n",
      "Epoch 00084: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 77s 514ms/step - loss: 0.1315 - accuracy: 0.9470 - recall: 0.9471 - f1: 0.9470 - val_loss: 0.1751 - val_accuracy: 0.9291 - val_recall: 0.9291 - val_f1: 0.9291\n",
      "Epoch 85/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9512 - recall: 0.9512 - f1: 0.9512\n",
      "Epoch 00085: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 76s 508ms/step - loss: 0.1216 - accuracy: 0.9513 - recall: 0.9513 - f1: 0.9513 - val_loss: 0.1831 - val_accuracy: 0.9268 - val_recall: 0.9267 - val_f1: 0.9268\n",
      "Epoch 86/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9512 - recall: 0.9513 - f1: 0.9512\n",
      "Epoch 00086: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 76s 509ms/step - loss: 0.1226 - accuracy: 0.9513 - recall: 0.9514 - f1: 0.9513 - val_loss: 0.1834 - val_accuracy: 0.9253 - val_recall: 0.9251 - val_f1: 0.9253\n",
      "Epoch 87/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9481 - recall: 0.9479 - f1: 0.9480\n",
      "Epoch 00087: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "150/150 [==============================] - 87s 582ms/step - loss: 0.1254 - accuracy: 0.9480 - recall: 0.9479 - f1: 0.9480 - val_loss: 0.1813 - val_accuracy: 0.9240 - val_recall: 0.9240 - val_f1: 0.9240\n",
      "Epoch 88/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9472 - recall: 0.9472 - f1: 0.9472\n",
      "Epoch 00088: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 83s 555ms/step - loss: 0.1273 - accuracy: 0.9473 - recall: 0.9473 - f1: 0.9473 - val_loss: 0.1796 - val_accuracy: 0.9274 - val_recall: 0.9273 - val_f1: 0.9274\n",
      "Epoch 89/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1196 - accuracy: 0.9520 - recall: 0.9521 - f1: 0.9520\n",
      "Epoch 00089: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 81s 542ms/step - loss: 0.1200 - accuracy: 0.9520 - recall: 0.9521 - f1: 0.9520 - val_loss: 0.1832 - val_accuracy: 0.9221 - val_recall: 0.9221 - val_f1: 0.9221\n",
      "Epoch 90/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9512 - recall: 0.9510 - f1: 0.9512\n",
      "Epoch 00090: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 78s 517ms/step - loss: 0.1218 - accuracy: 0.9510 - recall: 0.9509 - f1: 0.9510 - val_loss: 0.1848 - val_accuracy: 0.9227 - val_recall: 0.9228 - val_f1: 0.9227\n",
      "Epoch 91/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9522 - recall: 0.9523 - f1: 0.9522\n",
      "Epoch 00091: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 76s 504ms/step - loss: 0.1223 - accuracy: 0.9520 - recall: 0.9521 - f1: 0.9520 - val_loss: 0.1797 - val_accuracy: 0.9254 - val_recall: 0.9253 - val_f1: 0.9254\n",
      "Epoch 92/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 0.9484 - recall: 0.9482 - f1: 0.9484\n",
      "Epoch 00092: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 75s 501ms/step - loss: 0.1267 - accuracy: 0.9485 - recall: 0.9483 - f1: 0.9485 - val_loss: 0.1894 - val_accuracy: 0.9227 - val_recall: 0.9226 - val_f1: 0.9227\n",
      "Epoch 93/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9497 - recall: 0.9497 - f1: 0.9497\n",
      "Epoch 00093: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "150/150 [==============================] - 75s 502ms/step - loss: 0.1266 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495 - val_loss: 0.1789 - val_accuracy: 0.9251 - val_recall: 0.9249 - val_f1: 0.9251\n",
      "Epoch 94/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9491 - recall: 0.9489 - f1: 0.9491\n",
      "Epoch 00094: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 75s 500ms/step - loss: 0.1259 - accuracy: 0.9490 - recall: 0.9488 - f1: 0.9490 - val_loss: 0.1839 - val_accuracy: 0.9239 - val_recall: 0.9236 - val_f1: 0.9239\n",
      "Epoch 95/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9521 - recall: 0.9523 - f1: 0.9521\n",
      "Epoch 00095: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 76s 508ms/step - loss: 0.1190 - accuracy: 0.9524 - recall: 0.9526 - f1: 0.9524 - val_loss: 0.1810 - val_accuracy: 0.9256 - val_recall: 0.9255 - val_f1: 0.9256\n",
      "Epoch 96/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9504 - recall: 0.9504 - f1: 0.9504\n",
      "Epoch 00096: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 72s 481ms/step - loss: 0.1219 - accuracy: 0.9504 - recall: 0.9504 - f1: 0.9504 - val_loss: 0.1777 - val_accuracy: 0.9271 - val_recall: 0.9271 - val_f1: 0.9271\n",
      "Epoch 97/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488\n",
      "Epoch 00097: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 73s 486ms/step - loss: 0.1251 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488 - val_loss: 0.1876 - val_accuracy: 0.9224 - val_recall: 0.9224 - val_f1: 0.9224\n",
      "Epoch 98/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9493 - recall: 0.9491 - f1: 0.9493\n",
      "Epoch 00098: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 78s 518ms/step - loss: 0.1261 - accuracy: 0.9491 - recall: 0.9489 - f1: 0.9491 - val_loss: 0.1856 - val_accuracy: 0.9240 - val_recall: 0.9239 - val_f1: 0.9240\n",
      "Epoch 99/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9492 - recall: 0.9492 - f1: 0.9492\n",
      "Epoch 00099: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "150/150 [==============================] - 77s 516ms/step - loss: 0.1229 - accuracy: 0.9494 - recall: 0.9493 - f1: 0.9494 - val_loss: 0.1835 - val_accuracy: 0.9228 - val_recall: 0.9226 - val_f1: 0.9228\n",
      "Epoch 100/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1283 - accuracy: 0.9466 - recall: 0.9466 - f1: 0.9466\n",
      "Epoch 00100: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 79s 530ms/step - loss: 0.1281 - accuracy: 0.9467 - recall: 0.9467 - f1: 0.9467 - val_loss: 0.1781 - val_accuracy: 0.9268 - val_recall: 0.9268 - val_f1: 0.9268\n",
      "Epoch 101/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9500 - recall: 0.9499 - f1: 0.9500\n",
      "Epoch 00101: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 469ms/step - loss: 0.1237 - accuracy: 0.9499 - recall: 0.9498 - f1: 0.9499 - val_loss: 0.1856 - val_accuracy: 0.9229 - val_recall: 0.9227 - val_f1: 0.9228\n",
      "Epoch 102/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9481 - recall: 0.9482 - f1: 0.9481\n",
      "Epoch 00102: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 75s 502ms/step - loss: 0.1229 - accuracy: 0.9480 - recall: 0.9481 - f1: 0.9480 - val_loss: 0.1868 - val_accuracy: 0.9236 - val_recall: 0.9234 - val_f1: 0.9236\n",
      "Epoch 103/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9508\n",
      "Epoch 00103: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "150/150 [==============================] - 76s 504ms/step - loss: 0.1234 - accuracy: 0.9509 - recall: 0.9508 - f1: 0.9509 - val_loss: 0.1868 - val_accuracy: 0.9238 - val_recall: 0.9238 - val_f1: 0.9238\n",
      "Epoch 104/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1212 - accuracy: 0.9496 - recall: 0.9496 - f1: 0.9496\n",
      "Epoch 00104: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 76s 510ms/step - loss: 0.1216 - accuracy: 0.9496 - recall: 0.9496 - f1: 0.9496 - val_loss: 0.1813 - val_accuracy: 0.9270 - val_recall: 0.9269 - val_f1: 0.9270\n",
      "Epoch 105/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9509 - recall: 0.9508 - f1: 0.9509\n",
      "Epoch 00105: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 75s 498ms/step - loss: 0.1189 - accuracy: 0.9510 - recall: 0.9509 - f1: 0.9510 - val_loss: 0.1838 - val_accuracy: 0.9219 - val_recall: 0.9218 - val_f1: 0.9219\n",
      "Epoch 106/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9500 - recall: 0.9500 - f1: 0.9500\n",
      "Epoch 00106: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 75s 502ms/step - loss: 0.1234 - accuracy: 0.9501 - recall: 0.9501 - f1: 0.9501 - val_loss: 0.1833 - val_accuracy: 0.9265 - val_recall: 0.9265 - val_f1: 0.9265\n",
      "Epoch 107/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9482 - recall: 0.9485 - f1: 0.9482\n",
      "Epoch 00107: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "150/150 [==============================] - 77s 515ms/step - loss: 0.1241 - accuracy: 0.9484 - recall: 0.9486 - f1: 0.9484 - val_loss: 0.1889 - val_accuracy: 0.9211 - val_recall: 0.9210 - val_f1: 0.9211\n",
      "Epoch 108/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9493 - recall: 0.9492 - f1: 0.9493\n",
      "Epoch 00108: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 76s 506ms/step - loss: 0.1246 - accuracy: 0.9492 - recall: 0.9491 - f1: 0.9492 - val_loss: 0.1825 - val_accuracy: 0.9229 - val_recall: 0.9227 - val_f1: 0.9229\n",
      "Epoch 109/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9508\n",
      "Epoch 00109: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 76s 505ms/step - loss: 0.1223 - accuracy: 0.9507 - recall: 0.9506 - f1: 0.9507 - val_loss: 0.1822 - val_accuracy: 0.9259 - val_recall: 0.9258 - val_f1: 0.9259\n",
      "Epoch 110/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9491 - recall: 0.9490 - f1: 0.9491\n",
      "Epoch 00110: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 76s 508ms/step - loss: 0.1261 - accuracy: 0.9486 - recall: 0.9485 - f1: 0.9486 - val_loss: 0.1786 - val_accuracy: 0.9251 - val_recall: 0.9251 - val_f1: 0.9251\n",
      "Epoch 111/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9508\n",
      "Epoch 00111: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "150/150 [==============================] - 76s 509ms/step - loss: 0.1258 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9508 - val_loss: 0.1849 - val_accuracy: 0.9239 - val_recall: 0.9237 - val_f1: 0.9239\n",
      "Epoch 112/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 0.9453 - recall: 0.9452 - f1: 0.9453\n",
      "Epoch 00112: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 463ms/step - loss: 0.1261 - accuracy: 0.9452 - recall: 0.9451 - f1: 0.9452 - val_loss: 0.1762 - val_accuracy: 0.9272 - val_recall: 0.9271 - val_f1: 0.9272\n",
      "Epoch 113/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 0.9484 - recall: 0.9483 - f1: 0.9484\n",
      "Epoch 00113: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 68s 457ms/step - loss: 0.1253 - accuracy: 0.9483 - recall: 0.9482 - f1: 0.9483 - val_loss: 0.1781 - val_accuracy: 0.9265 - val_recall: 0.9265 - val_f1: 0.9265\n",
      "Epoch 114/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9495 - recall: 0.9494 - f1: 0.9495\n",
      "Epoch 00114: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 460ms/step - loss: 0.1200 - accuracy: 0.9495 - recall: 0.9494 - f1: 0.9495 - val_loss: 0.1853 - val_accuracy: 0.9225 - val_recall: 0.9224 - val_f1: 0.9225\n",
      "Epoch 115/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9486 - recall: 0.9485 - f1: 0.9486\n",
      "Epoch 00115: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00115: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "150/150 [==============================] - 78s 517ms/step - loss: 0.1220 - accuracy: 0.9485 - recall: 0.9485 - f1: 0.9485 - val_loss: 0.1787 - val_accuracy: 0.9256 - val_recall: 0.9255 - val_f1: 0.9256\n",
      "Epoch 116/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9494 - recall: 0.9494 - f1: 0.9494\n",
      "Epoch 00116: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 81s 540ms/step - loss: 0.1226 - accuracy: 0.9494 - recall: 0.9495 - f1: 0.9494 - val_loss: 0.1774 - val_accuracy: 0.9251 - val_recall: 0.9249 - val_f1: 0.9251\n",
      "Epoch 117/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9505 - recall: 0.9505 - f1: 0.9505\n",
      "Epoch 00117: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 76s 506ms/step - loss: 0.1225 - accuracy: 0.9506 - recall: 0.9506 - f1: 0.9506 - val_loss: 0.1780 - val_accuracy: 0.9264 - val_recall: 0.9264 - val_f1: 0.9264\n",
      "Epoch 118/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1171 - accuracy: 0.9517 - recall: 0.9517 - f1: 0.9517\n",
      "Epoch 00118: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 458ms/step - loss: 0.1171 - accuracy: 0.9518 - recall: 0.9517 - f1: 0.9518 - val_loss: 0.1783 - val_accuracy: 0.9262 - val_recall: 0.9261 - val_f1: 0.9262\n",
      "Epoch 119/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1209 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507\n",
      "Epoch 00119: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 466ms/step - loss: 0.1207 - accuracy: 0.9507 - recall: 0.9507 - f1: 0.9507 - val_loss: 0.1834 - val_accuracy: 0.9239 - val_recall: 0.9238 - val_f1: 0.9239\n",
      "Epoch 120/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9509 - recall: 0.9509 - f1: 0.9509\n",
      "Epoch 00120: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 468ms/step - loss: 0.1279 - accuracy: 0.9508 - recall: 0.9509 - f1: 0.9508 - val_loss: 0.1788 - val_accuracy: 0.9246 - val_recall: 0.9245 - val_f1: 0.9246\n",
      "Epoch 121/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9508 - recall: 0.9505 - f1: 0.9508\n",
      "Epoch 00121: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 71s 474ms/step - loss: 0.1203 - accuracy: 0.9507 - recall: 0.9505 - f1: 0.9507 - val_loss: 0.1940 - val_accuracy: 0.9194 - val_recall: 0.9191 - val_f1: 0.9194\n",
      "Epoch 122/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1278 - accuracy: 0.9508 - recall: 0.9509 - f1: 0.9508\n",
      "Epoch 00122: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00122: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "150/150 [==============================] - 73s 489ms/step - loss: 0.1278 - accuracy: 0.9507 - recall: 0.9509 - f1: 0.9507 - val_loss: 0.1770 - val_accuracy: 0.9253 - val_recall: 0.9253 - val_f1: 0.9253\n",
      "Epoch 123/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9517 - recall: 0.9517 - f1: 0.9517\n",
      "Epoch 00123: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 468ms/step - loss: 0.1235 - accuracy: 0.9518 - recall: 0.9518 - f1: 0.9518 - val_loss: 0.1783 - val_accuracy: 0.9240 - val_recall: 0.9238 - val_f1: 0.9240\n",
      "Epoch 124/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9480 - recall: 0.9480 - f1: 0.9480\n",
      "Epoch 00124: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 72s 477ms/step - loss: 0.1230 - accuracy: 0.9481 - recall: 0.9481 - f1: 0.9481 - val_loss: 0.1832 - val_accuracy: 0.9227 - val_recall: 0.9226 - val_f1: 0.9227\n",
      "Epoch 125/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9517 - recall: 0.9517 - f1: 0.9517\n",
      "Epoch 00125: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 465ms/step - loss: 0.1239 - accuracy: 0.9518 - recall: 0.9518 - f1: 0.9518 - val_loss: 0.1801 - val_accuracy: 0.9242 - val_recall: 0.9242 - val_f1: 0.9242\n",
      "Epoch 126/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 0.9489 - recall: 0.9487 - f1: 0.9488\n",
      "Epoch 00126: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00126: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "150/150 [==============================] - 69s 460ms/step - loss: 0.1222 - accuracy: 0.9491 - recall: 0.9489 - f1: 0.9491 - val_loss: 0.1810 - val_accuracy: 0.9240 - val_recall: 0.9239 - val_f1: 0.9240\n",
      "Epoch 127/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9494 - recall: 0.9493 - f1: 0.9494\n",
      "Epoch 00127: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 464ms/step - loss: 0.1274 - accuracy: 0.9494 - recall: 0.9493 - f1: 0.9494 - val_loss: 0.1750 - val_accuracy: 0.9269 - val_recall: 0.9268 - val_f1: 0.9269\n",
      "Epoch 128/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1284 - accuracy: 0.9484 - recall: 0.9484 - f1: 0.9484\n",
      "Epoch 00128: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 71s 471ms/step - loss: 0.1284 - accuracy: 0.9484 - recall: 0.9483 - f1: 0.9484 - val_loss: 0.1867 - val_accuracy: 0.9227 - val_recall: 0.9226 - val_f1: 0.9227\n",
      "Epoch 129/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1300 - accuracy: 0.9468 - recall: 0.9469 - f1: 0.9468\n",
      "Epoch 00129: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 465ms/step - loss: 0.1306 - accuracy: 0.9467 - recall: 0.9468 - f1: 0.9467 - val_loss: 0.1824 - val_accuracy: 0.9232 - val_recall: 0.9232 - val_f1: 0.9232\n",
      "Epoch 130/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488\n",
      "Epoch 00130: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00130: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "150/150 [==============================] - 70s 469ms/step - loss: 0.1232 - accuracy: 0.9485 - recall: 0.9486 - f1: 0.9485 - val_loss: 0.1792 - val_accuracy: 0.9250 - val_recall: 0.9249 - val_f1: 0.9250\n",
      "Epoch 131/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9498 - recall: 0.9497 - f1: 0.9498\n",
      "Epoch 00131: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 464ms/step - loss: 0.1258 - accuracy: 0.9497 - recall: 0.9497 - f1: 0.9497 - val_loss: 0.1891 - val_accuracy: 0.9226 - val_recall: 0.9224 - val_f1: 0.9226\n",
      "Epoch 132/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9504 - recall: 0.9503 - f1: 0.9504\n",
      "Epoch 00132: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 470ms/step - loss: 0.1234 - accuracy: 0.9505 - recall: 0.9503 - f1: 0.9505 - val_loss: 0.1910 - val_accuracy: 0.9186 - val_recall: 0.9185 - val_f1: 0.9186\n",
      "Epoch 133/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9498 - recall: 0.9498 - f1: 0.9498\n",
      "Epoch 00133: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 466ms/step - loss: 0.1219 - accuracy: 0.9499 - recall: 0.9499 - f1: 0.9499 - val_loss: 0.1866 - val_accuracy: 0.9217 - val_recall: 0.9217 - val_f1: 0.9217\n",
      "Epoch 134/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9497 - recall: 0.9497 - f1: 0.9497\n",
      "Epoch 00134: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00134: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "150/150 [==============================] - 71s 477ms/step - loss: 0.1226 - accuracy: 0.9495 - recall: 0.9495 - f1: 0.9495 - val_loss: 0.1887 - val_accuracy: 0.9229 - val_recall: 0.9227 - val_f1: 0.9229\n",
      "Epoch 135/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9478 - recall: 0.9479 - f1: 0.9478\n",
      "Epoch 00135: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 469ms/step - loss: 0.1241 - accuracy: 0.9478 - recall: 0.9479 - f1: 0.9478 - val_loss: 0.1840 - val_accuracy: 0.9223 - val_recall: 0.9221 - val_f1: 0.9223\n",
      "Epoch 136/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9519 - recall: 0.9519 - f1: 0.9519\n",
      "Epoch 00136: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 469ms/step - loss: 0.1214 - accuracy: 0.9520 - recall: 0.9520 - f1: 0.9520 - val_loss: 0.1876 - val_accuracy: 0.9201 - val_recall: 0.9201 - val_f1: 0.9201\n",
      "Epoch 137/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9496 - recall: 0.9498 - f1: 0.9496\n",
      "Epoch 00137: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 464ms/step - loss: 0.1269 - accuracy: 0.9499 - recall: 0.9501 - f1: 0.9499 - val_loss: 0.1848 - val_accuracy: 0.9256 - val_recall: 0.9254 - val_f1: 0.9256\n",
      "Epoch 138/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9498 - recall: 0.9498 - f1: 0.9498\n",
      "Epoch 00138: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "150/150 [==============================] - 70s 465ms/step - loss: 0.1262 - accuracy: 0.9498 - recall: 0.9498 - f1: 0.9498 - val_loss: 0.1835 - val_accuracy: 0.9229 - val_recall: 0.9228 - val_f1: 0.9229\n",
      "Epoch 139/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9505 - recall: 0.9505 - f1: 0.9505\n",
      "Epoch 00139: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 71s 470ms/step - loss: 0.1215 - accuracy: 0.9503 - recall: 0.9503 - f1: 0.9503 - val_loss: 0.1809 - val_accuracy: 0.9248 - val_recall: 0.9248 - val_f1: 0.9248\n",
      "Epoch 140/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9500 - recall: 0.9499 - f1: 0.9500\n",
      "Epoch 00140: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 73s 484ms/step - loss: 0.1204 - accuracy: 0.9498 - recall: 0.9497 - f1: 0.9498 - val_loss: 0.1777 - val_accuracy: 0.9253 - val_recall: 0.9254 - val_f1: 0.9253\n",
      "Epoch 141/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9477 - recall: 0.9478 - f1: 0.9477\n",
      "Epoch 00141: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 468ms/step - loss: 0.1273 - accuracy: 0.9476 - recall: 0.9477 - f1: 0.9476 - val_loss: 0.1902 - val_accuracy: 0.9216 - val_recall: 0.9215 - val_f1: 0.9216\n",
      "Epoch 142/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1285 - accuracy: 0.9471 - recall: 0.9469 - f1: 0.9471\n",
      "Epoch 00142: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00142: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "150/150 [==============================] - 70s 466ms/step - loss: 0.1281 - accuracy: 0.9474 - recall: 0.9472 - f1: 0.9474 - val_loss: 0.1778 - val_accuracy: 0.9292 - val_recall: 0.9291 - val_f1: 0.9292\n",
      "Epoch 143/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9513 - recall: 0.9513 - f1: 0.9513\n",
      "Epoch 00143: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 72s 478ms/step - loss: 0.1218 - accuracy: 0.9513 - recall: 0.9513 - f1: 0.9513 - val_loss: 0.1850 - val_accuracy: 0.9242 - val_recall: 0.9239 - val_f1: 0.9241\n",
      "Epoch 144/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1196 - accuracy: 0.9509 - recall: 0.9507 - f1: 0.9509\n",
      "Epoch 00144: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 71s 476ms/step - loss: 0.1197 - accuracy: 0.9508 - recall: 0.9507 - f1: 0.9508 - val_loss: 0.1885 - val_accuracy: 0.9215 - val_recall: 0.9214 - val_f1: 0.9215\n",
      "Epoch 145/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9477 - recall: 0.9476 - f1: 0.9476\n",
      "Epoch 00145: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 68s 456ms/step - loss: 0.1265 - accuracy: 0.9477 - recall: 0.9477 - f1: 0.9477 - val_loss: 0.1750 - val_accuracy: 0.9267 - val_recall: 0.9268 - val_f1: 0.9267\n",
      "Epoch 146/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9528 - recall: 0.9523 - f1: 0.9527\n",
      "Epoch 00146: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00146: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "150/150 [==============================] - 67s 450ms/step - loss: 0.1178 - accuracy: 0.9526 - recall: 0.9522 - f1: 0.9526 - val_loss: 0.1846 - val_accuracy: 0.9221 - val_recall: 0.9219 - val_f1: 0.9221\n",
      "Epoch 147/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9490 - recall: 0.9489 - f1: 0.9490\n",
      "Epoch 00147: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 70s 464ms/step - loss: 0.1252 - accuracy: 0.9491 - recall: 0.9489 - f1: 0.9491 - val_loss: 0.1806 - val_accuracy: 0.9264 - val_recall: 0.9263 - val_f1: 0.9264\n",
      "Epoch 148/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 0.9493 - recall: 0.9493 - f1: 0.9493\n",
      "Epoch 00148: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 458ms/step - loss: 0.1224 - accuracy: 0.9494 - recall: 0.9493 - f1: 0.9494 - val_loss: 0.1852 - val_accuracy: 0.9243 - val_recall: 0.9243 - val_f1: 0.9243\n",
      "Epoch 149/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1168 - accuracy: 0.9526 - recall: 0.9527 - f1: 0.9526\n",
      "Epoch 00149: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 68s 451ms/step - loss: 0.1168 - accuracy: 0.9525 - recall: 0.9525 - f1: 0.9525 - val_loss: 0.1813 - val_accuracy: 0.9253 - val_recall: 0.9251 - val_f1: 0.9253\n",
      "Epoch 150/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9514 - recall: 0.9513 - f1: 0.9514\n",
      "Epoch 00150: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 459ms/step - loss: 0.1176 - accuracy: 0.9514 - recall: 0.9513 - f1: 0.9514 - val_loss: 0.1852 - val_accuracy: 0.9228 - val_recall: 0.9227 - val_f1: 0.9228\n",
      "Epoch 151/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9508 - recall: 0.9510 - f1: 0.9508\n",
      "Epoch 00151: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 68s 457ms/step - loss: 0.1228 - accuracy: 0.9510 - recall: 0.9511 - f1: 0.9510 - val_loss: 0.1762 - val_accuracy: 0.9259 - val_recall: 0.9257 - val_f1: 0.9258\n",
      "Epoch 152/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9476 - recall: 0.9476 - f1: 0.9476\n",
      "Epoch 00152: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 461ms/step - loss: 0.1246 - accuracy: 0.9476 - recall: 0.9476 - f1: 0.9476 - val_loss: 0.1871 - val_accuracy: 0.9232 - val_recall: 0.9229 - val_f1: 0.9232\n",
      "Epoch 153/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9492 - recall: 0.9493 - f1: 0.9492\n",
      "Epoch 00153: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00153: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.1240 - accuracy: 0.9488 - recall: 0.9489 - f1: 0.9488 - val_loss: 0.1841 - val_accuracy: 0.9249 - val_recall: 0.9247 - val_f1: 0.9249\n",
      "Epoch 154/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9530 - recall: 0.9529 - f1: 0.9530\n",
      "Epoch 00154: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 457ms/step - loss: 0.1175 - accuracy: 0.9529 - recall: 0.9529 - f1: 0.9529 - val_loss: 0.1780 - val_accuracy: 0.9250 - val_recall: 0.9252 - val_f1: 0.9251\n",
      "Epoch 155/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9501 - recall: 0.9500 - f1: 0.9501\n",
      "Epoch 00155: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 75s 503ms/step - loss: 0.1216 - accuracy: 0.9499 - recall: 0.9498 - f1: 0.9499 - val_loss: 0.1857 - val_accuracy: 0.9241 - val_recall: 0.9241 - val_f1: 0.9241\n",
      "Epoch 156/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9503 - recall: 0.9505 - f1: 0.9503\n",
      "Epoch 00156: val_loss did not improve from 0.17388\n",
      "150/150 [==============================] - 69s 462ms/step - loss: 0.1177 - accuracy: 0.9502 - recall: 0.9504 - f1: 0.9502 - val_loss: 0.1867 - val_accuracy: 0.9228 - val_recall: 0.9227 - val_f1: 0.9228\n",
      "Epoch 157/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9507 - recall: 0.9505 - f1: 0.9507\n",
      "Epoch 00157: val_loss did not improve from 0.17388\n",
      "\n",
      "Epoch 00157: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      "150/150 [==============================] - 68s 454ms/step - loss: 0.1198 - accuracy: 0.9507 - recall: 0.9505 - f1: 0.9507 - val_loss: 0.1775 - val_accuracy: 0.9274 - val_recall: 0.9272 - val_f1: 0.9274\n",
      "Epoch 158/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9537 - recall: 0.9536 - f1: 0.9537\n",
      "Epoch 00158: val_loss improved from 0.17388 to 0.17187, saving model to batch_relu_validation_200-158-0.953700-0.927400.h5\n",
      "150/150 [==============================] - 68s 453ms/step - loss: 0.1174 - accuracy: 0.9537 - recall: 0.9536 - f1: 0.9537 - val_loss: 0.1719 - val_accuracy: 0.9274 - val_recall: 0.9274 - val_f1: 0.9274\n",
      "Epoch 159/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9502 - recall: 0.9505 - f1: 0.9503\n",
      "Epoch 00159: val_loss did not improve from 0.17187\n",
      "150/150 [==============================] - 69s 458ms/step - loss: 0.1251 - accuracy: 0.9500 - recall: 0.9502 - f1: 0.9500 - val_loss: 0.1801 - val_accuracy: 0.9276 - val_recall: 0.9277 - val_f1: 0.9276\n",
      "Epoch 160/160\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.1120 - accuracy: 0.9548 - recall: 0.9545 - f1: 0.9548\n",
      "Epoch 00160: val_loss did not improve from 0.17187\n",
      "150/150 [==============================] - 69s 463ms/step - loss: 0.1126 - accuracy: 0.9548 - recall: 0.9545 - f1: 0.9548 - val_loss: 0.1837 - val_accuracy: 0.9241 - val_recall: 0.9240 - val_f1: 0.9241\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with the size 72x72, the window size of the images to be fed\n",
    "batch_normalization = True\n",
    "EPOCHS = 160\n",
    "STEPS_PER_EPOCH = 150\n",
    "activation = \"LeakyReLU\"\n",
    "model = cnn_model(\n",
    "    shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "# Train the model with batch\n",
    "history = model.train()\n",
    "# model.save(\"batch_relu_validation_200.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "plt.savefig(\"batch_relu_validation_200.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,962\n",
      "Trainable params: 2,278,274\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 72, 72, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 36, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 36, 36, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,280,962\n",
      "Trainable params: 2,278,274\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_distributed_function_63110756 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Processing image => data/test_set_images/test_1/test_1.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_2/test_2.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_3/test_3.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_4/test_4.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_5/test_5.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_6/test_6.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_7/test_7.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_8/test_8.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_9/test_9.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_10/test_10.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_11/test_11.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_12/test_12.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_13/test_13.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_14/test_14.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_15/test_15.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_16/test_16.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_17/test_17.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_18/test_18.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_19/test_19.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_20/test_20.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_21/test_21.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_22/test_22.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_23/test_23.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_24/test_24.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_25/test_25.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_26/test_26.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_27/test_27.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_28/test_28.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_29/test_29.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_30/test_30.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_31/test_31.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_32/test_32.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_33/test_33.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_34/test_34.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_35/test_35.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_36/test_36.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_37/test_37.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_38/test_38.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_39/test_39.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_40/test_40.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_41/test_41.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_42/test_42.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_43/test_43.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_44/test_44.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_45/test_45.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_46/test_46.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_47/test_47.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_48/test_48.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_49/test_49.png\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Processing image => data/test_set_images/test_50/test_50.png\n"
     ]
    }
   ],
   "source": [
    "from helpers import *\n",
    "\n",
    "# from cnn_model import cnn_model\n",
    "\n",
    "# Instantiate the model\n",
    "batch_normalization = True\n",
    "activation = \"LeakyReLU\"\n",
    "model = cnn_model(\n",
    "    shape=(72, 72, 3), batch_normalization=batch_normalization, activation=activation\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model.load(\"batch_relu_validation_200-074-0.946167-0.927667.h5\")\n",
    "\n",
    "# Print a summary to make sure the correct model is used\n",
    "model.model.summary()\n",
    "\n",
    "# We add all test images to an array, used later for generating a submission\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = \"data/test_set_images/test_\" + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "    image_filenames.append(image_filename)\n",
    "\n",
    "# Set-up submission filename\n",
    "submission_filename = \"batch_relu_validation_200-074-0.946167-0.927667.csv\"\n",
    "\n",
    "# Generates the submission\n",
    "generate_submission(model, submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
