{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y_trainRaw, tX_trainRaw, ids_train = load_csv_data(DATA_TRAIN_PATH)\n",
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "y_testRaw, tX_testRaw, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_numerical_categorical(x,cat_cols):\n",
    "    x_num = np.delete(x,cat_cols,axis = 1)\n",
    "    x_cat = x[:,cat_cols]\n",
    "    return x_num, x_cat\n",
    "\n",
    "def replace_undef_val_with_nan(x):\n",
    "    return np.where(x == -999.0, np.nan, x)\n",
    "\n",
    "def nan_standardize_fit(x):                                \n",
    "    mean = np.nanmean(x, axis = 0)\n",
    "    std = np.nanstd(x, axis = 0)\n",
    "    return (x - mean)/std , mean, std\n",
    "\n",
    "def nan_standardize_with_median_fit(x):                               \n",
    "    median = np.nanmedian(x, axis = 0)\n",
    "    iqr,_,_ = calculate_iqr(x)  \n",
    "    return 2*(x - median)/iqr , median, iqr\n",
    "\n",
    "def nan_standardize_with_median_transform(x,median,iqr):                             \n",
    "    return 2*(x - median)/iqr \n",
    "    \n",
    "def nan_standardize_transform(x,mean,std):\n",
    "    return (x - mean)/std\n",
    "\n",
    "def relabel_y_non_negative(y):\n",
    "    new_y = y.copy()\n",
    "    new_y[new_y == -1] = 0\n",
    "    return new_y\n",
    " \n",
    "def relabel_y_negative(y):\n",
    "    new_y = y.copy()\n",
    "    new_y[new_y == 0] = -1\n",
    "    return new_y\n",
    "        \n",
    "def replace_nan_val_with_mean(x):\n",
    "    means = np.nanmean(x,axis = 0)\n",
    "    n_cols = x.shape[1]\n",
    "    new_x = x.copy()\n",
    "    for i in range(n_cols):\n",
    "        new_x[:,i] = np.where(np.isnan(new_x[:,i]), means[i], new_x[:,i])\n",
    "    return new_x\n",
    "\n",
    "def replace_nan_val_with_zero(x):\n",
    "    n_cols = x.shape[1]\n",
    "    new_x = x.copy()\n",
    "    for i in range(n_cols):\n",
    "        new_x[:,i] = np.where(np.isnan(new_x[:,i]), 0, new_x[:,i])\n",
    "    return new_x\n",
    "\n",
    "def calculate_iqr(x):\n",
    "    q1 = np.quantile(x,0.25,axis = 0)\n",
    "    q3 = np.quantile(x,0.75,axis = 0)\n",
    "    return q3 - q1, q1, q3\n",
    "\n",
    "def replace_iqr_outliers(x):\n",
    "    iqr, q1, q3= calculate_iqr(x)\n",
    "    upper_bound = q3 + iqr * 1.5\n",
    "    lower_bound = q1 - iqr * 1.5\n",
    "    x_trunc_up = np.where(x > upper_bound,upper_bound,x)\n",
    "    x_trunc_low = np.where(x_trunc_up < lower_bound,lower_bound,x_trunc_up)\n",
    "    return x_trunc_low\n",
    "\n",
    "def replace_nan_val_with_median(x):\n",
    "    medians = np.nanmedian(x,axis = 0)\n",
    "    n_cols = x.shape[1]\n",
    "    new_x = x.copy()\n",
    "    for i in range(n_cols):\n",
    "        new_x[:,i] = np.where(np.isnan(new_x[:,i]), medians[i], new_x[:,i])\n",
    "    return new_x\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    unique_vals = set(x.ravel())\n",
    "    # print(unique_vals)\n",
    "    n_cols = len(unique_vals) - 1 \n",
    "    ohe_x = np.zeros((x.shape[0],n_cols))\n",
    "    for (row,col) in enumerate(x):\n",
    "        if col < n_cols:\n",
    "            ohe_x[int(row),int(col)] = 1\n",
    "    return ohe_x\n",
    "\n",
    "def add_bias(x):\n",
    "    return np.hstack((np.ones(x.shape[0]).reshape(-1,1),x))\n",
    "   \n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data based on the given ratio: TODO\n",
    "    # ***************************************************\n",
    "    n_train = round(y.shape[0]*ratio)\n",
    "    idx = np.random.permutation(range(y.shape[0]))\n",
    "    x_shuffled = x[idx]\n",
    "    y_shuffled = y[idx]\n",
    "    return x_shuffled[:n_train],y_shuffled[:n_train],x_shuffled[n_train:],y_shuffled[n_train:]\n",
    "    \n",
    "    \n",
    "def multiHistPlots(x,figsize = (15,15)):\n",
    "    n = x.shape[1]\n",
    "    n_rows = np.ceil(np.sqrt(n)).astype(np.int64)\n",
    "    n_cols = np.floor(np.sqrt(n)).astype(np.int64)\n",
    "\n",
    "    if n_rows * n_cols < n:\n",
    "        n_cols = np.ceil(np.sqrt(n)).astype(np.int64)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows = n_rows, ncols = n_cols, figsize = figsize)\n",
    "\n",
    "    c = 0\n",
    "    for row in range(n_rows):\n",
    "        for col in range(n_cols):\n",
    "            if n > 1:\n",
    "                ax = axes[row][col]\n",
    "            else:\n",
    "                ax = axes\n",
    "            if c < x.shape[1]:\n",
    "                ax.hist(x[:,c], label = 'feature_{:d}'.format(c),density = True)\n",
    "                ax.legend(loc = 'upper left')\n",
    "                ax.set_ylabel('Probability')\n",
    "                ax.set_xlabel('Value')\n",
    "            c += 1\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions needed for the log and reg_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"Apply sigmoid function on t\n",
    "    \n",
    "    Args: \n",
    "        t=>(numpy.array): Values to apply sigmoid function\n",
    "    \n",
    "    Returns:\n",
    "        => numpy.array: Calculated values of sigmoid\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1.0 / (1.0 + np.exp(-t))\n",
    "\n",
    "\n",
    "def calculate_loss_log(y, tx, w):\n",
    "    \"\"\"Compute the cost of log_regression\n",
    "    \n",
    "    Args: \n",
    "        y =>(numpy.array): Target values\n",
    "        tx =>(numpy.array): Transposed features\n",
    "        w => (numpy.array): Weigths \n",
    "          \n",
    "    Returns:\n",
    "        => numpy.array: Calculated loss\n",
    "    \"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "    return np.squeeze(- loss)\n",
    "\n",
    "    \n",
    "def calculate_gradient_log(y, tx, w,):\n",
    "    \"\"\"Compute the gradient of loss for log_regression\n",
    "    \n",
    "    Args: \n",
    "        y =>(numpy.array): Target values\n",
    "        tx => (numpy.array): Transposed features\n",
    "        w => (numpy.array): Weigths \n",
    "          \n",
    "    Returns:\n",
    "        => numpy.array: Calculated logistic gradient\n",
    "    \"\"\"\n",
    "\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    grad = tx.T.dot(pred - y)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def learning_by_gradient_descent_log(y, tx, w, gamma):\n",
    "    \"\"\"Compute the gradient descen using logistic regression\n",
    "    \n",
    "    Args: \n",
    "        y =>(numpy.array): Target values\n",
    "        tx => (numpy.array): Transposed features\n",
    "        w => (numpy.array): Weigths \n",
    "        gamma=> (float): the gamma to use.\n",
    "        \n",
    "    Returns:\n",
    "        w =>(numpy.array): Calculated Weights\n",
    "        loss => (numpy.array): Calculated Loss\n",
    "    \"\"\"\n",
    "\n",
    "    loss = calculate_loss_log(y, tx, w) \n",
    "    grad = calculate_gradient_log(y, tx, w)\n",
    "    w -= gamma * grad\n",
    "    return loss, w\n",
    "\n",
    "\n",
    "def learning_by_reg_gradient_descent_log(y, tx, w, gamma, lambda_=0):\n",
    "    \"\"\"Compute the gradient descen using logistic regression\n",
    "    \n",
    "    Args: \n",
    "        y =>(numpy.array): Target values\n",
    "        tx => (numpy.array): Transposed features\n",
    "        w => (numpy.array): Weigths \n",
    "        gamma=> (float): the gamma to use.\n",
    "        \n",
    "    Returns: \n",
    "        w =>(numpy.array): Calculated Weights\n",
    "        loss => (numpy.array): Calculated Loss\n",
    "    \"\"\"\n",
    "\n",
    "    loss = calculate_loss_log(y, tx, w) + lambda_ * np.squeeze(w.T.dot(w))\n",
    "    grad = calculate_gradient_log(y, tx, w) + 2 * lambda_ * w\n",
    "    w -= gamma * grad\n",
    "    return loss, w\n",
    "\n",
    "def logistic_regression(y, tx, w_initial, max_iters, gamma):\n",
    "    \"\"\"Implement logistic regression using gradient descent\n",
    "    \n",
    "    Args: \n",
    "      y =>(numpy.array): Target values\n",
    "      tx => (numpy.array): Transposed features\n",
    "      w_initial => (numpy.array): Initial Weigths \n",
    "      max_iters => (int): number of iterations.\n",
    "      gamma=> (float): the gamma to use.\n",
    "          \n",
    "    Returns: \n",
    "        w =>(numpy.array): Calculated Weights\n",
    "        loss => (numpy.array): Calculated Loss\n",
    "    \"\"\"\n",
    "\n",
    "    assert max_iters > 0, \"max_iters should be a positive number\"\n",
    "    assert y.shape[0] == tx.shape[0], \"y and tx should have the same number of entries (rows)\"\n",
    "    assert tx.shape[1] == w_initial.shape[0], \"initial_w should be the same degree as tx\"\n",
    "    \n",
    "    print_every = 50\n",
    "    w = w_initial\n",
    "    losses =[]\n",
    "    for n_iter in range(max_iters+1):\n",
    "        loss, w = learning_by_gradient_descent_log(y, tx, w, gamma)\n",
    "        if (n_iter % print_every == 0):\n",
    "            # print average loss for the last print_every iterations\n",
    "            print('iteration\\t', str(n_iter), loss)\n",
    "            losses.append(loss)\n",
    "\n",
    "            \n",
    "    loss = calculate_loss_log(y, tx, w)\n",
    "    \n",
    "    return w, loss,losses\n",
    "\n",
    "def reg_logistic_regression(y, tx, w_initial, max_iters, gamma,lambda_):\n",
    "    \"\"\"Implement logistic regression using gradient descent\n",
    "    \n",
    "    Args: \n",
    "        y =>(numpy.array): Target values\n",
    "        tx => (numpy.array): Transposed features\n",
    "        w_initial => (numpy.array): Initial Weigths \n",
    "        max_iters => (int): number of iterations.\n",
    "        gamma=> (float): the gamma to use.\n",
    "          \n",
    "    Returns: \n",
    "        w =>(numpy.array): Calculated Weights\n",
    "        loss => (numpy.array): Calculated Loss\n",
    "    \"\"\"\n",
    "\n",
    "    assert max_iters > 0, \"max_iters should be a positive number\"\n",
    "    assert y.shape[0] == tx.shape[0], \"y and tx should have the same number of entries (rows)\"\n",
    "    assert tx.shape[1] == w_initial.shape[0], \"initial_w should be the same degree as tx\"\n",
    "    \n",
    "    print_every = 50\n",
    "    w = w_initial\n",
    "    losses =[]\n",
    "    for n_iter in range(max_iters+1):\n",
    "        loss, w = learning_by_reg_gradient_descent_log(y, tx, w, gamma,lambda_)\n",
    "        if (n_iter % print_every == 0):\n",
    "            # print average loss for the last print_every iterations\n",
    "            print('iteration\\t', str(n_iter), loss)\n",
    "            losses.append(loss)\n",
    "\n",
    "            \n",
    "    loss = calculate_loss_log(y, tx, w)\n",
    "    \n",
    "    return w, loss,losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [22]\n",
    "full_x_train_num, full_x_train_cat = split_numerical_categorical(tX_trainRaw,cat_cols)\n",
    "# Treat numerical values\n",
    "full_x_train_num_nan = replace_undef_val_with_nan(full_x_train_num)\n",
    "full_x_train_num_nan_std, train_mean, train_std = nan_standardize_fit(full_x_train_num_nan)\n",
    "# full_x_train_num_valid_std = replace_nan_val_with_mean(full_x_train_num_nan_std)\n",
    "full_x_train_num_valid_std = replace_nan_val_with_median(full_x_train_num_nan_std)\n",
    "full_x_train_num_valid_std = replace_iqr_outliers(full_x_train_num_valid_std)\n",
    "# Treat categorical values\n",
    "full_x_train_ohe_cat = one_hot_encode(full_x_train_cat)\n",
    "full_x_train = np.hstack((add_bias(full_x_train_num_valid_std),full_x_train_ohe_cat))\n",
    "# Treat labels\n",
    "full_y_train = y_trainRaw\n",
    "full_y_train = relabel_y_non_negative(full_y_train).reshape(-1,1)\n",
    "full_y_train = full_y_train.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val = split_data(full_x_train,full_y_train,0.8)\n",
    "y_val = relabel_y_negative(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration\t 0 428857.59690654144\n",
      "iteration\t 50 580522.6095472391\n",
      "iteration\t 100 311131.3372013471\n",
      "iteration\t 150 466770.90033078357\n",
      "iteration\t 200 160856.445716183\n",
      "iteration\t 250 495457.7480082634\n",
      "iteration\t 300 209870.6693670733\n",
      "iteration\t 350 535390.0085533891\n",
      "iteration\t 400 153224.90874038823\n",
      "iteration\t 450 475430.2977494928\n",
      "iteration\t 500 248618.36353900243\n"
     ]
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "gamma = 0.00005\n",
    "w_initial = np.ones((full_x_train.shape[1], 1))\n",
    "weights, loss_tr,losses = logistic_regression(y_train, x_train, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7058"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(weights, x_val)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [22]\n",
    "x_test_num, x_test_cat = split_numerical_categorical(tX_testRaw,cat_cols)\n",
    "# Treat numerical values\n",
    "x_test_num_nan = replace_undef_val_with_nan(x_test_num)\n",
    "x_test_num_nan_std = nan_standardize_transform(x_test_num_nan,train_mean,train_std)\n",
    "# x_test_num_nan_std = nan_standardize_with_median_transform(x_test_num_nan,train_median,train_std)\n",
    "x_test_num_valid_std = replace_nan_val_with_median(x_test_num_nan_std)\n",
    "x_test_num_valid_std = replace_iqr_outliers(x_test_num_valid_std)\n",
    "# x_test_num_valid_std = replace_nan_val_with_mean(x_test_num_nan_std)\n",
    "x_test_ohe_cat = one_hot_encode(x_test_cat)\n",
    "x_test = np.hstack((add_bias(x_test_num_valid_std),x_test_ohe_cat))\n",
    "# Treat labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(weights, x_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../results/log_reg.csv' \n",
    "create_csv_submission(ids_test, y_pred[:], OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/log_reg.csv') as input, open('../results/log_reg_cleanded.csv', 'w', newline='') as output:\n",
    "    writer = csv.writer(output)\n",
    "    for row in csv.reader(input):\n",
    "        if any(field.strip() for field in row):\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import linear_model, preprocessing, metrics\n",
    "\n",
    "# model = linear_model.SGDClassifier(max_iter=max_iters)\n",
    "# model.fit(x_train,y_train)\n",
    "# predict_y = model.predict(x_val)\n",
    "# predict_y = relabel_y_negative(predict_y)\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# accuracy_score(y_val,predict_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_iters = 1000\n",
    "# gamma = 0.00001\n",
    "# w_initial = np.zeros((x_train_prep.shape[1], 1))\n",
    "# lambdas = np.logspace(-4, 0, 10)\n",
    "# for i,lambda_ in enumerate(lambdas):\n",
    "#         weights, loss_tr,losses = logistic_regression(y_train_prep, x_train_prep, w_initial, max_iters, gamma)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_range = np.arange(0.0, 1.6e-8, 2.e-9)\n",
    "# (lambda_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = predict_labels(weights, x_val)\n",
    "# y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_PATH = '../results/log_reg.csv' \n",
    "# create_csv_submission(ids_test, y_pred[:], OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
