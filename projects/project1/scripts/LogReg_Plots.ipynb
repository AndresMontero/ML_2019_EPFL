{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y_trainRaw, tX_trainRaw, ids_train = load_csv_data(DATA_TRAIN_PATH)\n",
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "y_testRaw, tX_testRaw, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    # ***************************************************\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly\n",
    "\n",
    "def split_numerical_categorical(x,cat_cols):\n",
    "    x_num = np.delete(x,cat_cols,axis = 1)\n",
    "    x_cat = x[:,cat_cols]\n",
    "    return x_num, x_cat\n",
    "\n",
    "def replace_undef_val_with_nan(x):\n",
    "    return np.where(x == -999.0, np.nan, x)\n",
    "\n",
    "def nan_standardize_fit(x):                                \n",
    "    mean = np.nanmean(x, axis = 0)\n",
    "    std = np.nanstd(x, axis = 0)\n",
    "    return (x - mean)/std , mean, std\n",
    "\n",
    "def nan_standardize_with_median_fit(x):                               \n",
    "    median = np.nanmedian(x, axis = 0)\n",
    "    iqr,_,_ = calculate_iqr(x)  \n",
    "    return 2*(x - median)/iqr , median, iqr\n",
    "\n",
    "def nan_standardize_with_median_transform(x,median,iqr):                             \n",
    "    return 2*(x - median)/iqr \n",
    "    \n",
    "def nan_standardize_transform(x,mean,std):\n",
    "    return (x - mean)/std\n",
    "\n",
    "def relabel_y_non_negative(y):\n",
    "    new_y = y.copy()\n",
    "    new_y[new_y == -1] = 0\n",
    "    return new_y\n",
    " \n",
    "def relabel_y_negative(y):\n",
    "    new_y = y.copy()\n",
    "    new_y[new_y == 0] = -1\n",
    "    return new_y\n",
    "        \n",
    "def replace_nan_val_with_mean(x):\n",
    "    means = np.nanmean(x,axis = 0)\n",
    "    n_cols = x.shape[1]\n",
    "    new_x = x.copy()\n",
    "    for i in range(n_cols):\n",
    "        new_x[:,i] = np.where(np.isnan(new_x[:,i]), means[i], new_x[:,i])\n",
    "    return new_x\n",
    "\n",
    "def replace_nan_val_with_zero(x):\n",
    "    n_cols = x.shape[1]\n",
    "    new_x = x.copy()\n",
    "    for i in range(n_cols):\n",
    "        new_x[:,i] = np.where(np.isnan(new_x[:,i]), 0, new_x[:,i])\n",
    "    return new_x\n",
    "\n",
    "def calculate_iqr(x):\n",
    "    q1 = np.quantile(x,0.25,axis = 0)\n",
    "    q3 = np.quantile(x,0.75,axis = 0)\n",
    "    return q3 - q1, q1, q3\n",
    "\n",
    "def replace_iqr_outliers(x):\n",
    "    iqr, q1, q3= calculate_iqr(x)\n",
    "    upper_bound = q3 + iqr * 1.5\n",
    "    lower_bound = q1 - iqr * 1.5\n",
    "    x_trunc_up = np.where(x > upper_bound,upper_bound,x)\n",
    "    x_trunc_low = np.where(x_trunc_up < lower_bound,lower_bound,x_trunc_up)\n",
    "    return x_trunc_low\n",
    "\n",
    "def replace_nan_val_with_median(x):\n",
    "    medians = np.nanmedian(x,axis = 0)\n",
    "    n_cols = x.shape[1]\n",
    "    new_x = x.copy()\n",
    "    for i in range(n_cols):\n",
    "        new_x[:,i] = np.where(np.isnan(new_x[:,i]), medians[i], new_x[:,i])\n",
    "    return new_x\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    unique_vals = set(x.ravel())\n",
    "    # print(unique_vals)\n",
    "    n_cols = len(unique_vals) - 1 \n",
    "    ohe_x = np.zeros((x.shape[0],n_cols))\n",
    "    for (row,col) in enumerate(x):\n",
    "        if col < n_cols:\n",
    "            ohe_x[int(row),int(col)] = 1\n",
    "    return ohe_x\n",
    "\n",
    "def add_bias(x):\n",
    "    return np.hstack((np.ones(x.shape[0]).reshape(-1,1),x))\n",
    "   \n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data based on the given ratio: TODO\n",
    "    # ***************************************************\n",
    "    n_train = round(y.shape[0]*ratio)\n",
    "    idx = np.random.permutation(range(y.shape[0]))\n",
    "    x_shuffled = x[idx]\n",
    "    y_shuffled = y[idx]\n",
    "    return x_shuffled[:n_train],y_shuffled[:n_train],x_shuffled[n_train:],y_shuffled[n_train:]\n",
    "    \n",
    "    \n",
    "def multiHistPlots(x,figsize = (15,15)):\n",
    "    n = x.shape[1]\n",
    "    n_rows = np.ceil(np.sqrt(n)).astype(np.int64)\n",
    "    n_cols = np.floor(np.sqrt(n)).astype(np.int64)\n",
    "\n",
    "    if n_rows * n_cols < n:\n",
    "        n_cols = np.ceil(np.sqrt(n)).astype(np.int64)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows = n_rows, ncols = n_cols, figsize = figsize)\n",
    "\n",
    "    c = 0\n",
    "    for row in range(n_rows):\n",
    "        for col in range(n_cols):\n",
    "            if n > 1:\n",
    "                ax = axes[row][col]\n",
    "            else:\n",
    "                ax = axes\n",
    "            if c < x.shape[1]:\n",
    "                ax.hist(x[:,c], label = 'feature_{:d}'.format(c),density = True)\n",
    "                ax.legend(loc = 'upper left')\n",
    "                ax.set_ylabel('Probability')\n",
    "                ax.set_xlabel('Value')\n",
    "            c += 1\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions needed for the log and reg_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"Apply sigmoid function on t\n",
    "    \n",
    "    Args: \n",
    "        t=>(numpy.array): Values to apply sigmoid function\n",
    "    \n",
    "    Returns:\n",
    "        => numpy.array: Calculated values of sigmoid\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1.0 / (1.0 + np.exp(-t))\n",
    "\n",
    "\n",
    "def calculate_loss_log(y, tx, w):\n",
    "    \"\"\"Compute the cost of log_regression\n",
    "    \n",
    "    Args: \n",
    "        y =>(numpy.array): Target values\n",
    "        tx =>(numpy.array): Transposed features\n",
    "        w => (numpy.array): Weigths \n",
    "          \n",
    "    Returns:\n",
    "        => numpy.array: Calculated loss\n",
    "    \"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "    return np.squeeze(- loss)\n",
    "\n",
    "    \n",
    "def calculate_gradient_log(y, tx, w,):\n",
    "    \"\"\"Compute the gradient of loss for log_regression\n",
    "    \n",
    "    Args: \n",
    "        y =>(numpy.array): Target values\n",
    "        tx => (numpy.array): Transposed features\n",
    "        w => (numpy.array): Weigths \n",
    "          \n",
    "    Returns:\n",
    "        => numpy.array: Calculated logistic gradient\n",
    "    \"\"\"\n",
    "\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    grad = tx.T.dot(pred - y)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def learning_by_gradient_descent_log(y, tx, w, gamma):\n",
    "    \"\"\"Compute the gradient descen using logistic regression\n",
    "    \n",
    "    Args: \n",
    "        y =>(numpy.array): Target values\n",
    "        tx => (numpy.array): Transposed features\n",
    "        w => (numpy.array): Weigths \n",
    "        gamma=> (float): the gamma to use.\n",
    "        \n",
    "    Returns:\n",
    "        w =>(numpy.array): Calculated Weights\n",
    "        loss => (numpy.array): Calculated Loss\n",
    "    \"\"\"\n",
    "\n",
    "    loss = calculate_loss_log(y, tx, w) \n",
    "    grad = calculate_gradient_log(y, tx, w)\n",
    "    w -= gamma * grad\n",
    "    return loss, w\n",
    "\n",
    "\n",
    "def learning_by_reg_gradient_descent_log(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"Compute the gradient descen using logistic regression\n",
    "    \n",
    "    Args: \n",
    "        y =>(numpy.array): Target values\n",
    "        tx => (numpy.array): Transposed features\n",
    "        w => (numpy.array): Weigths \n",
    "        gamma=> (float): the gamma to use.\n",
    "        \n",
    "    Returns: \n",
    "        w =>(numpy.array): Calculated Weights\n",
    "        loss => (numpy.array): Calculated Loss\n",
    "    \"\"\"\n",
    "\n",
    "    loss = calculate_loss_log(y, tx, w) + lambda_ * np.squeeze(w.T.dot(w))\n",
    "    grad = calculate_gradient_log(y, tx, w) + 2 * lambda_ * w\n",
    "    w -= gamma * grad\n",
    "    return loss, w\n",
    "\n",
    "def logistic_regression(y, tx, w_initial, max_iters, gamma):\n",
    "    \"\"\"Implement logistic regression using gradient descent\n",
    "    \n",
    "    Args: \n",
    "      y =>(numpy.array): Target values\n",
    "      tx => (numpy.array): Transposed features\n",
    "      w_initial => (numpy.array): Initial Weigths \n",
    "      max_iters => (int): number of iterations.\n",
    "      gamma=> (float): the gamma to use.\n",
    "          \n",
    "    Returns: \n",
    "        w =>(numpy.array): Calculated Weights\n",
    "        loss => (numpy.array): Calculated Loss\n",
    "    \"\"\"\n",
    "\n",
    "    assert max_iters > 0, \"max_iters should be a positive number\"\n",
    "    assert y.shape[0] == tx.shape[0], \"y and tx should have the same number of entries (rows)\"\n",
    "    assert tx.shape[1] == w_initial.shape[0], \"initial_w should be the same degree as tx\"\n",
    "    \n",
    "    print_every = 250\n",
    "    w = w_initial\n",
    "    losses =[]\n",
    "    for n_iter in range(max_iters+1):\n",
    "        loss, w = learning_by_gradient_descent_log(y, tx, w, gamma)\n",
    "        if (n_iter % print_every == 0):\n",
    "            # print average loss for the last print_every iterations\n",
    "            print('iteration\\t', str(n_iter), loss)\n",
    "            losses.append(loss)\n",
    "\n",
    "            \n",
    "    loss = learning_by_gradient_descent_log(y, tx, w, gamma)\n",
    "    \n",
    "    return w, loss,losses\n",
    "\n",
    "def reg_logistic_regression(y, tx, w_initial, max_iters, gamma,lambda_):\n",
    "    \"\"\"Implement logistic regression using gradient descent\n",
    "    \n",
    "    Args: \n",
    "        y =>(numpy.array): Target values\n",
    "        tx => (numpy.array): Transposed features\n",
    "        w_initial => (numpy.array): Initial Weigths \n",
    "        max_iters => (int): number of iterations.\n",
    "        gamma=> (float): the gamma to use.\n",
    "          \n",
    "    Returns: \n",
    "        w =>(numpy.array): Calculated Weights\n",
    "        loss => (numpy.array): Calculated Loss\n",
    "    \"\"\"\n",
    "\n",
    "    assert max_iters > 0, \"max_iters should be a positive number\"\n",
    "    assert y.shape[0] == tx.shape[0], \"y and tx should have the same number of entries (rows)\"\n",
    "    assert tx.shape[1] == w_initial.shape[0], \"initial_w should be the same degree as tx\"\n",
    "    \n",
    "    print_every = 250\n",
    "    w = w_initial\n",
    "    losses =[]\n",
    "    for n_iter in range(max_iters+1):\n",
    "        loss, w = learning_by_reg_gradient_descent_log(y, tx, w, gamma,lambda_)\n",
    "        if (n_iter % print_every == 0):\n",
    "            # print average loss for the last print_every iterations\n",
    "            print('iteration\\t', str(n_iter), loss)\n",
    "            losses.append(loss)\n",
    "\n",
    "            \n",
    "    loss = learning_by_reg_gradient_descent_log(y, tx, w, gamma,lambda_)\n",
    "    \n",
    "    return w, loss,losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [22]\n",
    "full_x_train_num, full_x_train_cat = split_numerical_categorical(tX_trainRaw,cat_cols)\n",
    "# Treat numerical values\n",
    "full_x_train_num_nan = replace_undef_val_with_nan(full_x_train_num)\n",
    "full_x_train_num_nan_std, train_mean, train_std = nan_standardize_fit(full_x_train_num_nan)\n",
    "# full_x_train_num_valid_std = replace_nan_val_with_mean(full_x_train_num_nan_std)\n",
    "full_x_train_num_valid_std = replace_nan_val_with_median(full_x_train_num_nan_std)\n",
    "full_x_train_num_valid_std = replace_iqr_outliers(full_x_train_num_valid_std)\n",
    "# Treat categorical values\n",
    "full_x_train_ohe_cat = one_hot_encode(full_x_train_cat)\n",
    "x_train_poly = build_poly(full_x_train_num_valid_std,3)\n",
    "full_x_train = np.hstack((x_train_poly,full_x_train_ohe_cat))\n",
    "# Treat labels\n",
    "full_y_train = y_trainRaw\n",
    "full_y_train = relabel_y_non_negative(full_y_train).reshape(-1,1)\n",
    "full_y_train = full_y_train.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val = split_data(full_x_train,full_y_train,0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [22]\n",
    "x_test_num, x_test_cat = split_numerical_categorical(tX_testRaw,cat_cols)\n",
    "# Treat numerical values\n",
    "x_test_num_nan = replace_undef_val_with_nan(x_test_num)\n",
    "x_test_num_nan_std = nan_standardize_transform(x_test_num_nan,train_mean,train_std)\n",
    "# x_test_num_nan_std = nan_standardize_with_median_transform(x_test_num_nan,train_median,train_std)\n",
    "x_test_num_valid_std = replace_nan_val_with_median(x_test_num_nan_std)\n",
    "x_test_num_valid_std = replace_iqr_outliers(x_test_num_valid_std)\n",
    "# x_test_num_valid_std = replace_nan_val_with_mean(x_test_num_nan_std)\n",
    "x_test_ohe_cat = one_hot_encode(x_test_cat)\n",
    "x_test_poly = build_poly(x_test_num_valid_std,3)\n",
    "x_test = np.hstack((x_test_poly,x_test_ohe_cat))\n",
    "# Treat labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 1000\n",
    "gamma = 0.000001\n",
    "w_initial = np.zeros((full_x_train.shape[1], 1))\n",
    "# weights, loss_tr,losses = logistic_regression(y_train, x_train, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val = relabel_y_negative(y_val)\n",
    "# y_pred = predict_labels(weights, x_val)\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# accuracy_score(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degrees = range(1, 6)\n",
    "# lambdas = np.logspace(-4, 0, 10)\n",
    "\n",
    "# rmse_tr = np.zeros((len(lambdas),len(degrees)))\n",
    "# rmse_val = np.zeros((len(lambdas),len(degrees)))\n",
    "# for index_lambda_, lambda_ in enumerate(lambdas):\n",
    "#     print(\"LAMBDA: \", lambda_)\n",
    "#     for index_degree, degree in enumerate(degrees):\n",
    "#             print(\"DEGREEE: \", degree)\n",
    "#             x_tr_pol = build_poly(x_train,degree)\n",
    "#             x_val_pol = build_poly(x_val,degree)\n",
    "#             w_initial = np.zeros((x_tr_pol.shape[1], 1))\n",
    "#             weights,_,_ = reg_logistic_regression(y_train, x_tr_pol, w_initial, max_iters, gamma,lambda_)\n",
    "#             loss_train,_ = learning_by_reg_gradient_descent_log(y_train, x_tr_pol, weights, gamma,lambda_)\n",
    "#             loss_val,_ = learning_by_reg_gradient_descent_log(y_val, x_val_pol, weights, gamma,lambda_)\n",
    "# #             rmse_tr.append(np.sqrt(2*loss_train))        \n",
    "# #             rmse_val.append(np.sqrt(2*loss_val))        \n",
    "#             rmse_tr[index_lambda_, index_degree] = np.sqrt(2*loss_train)\n",
    "#             rmse_val[index_lambda_, index_degree] = np.sqrt(2*loss_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(rmse_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.armgin(rmse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figures(x_dim, y_dim1, y_dim2, title, legend):\n",
    "    plt.figure(1, figsize=(18,12))\n",
    "    plt.title(title)\n",
    "    plt.plot(x_dim, y_dim1)\n",
    "    plt.plot(x_dim, y_dim2, 'r')\n",
    "    plt.grid(True)\n",
    "    plt.legend(legend)\n",
    "    plt.show()\n",
    "plot_figures(degrees, rmse_tr[0], rmse_val[0], \"Train vs Validation\", ['Train', 'Validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogReg with the best degree and gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration\t 0 138629.4361119856\n",
      "iteration\t 250 84725.66497321552\n",
      "iteration\t 500 83314.93088593178\n",
      "iteration\t 750 82787.21469085386\n",
      "iteration\t 1000 82494.51500940116\n"
     ]
    }
   ],
   "source": [
    "# x_tr_pol = build_poly(x_train,3)\n",
    "# x_val_pol = build_poly(x_val,3)\n",
    "w_initial = np.zeros((x_train.shape[1], 1))\n",
    "weights,_,_ = logistic_regression(y_train, x_train, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81084"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val = relabel_y_negative(y_val)\n",
    "y_pred = predict_labels(weights, x_val)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RegLogReg with the best lambda, degree and gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration\t 0 138629.4361119856\n",
      "iteration\t 250 84726.22275867395\n",
      "iteration\t 500 83315.7173661692\n",
      "iteration\t 750 82788.14487469665\n",
      "iteration\t 1000 82495.55465048923\n"
     ]
    }
   ],
   "source": [
    "# x_tr_pol = build_poly(x_train,3)\n",
    "# x_val_pol = build_poly(x_val,3)\n",
    "w_initial = np.zeros((x_train.shape[1], 1))\n",
    "# weights,_,_ = reg_logistic_regression(y_train, x_train, w_initial, max_iters, gamma,0.0001)\n",
    "weights,_,_ = reg_logistic_regression(y_train, x_train, w_initial, max_iters, gamma, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81084"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val = relabel_y_negative(y_val)\n",
    "y_pred = predict_labels(weights, x_val)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test_pol = build_poly(x_test,3)\n",
    "y_pred = predict_labels(weights, x_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../results/reg_log_reg.csv' \n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/reg_log_reg.csv') as input, open('../results/reg_log_reg_cleaned.csv', 'w', newline='') as output:\n",
    "    writer = csv.writer(output)\n",
    "    for row in csv.reader(input):\n",
    "        if any(field.strip() for field in row):\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import linear_model, preprocessing, metrics\n",
    "\n",
    "# model = linear_model.SGDClassifier(max_iter=max_iters)\n",
    "# model.fit(x_train,y_train)\n",
    "# predict_y = model.predict(x_val)\n",
    "# predict_y = relabel_y_negative(predict_y)\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# accuracy_score(y_val,predict_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_iters = 1000\n",
    "# gamma = 0.00001\n",
    "# w_initial = np.zeros((x_train_prep.shape[1], 1))\n",
    "# lambdas = np.logspace(-4, 0, 10)\n",
    "# for i,lambda_ in enumerate(lambdas):\n",
    "#         weights, loss_tr,losses = logistic_regression(y_train_prep, x_train_prep, w_initial, max_iters, gamma)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_range = np.arange(0.0, 1.6e-8, 2.e-9)\n",
    "# (lambda_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = predict_labels(weights, x_val)\n",
    "# y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_PATH = '../results/log_reg.csv' \n",
    "# create_csv_submission(ids_test, y_pred[:], OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
