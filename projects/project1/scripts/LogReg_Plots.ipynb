{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y_train, x_train, ids_train = load_csv_data(DATA_TRAIN_PATH)\n",
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "y_test, x_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions needed for the log and reg_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"Apply sigmoid function on t\n",
    "    Args: t=>(numpy.array): Values to apply sigmoid function\n",
    "    Returns:=> numpy.array: Calculated values of sigmoid\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1.0 / (1.0 + np.exp(-t))\n",
    "\n",
    "\n",
    "def calculate_loss_log(y, tx, w):\n",
    "    \"\"\"Compute the cost of log_regression\n",
    "    Args: y =>(numpy.array): Target values\n",
    "          tx =>(numpy.array): Transposed features\n",
    "          w => (numpy.array): Weigths \n",
    "    Returns:=> numpy.array: Calculated loss\n",
    "    \"\"\"\n",
    "    pred = np.squeeze(sigmoid(tx.dot(w)))\n",
    "#     loss = np.sum(np.log(1 + np.exp(pred))) - y.T.dot(pred)\n",
    "    loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "    print(\"calculate_loss_log\", w.shape)\n",
    "\n",
    "    return np.squeeze(- loss)\n",
    "    \n",
    "#     exp_ = np.exp(tx.dot(w))\n",
    "#     log_ = np.log(1 + exp_)\n",
    "#     y_ = y * tx.dot(w)\n",
    "#     sum_ = np.sum(log_ - y_ , axis=0)\n",
    "#     return sum_\n",
    "\n",
    "def calculate_gradient_log(y, tx, w,):\n",
    "    \"\"\"Compute the gradient of loss for log_regression\n",
    "    Args: y =>(numpy.array): Target values\n",
    "          tx => (numpy.array): Transposed features\n",
    "          w => (numpy.array): Weigths \n",
    "    Returns:=> numpy.array: Calculated logistic gradient\n",
    "    \"\"\"\n",
    "\n",
    "    pred =  sigmoid(tx.dot(w))\n",
    "    gradient = np.dot(tx.T,(pred - y))\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def learning_by_gradient_descent_log(y, tx, w, gamma):\n",
    "    \"\"\"Compute the gradient descen using logistic regression\n",
    "    Args: y =>(numpy.array): Target values\n",
    "          tx => (numpy.array): Transposed features\n",
    "          w => (numpy.array): Weigths \n",
    "          gamma=> (float): the gamma to use.\n",
    "    Returns: w =>(numpy.array): Calculated Weights\n",
    "             loss => (numpy.array): Calculated Loss\n",
    "    \"\"\"\n",
    "\n",
    "    loss = calculate_loss_log(y, tx, w)\n",
    "    grad = calculate_gradient_log(y, tx, w)\n",
    "    print(\"learning_by_gradient_descent_log\", w.shape)\n",
    "    print(\"learning_by_gradient_descent_log\", grad.shape)\n",
    "\n",
    "    w = w - gamma * grad\n",
    "\n",
    "    return loss, w\n",
    "\n",
    "\n",
    "def logistic_regression(y, tx, w_initial, max_iters, gamma):\n",
    "    \"\"\"Implement logistic regression using gradient descent\n",
    "    Args: y =>(numpy.array): Target values\n",
    "          tx => (numpy.array): Transposed features\n",
    "          w_initial => (numpy.array): Initial Weigths \n",
    "          max_iters => (int): number of iterations.\n",
    "          gamma=> (float): the gamma to use.\n",
    "    Returns: w =>(numpy.array): Calculated Weights\n",
    "             loss => (numpy.array): Calculated Loss\n",
    "    \"\"\"\n",
    "\n",
    "    assert max_iters > 0, \"max_iters should be a positive number\"\n",
    "    assert y.shape[0] == tx.shape[0], \"y and tx should have the same number of entries (rows)\"\n",
    "    assert tx.shape[1] == w_initial.shape[0], \"initial_w should be the same degree as tx\"\n",
    "    \n",
    "    print_every = 10\n",
    "    w = w_initial\n",
    "    # change labels from (-1, 1) to (0, 1) should we use this????\n",
    "#     y[np.where(y == -1)] = 0\n",
    "    losses =[]\n",
    "    for n_iter in range(max_iters):\n",
    "        loss, w = learning_by_gradient_descent_log(y, tx, w, gamma)\n",
    "        losses.append(loss)\n",
    "        if (n_iter % print_every == 0):\n",
    "            # print average loss for the last print_every iterations\n",
    "            print('iteration\\t', str(n_iter))\n",
    "            \n",
    "    loss = calculate_loss_log(y, tx, w)\n",
    "    \n",
    "    return w, loss,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 30)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n",
    "x_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000,)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 1)\n",
      "calculate_loss_log (30, 1)\n",
      "learning_by_gradient_descent_log (30, 1)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "iteration\t 0\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "iteration\t 10\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "calculate_loss_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "learning_by_gradient_descent_log (30, 10)\n",
      "calculate_loss_log (30, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AndresMontero\\.conda\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "max_iters = 20\n",
    "threshold = 1e-8\n",
    "gamma = 0.01\n",
    "losses = []\n",
    "w_initial = np.zeros((x_train.shape[1], 1))\n",
    "print(w_initial.shape)\n",
    "\n",
    "y_tmp = y_train[:10]\n",
    "x_tmp = x_train[:10]\n",
    "\n",
    "weights, loss_tr, losses = logistic_regression(y_tmp, x_tmp, w_initial, max_iters, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.005865"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 10)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(weights, x_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../results/log_reg.csv' \n",
    "y_pred = predict_labels(weights, x_test)\n",
    "create_csv_submission(ids_test, y_pred[:], OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # least squares: TODO\n",
    "    # returns mse, and optimal weights\n",
    "    # ***************************************************\n",
    "    \n",
    "    a = np.dot(tx.T,tx)\n",
    "    b = np.dot(tx.T,y)\n",
    "    \n",
    "    w_star = np.linalg.solve(a,b)\n",
    "    \n",
    "    return w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = least_squares(y_train, x_train)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(weights, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
