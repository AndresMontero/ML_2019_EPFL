{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'  \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_utils import *\n",
    "from proj1_visualization import *\n",
    "from proj1_cross_validation import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [22]\n",
    "tX_num, tX_cat = split_numerical_categorical(tX,cat_cols)\n",
    "tX_test_num, tX_test_cat = split_numerical_categorical(tX_test,cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat numerical values\n",
    "best_degree = 5\n",
    "full_x_train_num_nan = replace_undef_val_with_nan(tX_num)\n",
    "full_x_train_num_std, train_mean, train_std = nan_standardize_fit(full_x_train_num_nan)\n",
    "full_x_train_num_valid = replace_nan_val_with_median(full_x_train_num_std)\n",
    "full_x_train_num_valid = replace_iqr_outliers(full_x_train_num_valid)\n",
    "# Treat categorical values\n",
    "full_x_train_ohe_cat = one_hot_encode(tX_cat)\n",
    "full_x_train_poly = build_poly(full_x_train_num_valid , best_degree)\n",
    "full_x_train = np.hstack((full_x_train_poly,full_x_train_ohe_cat))\n",
    "# Treat labels\n",
    "full_y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val = split_data(full_x_train,full_y_train,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat numerical values\n",
    "x_test_num_nan = replace_undef_val_with_nan(tX_test_num)\n",
    "x_test_num_nan_std = nan_standardize_transform(x_test_num_nan,train_mean,train_std)\n",
    "x_test_num_valid_std = replace_nan_val_with_median(x_test_num_nan_std)\n",
    "x_test_num_valid_std = replace_iqr_outliers(x_test_num_valid_std)\n",
    "# Treat categorical values\n",
    "x_test_ohe_cat = one_hot_encode(tX_test_cat)\n",
    "x_test_poly = build_poly(x_test_num_valid_std , best_degree)\n",
    "x_test = np.hstack((x_test_poly,x_test_ohe_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD(1/1999): loss=0.49999945001730517, w0=-3.1503000000000004e-07, w1=1.1441435393875164e-07\n",
      "GD(51/1999): loss=0.4999719543776745, w0=-1.6065761678670137e-05, w1=5.834938817029493e-06\n",
      "GD(101/1999): loss=0.49994446558980826, w0=-3.1814986957753016e-05, w1=1.1555084412194887e-05\n",
      "GD(151/1999): loss=0.49991698365161474, w0=-4.756270600479256e-05, w1=1.7274851172060363e-05\n",
      "GD(201/1999): loss=0.49988950856100184, w0=-6.330891898731048e-05, w1=2.299423912924805e-05\n",
      "GD(251/1999): loss=0.49986204031587933, w0=-7.905362607280616e-05, w1=2.8713248316376766e-05\n",
      "GD(301/1999): loss=0.4998345789141566, w0=-9.479682742875676e-05, w1=3.443187876606203e-05\n",
      "GD(351/1999): loss=0.4998071243537451, w0=-0.00011053852322261727, w1=4.015013051091601e-05\n",
      "GD(401/1999): loss=0.4997796766325555, w0=-0.00012627871362182045, w1=4.586800358354759e-05\n",
      "GD(451/1999): loss=0.4997522357485006, w0=-0.0001420173987937767, w1=5.158549801656232e-05\n",
      "GD(501/1999): loss=0.499724801699493, w0=-0.00015775457890587435, w1=5.730261384256248e-05\n",
      "GD(551/1999): loss=0.4996973744834461, w0=-0.0001734902541254792, w1=6.3019351094147e-05\n",
      "GD(601/1999): loss=0.4996699540982744, w0=-0.00018922442461993518, w1=6.873570980391155e-05\n",
      "GD(651/1999): loss=0.499642540541893, w0=-0.00020495709055656376, w1=7.44516900044484e-05\n",
      "GD(701/1999): loss=0.4996151338122173, w0=-0.00022068825210266448, w1=8.016729172834665e-05\n",
      "GD(751/1999): loss=0.4995877339071641, w0=-0.00023641790942551438, w1=8.588251500819195e-05\n",
      "GD(801/1999): loss=0.4995603408246503, w0=-0.00025214606269236835, w1=9.159735987656668e-05\n",
      "GD(851/1999): loss=0.49953295456259383, w0=-0.00026787271207045924, w1=9.731182636604995e-05\n",
      "GD(901/1999): loss=0.49950557511891347, w0=-0.00028359785772699735, w1=0.00010302591450921758\n",
      "GD(951/1999): loss=0.4994782024915281, w0=-0.0002993214998291712, w1=0.00010873962433864199\n",
      "GD(1001/1999): loss=0.49945083667835816, w0=-0.00031504363854414663, w1=0.00011445295588689245\n",
      "GD(1051/1999): loss=0.49942347767732426, w0=-0.000330764274039068, w1=0.00012016590918653467\n",
      "GD(1101/1999): loss=0.49939612548634776, w0=-0.0003464834064810567, w1=0.00012587848427013132\n",
      "GD(1151/1999): loss=0.4993687801033509, w0=-0.00036220103603721215, w1=0.0001315906811702417\n",
      "GD(1201/1999): loss=0.4993414415262565, w0=-0.0003779171628746122, w1=0.00013730249991942165\n",
      "GD(1251/1999): loss=0.4993141097529881, w0=-0.0003936317871603118, w1=0.00014301394055022393\n",
      "GD(1301/1999): loss=0.49928678478147037, w0=-0.00040934490906134385, w1=0.00014872500309519762\n",
      "GD(1351/1999): loss=0.49925946660962767, w0=-0.00042505652874471944, w1=0.0001544356875868891\n",
      "GD(1401/1999): loss=0.4992321552353863, w0=-0.00044076664637742705, w1=0.00016014599405784103\n",
      "GD(1451/1999): loss=0.49920485065667225, w0=-0.00045647526212643327, w1=0.00016585592254059263\n",
      "GD(1501/1999): loss=0.4991775528714129, w0=-0.00047218237615868225, w1=0.0001715654730676802\n",
      "GD(1551/1999): loss=0.4991502618775363, w0=-0.00048788798864109656, w1=0.00017727464567163654\n",
      "GD(1601/1999): loss=0.4991229776729707, w0=-0.0005035920997405759, w1=0.00018298344038499107\n",
      "GD(1651/1999): loss=0.49909570025564537, w0=-0.0005192947096239981, w1=0.0001886918572402701\n",
      "GD(1701/1999): loss=0.4990684296234907, w0=-0.0005349958184582193, w1=0.00019439989626999653\n",
      "GD(1751/1999): loss=0.49904116577443686, w0=-0.0005506954264100726, w1=0.00020010755750668992\n",
      "GD(1801/1999): loss=0.4990139087064159, w0=-0.00056639353364637, w1=0.0002058148409828668\n",
      "GD(1851/1999): loss=0.498986658417359, w0=-0.0005820901403339, w1=0.00021152174673103998\n",
      "GD(1901/1999): loss=0.4989594149051999, w0=-0.0005977852466394307, w1=0.00021722827478371922\n",
      "GD(1951/1999): loss=0.4989321781678719, w0=-0.0006134788527297065, w1=0.00022293442517341098\n",
      "GD(1/1999): loss=0.49999798083955754, w0=-3.1503000000000004e-07, w1=1.1441435393875164e-07\n",
      "GD(51/1999): loss=0.4998970837281934, w0=-1.606068371248464e-05, w1=5.834489503860933e-06\n",
      "GD(101/1999): loss=0.4997963059335669, w0=-3.179487986585314e-05, w1=1.1553305319104635e-05\n",
      "GD(151/1999): loss=0.4996956472834236, w0=-4.751762699441122e-05, w1=1.727086263299731e-05\n",
      "GD(201/1999): loss=0.4995951076057695, w0=-6.322893362596754e-05, w1=2.2987162278233256e-05\n",
      "GD(251/1999): loss=0.4994946867288719, w0=-7.892880828183856e-05, w1=2.8702205086874116e-05\n",
      "GD(301/1999): loss=0.49939438448125734, w0=-9.461725947685366e-05, w1=3.441599189034933e-05\n",
      "GD(351/1999): loss=0.4992942006917134, w0=-0.0001102942957193601, w1=4.0128523519456734e-05\n",
      "GD(401/1999): loss=0.49919413518928657, w0=-0.00012595992551122804, w1=4.5839800804362884e-05\n",
      "GD(451/1999): loss=0.4990941878032823, w0=-0.00014161415734785545, w1=5.1549824574603656e-05\n",
      "GD(501/1999): loss=0.4989943583632649, w0=-0.00015725699971817295, w1=5.72585956590848e-05\n",
      "GD(551/1999): loss=0.4988946466990569, w0=-0.00017288846110464907, w1=6.296611488608213e-05\n",
      "GD(601/1999): loss=0.4987950526407389, w0=-0.00018850854998329533, w1=6.867238308324238e-05\n",
      "GD(651/1999): loss=0.49869557601864883, w0=-0.00020411727482367082, w1=7.437740107758347e-05\n",
      "GD(701/1999): loss=0.4985962166633812, w0=-0.00021971464408888724, w1=8.008116969549498e-05\n",
      "GD(751/1999): loss=0.49849697440578866, w0=-0.0002353006662356139, w1=8.578368976273876e-05\n",
      "GD(801/1999): loss=0.49839784907697837, w0=-0.0002508753497140831, w1=9.148496210444933e-05\n",
      "GD(851/1999): loss=0.4982988405083144, w0=-0.0002664387029680943, w1=9.718498754513431e-05\n",
      "GD(901/1999): loss=0.4981999485314164, w0=-0.0002819907344350197, w1=0.00010288376690867515\n",
      "GD(951/1999): loss=0.4981011729781589, w0=-0.0002975314525458092, w1=0.00010858130101832719\n",
      "GD(1001/1999): loss=0.498002513680671, w0=-0.00031306086572499456, w1=0.00011427759069672067\n",
      "GD(1051/1999): loss=0.4979039704713365, w0=-0.00032857898239069527, w1=0.0001199726367658607\n",
      "GD(1101/1999): loss=0.49780554318279263, w0=-0.00034408581095462306, w1=0.00012566644004712805\n",
      "GD(1151/1999): loss=0.49770723164793057, w0=-0.0003595813598220869, w1=0.00013135900136127958\n",
      "GD(1201/1999): loss=0.4976090356998948, w0=-0.00037506563739199785, w1=0.0001370503215284486\n",
      "GD(1251/1999): loss=0.49751095517208227, w0=-0.00039053865205687367, w1=0.0001427404013681456\n",
      "GD(1301/1999): loss=0.49741298989814237, w0=-0.0004060004122028447, w1=0.0001484292416992584\n",
      "GD(1351/1999): loss=0.4973151397119763, w0=-0.00042145092620965793, w1=0.00015411684334005302\n",
      "GD(1401/1999): loss=0.49721740444773754, w0=-0.00043689020245068203, w1=0.00015980320710817377\n",
      "GD(1451/1999): loss=0.49711978393982964, w0=-0.0004523182492929129, w1=0.00016548833382064394\n",
      "GD(1501/1999): loss=0.49702227802290827, w0=-0.00046773507509697746, w1=0.00017117222429386628\n",
      "GD(1551/1999): loss=0.49692488653187883, w0=-0.00048314068821713943, w1=0.00017685487934362344\n",
      "GD(1601/1999): loss=0.4968276093018968, w0=-0.0004985350970013039, w1=0.00018253629978507834\n",
      "GD(1651/1999): loss=0.4967304461683676, w0=-0.0005139183097910223, w1=0.00018821648643277498\n",
      "GD(1701/1999): loss=0.4966333969669461, w0=-0.000529290334921497, w1=0.0001938954401006384\n",
      "GD(1751/1999): loss=0.49653646153353487, w0=-0.0005446511807215867, w1=0.00019957316160197562\n",
      "GD(1801/1999): loss=0.49643963970428684, w0=-0.0005600008555138113, w1=0.000205249651749476\n",
      "GD(1851/1999): loss=0.4963429313156018, w0=-0.0005753393676143565, w1=0.00021092491135521154\n",
      "GD(1901/1999): loss=0.49624633620412756, w0=-0.0005906667253330778, w1=0.00021659894123063757\n",
      "GD(1951/1999): loss=0.49614985420675994, w0=-0.0006059829369735077, w1=0.00022227174218659315\n",
      "GD(1/1999): loss=0.4999973303015541, w0=-3.1503000000000004e-07, w1=1.1441435393875164e-07\n",
      "GD(51/1999): loss=0.49986394187343297, w0=-1.606027652947538e-05, w1=5.834619748892216e-06\n",
      "GD(101/1999): loss=0.49973074242483495, w0=-3.179326885603748e-05, w1=1.1553820980908391e-05\n",
      "GD(151/1999): loss=0.4995977316224855, w0=-4.751401811524438e-05, w1=1.7272018570344613e-05\n",
      "GD(201/1999): loss=0.4994649091337845, w0=-6.322253543075437e-05, w1=2.2989213036807534e-05\n",
      "GD(251/1999): loss=0.4993322746268051, w0=-7.891883191434044e-05, w1=2.8705404899154458e-05\n",
      "GD(301/1999): loss=0.49919982777029076, w0=-9.460291866590456e-05, w1=3.442059467549464e-05\n",
      "GD(351/1999): loss=0.49906756823365594, w0=-0.00011027480677349211, w1=4.013478288319069e-05\n",
      "GD(401/1999): loss=0.49893549568698303, w0=-0.00012593450731330607, w1=4.5847970038859835e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD(451/1999): loss=0.49880360980102084, w0=-0.00014158203134972153, w1=5.156015665837525e-05\n",
      "GD(501/1999): loss=0.49867191024718394, w0=-0.00015721738993529987, w1=5.727134325686747e-05\n",
      "GD(551/1999): loss=0.49854039669755074, w0=-0.00017284059411080293, w1=6.298153034872556e-05\n",
      "GD(601/1999): loss=0.49840906882486136, w0=-0.00018845165490520742, w1=6.869071844759857e-05\n",
      "GD(651/1999): loss=0.4982779263025176, w0=-0.00020405058333571906, w1=7.43989080663969e-05\n",
      "GD(701/1999): loss=0.49814696880457987, w0=-0.00021963739040778703, w1=8.01060997172934e-05\n",
      "GD(751/1999): loss=0.4980161960057666, w0=-0.00023521208711511782, w1=8.58122939117248e-05\n",
      "GD(801/1999): loss=0.4978856075814526, w0=-0.0002507746844396896, w1=9.151749116039321e-05\n",
      "GD(851/1999): loss=0.49775520320766764, w0=-0.0002663251933517663, w1=9.722169197326711e-05\n",
      "GD(901/1999): loss=0.4976249825610944, w0=-0.00028186362480991164, w1=0.00010292489685958282\n",
      "GD(951/1999): loss=0.49749494531906824, w0=-0.0002973899897610039, w1=0.00010862710632784571\n",
      "GD(1001/1999): loss=0.49736509115957395, w0=-0.0003129042991402495, w1=0.0001143283208858319\n",
      "GD(1051/1999): loss=0.49723541976124647, w0=-0.00032840656387119615, w1=0.00012002854104058891\n",
      "GD(1101/1999): loss=0.49710593080336696, w0=-0.00034389679486574945, w1=0.00012572776729843753\n",
      "GD(1151/1999): loss=0.4969766239658638, w0=-0.00035937500302418414, w1=0.00013142600016497266\n",
      "GD(1201/1999): loss=0.4968474989293087, w0=-0.0003748411992351599, w1=0.00013712324014506514\n",
      "GD(1251/1999): loss=0.49671855537491777, w0=-0.00039029539437573474, w1=0.00014281948774286238\n",
      "GD(1301/1999): loss=0.4965897929845479, w0=-0.00040573759931137904, w1=0.00014851474346179027\n",
      "GD(1351/1999): loss=0.4964612114406965, w0=-0.0004211678248959899, w1=0.00015420900780455397\n",
      "GD(1401/1999): loss=0.49633281042649985, w0=-0.0004365860819719046, w1=0.00015990228127313967\n",
      "GD(1451/1999): loss=0.49620458962573116, w0=-0.0004519923813699144, w1=0.0001655945643688156\n",
      "GD(1501/1999): loss=0.4960765487228001, w0=-0.0004673867339092794, w1=0.00017128585759213335\n",
      "GD(1551/1999): loss=0.49594868740275083, w0=-0.00048276915039774127, w1=0.00017697616144292895\n",
      "GD(1601/1999): loss=0.4958210053512596, w0=-0.0004981396416315376, w1=0.00018266547642032454\n",
      "GD(1651/1999): loss=0.4956935022546355, w0=-0.000513498218395417, w1=0.00018835380302272936\n",
      "GD(1701/1999): loss=0.49556617779981715, w0=-0.0005288448914626507, w1=0.00019404114174784127\n",
      "GD(1751/1999): loss=0.49543903167437214, w0=-0.0005441796715950476, w1=0.0001997274930926478\n",
      "GD(1801/1999): loss=0.4953120635664951, w0=-0.0005595025695429687, w1=0.00020541285755342734\n",
      "GD(1851/1999): loss=0.49518527316500716, w0=-0.0005748135960453391, w1=0.0002110972356257508\n",
      "GD(1901/1999): loss=0.49505866015935374, w0=-0.000590112761829664, w1=0.00021678062780448228\n",
      "GD(1951/1999): loss=0.49493222423960337, w0=-0.0006054000776120406, w1=0.00022246303458378115\n",
      "1\n",
      "0\n",
      "0.69064\n",
      "[[0.69064]\n",
      " [0.6566 ]\n",
      " [0.66808]]\n"
     ]
    }
   ],
   "source": [
    "method_flag = 1\n",
    "degrees = np.asarray(range(1,4))\n",
    "lambdas = np.asarray([0])\n",
    "best_degree, best_lambda, accuracy_score, accuracy_scores_grid = degree_lambda_grid_search(y,tX,[22],0.8,method_flag,degrees,lambdas)\n",
    "print(best_degree)\n",
    "print(best_lambda)\n",
    "print(accuracy_score)\n",
    "print(accuracy_scores_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(50/2000): loss=0.4999560328730805, w(0)=-1.7999494721739857e-05, w1=3.5480791707477293e-06\n",
      "SGD(100/2000): loss=0.5001926794149707, w(0)=-3.399670654765714e-05, w1=1.0489958052191082e-05\n",
      "SGD(150/2000): loss=0.4996817964838604, w(0)=-5.1992623872818504e-05, w1=1.3375506850854295e-05\n",
      "SGD(200/2000): loss=0.49961034577844315, w(0)=-7.198936799854934e-05, w1=2.36637956134491e-05\n",
      "SGD(250/2000): loss=0.5003647068815094, w(0)=-8.39825402078926e-05, w1=2.823389416660846e-05\n",
      "SGD(300/2000): loss=0.49992163320844546, w(0)=-0.00010997139578886805, w1=3.3456361532792676e-05\n",
      "SGD(350/2000): loss=0.49938066192564445, w(0)=-0.0001299581496448557, w1=3.814024383261656e-05\n",
      "SGD(400/2000): loss=0.4995744662692441, w(0)=-0.00014594592209955548, w1=3.7837670888478076e-05\n",
      "SGD(450/2000): loss=0.5005391964889622, w(0)=-0.00015792810810679348, w1=4.704865296964899e-05\n",
      "SGD(500/2000): loss=0.5001973942162212, w(0)=-0.00017790936626798227, w1=5.55638469687169e-05\n",
      "SGD(550/2000): loss=0.5000674954478292, w(0)=-0.0001958924469887238, w1=5.868068685671567e-05\n",
      "SGD(600/2000): loss=0.5004918472268828, w(0)=-0.00022186905190885135, w1=6.267544022026408e-05\n",
      "SGD(650/2000): loss=0.5002505385346953, w(0)=-0.00023985106889768746, w1=7.30116297486714e-05\n",
      "SGD(700/2000): loss=0.4990493888375584, w(0)=-0.00025782371918221456, w1=7.762022002396391e-05\n",
      "SGD(750/2000): loss=0.5007738802967074, w(0)=-0.00027978567844677945, w1=8.479849398883905e-05\n",
      "SGD(800/2000): loss=0.49809226188251243, w(0)=-0.00029176236020024934, w1=9.123099353895322e-05\n",
      "SGD(850/2000): loss=0.5002930195956794, w(0)=-0.00030973834919809123, w1=9.552125074055433e-05\n",
      "SGD(900/2000): loss=0.5000849010732262, w(0)=-0.00032170122331118663, w1=0.00010075355381298452\n",
      "SGD(950/2000): loss=0.4980349268552071, w(0)=-0.0003376639384979387, w1=0.00010765821233118472\n",
      "SGD(1000/2000): loss=0.49878057434236683, w(0)=-0.00034763525044063427, w1=0.00011775126307879766\n",
      "SGD(1050/2000): loss=0.4990225101039216, w(0)=-0.00036558883898662976, w1=0.00011946586140824155\n",
      "SGD(1100/2000): loss=0.5000302092802922, w(0)=-0.0003735535417726031, w1=0.00012796704716078327\n",
      "SGD(1150/2000): loss=0.4990667925305608, w(0)=-0.00038152290154241713, w1=0.00012298605273773798\n",
      "SGD(1200/2000): loss=0.4976173595398367, w(0)=-0.0003974818212901733, w1=0.00013312716524241272\n",
      "SGD(1250/2000): loss=0.4986969984462488, w(0)=-0.0004094494816705126, w1=0.00014056167148427117\n",
      "SGD(1300/2000): loss=0.5018902653354955, w(0)=-0.00042341412532732376, w1=0.00014542229442775136\n",
      "SGD(1350/2000): loss=0.49787381410427634, w(0)=-0.0004533748483784811, w1=0.00015302090248925994\n",
      "SGD(1400/2000): loss=0.49730781804854607, w(0)=-0.00047332218088138775, w1=0.00016169357582730484\n",
      "SGD(1450/2000): loss=0.500661576393225, w(0)=-0.000485266044327785, w1=0.0001704231416664758\n",
      "SGD(1500/2000): loss=0.5023582370747631, w(0)=-0.0004992302048158798, w1=0.00018010323913305438\n",
      "SGD(1550/2000): loss=0.5008271657975527, w(0)=-0.000503179742577448, w1=0.00018774578503266286\n",
      "SGD(1600/2000): loss=0.4992700573074712, w(0)=-0.0005151187765492465, w1=0.00019320917594695136\n",
      "SGD(1650/2000): loss=0.49735079961097767, w(0)=-0.0005370638569065949, w1=0.00020569173130064909\n",
      "SGD(1700/2000): loss=0.5000045164655116, w(0)=-0.0005489929348877437, w1=0.00020735195886312845\n",
      "SGD(1750/2000): loss=0.5020237195598807, w(0)=-0.0005629660943298411, w1=0.00021383285442051444\n",
      "SGD(1800/2000): loss=0.49939057700448375, w(0)=-0.0005729136968917678, w1=0.00021675296254208498\n",
      "SGD(1850/2000): loss=0.4978467445558795, w(0)=-0.0005808698452519763, w1=0.00022680170134095067\n",
      "SGD(1900/2000): loss=0.49992766713005393, w(0)=-0.000596801398417472, w1=0.00023647837406464374\n",
      "SGD(1950/2000): loss=0.4977924118662855, w(0)=-0.0006087281988484144, w1=0.00023794870501171538\n",
      "SGD(2000/2000): loss=0.5030625413335145, w(0)=-0.0006226771360314885, w1=0.0002460389461568933\n",
      "SGD(50/2000): loss=0.5000667224233023, w(0)=-1.199748524317055e-05, w1=1.9022068554849966e-06\n",
      "SGD(100/2000): loss=0.49927915422399627, w(0)=-3.997504270670089e-05, w1=6.205879349942255e-06\n",
      "SGD(150/2000): loss=0.4996822927654351, w(0)=-4.7944760941946675e-05, w1=1.1552635526554412e-05\n",
      "SGD(200/2000): loss=0.5004508683288603, w(0)=-6.78994596909545e-05, w1=1.7625610953343185e-05\n",
      "SGD(250/2000): loss=0.49859196416802504, w(0)=-7.984749865458122e-05, w1=2.0587995577511925e-05\n",
      "SGD(300/2000): loss=0.4990982579625731, w(0)=-9.578437272968559e-05, w1=2.4723251728265006e-05\n",
      "SGD(350/2000): loss=0.49846444576331705, w(0)=-9.971641099391896e-05, w1=2.0373467267003593e-05\n",
      "SGD(400/2000): loss=0.4977456557015348, w(0)=-0.00011563464044298984, w1=2.0038807686669452e-05\n",
      "SGD(450/2000): loss=0.4962701472246318, w(0)=-0.00013754411309033497, w1=2.386346500751935e-05\n",
      "SGD(500/2000): loss=0.497741914210788, w(0)=-0.00015344873708556287, w1=3.0002867810959237e-05\n",
      "SGD(550/2000): loss=0.4960296159879617, w(0)=-0.00017133137411160974, w1=3.4252414092353773e-05\n",
      "SGD(600/2000): loss=0.495769454715381, w(0)=-0.00019119447199045415, w1=4.130328073805223e-05\n",
      "SGD(650/2000): loss=0.5014544723753619, w(0)=-0.00020106139794557758, w1=4.828033146085496e-05\n",
      "SGD(700/2000): loss=0.4942927400130552, w(0)=-0.0002209135183823495, w1=5.4144929398187386e-05\n",
      "SGD(750/2000): loss=0.5021327109585858, w(0)=-0.00023674684946145362, w1=6.681641803012967e-05\n",
      "SGD(800/2000): loss=0.4977870740142882, w(0)=-0.00025058299548054, w1=7.581629800672298e-05\n",
      "SGD(850/2000): loss=0.494910084582018, w(0)=-0.00026240692005097147, w1=7.718555775827922e-05\n",
      "SGD(900/2000): loss=0.5011121310874767, w(0)=-0.0002802220532879943, w1=8.054260836824795e-05\n",
      "SGD(950/2000): loss=0.5021607099917408, w(0)=-0.0003039994399230229, w1=8.81071129515282e-05\n",
      "SGD(1000/2000): loss=0.4979891235334622, w(0)=-0.00031777992958975453, w1=9.672938726943968e-05\n",
      "SGD(1050/2000): loss=0.4937641419632722, w(0)=-0.0003255667873066453, w1=0.00010174317415828217\n",
      "SGD(1100/2000): loss=0.4951162774775504, w(0)=-0.00033933617854086236, w1=0.00010783316029801273\n",
      "SGD(1150/2000): loss=0.49592313326298965, w(0)=-0.0003610805624053546, w1=0.00011602647545171457\n",
      "SGD(1200/2000): loss=0.4948951892051771, w(0)=-0.00038880357271100634, w1=0.0001282947044016858\n",
      "SGD(1250/2000): loss=0.5025665026522959, w(0)=-0.00040051972686603595, w1=0.00013322853622327218\n",
      "SGD(1300/2000): loss=0.49239565251348505, w(0)=-0.0004222330255248388, w1=0.00013974113563279322\n",
      "SGD(1350/2000): loss=0.4946794217705113, w(0)=-0.0004359157845921589, w1=0.00014479595066822583\n",
      "SGD(1400/2000): loss=0.49353319492025066, w(0)=-0.00045363565687021434, w1=0.00015303330180278479\n",
      "SGD(1450/2000): loss=0.4913435091757419, w(0)=-0.00047132742277350897, w1=0.00016304705680938303\n",
      "SGD(1500/2000): loss=0.49318246689542683, w(0)=-0.0004889963220357287, w1=0.00016987863797203503\n",
      "SGD(1550/2000): loss=0.4922816594728503, w(0)=-0.0004986099954811817, w1=0.000166548535694377\n",
      "SGD(1600/2000): loss=0.49260143269706375, w(0)=-0.0005142573440713929, w1=0.000174679997229214\n",
      "SGD(1650/2000): loss=0.5039465208941232, w(0)=-0.0005318729587398065, w1=0.00017638077167815382\n",
      "SGD(1700/2000): loss=0.4931382735075043, w(0)=-0.000547491998478004, w1=0.0001810653385201769\n",
      "SGD(1750/2000): loss=0.4891894785346663, w(0)=-0.0005610970308017202, w1=0.00018498273112648107\n",
      "SGD(1800/2000): loss=0.5091659779549885, w(0)=-0.0005806695488148911, w1=0.00019444320035198788\n",
      "SGD(1850/2000): loss=0.5045457237818477, w(0)=-0.0005962557275316456, w1=0.00019962386652138165\n",
      "SGD(1900/2000): loss=0.48281809943232623, w(0)=-0.0006217158629116671, w1=0.00020716131736855782\n",
      "SGD(1950/2000): loss=0.5080671854372798, w(0)=-0.0006312780611385714, w1=0.00021352847805164663\n",
      "SGD(2000/2000): loss=0.5051212722225311, w(0)=-0.0006408200864145556, w1=0.0002192087179551559\n",
      "SGD(50/2000): loss=0.5000802462076946, w(0)=-2.1990724906490057e-05, w1=1.6090510187800105e-05\n",
      "SGD(100/2000): loss=0.5002236483327558, w(0)=-1.9969324130842478e-05, w1=1.8736346317937352e-05\n",
      "SGD(150/2000): loss=0.4985671801879639, w(0)=-3.195188559700712e-05, w1=2.5305058328913418e-05\n",
      "SGD(200/2000): loss=0.4984146343679638, w(0)=-4.392296343802484e-05, w1=3.458526209723271e-05\n",
      "SGD(250/2000): loss=0.4984765972505082, w(0)=-6.588237823054785e-05, w1=3.698266354944256e-05\n",
      "SGD(300/2000): loss=0.4997979515674257, w(0)=-8.383062894298516e-05, w1=4.66901076001003e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(350/2000): loss=0.49802377980740137, w(0)=-9.976946632167817e-05, w1=5.3045904682887175e-05\n",
      "SGD(400/2000): loss=0.4985549066697162, w(0)=-0.00011368578373648375, w1=5.8173611147130814e-05\n",
      "SGD(450/2000): loss=0.5001220725584077, w(0)=-0.00012760792876787393, w1=6.941220978822826e-05\n",
      "SGD(500/2000): loss=0.49617258762761807, w(0)=-0.0001454994499132223, w1=7.057521877947946e-05\n",
      "SGD(550/2000): loss=0.5011661275757886, w(0)=-0.0001553972033067043, w1=7.806323806288272e-05\n",
      "SGD(600/2000): loss=0.49545429233765625, w(0)=-0.0001692830815274375, w1=8.32657790262584e-05\n",
      "SGD(650/2000): loss=0.5018283159412038, w(0)=-0.0001871594960338809, w1=8.656123624170673e-05\n",
      "SGD(700/2000): loss=0.4951778131186983, w(0)=-0.00020102646223981588, w1=8.914234176609244e-05\n",
      "SGD(750/2000): loss=0.493698591206773, w(0)=-0.00020686978903618468, w1=9.116747917620688e-05\n",
      "SGD(800/2000): loss=0.5071827756469156, w(0)=-0.00021869831090553265, w1=0.00010180492886650803\n",
      "SGD(850/2000): loss=0.4933540814078508, w(0)=-0.0002325205574984475, w1=0.00010554829969263606\n",
      "SGD(900/2000): loss=0.4935966047517324, w(0)=-0.0002563348937639334, w1=0.00011709091372540916\n",
      "SGD(950/2000): loss=0.49648284647694896, w(0)=-0.00027015326165020544, w1=0.0001220966862149023\n",
      "SGD(1000/2000): loss=0.4912207219217568, w(0)=-0.0002899641679210318, w1=0.00013207665057823103\n",
      "SGD(1050/2000): loss=0.49463684329017155, w(0)=-0.00031573256688178705, w1=0.00014084540015589034\n",
      "SGD(1100/2000): loss=0.5062785388022852, w(0)=-0.00033148217906155727, w1=0.0001477811788364738\n",
      "SGD(1150/2000): loss=0.49634207770809896, w(0)=-0.00034521761077892097, w1=0.00014823754534211618\n",
      "SGD(1200/2000): loss=0.46854348965088377, w(0)=-0.00036291323839720814, w1=0.00015161920493827116\n",
      "SGD(1250/2000): loss=0.4903855374135018, w(0)=-0.0003886258999079688, w1=0.0001628019650341983\n",
      "SGD(1300/2000): loss=0.4909250221106514, w(0)=-0.00041032605541049697, w1=0.0001653487561929071\n",
      "SGD(1350/2000): loss=0.4956406918873998, w(0)=-0.0004100371616579049, w1=0.00016404206424913925\n",
      "SGD(1400/2000): loss=0.4895433341127528, w(0)=-0.0004257084480491363, w1=0.00016823894177248446\n",
      "SGD(1450/2000): loss=0.5043260233247251, w(0)=-0.0004214734824994943, w1=0.00017484777729077659\n",
      "SGD(1500/2000): loss=0.5018663420707553, w(0)=-0.0004391117475967928, w1=0.0001790085227718372\n",
      "SGD(1550/2000): loss=0.490756763965349, w(0)=-0.00046071857378428787, w1=0.00018284594037173561\n",
      "SGD(1600/2000): loss=0.4932411066144259, w(0)=-0.00046832228184904196, w1=0.00018498593562472129\n",
      "SGD(1650/2000): loss=0.49760170777765206, w(0)=-0.0004819571161868218, w1=0.00019348473802801317\n",
      "SGD(1700/2000): loss=0.5024475998508087, w(0)=-0.0005035849362002307, w1=0.00019836187346216515\n",
      "SGD(1750/2000): loss=0.4901054805850202, w(0)=-0.0005292148027343757, w1=0.00021104289575217774\n",
      "SGD(1800/2000): loss=0.48784303525484046, w(0)=-0.000544859631998516, w1=0.00021783387738418442\n",
      "SGD(1850/2000): loss=0.5065475027271238, w(0)=-0.0005484266264011042, w1=0.0002267399971910868\n",
      "SGD(1900/2000): loss=0.4947850853667505, w(0)=-0.0005600362947219586, w1=0.00023507259717844457\n",
      "SGD(1950/2000): loss=0.4864396545816698, w(0)=-0.0005736629333965211, w1=0.00024246921380879376\n",
      "SGD(2000/2000): loss=0.4928374919480324, w(0)=-0.0005852398173770442, w1=0.00024493440799620397\n",
      "SGD(50/2000): loss=0.5002490920227982, w(0)=-1.1969295139704924e-05, w1=6.02189943436152e-06\n",
      "SGD(100/2000): loss=0.5091843199076831, w(0)=-2.587643367043747e-05, w1=1.0494610937369474e-05\n",
      "SGD(150/2000): loss=0.497776392468876, w(0)=-4.9763393527234736e-05, w1=1.5170712240104008e-05\n",
      "SGD(200/2000): loss=0.5019616204237414, w(0)=-5.957039125804292e-05, w1=1.9934171647097322e-05\n",
      "SGD(250/2000): loss=0.4943723905329528, w(0)=-6.932445387519504e-05, w1=2.576733943947653e-05\n",
      "SGD(300/2000): loss=0.49374411897831433, w(0)=-8.896899290019836e-05, w1=3.444793080631636e-05\n",
      "SGD(350/2000): loss=0.49559377553384615, w(0)=-0.00011057971617143066, w1=3.981840084006786e-05\n",
      "SGD(400/2000): loss=0.5610211038548495, w(0)=-0.00012204052221539958, w1=4.119016100847744e-05\n",
      "SGD(450/2000): loss=0.4960146161119667, w(0)=-0.00013555939140223527, w1=4.4099270614815495e-05\n",
      "SGD(500/2000): loss=0.5183642537800834, w(0)=-0.00014501503743286767, w1=5.060750431057528e-05\n",
      "SGD(550/2000): loss=0.4946482596177082, w(0)=-0.000160260774321557, w1=5.6817731194190775e-05\n",
      "SGD(600/2000): loss=0.5047707159229723, w(0)=-0.0001837242962126939, w1=6.266464272334547e-05\n",
      "SGD(650/2000): loss=0.48420391761496523, w(0)=-0.00020509544864016536, w1=6.509206670634563e-05\n",
      "SGD(700/2000): loss=0.4817532453787371, w(0)=-0.0002182398686419996, w1=7.340140692571113e-05\n",
      "SGD(750/2000): loss=0.5098101824196306, w(0)=-0.00023332968361222607, w1=7.50833838527457e-05\n",
      "SGD(800/2000): loss=0.513454589615135, w(0)=-0.00024038673758755894, w1=8.855494910441334e-05\n",
      "SGD(850/2000): loss=0.4819049180976527, w(0)=-0.0002496161584600734, w1=9.601018407225409e-05\n",
      "SGD(900/2000): loss=0.49608366560998585, w(0)=-0.0002665506733822458, w1=0.00010233479253772452\n",
      "SGD(950/2000): loss=0.5279842600245437, w(0)=-0.00028334916224430275, w1=0.00011082713764374474\n",
      "SGD(1000/2000): loss=0.47661231763299594, w(0)=-0.00030008609912687254, w1=0.00011605811435170174\n",
      "SGD(1050/2000): loss=0.5148245136092738, w(0)=-0.00032062485185414314, w1=0.00011823805449886143\n",
      "SGD(1100/2000): loss=0.376570911464382, w(0)=-0.00033532549041742805, w1=0.00013015737890836995\n",
      "SGD(1150/2000): loss=0.49164689501243664, w(0)=-0.00034435323040727436, w1=0.00013929771536897007\n",
      "SGD(1200/2000): loss=0.48343816890445573, w(0)=-0.00034894041580267636, w1=0.00013758383293822573\n",
      "SGD(1250/2000): loss=0.4702301357726219, w(0)=-0.0003754561160831443, w1=0.00014107635343046942\n",
      "SGD(1300/2000): loss=0.4491447483598692, w(0)=-0.00038994595469368467, w1=0.00014970441901986214\n",
      "SGD(1350/2000): loss=0.45818567003676114, w(0)=-0.000392673271450126, w1=0.00015503613435628814\n",
      "SGD(1400/2000): loss=0.5203126767617144, w(0)=-0.00041731667255715747, w1=0.00016488045893374095\n",
      "SGD(1450/2000): loss=0.48934357350372376, w(0)=-0.0004321331097901149, w1=0.00017051468412162293\n",
      "SGD(1500/2000): loss=0.48678753288336196, w(0)=-0.00044638633648104377, w1=0.00017323504979039077\n",
      "SGD(1550/2000): loss=0.5131086476217479, w(0)=-0.00044319266891167374, w1=0.00017733586644065006\n",
      "SGD(1600/2000): loss=0.5757559760722047, w(0)=-0.00045150791082131027, w1=0.00018212504007002206\n",
      "SGD(1650/2000): loss=0.4598233087636596, w(0)=-0.0004675457440583325, w1=0.0001899267376437276\n",
      "SGD(1700/2000): loss=0.33361451734888015, w(0)=-0.0004796409221800623, w1=0.0001972549111999468\n",
      "SGD(1750/2000): loss=0.5110306922851138, w(0)=-0.0004914920238439748, w1=0.00019885536808964213\n",
      "SGD(1800/2000): loss=0.4823767311189684, w(0)=-0.0005053887383727755, w1=0.0002104378297480741\n",
      "SGD(1850/2000): loss=0.5409231952149439, w(0)=-0.0005158158665698659, w1=0.00021896519592199624\n",
      "SGD(1900/2000): loss=0.45367619166986267, w(0)=-0.0005257912727156584, w1=0.000219545859427071\n",
      "SGD(1950/2000): loss=0.47435421425351376, w(0)=-0.0005397675352991039, w1=0.00023344408151602545\n",
      "SGD(2000/2000): loss=0.25651589605762065, w(0)=-0.0005518247067739395, w1=0.00024488312501671155\n",
      "SGD(50/2000): loss=0.5000156191100678, w(0)=-1.1989127913361543e-05, w1=-3.689668428131765e-07\n",
      "SGD(100/2000): loss=0.4991906610277399, w(0)=-2.1937692235737953e-05, w1=1.0864290242025162e-06\n",
      "SGD(150/2000): loss=0.49757493169586176, w(0)=-3.591528673392359e-05, w1=7.225062732861159e-06\n",
      "SGD(200/2000): loss=0.49995672847801714, w(0)=-3.988008379471947e-05, w1=1.1504180089085314e-05\n",
      "SGD(250/2000): loss=0.4983623893876479, w(0)=-6.360802554907823e-05, w1=1.4138656703792546e-05\n",
      "SGD(300/2000): loss=0.5002607588799717, w(0)=-7.71059561475871e-05, w1=7.669087202575038e-06\n",
      "SGD(350/2000): loss=0.503592255045969, w(0)=-8.480778035147235e-05, w1=1.566952362035152e-05\n",
      "SGD(400/2000): loss=0.5027662449275011, w(0)=-9.452333841373887e-05, w1=1.9832303655338873e-05\n",
      "SGD(450/2000): loss=0.5020923376210207, w(0)=-0.0001178887696518416, w1=3.028056821726613e-05\n",
      "SGD(500/2000): loss=0.491512563983765, w(0)=-0.00012952913406269194, w1=4.086557722145781e-05\n",
      "SGD(550/2000): loss=0.5056829370673107, w(0)=-0.00013906273267863737, w1=5.300806248854202e-05\n",
      "SGD(600/2000): loss=0.4952736151125559, w(0)=-0.00015028959491670293, w1=6.105200600056071e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(650/2000): loss=0.4936524960104771, w(0)=-0.0001698398400417495, w1=6.527983624782829e-05\n",
      "SGD(700/2000): loss=0.44446132218736895, w(0)=-0.00018862806613884303, w1=6.910166864176832e-05\n",
      "SGD(750/2000): loss=0.49566579168689695, w(0)=-0.00020851328214604537, w1=7.337558238246342e-05\n",
      "SGD(800/2000): loss=0.48530526170458227, w(0)=-0.0002240096306553932, w1=8.422143684947854e-05\n",
      "SGD(850/2000): loss=0.5077582902571349, w(0)=-0.0002405508154003204, w1=8.97949147277002e-05\n",
      "SGD(900/2000): loss=0.49644595668857816, w(0)=-0.0002622126951201993, w1=9.197231633082656e-05\n",
      "SGD(950/2000): loss=0.5040680234408969, w(0)=-0.00027122442664188315, w1=9.796916326162179e-05\n",
      "SGD(1000/2000): loss=0.4736012845469952, w(0)=-0.0002891467345921166, w1=0.00011064026626367453\n",
      "SGD(1050/2000): loss=0.5103118100864821, w(0)=-0.0003099829354390571, w1=0.00011802846187998188\n",
      "SGD(1100/2000): loss=0.46531659555814003, w(0)=-0.0003208776781153035, w1=0.0001235027509914114\n",
      "SGD(1150/2000): loss=0.5074804291614698, w(0)=-0.00034352681650745906, w1=0.0001326888556814571\n",
      "SGD(1200/2000): loss=0.45451311013731704, w(0)=-0.0003594728517577766, w1=0.00013363903583358294\n",
      "SGD(1250/2000): loss=0.48178015973440325, w(0)=-0.0003710396463003891, w1=0.00014444601383371142\n",
      "SGD(1300/2000): loss=0.5163660501151743, w(0)=-0.00038758822414528747, w1=0.0001487086896844431\n",
      "SGD(1350/2000): loss=0.4262636040717179, w(0)=-0.0003941300084055413, w1=0.00015822043249655613\n",
      "SGD(1400/2000): loss=0.45336591263052634, w(0)=-0.0003948346667506929, w1=0.0001615698113082419\n",
      "SGD(1450/2000): loss=0.5293884329060193, w(0)=-0.0004033954373554895, w1=0.00016772301735097427\n",
      "SGD(1500/2000): loss=0.4602099322554248, w(0)=-0.00040766802537297355, w1=0.00016474928913274049\n",
      "SGD(1550/2000): loss=0.46646289025054904, w(0)=-0.0004244359554462324, w1=0.00016720112673249528\n",
      "SGD(1600/2000): loss=0.48959541374331345, w(0)=-0.0004377771507634129, w1=0.00016751063509384722\n",
      "SGD(1650/2000): loss=0.5130946186755165, w(0)=-0.0004432230408721441, w1=0.00017158738126695945\n",
      "SGD(1700/2000): loss=0.5234124827310703, w(0)=-0.00045786400777454603, w1=0.00018286109802892835\n",
      "SGD(1750/2000): loss=0.4716674221254826, w(0)=-0.0004686657503731833, w1=0.00018513453397428435\n",
      "SGD(1800/2000): loss=0.49128836815813975, w(0)=-0.00048460721217163694, w1=0.00018881466421481413\n",
      "SGD(1850/2000): loss=0.40803392280891554, w(0)=-0.0005081215646650651, w1=0.00019785285017334214\n",
      "SGD(1900/2000): loss=0.48036923846506924, w(0)=-0.0005278292331695989, w1=0.00020274164320362208\n",
      "SGD(1950/2000): loss=0.4411920883151403, w(0)=-0.0005433117545533617, w1=0.00020314905918423495\n",
      "SGD(2000/2000): loss=0.4599637396124563, w(0)=-0.0005695161400823084, w1=0.00020761536455222716\n",
      "1\n",
      "0\n",
      "0.69008\n",
      "[[0.69008]\n",
      " [0.6566 ]\n",
      " [0.67148]\n",
      " [0.6566 ]\n",
      " [0.66862]]\n"
     ]
    }
   ],
   "source": [
    "method_flag = 2\n",
    "degrees = np.asarray(range(1,6))\n",
    "lambdas = np.asarray([0])\n",
    "best_degree, best_lambda, accuracy_score, accuracy_scores_grid = degree_lambda_grid_search(y,tX,[22],0.8,method_flag,degrees,lambdas)\n",
    "print(best_degree)\n",
    "print(best_lambda)\n",
    "print(accuracy_score)\n",
    "print(accuracy_scores_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "0.6566\n",
      "[[0.6566]\n",
      " [0.3434]\n",
      " [0.3434]\n",
      " [0.3434]\n",
      " [0.6566]\n",
      " [0.6566]\n",
      " [0.6566]\n",
      " [0.3434]\n",
      " [0.3434]\n",
      " [0.6566]]\n"
     ]
    }
   ],
   "source": [
    "method_flag = 3\n",
    "degrees = np.asarray(range(1,11))\n",
    "lambdas = np.asarray([0])\n",
    "best_degree, best_lambda, accuracy_score, accuracy_scores_grid = degree_lambda_grid_search(y,tX,[22],0.8,method_flag,degrees,lambdas)\n",
    "print(best_degree)\n",
    "print(best_lambda)\n",
    "print(accuracy_score)\n",
    "print(accuracy_scores_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1e-05\n",
      "0.81796\n",
      "[[0.7472  0.7472  0.7472  0.7472  0.7472  0.7472  0.74716 0.74722 0.74524\n",
      "  0.7368  0.7141 ]\n",
      " [0.7976  0.7976  0.7976  0.7976  0.7976  0.79758 0.79764 0.79778 0.79572\n",
      "  0.78564 0.7342 ]\n",
      " [0.81312 0.81312 0.81312 0.81312 0.81316 0.81308 0.81294 0.81306 0.80954\n",
      "  0.79208 0.74434]\n",
      " [0.81652 0.81652 0.81652 0.81652 0.8166  0.81652 0.8163  0.8158  0.8109\n",
      "  0.79326 0.75144]\n",
      " [0.81682 0.81682 0.8168  0.81688 0.817   0.81688 0.81668 0.81608 0.81152\n",
      "  0.7954  0.75406]\n",
      " [0.8169  0.8169  0.8169  0.8169  0.817   0.817   0.81724 0.81592 0.81226\n",
      "  0.79798 0.757  ]\n",
      " [0.81764 0.81766 0.81772 0.81776 0.8177  0.81796 0.8177  0.8162  0.81238\n",
      "  0.7986  0.75744]\n",
      " [0.81658 0.8166  0.81656 0.8165  0.8172  0.81764 0.81724 0.81626 0.81274\n",
      "  0.79916 0.75818]\n",
      " [0.81654 0.81656 0.81668 0.81678 0.81752 0.81786 0.81766 0.81642 0.81342\n",
      "  0.8     0.75708]]\n"
     ]
    }
   ],
   "source": [
    "method_flag = 4\n",
    "degrees = np.asarray(range(1,10))\n",
    "lambdas = np.logspace(-10,0,11)\n",
    "best_degree, best_lambda, accuracy_score, accuracy_scores_grid = degree_lambda_grid_search(y,tX,[22],0.8,method_flag,degrees,lambdas)\n",
    "print(best_degree)\n",
    "print(best_lambda)\n",
    "print(accuracy_score)\n",
    "print(accuracy_scores_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 103384.51325763622\n",
      "#Iteration: 500, Loss: 102473.3534760032\n",
      "#Iteration: 750, Loss: 101989.73481291284\n",
      "#Iteration: 1000, Loss: 101711.39709745989\n",
      "#Iteration: 1250, Loss: 101545.3295208861\n",
      "#Iteration: 1500, Loss: 101443.84265658101\n",
      "#Iteration: 1750, Loss: 101380.64654471242\n",
      "#Iteration: 2000, Loss: 101340.64371542702\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 86557.5528287275\n",
      "#Iteration: 500, Loss: 85379.48974339946\n",
      "#Iteration: 750, Loss: 84967.39846623022\n",
      "#Iteration: 1000, Loss: 84743.50226234547\n",
      "#Iteration: 1250, Loss: 84598.95296357662\n",
      "#Iteration: 1500, Loss: 84498.19457693736\n",
      "#Iteration: 1750, Loss: 84425.13566641122\n",
      "#Iteration: 2000, Loss: 84370.88533114741\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 85004.81546502894\n",
      "#Iteration: 500, Loss: 83617.21034856312\n",
      "#Iteration: 750, Loss: 83093.430842894\n",
      "#Iteration: 1000, Loss: 82801.74686093649\n",
      "#Iteration: 1250, Loss: 82609.61905087414\n",
      "#Iteration: 1500, Loss: 82470.81708593006\n",
      "#Iteration: 1750, Loss: 82364.77216959509\n",
      "#Iteration: 2000, Loss: 82280.78515168035\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 103384.51325763622\n",
      "#Iteration: 500, Loss: 102473.3534760032\n",
      "#Iteration: 750, Loss: 101989.73481291284\n",
      "#Iteration: 1000, Loss: 101711.39709745989\n",
      "#Iteration: 1250, Loss: 101545.3295208861\n",
      "#Iteration: 1500, Loss: 101443.84265658101\n",
      "#Iteration: 1750, Loss: 101380.64654471242\n",
      "#Iteration: 2000, Loss: 101340.64371542702\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 86557.5528287275\n",
      "#Iteration: 500, Loss: 85379.48974339946\n",
      "#Iteration: 750, Loss: 84967.39846623022\n",
      "#Iteration: 1000, Loss: 84743.50226234547\n",
      "#Iteration: 1250, Loss: 84598.95296357662\n",
      "#Iteration: 1500, Loss: 84498.19457693736\n",
      "#Iteration: 1750, Loss: 84425.13566641122\n",
      "#Iteration: 2000, Loss: 84370.88533114741\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 85004.81546502894\n",
      "#Iteration: 500, Loss: 83617.21034856312\n",
      "#Iteration: 750, Loss: 83093.430842894\n",
      "#Iteration: 1000, Loss: 82801.74686093649\n",
      "#Iteration: 1250, Loss: 82609.61905087414\n",
      "#Iteration: 1500, Loss: 82470.81708593006\n",
      "#Iteration: 1750, Loss: 82364.77216959509\n",
      "#Iteration: 2000, Loss: 82280.78515168035\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 103384.51325763622\n",
      "#Iteration: 500, Loss: 102473.3534760032\n",
      "#Iteration: 750, Loss: 101989.73481291284\n",
      "#Iteration: 1000, Loss: 101711.39709745989\n",
      "#Iteration: 1250, Loss: 101545.3295208861\n",
      "#Iteration: 1500, Loss: 101443.84265658101\n",
      "#Iteration: 1750, Loss: 101380.64654471242\n",
      "#Iteration: 2000, Loss: 101340.64371542702\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 86557.5528287275\n",
      "#Iteration: 500, Loss: 85379.48974339946\n",
      "#Iteration: 750, Loss: 84967.39846623022\n",
      "#Iteration: 1000, Loss: 84743.50226234547\n",
      "#Iteration: 1250, Loss: 84598.95296357662\n",
      "#Iteration: 1500, Loss: 84498.19457693736\n",
      "#Iteration: 1750, Loss: 84425.13566641122\n",
      "#Iteration: 2000, Loss: 84370.88533114741\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 85004.81546502894\n",
      "#Iteration: 500, Loss: 83617.21034856312\n",
      "#Iteration: 750, Loss: 83093.430842894\n",
      "#Iteration: 1000, Loss: 82801.74686093649\n",
      "#Iteration: 1250, Loss: 82609.61905087414\n",
      "#Iteration: 1500, Loss: 82470.81708593006\n",
      "#Iteration: 1750, Loss: 82364.77216959509\n",
      "#Iteration: 2000, Loss: 82280.78515168035\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 103384.51325763622\n",
      "#Iteration: 500, Loss: 102473.3534760032\n",
      "#Iteration: 750, Loss: 101989.73481291284\n",
      "#Iteration: 1000, Loss: 101711.39709745989\n",
      "#Iteration: 1250, Loss: 101545.3295208861\n",
      "#Iteration: 1500, Loss: 101443.84265658101\n",
      "#Iteration: 1750, Loss: 101380.64654471242\n",
      "#Iteration: 2000, Loss: 101340.64371542702\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 86557.5528287275\n",
      "#Iteration: 500, Loss: 85379.48974339946\n",
      "#Iteration: 750, Loss: 84967.39846623022\n",
      "#Iteration: 1000, Loss: 84743.50226234547\n",
      "#Iteration: 1250, Loss: 84598.95296357662\n",
      "#Iteration: 1500, Loss: 84498.19457693736\n",
      "#Iteration: 1750, Loss: 84425.13566641122\n",
      "#Iteration: 2000, Loss: 84370.88533114741\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 85004.81546502894\n",
      "#Iteration: 500, Loss: 83617.21034856312\n",
      "#Iteration: 750, Loss: 83093.430842894\n",
      "#Iteration: 1000, Loss: 82801.74686093649\n",
      "#Iteration: 1250, Loss: 82609.61905087414\n",
      "#Iteration: 1500, Loss: 82470.81708593006\n",
      "#Iteration: 1750, Loss: 82364.77216959509\n",
      "#Iteration: 2000, Loss: 82280.78515168035\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 103384.51325763622\n",
      "#Iteration: 500, Loss: 102473.3534760032\n",
      "#Iteration: 750, Loss: 101989.73481291284\n",
      "#Iteration: 1000, Loss: 101711.39709745989\n",
      "#Iteration: 1250, Loss: 101545.3295208861\n",
      "#Iteration: 1500, Loss: 101443.84265658101\n",
      "#Iteration: 1750, Loss: 101380.64654471242\n",
      "#Iteration: 2000, Loss: 101340.64371542702\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 86557.5528287275\n",
      "#Iteration: 500, Loss: 85379.48974339946\n",
      "#Iteration: 750, Loss: 84967.39846623022\n",
      "#Iteration: 1000, Loss: 84743.50226234547\n",
      "#Iteration: 1250, Loss: 84598.95296357662\n",
      "#Iteration: 1500, Loss: 84498.19457693736\n",
      "#Iteration: 1750, Loss: 84425.13566641122\n",
      "#Iteration: 2000, Loss: 84370.88533114741\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 85004.81546502894\n",
      "#Iteration: 500, Loss: 83617.21034856312\n",
      "#Iteration: 750, Loss: 83093.430842894\n",
      "#Iteration: 1000, Loss: 82801.74686093649\n",
      "#Iteration: 1250, Loss: 82609.61905087414\n",
      "#Iteration: 1500, Loss: 82470.81708593006\n",
      "#Iteration: 1750, Loss: 82364.77216959509\n",
      "#Iteration: 2000, Loss: 82280.78515168035\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 103384.51325763622\n",
      "#Iteration: 500, Loss: 102473.3534760032\n",
      "#Iteration: 750, Loss: 101989.73481291284\n",
      "#Iteration: 1000, Loss: 101711.39709745989\n",
      "#Iteration: 1250, Loss: 101545.3295208861\n",
      "#Iteration: 1500, Loss: 101443.84265658101\n",
      "#Iteration: 1750, Loss: 101380.64654471242\n",
      "#Iteration: 2000, Loss: 101340.64371542702\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 86557.5528287275\n",
      "#Iteration: 500, Loss: 85379.48974339946\n",
      "#Iteration: 750, Loss: 84967.39846623022\n",
      "#Iteration: 1000, Loss: 84743.50226234547\n",
      "#Iteration: 1250, Loss: 84598.95296357662\n",
      "#Iteration: 1500, Loss: 84498.19457693736\n",
      "#Iteration: 1750, Loss: 84425.13566641122\n",
      "#Iteration: 2000, Loss: 84370.88533114741\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 85004.81546502894\n",
      "#Iteration: 500, Loss: 83617.21034856312\n",
      "#Iteration: 750, Loss: 83093.430842894\n",
      "#Iteration: 1000, Loss: 82801.74686093649\n",
      "#Iteration: 1250, Loss: 82609.61905087414\n",
      "#Iteration: 1500, Loss: 82470.81708593006\n",
      "#Iteration: 1750, Loss: 82364.77216959509\n",
      "#Iteration: 2000, Loss: 82280.78515168035\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 103384.51325763622\n",
      "#Iteration: 500, Loss: 102473.3534760032\n",
      "#Iteration: 750, Loss: 101989.73481291284\n",
      "#Iteration: 1000, Loss: 101711.39709745989\n",
      "#Iteration: 1250, Loss: 101545.3295208861\n",
      "#Iteration: 1500, Loss: 101443.84265658101\n",
      "#Iteration: 1750, Loss: 101380.64654471242\n",
      "#Iteration: 2000, Loss: 101340.64371542702\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 86557.5528287275\n",
      "#Iteration: 500, Loss: 85379.48974339946\n",
      "#Iteration: 750, Loss: 84967.39846623022\n",
      "#Iteration: 1000, Loss: 84743.50226234547\n",
      "#Iteration: 1250, Loss: 84598.95296357662\n",
      "#Iteration: 1500, Loss: 84498.19457693736\n",
      "#Iteration: 1750, Loss: 84425.13566641122\n",
      "#Iteration: 2000, Loss: 84370.88533114741\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 85004.81546502894\n",
      "#Iteration: 500, Loss: 83617.21034856312\n",
      "#Iteration: 750, Loss: 83093.430842894\n",
      "#Iteration: 1000, Loss: 82801.74686093649\n",
      "#Iteration: 1250, Loss: 82609.61905087414\n",
      "#Iteration: 1500, Loss: 82470.81708593006\n",
      "#Iteration: 1750, Loss: 82364.77216959509\n",
      "#Iteration: 2000, Loss: 82280.78515168035\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 103384.51325763622\n",
      "#Iteration: 500, Loss: 102473.3534760032\n",
      "#Iteration: 750, Loss: 101989.73481291284\n",
      "#Iteration: 1000, Loss: 101711.39709745989\n",
      "#Iteration: 1250, Loss: 101545.3295208861\n",
      "#Iteration: 1500, Loss: 101443.84265658101\n",
      "#Iteration: 1750, Loss: 101380.64654471242\n",
      "#Iteration: 2000, Loss: 101340.64371542702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 86557.5528287275\n",
      "#Iteration: 500, Loss: 85379.48974339946\n",
      "#Iteration: 750, Loss: 84967.39846623022\n",
      "#Iteration: 1000, Loss: 84743.50226234547\n",
      "#Iteration: 1250, Loss: 84598.95296357662\n",
      "#Iteration: 1500, Loss: 84498.19457693736\n",
      "#Iteration: 1750, Loss: 84425.13566641122\n",
      "#Iteration: 2000, Loss: 84370.88533114741\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 85004.81546502894\n",
      "#Iteration: 500, Loss: 83617.21034856312\n",
      "#Iteration: 750, Loss: 83093.430842894\n",
      "#Iteration: 1000, Loss: 82801.74686093649\n",
      "#Iteration: 1250, Loss: 82609.61905087414\n",
      "#Iteration: 1500, Loss: 82470.81708593006\n",
      "#Iteration: 1750, Loss: 82364.77216959509\n",
      "#Iteration: 2000, Loss: 82280.78515168035\n",
      "3\n",
      "1e-07\n",
      "0.81456\n",
      "[[0.74986 0.74986 0.74986 0.74986 0.74986 0.74986 0.74986 0.74986]\n",
      " [0.81092 0.81092 0.81092 0.81092 0.81092 0.81092 0.81092 0.81092]\n",
      " [0.81456 0.81456 0.81456 0.81456 0.81456 0.81456 0.81456 0.81456]]\n"
     ]
    }
   ],
   "source": [
    "method_flag = 5\n",
    "degrees = np.asarray(range(1,4))\n",
    "lambdas = np.logspace(-7,0,8)\n",
    "best_degree, best_lambda, accuracy_score, accuracy_scores_grid = degree_lambda_grid_search(y,tX,[22],0.8,method_flag,degrees,lambdas)\n",
    "print(best_degree)\n",
    "print(best_lambda)\n",
    "print(accuracy_score)\n",
    "print(accuracy_scores_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 103384.51325763622\n",
      "#Iteration: 500, Loss: 102473.3534760032\n",
      "#Iteration: 750, Loss: 101989.73481291284\n",
      "#Iteration: 1000, Loss: 101711.39709745989\n",
      "#Iteration: 1250, Loss: 101545.3295208861\n",
      "#Iteration: 1500, Loss: 101443.84265658101\n",
      "#Iteration: 1750, Loss: 101380.64654471242\n",
      "#Iteration: 2000, Loss: 101340.64371542702\n",
      "#Iteration: 0, Loss: 138629.4361119856\n",
      "#Iteration: 250, Loss: 86557.5528287275\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-460c2fd0c2f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdegrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlambdas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbest_degree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_scores_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdegree_lambda_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod_flag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_degree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_lambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Docs_Adrian/EPFL/Courses/Machine_learning/ML_2019_EPFL/projects/project1/scripts/proj1_cross_validation.py\u001b[0m in \u001b[0;36mdegree_lambda_grid_search\u001b[0;34m(y, x, cat_cols, ratio_train, method_flag, degrees, lambdas, gamma, max_iters, seed)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;31m# Logistic regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelabel_y_non_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mw_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_initial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmethod_flag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Docs_Adrian/EPFL/Courses/Machine_learning/ML_2019_EPFL/projects/project1/scripts/implementations.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(y, tx, w_initial, max_iters, gamma)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_initial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_by_gradient_descent_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;31m# print average loss for the last print_every iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Docs_Adrian/EPFL/Courses/Machine_learning/ML_2019_EPFL/projects/project1/scripts/implementations_utils.py\u001b[0m in \u001b[0;36mlearning_by_gradient_descent_log\u001b[0;34m(y, tx, w, gamma)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_loss_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_gradient_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Docs_Adrian/EPFL/Courses/Machine_learning/ML_2019_EPFL/projects/project1/scripts/implementations_utils.py\u001b[0m in \u001b[0;36mcalculate_gradient_log\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "method_flag = 5\n",
    "degrees = np.asarray(range(1,4))\n",
    "lambdas = np.logspace(-7,0,8)\n",
    "best_degree, best_lambda, accuracy_score, accuracy_scores_grid = degree_lambda_grid_search(y,tX,[22],0.8,method_flag,degrees,lambdas)\n",
    "print(best_degree)\n",
    "print(best_lambda)\n",
    "print(accuracy_score)\n",
    "print(accuracy_scores_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lambda = 1e-5\n",
    "lambda_ = best_lambda\n",
    "weights, loss = ridge_regression(y_train,x_train,lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = least_squares(y_train,x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test crossvalidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD(51/2000): loss=0.498008967536016, w0=-3.428704855974191e-05, w1=1.2542584521976463e-05\n",
      "GD(101/2000): loss=0.4961789938662344, w0=-6.817374129066865e-05, w1=2.5114613293158463e-05\n",
      "GD(151/2000): loss=0.494488630926867, w0=-0.00010167693092135272, w1=3.771292478816873e-05\n",
      "GD(201/2000): loss=0.4929194976044192, w0=-0.00013481234907622975, w1=5.0334568769426166e-05\n",
      "GD(251/2000): loss=0.4914558345624867, w0=-0.0001675946883977117, w1=6.2976791681685e-05\n",
      "GD(301/2000): loss=0.49008412401576157, w0=-0.00020003767849343835, w1=7.563702309685602e-05\n",
      "GD(351/2000): loss=0.48879276495615576, w0=-0.00023215415617703437, w1=8.831286313174699e-05\n",
      "GD(401/2000): loss=0.4875717957253721, w0=-0.0002639561304351158, w1=0.0001010020707671358\n",
      "GD(451/2000): loss=0.48641265701427655, w0=-0.00029545484252036197, w1=0.0001137025530019339\n",
      "GD(501/2000): loss=0.48530798938182923, w0=-0.0003266608215400682, w1=0.00012641235478113563\n",
      "GD(551/2000): loss=0.4842514602505461, w0=-0.0003575839358814899, w1=0.00013912964964082073\n",
      "GD(601/2000): loss=0.4832376160731641, w0=-0.00038823344078933535, w1=0.0001518527310176998\n",
      "GD(651/2000): loss=0.48226175599492704, w0=-0.0004186180223867866, w1=0.00016458000417460242\n",
      "GD(701/2000): loss=0.48131982387345806, w0=-0.0004487458384092659, w1=0.00017730997869692104\n",
      "GD(751/2000): loss=0.48040831597708883, w0=-0.0004786245558997047, w1=0.00019004126151836797\n",
      "GD(801/2000): loss=0.479524202074234, w0=-0.0005082613860951545, w1=0.0002027725504374915\n",
      "GD(851/2000): loss=0.47866485796081437, w0=-0.0005376631167171172, w1=0.00021550262808926228\n",
      "GD(901/2000): loss=0.47782800775820233, w0=-0.0005668361418618231, w1=0.00022823035633868228\n",
      "GD(951/2000): loss=0.4770116745578695, w0=-0.0005957864896717794, w1=0.00024095467106581884\n",
      "GD(1001/2000): loss=0.47621413819695935, w0=-0.0006245198479561325, w1=0.0002536745773139288\n",
      "GD(1051/2000): loss=0.4754338991266297, w0=-0.0006530415879146609, w1=0.0002663891447744337\n",
      "GD(1101/2000): loss=0.4746696474866291, w0=-0.000681356786108453, w1=0.00027909750358444423\n",
      "GD(1151/2000): loss=0.4739202366290366, w0=-0.0007094702448094658, w1=0.0002917988404143238\n",
      "GD(1201/2000): loss=0.4731846604446012, w0=-0.0007373865108511169, w1=0.0003044923948244433\n",
      "GD(1251/2000): loss=0.47246203393949077, w0=-0.0007651098930927917, w1=0.0003171774558718101\n",
      "GD(1301/2000): loss=0.47175157659080846, w0=-0.0007926444786025825, w1=0.00032985335894867895\n",
      "GD(1351/2000): loss=0.4710525980780313, w0=-0.0008199941476546545, w1=0.0003425194828365641\n",
      "GD(1401/2000): loss=0.47036448604624914, w0=-0.0008471625876303248, w1=0.00035517524696028897\n",
      "GD(1451/2000): loss=0.4696866956072312, w0=-0.000874153305905184, w1=0.0003678201088278362\n",
      "GD(1501/2000): loss=0.4690187403271699, w0=-0.0009009696417983436, w1=0.000380453561642804\n",
      "GD(1551/2000): loss=0.4683601844865141, w0=-0.0009276147776541267, w1=0.00039307513207723587\n",
      "GD(1601/2000): loss=0.46771063642852995, w0=-0.0009540917491211915, w1=0.0004056843781934883\n",
      "GD(1651/2000): loss=0.46706974283988933, w0=-0.000980403454689149, w1=0.000418280887504624\n",
      "GD(1701/2000): loss=0.4664371838293627, w0=-0.0010065526645381957, w1=0.0004308642751635835\n",
      "GD(1751/2000): loss=0.46581266869013915, w0=-0.0010325420287530642, w1=0.0004434341822721\n",
      "GD(1801/2000): loss=0.46519593224791, w0=-0.001058374084948731, w1=0.00045599027430097525\n",
      "GD(1851/2000): loss=0.46458673171103715, w0=-0.0010840512653517138, w1=0.00046853223961393955\n",
      "GD(1901/2000): loss=0.4639848439512559, w0=-0.0011095759033774798, w1=0.0004810597880878894\n",
      "GD(1951/2000): loss=0.4633900631537063, w0=-0.0011349502397414342, w1=0.0004935726498228078\n",
      "GD(2001/2000): loss=0.46280219878394036, w0=-0.00116017642813812, w1=0.0005060705739351594\n",
      "GD(51/2000): loss=0.49802219216206556, w0=-3.4529973487376946e-05, w1=1.2606556830168696e-05\n",
      "GD(101/2000): loss=0.4961962040701439, w0=-6.864894483250921e-05, w1=2.5235181520065487e-05\n",
      "GD(151/2000): loss=0.49450254205150945, w0=-0.00010237385370646925, w1=3.788319136289729e-05\n",
      "GD(201/2000): loss=0.4929244289421242, w0=-0.0001357205546253391, w1=5.054807982194765e-05\n",
      "GD(251/2000): loss=0.49144741824077565, w0=-0.00016870389394012456, w1=6.322750459505658e-05\n",
      "GD(301/2000): loss=0.4900590634454973, w0=-0.00020133778118821595, w1=7.591927652197337e-05\n",
      "GD(351/2000): loss=0.4887486346574632, w0=-0.00023363525522436258, w1=8.862134927356708e-05\n",
      "GD(401/2000): loss=0.48750687567421036, w0=-0.0002656085455179987, w1=0.00010133180976635767\n",
      "GD(451/2000): loss=0.48632579576737356, w0=-0.0002972691289749514, w1=0.0001140488692499756\n",
      "GD(501/2000): loss=0.48519849117358665, w0=-0.00032862778261489955, w1=0.0001267708550189984\n",
      "GD(551/2000): loss=0.484118992040936, w0=-0.00035969463241128846, w1=0.00013949620270416814\n",
      "GD(601/2000): loss=0.4830821311845196, w0=-0.0003904791985775697, w1=0.00015222344910128816\n",
      "GD(651/2000): loss=0.4820834315280527, w0=-0.0004209904375625131, w1=0.0001649512254991462\n",
      "GD(701/2000): loss=0.4811190095566408, w0=-0.0004512367809977909, w1=0.00017767825147063844\n",
      "GD(751/2000): loss=0.4801854924896503, w0=-0.00048122617182293166, w1=0.00019040332909388348\n",
      "GD(801/2000): loss=0.47927994721129336, w0=-0.0005109660977960118, w1=0.0002031253375725414\n",
      "GD(851/2000): loss=0.4783998192780071, w0=-0.0005404636225829477, w1=0.00021584322822679644\n",
      "GD(901/2000): loss=0.4775428805627628, w0=-0.0005697254146039206, w1=0.00022855601982854295\n",
      "GD(951/2000): loss=0.4767071843028754, w0=-0.0005987577738021905, w1=0.00024126279425624103\n",
      "GD(1001/2000): loss=0.47589102649467857, w0=-0.0006275666564882835, w1=0.0002539626924466907\n",
      "GD(1051/2000): loss=0.4750929127298364, w0=-0.0006561576984011623, w1=0.0002666549106226314\n",
      "GD(1101/2000): loss=0.474311529697753, w0=-0.0006845362361174807, w1=0.0002793386967766021\n",
      "GD(1151/2000): loss=0.47354572068958994, w0=-0.0007127073269302859, w1=0.00029201334739291634\n",
      "GD(1201/2000): loss=0.4727944645345368, w0=-0.0007406757673095227, w1=0.00030467820439092526\n",
      "GD(1251/2000): loss=0.47205685748045506, w0=-0.0007684461100483631, w1=0.00031733265227395645\n",
      "GD(1301/2000): loss=0.4713320976007951, w0=-0.0007960226801916606, w1=0.0003299761154694482\n",
      "GD(1351/2000): loss=0.4706194713694727, w0=-0.000823409589835696, w1=0.00034260805584684196\n",
      "GD(1401/2000): loss=0.46991834209658867, w0=-0.0008506107518817678, w1=0.0003552279704007702\n",
      "GD(1451/2000): loss=0.46922813996174256, w0=-0.0008776298928200619, w1=0.0003678353890879714\n",
      "GD(1501/2000): loss=0.46854835341926265, w0=-0.000904470564614589, w1=0.0003804298728071981\n",
      "GD(1551/2000): loss=0.4678785217818719, w0=-0.0009311361557547092, w1=0.00039301101151215486\n",
      "GD(1601/2000): loss=0.4672182288168815, w0=-0.0009576299015339463, w1=0.0004055784224482235\n",
      "GD(1651/2000): loss=0.46656709721263906, w0=-0.0009839548936122896, w1=0.0004181317485043875\n",
      "GD(1701/2000): loss=0.4659247837932023, w0=-0.0010101140889140164, w1=0.00043067065667239075\n",
      "GD(1751/2000): loss=0.4652909753765601, w0=-0.001036110317909253, w1=0.00044319483660572973\n",
      "GD(1801/2000): loss=0.4646653851865866, w0=-0.001061946292323897, w1=0.00045570399927161105\n",
      "GD(1851/2000): loss=0.4640477497416615, w0=-0.001087624612319264, w1=0.0004681978756894924\n",
      "GD(1901/2000): loss=0.4634378261538049, w0=-0.0011131477731797397, w1=0.0004806762157502844\n",
      "GD(1951/2000): loss=0.46283538978153893, w0=-0.0011385181715439163, w1=0.0004931387871107051\n",
      "GD(2001/2000): loss=0.46224023218771254, w0=-0.0011637381112120822, w1=0.0005055853741576749\n",
      "[0.4628022  0.46224023]\n",
      "[0.96214375 0.96150278]\n",
      "[0.675592 0.675296]\n"
     ]
    }
   ],
   "source": [
    "method_flag = 1\n",
    "k_fold = 2\n",
    "degree = 5\n",
    "lambda_ = 1e-5\n",
    "gamma = 2.2e-6\n",
    "max_iters = 2000 \n",
    "losses_tr, losses_va, accuracy_scores = k_fold_cross_validation(y,tX,cat_cols,method_flag,k_fold,degree,lambda_,gamma,max_iters)\n",
    "print(losses_tr)\n",
    "print(losses_va)\n",
    "print(accuracy_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(50/2000): loss=0.501039671462663, w(0)=-2.1927381545026525e-05, w1=1.1068327802511765e-05\n",
      "SGD(100/2000): loss=0.49934926696556037, w(0)=-4.782160013950155e-05, w1=1.0102490916102534e-05\n",
      "SGD(150/2000): loss=0.49776999596434274, w(0)=-0.00010885720708351211, w1=1.5846895104484017e-05\n",
      "SGD(200/2000): loss=0.48007379750805756, w(0)=-0.00014597839527303615, w1=3.1917279663436526e-05\n",
      "SGD(250/2000): loss=0.4804912998071614, w(0)=-0.0002004454026782483, w1=3.7644390118481506e-05\n",
      "SGD(300/2000): loss=0.46663531940065395, w(0)=-0.0002462849903179082, w1=5.550906803256989e-05\n",
      "SGD(350/2000): loss=0.3395710925211699, w(0)=-0.0002955683543515372, w1=7.845587927932728e-05\n",
      "SGD(400/2000): loss=0.4815800425147755, w(0)=-0.0003224920181258003, w1=9.381807635984358e-05\n",
      "SGD(450/2000): loss=0.5276251490783027, w(0)=-0.00033795534764145027, w1=0.00011119067494515082\n",
      "SGD(500/2000): loss=0.4718079376536416, w(0)=-0.0004056496048556763, w1=0.00013378197232042892\n",
      "SGD(550/2000): loss=0.4809293228126758, w(0)=-0.00044041504653636053, w1=0.0001633004582411852\n",
      "SGD(600/2000): loss=0.5100223000293002, w(0)=-0.0004710470739812568, w1=0.0001788635841341258\n",
      "SGD(650/2000): loss=0.47414906455983113, w(0)=-0.0004753972222490671, w1=0.00019239606235729197\n",
      "SGD(700/2000): loss=0.5150511745776241, w(0)=-0.0005146187758329804, w1=0.00020749193472116627\n",
      "SGD(750/2000): loss=0.5114763538515504, w(0)=-0.0005534423277401455, w1=0.00021805404796662836\n",
      "SGD(800/2000): loss=0.5256648091595585, w(0)=-0.0005919953276482971, w1=0.00022615047530681352\n",
      "SGD(850/2000): loss=0.46312507651543416, w(0)=-0.0006125372034759673, w1=0.0002306963046073592\n",
      "SGD(900/2000): loss=0.4695314363851423, w(0)=-0.000661454368469456, w1=0.00024794408942907157\n",
      "SGD(950/2000): loss=0.5864424771775055, w(0)=-0.0007141798083323031, w1=0.00026544025255183993\n",
      "SGD(1000/2000): loss=0.5220294497727287, w(0)=-0.0007317860629065156, w1=0.00028887444880949255\n",
      "SGD(1050/2000): loss=0.4271284387949021, w(0)=-0.0007502684322783009, w1=0.00030119698573959975\n",
      "SGD(1100/2000): loss=0.4893128571861497, w(0)=-0.0007786913165648737, w1=0.0003094484173988442\n",
      "SGD(1150/2000): loss=0.5664932397827587, w(0)=-0.000802104737292623, w1=0.000337420315577236\n",
      "SGD(1200/2000): loss=0.36473646598454906, w(0)=-0.0008322190849466642, w1=0.00036601718784783733\n",
      "SGD(1250/2000): loss=0.469245735199732, w(0)=-0.0008704231634889501, w1=0.000377230038141852\n",
      "SGD(1300/2000): loss=0.46945450492126256, w(0)=-0.0009002336976391054, w1=0.00038484066142869297\n",
      "SGD(1350/2000): loss=0.4243334627703357, w(0)=-0.000924471748646001, w1=0.0004034195897584889\n",
      "SGD(1400/2000): loss=0.4056421402587027, w(0)=-0.0009774205338113227, w1=0.00042266155007570396\n",
      "SGD(1450/2000): loss=0.09034087310161616, w(0)=-0.0009780637869174886, w1=0.0004553691176946126\n",
      "SGD(1500/2000): loss=0.7438629231756454, w(0)=-0.0009880775624953463, w1=0.0004749941132440535\n",
      "SGD(1550/2000): loss=0.5228217515980359, w(0)=-0.0010140861960173156, w1=0.00048120275654976903\n",
      "SGD(1600/2000): loss=0.36197299899206065, w(0)=-0.0010340806184497302, w1=0.00048695551607615746\n",
      "SGD(1650/2000): loss=0.36197243275607704, w(0)=-0.0010518050453863159, w1=0.000493009806692758\n",
      "SGD(1700/2000): loss=0.24900788473729157, w(0)=-0.0010660312012780563, w1=0.0005024210784316032\n",
      "SGD(1750/2000): loss=0.39686263819703865, w(0)=-0.0010841216279210684, w1=0.0005061242244667138\n",
      "SGD(1800/2000): loss=0.4096250422157787, w(0)=-0.0010969040395573959, w1=0.0005171641923084966\n",
      "SGD(1850/2000): loss=0.5012206737780857, w(0)=-0.0011134079813565356, w1=0.0005246166438984276\n",
      "SGD(1900/2000): loss=0.5803908657863962, w(0)=-0.0011425949430443798, w1=0.0005305060740575669\n",
      "SGD(1950/2000): loss=0.5147019858651508, w(0)=-0.0011748917431366327, w1=0.0005251958796145883\n",
      "SGD(2000/2000): loss=0.4952750418785568, w(0)=-0.001192701428395058, w1=0.0005406520954827512\n",
      "SGD(50/2000): loss=0.49622846682689814, w(0)=-5.705645027182261e-05, w1=1.4430344508235711e-05\n",
      "SGD(100/2000): loss=0.5000038990445992, w(0)=-7.416100294184826e-05, w1=2.7751288377512102e-05\n",
      "SGD(150/2000): loss=0.4984893735441652, w(0)=-0.00011718869906824514, w1=4.537401013140371e-05\n",
      "SGD(200/2000): loss=0.5134425005084625, w(0)=-0.0001335655581863885, w1=6.191063054406054e-05\n",
      "SGD(250/2000): loss=0.49604283851253994, w(0)=-0.00017588546479947918, w1=7.614607057085654e-05\n",
      "SGD(300/2000): loss=0.4809057370612761, w(0)=-0.0002009742120331634, w1=9.104266830741398e-05\n",
      "SGD(350/2000): loss=0.49269225874402245, w(0)=-0.00024766023662878734, w1=0.00010548709831999028\n",
      "SGD(400/2000): loss=0.4962844474891147, w(0)=-0.00028089764691261224, w1=0.0001243583700495549\n",
      "SGD(450/2000): loss=0.4778929969076544, w(0)=-0.0003390897387338073, w1=0.00012874203397477953\n",
      "SGD(500/2000): loss=0.5072237304933443, w(0)=-0.0003801635613655709, w1=0.00014833172424376611\n",
      "SGD(550/2000): loss=0.27985406213055464, w(0)=-0.0004198050333102156, w1=0.00015592244142612896\n",
      "SGD(600/2000): loss=0.4857726846913521, w(0)=-0.0004427989744676444, w1=0.00017433579871531884\n",
      "SGD(650/2000): loss=0.35989412712317026, w(0)=-0.00047917046527169575, w1=0.00017905389169850557\n",
      "SGD(700/2000): loss=0.4790008243757417, w(0)=-0.0005543464204443437, w1=0.00020065902903088398\n",
      "SGD(750/2000): loss=0.4776920803829293, w(0)=-0.0005870529610871194, w1=0.0002131337157440471\n",
      "SGD(800/2000): loss=0.4542594457489679, w(0)=-0.0006075092297366543, w1=0.00022334292986154593\n",
      "SGD(850/2000): loss=0.437447673460792, w(0)=-0.0006638005287930183, w1=0.0002523430331191124\n",
      "SGD(900/2000): loss=0.33763340827596205, w(0)=-0.0006929715967537413, w1=0.0002646547422888963\n",
      "SGD(950/2000): loss=0.4682958727490398, w(0)=-0.0007495038269313104, w1=0.00027437436611645787\n",
      "SGD(1000/2000): loss=0.44886918262481423, w(0)=-0.0007807742447608363, w1=0.0002848705507318339\n",
      "SGD(1050/2000): loss=0.4033302820609689, w(0)=-0.0008131923861578055, w1=0.0002985697081218329\n",
      "SGD(1100/2000): loss=0.43366916537349204, w(0)=-0.0008373508067787776, w1=0.00031864779233130064\n",
      "SGD(1150/2000): loss=0.45239233061972567, w(0)=-0.0008738697893241662, w1=0.0003177203572054315\n",
      "SGD(1200/2000): loss=0.5104746304878399, w(0)=-0.0008662821072469353, w1=0.0003248529898289963\n",
      "SGD(1250/2000): loss=0.4795354870530231, w(0)=-0.0008924037522923554, w1=0.00033315712460128507\n",
      "SGD(1300/2000): loss=0.5252030026003796, w(0)=-0.0009182458679639021, w1=0.00033535096561317936\n",
      "SGD(1350/2000): loss=0.4758333038466377, w(0)=-0.0009514301643145854, w1=0.0003557869376071316\n",
      "SGD(1400/2000): loss=0.41463955511807143, w(0)=-0.0009612264324738009, w1=0.0003677216918631217\n",
      "SGD(1450/2000): loss=0.39954535466239666, w(0)=-0.000997622082292491, w1=0.00037727409455392007\n",
      "SGD(1500/2000): loss=0.384708213569975, w(0)=-0.0010118190047469272, w1=0.0003910519286058643\n",
      "SGD(1550/2000): loss=0.44854075005307037, w(0)=-0.0010143471736228146, w1=0.0004087248801203662\n",
      "SGD(1600/2000): loss=0.4791225408514774, w(0)=-0.0010375850725788825, w1=0.0004235259772623843\n",
      "SGD(1650/2000): loss=0.5482955117471754, w(0)=-0.0010779920692104577, w1=0.0004387532744422733\n",
      "SGD(1700/2000): loss=0.4040634604196185, w(0)=-0.0010922070202502875, w1=0.0004561269895995798\n",
      "SGD(1750/2000): loss=0.5080046121524512, w(0)=-0.0011127604048127855, w1=0.00046438189631943657\n",
      "SGD(1800/2000): loss=0.4891652108135847, w(0)=-0.0011561370067255975, w1=0.00046994715562567437\n",
      "SGD(1850/2000): loss=0.3646746625094704, w(0)=-0.001200817361597854, w1=0.0004850773682914351\n",
      "SGD(1900/2000): loss=0.28927261812687677, w(0)=-0.0012621754790807045, w1=0.0005102466738942798\n",
      "SGD(1950/2000): loss=0.48538643983543744, w(0)=-0.0012573553771377287, w1=0.0005274184708090023\n",
      "SGD(2000/2000): loss=0.3149794310473352, w(0)=-0.0013060407056816356, w1=0.00053208791122281\n",
      "[0.49527504 0.31497943]\n",
      "[0.96212014 0.96104572]\n",
      "[0.67208  0.667984]\n"
     ]
    }
   ],
   "source": [
    "method_flag = 2\n",
    "k_fold = 2\n",
    "degree = 5\n",
    "lambda_ = 1e-5\n",
    "gamma = 2.2e-6\n",
    "max_iters = 2000 \n",
    "losses_tr, losses_va, accuracy_scores = k_fold_cross_validation(y,tX,cat_cols,method_flag,k_fold,degree,lambda_,gamma,max_iters)\n",
    "print(losses_tr)\n",
    "print(losses_va)\n",
    "print(accuracy_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2730447  0.27282586]\n",
      "[  76886.14547321 1538054.20836699]\n",
      "[0.657896 0.656768]\n"
     ]
    }
   ],
   "source": [
    "method_flag = 3\n",
    "k_fold = 2\n",
    "degree = 5\n",
    "lambda_ = 1e-5\n",
    "gamma = 2.2e-6\n",
    "max_iters = 2000 \n",
    "losses_tr, losses_va, accuracy_scores = k_fold_cross_validation(y,tX,cat_cols,method_flag,k_fold,degree,lambda_,gamma,max_iters)\n",
    "print(losses_tr)\n",
    "print(losses_va)\n",
    "print(accuracy_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2730471  0.27283294]\n",
      "[0.73954686 0.7399415 ]\n",
      "[0.81504  0.815104]\n"
     ]
    }
   ],
   "source": [
    "method_flag = 4\n",
    "k_fold = 2\n",
    "degree = 5\n",
    "lambda_ = 1e-5\n",
    "gamma = 2.2e-6\n",
    "max_iters = 2000 \n",
    "losses_tr, losses_va, accuracy_scores = k_fold_cross_validation(y,tX,cat_cols,method_flag,k_fold,degree,lambda_,gamma,max_iters)\n",
    "print(losses_tr)\n",
    "print(losses_va)\n",
    "print(accuracy_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Iteration: 0, Loss: 86643.3975699927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adrianvillarroel/Documents/Docs_Adrian/EPFL/Courses/Machine_learning/ML_2019_EPFL/projects/project1/scripts/implementations_utils.py:41: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Iteration: 250, Loss: 123432.86348150649\n",
      "#Iteration: 500, Loss: nan\n",
      "#Iteration: 750, Loss: 121607.29719502994\n",
      "#Iteration: 1000, Loss: nan\n",
      "#Iteration: 1250, Loss: 121596.64501936092\n",
      "#Iteration: 1500, Loss: nan\n",
      "#Iteration: 1750, Loss: 121817.37628073568\n",
      "#Iteration: 2000, Loss: nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-11ae07eb27bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.2e-6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlosses_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_va\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_fold_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcat_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod_flag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_va\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Docs_Adrian/EPFL/Courses/Machine_learning/ML_2019_EPFL/projects/project1/scripts/proj1_cross_validation.py\u001b[0m in \u001b[0;36mk_fold_cross_validation\u001b[0;34m(y, x, cat_cols, method_flag, k_fold, degree, lambda_, gamma, max_iters, seed)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mloss_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_va\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcat_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod_flag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mlosses_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_tr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0mlosses_va\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_va\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0maccuracy_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "method_flag = 5\n",
    "k_fold = 2\n",
    "degree = 5\n",
    "lambda_ = 1e-5\n",
    "gamma = 2.2e-6\n",
    "max_iters = 2000 \n",
    "losses_tr, losses_va, accuracy_scores = k_fold_cross_validation(y,tX,cat_cols,method_flag,k_fold,degree,lambda_,gamma,max_iters)\n",
    "print(losses_tr)\n",
    "print(losses_va)\n",
    "print(accuracy_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Iteration: 0, Loss: 86643.3975699927\n",
      "#Iteration: 250, Loss: 123432.86349805204\n",
      "#Iteration: 500, Loss: nan\n",
      "#Iteration: 750, Loss: 121607.29724336513\n",
      "#Iteration: 1000, Loss: nan\n",
      "#Iteration: 1250, Loss: 121596.64506041558\n",
      "#Iteration: 1500, Loss: nan\n",
      "#Iteration: 1750, Loss: 121817.37631268478\n",
      "#Iteration: 2000, Loss: nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b00fa24dcedf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.2e-6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlosses_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_va\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_fold_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcat_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod_flag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_va\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Docs_Adrian/EPFL/Courses/Machine_learning/ML_2019_EPFL/projects/project1/scripts/proj1_cross_validation.py\u001b[0m in \u001b[0;36mk_fold_cross_validation\u001b[0;34m(y, x, cat_cols, method_flag, k_fold, degree, lambda_, gamma, max_iters, seed)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mloss_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_va\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcat_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod_flag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mlosses_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_tr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0mlosses_va\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_va\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0maccuracy_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "method_flag = 6\n",
    "k_fold = 2\n",
    "degree = 5\n",
    "lambda_ = 1e-5\n",
    "gamma = 2.2e-6\n",
    "max_iters = 2000 \n",
    "losses_tr, losses_va, accuracy_scores = k_fold_cross_validation(y,tX,cat_cols,method_flag,k_fold,degree,lambda_,gamma,max_iters)\n",
    "print(losses_tr)\n",
    "print(losses_va)\n",
    "print(accuracy_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(weights, x_val)\n",
    "y_val = relabel_y_negative(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29045  3725]\n",
      " [ 5553 11677]]\n",
      "0.81444\n",
      "0.7156778622211326\n",
      "0.6777132907719094\n",
      "0.7581482924295546\n"
     ]
    }
   ],
   "source": [
    "# Just to check\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, f1_score, recall_score, precision_score\n",
    "print(confusion_matrix(y_val,y_pred))\n",
    "print(accuracy_score(y_val,y_pred))\n",
    "print(f1_score(y_val,y_pred))\n",
    "print(recall_score(y_val,y_pred))\n",
    "print(precision_score(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.,  1.]), array([34602, 15398]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_label_y_counts(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../results/ridge_reg_submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, x_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (ml_2019)",
   "language": "python",
   "name": "ml_2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
