{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y_train, x_train, ids_train = load_csv_data(DATA_TRAIN_PATH)\n",
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "y_test, x_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['DER_mass_MMC', 'DER_mass_transverse_met_lep',\n",
    "       'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet',\n",
    "       'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt',\n",
    "       'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality',\n",
    "       'DER_lep_eta_centrality', 'PRI_tau_pt', 'PRI_tau_eta', 'PRI_tau_phi',\n",
    "       'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_met', 'PRI_met_phi',\n",
    "       'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt',\n",
    "       'PRI_jet_leading_eta', 'PRI_jet_leading_phi', 'PRI_jet_subleading_pt',\n",
    "       'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_all_pt']\n",
    "dtypes = 15 * [\"float\"] + ['float-angle'] + 2 * ['float'] + ['float-angle'] \\\n",
    "         + ['float'] + ['float-angle'] +['float'] + ['categorical'] + (2 * ['float']\\\n",
    "         + ['float-angle']) * 2 + ['float']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self,y,tX,cols,dtypes):\n",
    "        assert tX.shape[1] == len(cols), \"The number of columns of data does not match the number of column names\"\n",
    "        assert len(cols) == len(dtypes), \"The number f column names must match the number of dtypes\"\n",
    "        self.CATEGORICAL_TYPE = 'categorical'\n",
    "        self.FLOAT_TYPE = 'float'\n",
    "        self.FLOAT_ANGLE_TYPE = 'float-angle'\n",
    "        \n",
    "        self.cols = cols\n",
    "        self.dtypes = dtypes\n",
    "        features = list(tX.T)\n",
    "        dtypes_features = [list(x) for x in zip(dtypes, features)]\n",
    "        self.data_dict = dict(zip(cols,dtypes_features))\n",
    "        self.y = y\n",
    "        \n",
    "    def dropFeatures(self,cols):\n",
    "        for c in cols:\n",
    "            self.data_dict.pop(c) \n",
    "        self.cols = list(self.data_dict.keys())\n",
    "        self.dtypes = [self.data_dict.get(c)[0] for c in self.cols]\n",
    "        \n",
    "    def getFeatures(self, cols = None):\n",
    "        if not cols:\n",
    "            cols = self.cols\n",
    "        features_list = self.getFeaturesList(cols)\n",
    "        return np.hstack(features_list)\n",
    "    \n",
    "    def relabelYNonNegative(self):\n",
    "        self.y[self.y == -1] = 0        \n",
    "    \n",
    "    def relabelYNegative(self):\n",
    "        self.y[self.y == 0] = -1\n",
    "    \n",
    "    def getYLabels(self):\n",
    "        return self.y\n",
    "    \n",
    "    def getFeaturesList(self,cols):\n",
    "        return [self.data_dict.get(c)[1].reshape(-1,1) for c in cols]\n",
    "    \n",
    "    def replaceUndefVal(self,undef_orig = -999.0,undef_new = 0, cols = None):\n",
    "        if not cols:\n",
    "            cols = self.cols\n",
    "        features = self.getFeatures(cols)    \n",
    "        features = np.where(features == undef_orig, np.nan, features)\n",
    "        self.updateData(cols,features)\n",
    "    \n",
    "    def replaceNanVal(self,val = 0, cols = None):\n",
    "        if not cols:\n",
    "            cols = self.cols\n",
    "        features = self.getFeatures(cols)\n",
    "        features = np.where(np.isnan(features), val, features)\n",
    "        self.updateData(cols,features)\n",
    "        \n",
    "    def nanStandardize(self, cols = None):\n",
    "        if not cols:\n",
    "            cols = [c for c in self.cols \\\n",
    "                    if (self.data_dict.get(c)[0] == self.FLOAT_TYPE\\\n",
    "                        or self.data_dict.get(c)[0] == self.FLOAT_ANGLE_TYPE)]\n",
    "            \n",
    "        # Check if columns are float or float angle\n",
    "        for col in cols:\n",
    "            if self.data_dict.get(col)[0] != self.FLOAT_TYPE \\\n",
    "            and self.data_dict.get(col)[0] != self.FLOAT_ANGLE_TYPE:\n",
    "                raise ValueError('Cannot standardize categorical columns')\n",
    "        \n",
    "        features = self.getFeatures(cols)\n",
    "        mean = np.nanmean(features, axis = 0)\n",
    "        std = np.nanstd(features, axis = 0)\n",
    "        features = (features - mean)/std\n",
    "        \n",
    "        self.updateData(cols,features)\n",
    "   \n",
    "    def updateData(self,cols,features,dtypes_list = None):\n",
    "        \n",
    "        features_list = list(features.T)\n",
    "        if not dtypes_list:\n",
    "            dtypes_list = [self.data_dict.get(c)[0] for c in cols]\n",
    "        \n",
    "        dtypes_features = [list(x) for x in zip(dtypes_list, features_list)]\n",
    "        update_dict = dict(zip(cols,dtypes_features))\n",
    "        self.data_dict.update(update_dict)\n",
    "        \n",
    "    def oneHotEncode(self, cols = None):\n",
    "        if not cols:\n",
    "            cols = [c for c in self.cols if self.data_dict.get(c)[0] == self.CATEGORICAL_TYPE]\n",
    "        \n",
    "        # Check if columns are categorical\n",
    "        for col in cols:\n",
    "            if self.data_dict.get(col)[0] != self.CATEGORICAL_TYPE:\n",
    "                raise ValueError('Cannot one hot encode non-categorical columns')\n",
    "        features = self.getFeatures(cols)\n",
    "        n = features.shape[0]\n",
    "        idx_n = np.asarray(range(n),dtype = np.int64)\n",
    "        features_list = list(features.T.astype(np.int64))\n",
    "        ohe_features_list = [np.zeros((n,len(set(x))-1)) for x in features_list]\n",
    "        for (idx,x) in enumerate(ohe_features_list): \n",
    "            no_dummy_var_idx = (features_list[idx] < (x.shape[1] - 1)).astype(np.int64)\n",
    "            ndv_idx_n = idx_n[no_dummy_var_idx]\n",
    "            ndv_one_hot_idx = features_list[idx][no_dummy_var_idx]\n",
    "            x[ndv_idx_n,ndv_one_hot_idx] = 1 \n",
    "        features = np.hstack(ohe_features_list)\n",
    "        self.dropFeatures(cols)\n",
    "        \n",
    "        cols_sizes_list = [(c,len(set(x))) for (c,x) in zip(cols,features_list)]\n",
    "        ohe_cols = [[c+\"_\"+str(x) for x in range(s-1)] for (c,s) in cols_sizes_list]\n",
    "        ohe_cols = [ohe_col for ohe_col_sublist in ohe_cols for ohe_col in ohe_col_sublist]\n",
    "        dtypes_list = len(ohe_cols) * [self.CATEGORICAL_TYPE]\n",
    "        \n",
    "        self.cols = self.cols + ohe_cols\n",
    "        self.dtypes = self.dtypes + dtypes_list\n",
    "        self.updateData(ohe_cols,features,dtypes_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions needed for the log and reg_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"Apply sigmoid function on t\n",
    "    \n",
    "    Args: \n",
    "        t=>(numpy.array): Values to apply sigmoid function\n",
    "    \n",
    "    Returns:\n",
    "        => numpy.array: Calculated values of sigmoid\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1.0 / (1.0 + np.exp(-t))\n",
    "\n",
    "\n",
    "def calculate_loss_log(y, tx, w):\n",
    "    \"\"\"Compute the cost of log_regression\n",
    "    \n",
    "    Args: \n",
    "        y =>(numpy.array): Target values\n",
    "        tx =>(numpy.array): Transposed features\n",
    "        w => (numpy.array): Weigths \n",
    "          \n",
    "    Returns:\n",
    "        => numpy.array: Calculated loss\n",
    "    \"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "    return np.squeeze(- loss)\n",
    "\n",
    "    \n",
    "def calculate_gradient_log(y, tx, w,):\n",
    "    \"\"\"Compute the gradient of loss for log_regression\n",
    "    \n",
    "    Args: \n",
    "        y =>(numpy.array): Target values\n",
    "        tx => (numpy.array): Transposed features\n",
    "        w => (numpy.array): Weigths \n",
    "          \n",
    "    Returns:\n",
    "        => numpy.array: Calculated logistic gradient\n",
    "    \"\"\"\n",
    "\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    grad = tx.T.dot(pred - y)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def learning_by_gradient_descent_log(y, tx, w, gamma):\n",
    "    \"\"\"Compute the gradient descen using logistic regression\n",
    "    \n",
    "    Args: \n",
    "        y =>(numpy.array): Target values\n",
    "        tx => (numpy.array): Transposed features\n",
    "        w => (numpy.array): Weigths \n",
    "        gamma=> (float): the gamma to use.\n",
    "        \n",
    "    Returns:\n",
    "        w =>(numpy.array): Calculated Weights\n",
    "        loss => (numpy.array): Calculated Loss\n",
    "    \"\"\"\n",
    "\n",
    "    loss = calculate_loss_log(y, tx, w) \n",
    "    grad = calculate_gradient_log(y, tx, w)\n",
    "    w -= gamma * grad\n",
    "    return loss, w\n",
    "\n",
    "\n",
    "def learning_by_reg_gradient_descent_log(y, tx, w, gamma, lambda_=0):\n",
    "    \"\"\"Compute the gradient descen using logistic regression\n",
    "    \n",
    "    Args: \n",
    "        y =>(numpy.array): Target values\n",
    "        tx => (numpy.array): Transposed features\n",
    "        w => (numpy.array): Weigths \n",
    "        gamma=> (float): the gamma to use.\n",
    "        \n",
    "    Returns: \n",
    "        w =>(numpy.array): Calculated Weights\n",
    "        loss => (numpy.array): Calculated Loss\n",
    "    \"\"\"\n",
    "\n",
    "    loss = calculate_loss_log(y, tx, w) + lambda_ * np.squeeze(w.T.dot(w))\n",
    "    grad = calculate_gradient_log(y, tx, w) + 2 * lambda_ * w\n",
    "    w -= gamma * grad\n",
    "    return loss, w\n",
    "\n",
    "def logistic_regression(y, tx, w_initial, max_iters, gamma):\n",
    "    \"\"\"Implement logistic regression using gradient descent\n",
    "    \n",
    "    Args: \n",
    "      y =>(numpy.array): Target values\n",
    "      tx => (numpy.array): Transposed features\n",
    "      w_initial => (numpy.array): Initial Weigths \n",
    "      max_iters => (int): number of iterations.\n",
    "      gamma=> (float): the gamma to use.\n",
    "          \n",
    "    Returns: \n",
    "        w =>(numpy.array): Calculated Weights\n",
    "        loss => (numpy.array): Calculated Loss\n",
    "    \"\"\"\n",
    "\n",
    "    assert max_iters > 0, \"max_iters should be a positive number\"\n",
    "    assert y.shape[0] == tx.shape[0], \"y and tx should have the same number of entries (rows)\"\n",
    "    assert tx.shape[1] == w_initial.shape[0], \"initial_w should be the same degree as tx\"\n",
    "    \n",
    "    print_every = 1\n",
    "    w = w_initial\n",
    "    losses =[]\n",
    "    for n_iter in range(max_iters):\n",
    "        loss, w = learning_by_gradient_descent_log(y, tx, w, gamma)\n",
    "        losses.append(loss)\n",
    "        if (n_iter % print_every == 0):\n",
    "            # print average loss for the last print_every iterations\n",
    "            print('iteration\\t', str(n_iter), loss)\n",
    "            \n",
    "    loss = calculate_loss_log(y, tx, w)\n",
    "    \n",
    "    return w, loss,losses\n",
    "\n",
    "def reg_logistic_regression(y, tx, w_initial, max_iters, gamma,lambda_):\n",
    "    \"\"\"Implement logistic regression using gradient descent\n",
    "    Args: y =>(numpy.array): Target values\n",
    "          tx => (numpy.array): Transposed features\n",
    "          w_initial => (numpy.array): Initial Weigths \n",
    "          max_iters => (int): number of iterations.\n",
    "          gamma=> (float): the gamma to use.\n",
    "    Returns: w =>(numpy.array): Calculated Weights\n",
    "             loss => (numpy.array): Calculated Loss\n",
    "    \"\"\"\n",
    "\n",
    "    assert max_iters > 0, \"max_iters should be a positive number\"\n",
    "    assert y.shape[0] == tx.shape[0], \"y and tx should have the same number of entries (rows)\"\n",
    "    assert tx.shape[1] == w_initial.shape[0], \"initial_w should be the same degree as tx\"\n",
    "    \n",
    "    print_every = 1\n",
    "    w = w_initial\n",
    "    # change labels from (-1, 1) to (0, 1) should we use this????\n",
    "    y[np.where(y == -1)] = 0\n",
    "    losses =[]\n",
    "    for n_iter in range(max_iters):\n",
    "        loss, w = learning_by_reg_gradient_descent_log(y, tx, w, gamma,lambda_)\n",
    "        losses.append(loss)\n",
    "        if (n_iter % print_every == 0):\n",
    "            # print average loss for the last print_every iterations\n",
    "            print('iteration\\t', str(n_iter), loss)\n",
    "            \n",
    "    loss = calculate_loss_log(y, tx, w)\n",
    "    \n",
    "    return w, loss,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data preprocessor\n",
    "data_prep = DataPreprocessor(y_train,x_train,feature_cols,dtypes)\n",
    "data_prep.replaceUndefVal(undef_new = np.nan)\n",
    "data_prep.nanStandardize()\n",
    "data_prep.replaceNanVal()\n",
    "data_prep.oneHotEncode()\n",
    "x_train_prep = data_prep.getFeatures()\n",
    "\n",
    "data_prep.relabelYNonNegative()\n",
    "y_train_prep = data_prep.getYLabels()\n",
    "y_train_prep = y_train_prep.reshape(-1,1)\n",
    "y_train_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8418692000542485"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(x_train_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration\t 0 173286.79513998044\n",
      "iteration\t 1 181850.0319150082\n",
      "iteration\t 2 204923.38393836078\n",
      "iteration\t 3 270407.5878468261\n",
      "iteration\t 4 174017.59068426667\n",
      "iteration\t 5 204739.62065891564\n",
      "iteration\t 6 190289.47846737824\n",
      "iteration\t 7 235081.39237588036\n",
      "iteration\t 8 178568.55982303424\n",
      "iteration\t 9 210111.08020084727\n"
     ]
    }
   ],
   "source": [
    "max_iters = 10\n",
    "gamma = 0.00001\n",
    "losses = []\n",
    "w_initial = np.zeros((x_train_prep.shape[1], 1))\n",
    "lambda_ = 0.1\n",
    "\n",
    "weights, loss_tr, losses = reg_logistic_regression(y_train_prep, x_train_prep, w_initial, max_iters, gamma, lambda_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[173286.79513998044,\n",
       " 181850.0319150082,\n",
       " 204923.38393836078,\n",
       " 270407.5878468261,\n",
       " 174017.59068426667,\n",
       " 204739.62065891564,\n",
       " 190289.47846737824,\n",
       " 235081.39237588036,\n",
       " 178568.55982303424,\n",
       " 210111.08020084727]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(182078.85282157)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_test = DataPreprocessor(y_test, x_test,feature_cols,dtypes)\n",
    "data_prep_test.replaceUndefVal(undef_new = np.nan)\n",
    "data_prep_test.replaceNanVal()\n",
    "data_prep_test.nanStandardize()\n",
    "data_prep_test.oneHotEncode()\n",
    "x_test_prep = data_prep_test.getFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(weights, x_test_prep)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5507604208095903"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_PATH = '../results/log_reg.csv' \n",
    "# create_csv_submission(ids_test, y_pred[:], OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss and gradient.\"\"\"\n",
    "    num_samples = y.shape[0]\n",
    "    loss = calculate_loss_log(y, tx, w) + lambda_ * np.squeeze(w.T.dot(w))\n",
    "    gradient = calculate_gradient_log(y, tx, w) + 2 * lambda_ * w\n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, gradient = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    w -= gamma * gradient\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10000\n",
    "gamma = 0.0000001\n",
    "lambda_ = 0.1\n",
    "threshold = 1e-8\n",
    "losses = []\n",
    "\n",
    "# build tx\n",
    "\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_penalized_gradient(y_train, x_train_prep, w_initial, gamma, lambda_)\n",
    "    # log info\n",
    "    if iter % 100 == 0:\n",
    "        print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "    # converge criterion\n",
    "    losses.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
